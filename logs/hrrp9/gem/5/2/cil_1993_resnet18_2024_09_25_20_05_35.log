2024-09-25 20:05:35,254 [trainer.py] => config: ./exps/gem.json
2024-09-25 20:05:35,254 [trainer.py] => prefix: cil
2024-09-25 20:05:35,254 [trainer.py] => dataset: hrrp9
2024-09-25 20:05:35,254 [trainer.py] => memory_size: 500
2024-09-25 20:05:35,254 [trainer.py] => memory_per_class: 20
2024-09-25 20:05:35,254 [trainer.py] => fixed_memory: False
2024-09-25 20:05:35,254 [trainer.py] => shuffle: True
2024-09-25 20:05:35,254 [trainer.py] => init_cls: 5
2024-09-25 20:05:35,254 [trainer.py] => increment: 2
2024-09-25 20:05:35,254 [trainer.py] => model_name: gem
2024-09-25 20:05:35,254 [trainer.py] => convnet_type: resnet18
2024-09-25 20:05:35,254 [trainer.py] => init_train: False
2024-09-25 20:05:35,254 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-09-25 20:05:35,254 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-09-25 20:05:35,254 [trainer.py] => device: [device(type='cuda', index=0)]
2024-09-25 20:05:35,255 [trainer.py] => seed: 1993
2024-09-25 20:05:35,255 [trainer.py] => epochs: 150
2024-09-25 20:05:35,255 [trainer.py] => lrate: 0.1
2024-09-25 20:05:35,255 [trainer.py] => milestones: [50, 80, 120]
2024-09-25 20:05:35,255 [trainer.py] => lrate_decay: 0.1
2024-09-25 20:05:35,255 [trainer.py] => momentum: 0.1
2024-09-25 20:05:35,255 [trainer.py] => batch_size: 128
2024-09-25 20:05:35,255 [trainer.py] => weight_decay: 0.0002
2024-09-25 20:05:35,255 [trainer.py] => num_workers: 4
2024-09-25 20:05:35,922 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-09-25 20:05:36,349 [trainer.py] => All params: 3843904
2024-09-25 20:05:36,350 [trainer.py] => Trainable params: 3843904
2024-09-25 20:05:36,374 [gem.py] => Learning on 0-5
2024-09-25 20:05:36,429 [gem.py] => init_train?---False
2024-09-25 20:05:37,127 [base.py] => Reducing exemplars...(100 per classes)
2024-09-25 20:05:37,127 [base.py] => Constructing exemplars...(100 per classes)
2024-09-25 20:05:42,399 [gem.py] => Exemplar size: 500
2024-09-25 20:05:42,400 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-09-25 20:05:42,400 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-09-25 20:05:42,400 [trainer.py] => CNN top1 curve: [89.93]
2024-09-25 20:05:42,400 [trainer.py] => CNN top5 curve: [100.0]
2024-09-25 20:05:42,400 [trainer.py] => NME top1 curve: [90.0]
2024-09-25 20:05:42,400 [trainer.py] => NME top5 curve: [100.0]

2024-09-25 20:05:42,400 [trainer.py] => Average Accuracy (CNN): 89.93
2024-09-25 20:05:42,400 [trainer.py] => Average Accuracy (NME): 90.0
2024-09-25 20:05:42,400 [trainer.py] => All params: 3846469
2024-09-25 20:05:42,401 [trainer.py] => Trainable params: 3846469
2024-09-25 20:05:42,402 [gem.py] => Learning on 5-7
2024-09-25 20:05:45,849 [gem.py] => Task 1, Epoch 1/150 => Loss 0.322, Train_accy 89.22, Test_accy 61.62
2024-09-25 20:06:00,237 [gem.py] => Task 1, Epoch 6/150 => Loss 0.010, Train_accy 99.98, Test_accy 61.05
2024-09-25 20:06:14,669 [gem.py] => Task 1, Epoch 11/150 => Loss 0.004, Train_accy 100.00, Test_accy 60.95
2024-09-25 20:06:29,900 [gem.py] => Task 1, Epoch 16/150 => Loss 0.002, Train_accy 100.00, Test_accy 62.81
2024-09-25 20:06:45,849 [gem.py] => Task 1, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.50
2024-09-25 20:07:00,930 [gem.py] => Task 1, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.45
2024-09-25 20:07:16,993 [gem.py] => Task 1, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.83
2024-09-25 20:07:32,131 [gem.py] => Task 1, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.98
2024-09-25 20:07:47,353 [gem.py] => Task 1, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.19
2024-09-25 20:08:02,684 [gem.py] => Task 1, Epoch 46/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.57
2024-09-25 20:08:18,287 [gem.py] => Task 1, Epoch 51/150 => Loss 0.001, Train_accy 100.00, Test_accy 65.88
2024-09-25 20:08:33,340 [gem.py] => Task 1, Epoch 56/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.90
2024-09-25 20:08:48,874 [gem.py] => Task 1, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.19
2024-09-25 20:09:04,195 [gem.py] => Task 1, Epoch 66/150 => Loss 0.001, Train_accy 100.00, Test_accy 65.26
2024-09-25 20:09:20,727 [gem.py] => Task 1, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.12
2024-09-25 20:09:36,893 [gem.py] => Task 1, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.00
2024-09-25 20:09:52,918 [gem.py] => Task 1, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.07
2024-09-25 20:10:08,442 [gem.py] => Task 1, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.19
2024-09-25 20:10:24,118 [gem.py] => Task 1, Epoch 91/150 => Loss 0.001, Train_accy 100.00, Test_accy 65.86
2024-09-25 20:10:40,376 [gem.py] => Task 1, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.57
2024-09-25 20:10:56,382 [gem.py] => Task 1, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 64.98
2024-09-25 20:11:12,124 [gem.py] => Task 1, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.26
2024-09-25 20:11:28,025 [gem.py] => Task 1, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 64.60
2024-09-25 20:11:42,966 [gem.py] => Task 1, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.19
2024-09-25 20:11:59,438 [gem.py] => Task 1, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 64.36
2024-09-25 20:12:15,518 [gem.py] => Task 1, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.07
2024-09-25 20:12:31,265 [gem.py] => Task 1, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.07
2024-09-25 20:12:47,213 [gem.py] => Task 1, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.62
2024-09-25 20:13:02,979 [gem.py] => Task 1, Epoch 141/150 => Loss 0.001, Train_accy 100.00, Test_accy 65.40
2024-09-25 20:13:18,657 [gem.py] => Task 1, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.55
2024-09-25 20:13:31,189 [gem.py] => Task 1, Epoch 150/150 => Loss 0.001, Train_accy 100.00
2024-09-25 20:13:31,203 [base.py] => Reducing exemplars...(71 per classes)
2024-09-25 20:13:32,332 [base.py] => Constructing exemplars...(71 per classes)
2024-09-25 20:13:35,060 [gem.py] => Exemplar size: 497
2024-09-25 20:13:35,061 [trainer.py] => CNN: {'total': 64.26, '00-04': 76.83, '05-06': 32.83, 'old': 76.83, 'new': 32.83}
2024-09-25 20:13:35,061 [trainer.py] => NME: {'total': 76.5, '00-04': 75.03, '05-06': 80.17, 'old': 75.03, 'new': 80.17}
2024-09-25 20:13:35,061 [trainer.py] => CNN top1 curve: [89.93, 64.26]
2024-09-25 20:13:35,061 [trainer.py] => CNN top5 curve: [100.0, 98.6]
2024-09-25 20:13:35,061 [trainer.py] => NME top1 curve: [90.0, 76.5]
2024-09-25 20:13:35,061 [trainer.py] => NME top5 curve: [100.0, 98.67]

2024-09-25 20:13:35,061 [trainer.py] => Average Accuracy (CNN): 77.095
2024-09-25 20:13:35,061 [trainer.py] => Average Accuracy (NME): 83.25
2024-09-25 20:13:35,062 [trainer.py] => All params: 3847495
2024-09-25 20:13:35,063 [trainer.py] => Trainable params: 3847495
2024-09-25 20:13:35,064 [gem.py] => Learning on 7-9
2024-09-25 20:13:40,202 [gem.py] => Task 2, Epoch 1/150 => Loss 0.186, Train_accy 94.40, Test_accy 50.98
2024-09-25 20:14:05,459 [gem.py] => Task 2, Epoch 6/150 => Loss 0.005, Train_accy 100.00, Test_accy 56.74
2024-09-25 20:14:30,525 [gem.py] => Task 2, Epoch 11/150 => Loss 0.002, Train_accy 100.00, Test_accy 56.54
2024-09-25 20:14:54,945 [gem.py] => Task 2, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.39
2024-09-25 20:15:18,932 [gem.py] => Task 2, Epoch 21/150 => Loss 0.018, Train_accy 99.52, Test_accy 55.65
2024-09-25 20:15:42,359 [gem.py] => Task 2, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.63
2024-09-25 20:16:07,370 [gem.py] => Task 2, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.39
2024-09-25 20:16:31,842 [gem.py] => Task 2, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 58.76
2024-09-25 20:16:56,752 [gem.py] => Task 2, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.57
2024-09-25 20:17:23,150 [gem.py] => Task 2, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 57.31
2024-09-25 20:17:49,411 [gem.py] => Task 2, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.72
2024-09-25 20:18:15,640 [gem.py] => Task 2, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.50
2024-09-25 20:18:41,583 [gem.py] => Task 2, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.74
2024-09-25 20:19:06,721 [gem.py] => Task 2, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.61
2024-09-25 20:19:32,562 [gem.py] => Task 2, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.67
2024-09-25 20:19:58,100 [gem.py] => Task 2, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.37
2024-09-25 20:20:23,249 [gem.py] => Task 2, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.20
2024-09-25 20:20:49,282 [gem.py] => Task 2, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.52
2024-09-25 20:21:07,564 [gem.py] => Task 2, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.44
2024-09-25 20:21:26,485 [gem.py] => Task 2, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.17
2024-09-25 20:21:45,839 [gem.py] => Task 2, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.24
2024-09-25 20:22:04,321 [gem.py] => Task 2, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 57.06
2024-09-25 20:22:23,218 [gem.py] => Task 2, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.85
2024-09-25 20:22:41,077 [gem.py] => Task 2, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.61
2024-09-25 20:22:59,323 [gem.py] => Task 2, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.94
2024-09-25 20:23:18,089 [gem.py] => Task 2, Epoch 126/150 => Loss 0.001, Train_accy 100.00, Test_accy 56.89
2024-09-25 20:23:37,069 [gem.py] => Task 2, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.81
2024-09-25 20:23:55,752 [gem.py] => Task 2, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 57.02
2024-09-25 20:24:14,843 [gem.py] => Task 2, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.61
2024-09-25 20:24:35,362 [gem.py] => Task 2, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 57.06
2024-09-25 20:24:52,482 [gem.py] => Task 2, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-09-25 20:24:52,491 [base.py] => Reducing exemplars...(55 per classes)
2024-09-25 20:24:54,265 [base.py] => Constructing exemplars...(55 per classes)
2024-09-25 20:24:57,256 [gem.py] => Exemplar size: 495
2024-09-25 20:24:57,256 [trainer.py] => CNN: {'total': 56.81, '00-04': 63.23, '05-06': 15.25, '07-08': 82.33, 'old': 49.52, 'new': 82.33}
2024-09-25 20:24:57,256 [trainer.py] => NME: {'total': 62.46, '00-04': 60.23, '05-06': 46.25, '07-08': 84.25, 'old': 56.24, 'new': 84.25}
2024-09-25 20:24:57,257 [trainer.py] => CNN top1 curve: [89.93, 64.26, 56.81]
2024-09-25 20:24:57,257 [trainer.py] => CNN top5 curve: [100.0, 98.6, 94.94]
2024-09-25 20:24:57,257 [trainer.py] => NME top1 curve: [90.0, 76.5, 62.46]
2024-09-25 20:24:57,257 [trainer.py] => NME top5 curve: [100.0, 98.67, 95.41]

2024-09-25 20:24:57,257 [trainer.py] => Average Accuracy (CNN): 70.33333333333333
2024-09-25 20:24:57,257 [trainer.py] => Average Accuracy (NME): 76.32000000000001
2024-09-25 20:24:57,258 [trainer.py] => Forgetting (CNN): 22.140000000000004
