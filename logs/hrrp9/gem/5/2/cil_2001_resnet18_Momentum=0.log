2024-10-28 10:28:45,990 [trainer.py] => config: ./exps/gem.json
2024-10-28 10:28:45,990 [trainer.py] => prefix: cil
2024-10-28 10:28:45,990 [trainer.py] => dataset: hrrp9
2024-10-28 10:28:45,990 [trainer.py] => memory_size: 500
2024-10-28 10:28:45,990 [trainer.py] => memory_per_class: 20
2024-10-28 10:28:45,990 [trainer.py] => fixed_memory: False
2024-10-28 10:28:45,990 [trainer.py] => shuffle: True
2024-10-28 10:28:45,990 [trainer.py] => init_cls: 5
2024-10-28 10:28:45,990 [trainer.py] => increment: 2
2024-10-28 10:28:45,990 [trainer.py] => model_name: gem
2024-10-28 10:28:45,990 [trainer.py] => convnet_type: resnet18
2024-10-28 10:28:45,990 [trainer.py] => init_train: False
2024-10-28 10:28:45,990 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42503.pth
2024-10-28 10:28:45,990 [trainer.py] => fc_path1: checkpoints/init_train/fc_42503.pth
2024-10-28 10:28:45,991 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_35172.pth
2024-10-28 10:28:45,991 [trainer.py] => fc_path: checkpoints/init_train/fc_35172.pth
2024-10-28 10:28:45,991 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-28 10:28:45,991 [trainer.py] => seed: 2001
2024-10-28 10:28:45,991 [trainer.py] => epochs: 150
2024-10-28 10:28:45,991 [trainer.py] => lrate: 0.1
2024-10-28 10:28:45,991 [trainer.py] => milestones: [50, 80, 120]
2024-10-28 10:28:45,991 [trainer.py] => lrate_decay: 0.1
2024-10-28 10:28:45,991 [trainer.py] => momentum: 0
2024-10-28 10:28:45,991 [trainer.py] => batch_size: 128
2024-10-28 10:28:45,991 [trainer.py] => weight_decay: 0.0002
2024-10-28 10:28:45,991 [trainer.py] => num_workers: 4
2024-10-28 10:28:46,656 [data_manager.py] => [3, 5, 1, 7, 2, 8, 6, 4, 0]
2024-10-28 10:28:47,931 [trainer.py] => All params: 3843904
2024-10-28 10:28:47,932 [trainer.py] => Trainable params: 3843904
2024-10-28 10:28:47,935 [gem.py] => Learning on 0-5
2024-10-28 10:28:47,996 [gem.py] => init_train?---False
2024-10-28 10:28:48,607 [base.py] => Reducing exemplars...(100 per classes)
2024-10-28 10:28:48,608 [base.py] => Constructing exemplars...(100 per classes)
2024-10-28 10:28:54,898 [trainer.py] => All params: 3846469
2024-10-28 10:28:55,727 [gem.py] => Exemplar size: 500
2024-10-28 10:28:55,727 [trainer.py] => CNN: {'total': 90.13, '00-04': 90.13, 'old': 0, 'new': 90.13}
2024-10-28 10:28:55,727 [trainer.py] => NME: {'total': 89.53, '00-04': 89.53, 'old': 0, 'new': 89.53}
2024-10-28 10:28:55,727 [trainer.py] => CNN top1 curve: [90.13]
2024-10-28 10:28:55,727 [trainer.py] => CNN top5 curve: [100.0]
2024-10-28 10:28:55,727 [trainer.py] => NME top1 curve: [89.53]
2024-10-28 10:28:55,727 [trainer.py] => NME top5 curve: [100.0]

2024-10-28 10:28:55,728 [trainer.py] => Average Accuracy (CNN): 90.13
2024-10-28 10:28:55,728 [trainer.py] => Average Accuracy (NME): 89.53
2024-10-28 10:28:55,728 [trainer.py] => All params: 3846469
2024-10-28 10:28:55,728 [trainer.py] => Trainable params: 3846469
2024-10-28 10:28:55,729 [gem.py] => Learning on 5-7
2024-10-28 10:29:02,241 [gem.py] => Task 1, Epoch 1/150 => Loss 0.294, Train_accy 90.60, Test_accy 51.57
2024-10-28 10:29:26,906 [gem.py] => Task 1, Epoch 6/150 => Loss 0.006, Train_accy 100.00, Test_accy 59.10
2024-10-28 10:29:53,187 [gem.py] => Task 1, Epoch 11/150 => Loss 0.003, Train_accy 100.00, Test_accy 59.57
2024-10-28 10:30:18,990 [gem.py] => Task 1, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 60.93
2024-10-28 10:30:46,763 [gem.py] => Task 1, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.10
2024-10-28 10:31:11,999 [gem.py] => Task 1, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.21
2024-10-28 10:31:40,022 [gem.py] => Task 1, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 61.05
2024-10-28 10:32:08,024 [gem.py] => Task 1, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.36
2024-10-28 10:32:37,371 [gem.py] => Task 1, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.33
2024-10-28 10:33:05,200 [gem.py] => Task 1, Epoch 46/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.71
2024-10-28 10:33:35,544 [gem.py] => Task 1, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.76
2024-10-28 10:34:05,521 [gem.py] => Task 1, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.05
2024-10-28 10:34:35,900 [gem.py] => Task 1, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.05
2024-10-28 10:35:08,679 [gem.py] => Task 1, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.02
2024-10-28 10:35:41,041 [gem.py] => Task 1, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.14
2024-10-28 10:36:12,986 [gem.py] => Task 1, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.05
2024-10-28 10:36:44,507 [gem.py] => Task 1, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.88
2024-10-28 10:37:17,120 [gem.py] => Task 1, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.57
2024-10-28 10:37:47,935 [gem.py] => Task 1, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.95
2024-10-28 10:38:19,204 [gem.py] => Task 1, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.10
2024-10-28 10:38:49,966 [gem.py] => Task 1, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.21
2024-10-28 10:39:22,447 [gem.py] => Task 1, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.86
2024-10-28 10:39:53,859 [gem.py] => Task 1, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.88
2024-10-28 10:40:24,107 [gem.py] => Task 1, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.07
2024-10-28 10:40:51,785 [gem.py] => Task 1, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.38
2024-10-28 10:41:21,045 [gem.py] => Task 1, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.79
2024-10-28 10:41:47,445 [gem.py] => Task 1, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.31
2024-10-28 10:42:13,725 [gem.py] => Task 1, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.24
2024-10-28 10:42:38,953 [gem.py] => Task 1, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.12
2024-10-28 10:43:05,244 [gem.py] => Task 1, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.62
2024-10-28 10:43:26,052 [gem.py] => Task 1, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-10-28 10:43:26,053 [base.py] => Reducing exemplars...(71 per classes)
2024-10-28 10:43:27,730 [base.py] => Constructing exemplars...(71 per classes)
2024-10-28 10:43:30,144 [trainer.py] => All params: 3847495
2024-10-28 10:43:31,324 [gem.py] => Exemplar size: 497
2024-10-28 10:43:31,325 [trainer.py] => CNN: {'total': 63.17, '00-04': 72.1, '05-06': 40.83, 'old': 72.1, 'new': 40.83}
2024-10-28 10:43:31,325 [trainer.py] => NME: {'total': 65.48, '00-04': 63.77, '05-06': 69.75, 'old': 63.77, 'new': 69.75}
2024-10-28 10:43:31,325 [trainer.py] => CNN top1 curve: [90.13, 63.17]
2024-10-28 10:43:31,325 [trainer.py] => CNN top5 curve: [100.0, 97.43]
2024-10-28 10:43:31,325 [trainer.py] => NME top1 curve: [89.53, 65.48]
2024-10-28 10:43:31,325 [trainer.py] => NME top5 curve: [100.0, 96.86]

2024-10-28 10:43:31,325 [trainer.py] => Average Accuracy (CNN): 76.65
2024-10-28 10:43:31,325 [trainer.py] => Average Accuracy (NME): 77.505
2024-10-28 10:43:31,326 [trainer.py] => All params: 3847495
2024-10-28 10:43:31,326 [trainer.py] => Trainable params: 3847495
2024-10-28 10:43:31,327 [gem.py] => Learning on 7-9
2024-10-28 10:43:36,961 [gem.py] => Task 2, Epoch 1/150 => Loss 0.285, Train_accy 90.22, Test_accy 39.96
2024-10-28 10:44:04,615 [gem.py] => Task 2, Epoch 6/150 => Loss 0.008, Train_accy 100.00, Test_accy 52.20
2024-10-28 10:44:36,447 [gem.py] => Task 2, Epoch 11/150 => Loss 0.003, Train_accy 100.00, Test_accy 51.37
2024-10-28 10:45:08,390 [gem.py] => Task 2, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 50.46
2024-10-28 10:45:40,481 [gem.py] => Task 2, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 50.22
2024-10-28 10:46:10,999 [gem.py] => Task 2, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 49.80
2024-10-28 10:46:42,431 [gem.py] => Task 2, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 51.96
2024-10-28 10:47:10,868 [gem.py] => Task 2, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 48.15
2024-10-28 10:47:38,113 [gem.py] => Task 2, Epoch 41/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.46
2024-10-28 10:48:07,261 [gem.py] => Task 2, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.22
2024-10-28 10:48:36,937 [gem.py] => Task 2, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.85
2024-10-28 10:49:06,620 [gem.py] => Task 2, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.96
2024-10-28 10:49:36,238 [gem.py] => Task 2, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.83
2024-10-28 10:50:06,103 [gem.py] => Task 2, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.93
2024-10-28 10:50:35,273 [gem.py] => Task 2, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.56
2024-10-28 10:51:06,146 [gem.py] => Task 2, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.17
2024-10-28 10:51:36,362 [gem.py] => Task 2, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.91
2024-10-28 10:52:06,898 [gem.py] => Task 2, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.67
2024-10-28 10:52:35,714 [gem.py] => Task 2, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.85
2024-10-28 10:53:05,136 [gem.py] => Task 2, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.85
2024-10-28 10:53:35,110 [gem.py] => Task 2, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.93
2024-10-28 10:54:04,687 [gem.py] => Task 2, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.94
2024-10-28 10:54:35,161 [gem.py] => Task 2, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.07
2024-10-28 10:55:05,703 [gem.py] => Task 2, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.06
2024-10-28 10:55:35,623 [gem.py] => Task 2, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.26
2024-10-28 10:56:06,764 [gem.py] => Task 2, Epoch 126/150 => Loss 0.001, Train_accy 100.00, Test_accy 48.15
2024-10-28 10:56:36,376 [gem.py] => Task 2, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.00
2024-10-28 10:57:05,788 [gem.py] => Task 2, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 48.00
2024-10-28 10:57:34,320 [gem.py] => Task 2, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.57
2024-10-28 10:58:04,307 [gem.py] => Task 2, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 47.89
2024-10-28 10:58:28,523 [gem.py] => Task 2, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-10-28 10:58:28,532 [base.py] => Reducing exemplars...(55 per classes)
2024-10-28 10:58:30,574 [base.py] => Constructing exemplars...(55 per classes)
2024-10-28 10:58:32,904 [trainer.py] => All params: 3848521
2024-10-28 10:58:34,172 [gem.py] => Exemplar size: 495
2024-10-28 10:58:34,172 [trainer.py] => CNN: {'total': 47.44, '00-04': 54.9, '05-06': 9.67, '07-08': 66.58, 'old': 41.98, 'new': 66.58}
2024-10-28 10:58:34,172 [trainer.py] => NME: {'total': 52.81, '00-04': 57.83, '05-06': 29.5, '07-08': 63.58, 'old': 49.74, 'new': 63.58}
2024-10-28 10:58:34,173 [trainer.py] => CNN top1 curve: [90.13, 63.17, 47.44]
2024-10-28 10:58:34,173 [trainer.py] => CNN top5 curve: [100.0, 97.43, 92.04]
2024-10-28 10:58:34,173 [trainer.py] => NME top1 curve: [89.53, 65.48, 52.81]
2024-10-28 10:58:34,173 [trainer.py] => NME top5 curve: [100.0, 96.86, 93.07]

2024-10-28 10:58:34,173 [trainer.py] => Average Accuracy (CNN): 66.91333333333334
2024-10-28 10:58:34,173 [trainer.py] => Average Accuracy (NME): 69.27333333333333
2024-10-28 10:58:34,174 [trainer.py] => Forgetting (CNN): 33.19499999999999
