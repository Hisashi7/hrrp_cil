2024-10-23 15:23:07,003 [trainer.py] => config: ./exps/icarl.json
2024-10-23 15:23:07,003 [trainer.py] => prefix: cil
2024-10-23 15:23:07,003 [trainer.py] => dataset: hrrp9
2024-10-23 15:23:07,003 [trainer.py] => memory_size: 400
2024-10-23 15:23:07,003 [trainer.py] => memory_per_class: 20
2024-10-23 15:23:07,003 [trainer.py] => fixed_memory: False
2024-10-23 15:23:07,003 [trainer.py] => shuffle: True
2024-10-23 15:23:07,003 [trainer.py] => init_cls: 5
2024-10-23 15:23:07,003 [trainer.py] => increment: 2
2024-10-23 15:23:07,003 [trainer.py] => model_name: icarl
2024-10-23 15:23:07,003 [trainer.py] => convnet_type: resnet18
2024-10-23 15:23:07,003 [trainer.py] => device: [device(type='cuda', index=5)]
2024-10-23 15:23:07,003 [trainer.py] => init_train: False
2024-10-23 15:23:07,003 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-23 15:23:07,004 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-23 15:23:07,004 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-23 15:23:07,004 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-23 15:23:07,004 [trainer.py] => seed: 1993
2024-10-23 15:23:07,004 [trainer.py] => init_epoch: 0
2024-10-23 15:23:07,004 [trainer.py] => init_lr: 0.1
2024-10-23 15:23:07,004 [trainer.py] => init_milestones: [60, 120, 170]
2024-10-23 15:23:07,004 [trainer.py] => init_lr_decay: 0.1
2024-10-23 15:23:07,004 [trainer.py] => init_weight_decay: 0.0005
2024-10-23 15:23:07,004 [trainer.py] => epochs: 150
2024-10-23 15:23:07,004 [trainer.py] => lrate: 0.1
2024-10-23 15:23:07,004 [trainer.py] => milestones: [80, 120]
2024-10-23 15:23:07,004 [trainer.py] => lrate_decay: 0.1
2024-10-23 15:23:07,004 [trainer.py] => momentum: 0.9
2024-10-23 15:23:07,004 [trainer.py] => batch_size: 128
2024-10-23 15:23:07,004 [trainer.py] => weight_decay: 0.0002
2024-10-23 15:23:07,004 [trainer.py] => num_workers: 8
2024-10-23 15:23:07,004 [trainer.py] => T: 2
2024-10-23 15:23:07,714 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-23 15:23:08,289 [trainer.py] => All params: 3843904
2024-10-23 15:23:08,289 [trainer.py] => Trainable params: 3843904
2024-10-23 15:23:08,292 [icarl.py] => Learning on 0-5
2024-10-23 15:23:08,517 [icarl.py] => init_train?---False
2024-10-23 15:23:09,425 [base.py] => Reducing exemplars...(80 per classes)
2024-10-23 15:23:09,425 [base.py] => Constructing exemplars...(80 per classes)
2024-10-23 15:23:14,998 [trainer.py] => All params: 3846469
2024-10-23 15:23:16,110 [icarl.py] => Exemplar size: 400
2024-10-23 15:23:16,111 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-23 15:23:16,111 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-23 15:23:16,111 [trainer.py] => CNN top1 curve: [89.93]
2024-10-23 15:23:16,111 [trainer.py] => CNN top5 curve: [100.0]
2024-10-23 15:23:16,111 [trainer.py] => NME top1 curve: [90.0]
2024-10-23 15:23:16,111 [trainer.py] => NME top5 curve: [100.0]

2024-10-23 15:23:16,111 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-23 15:23:16,111 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-23 15:23:16,111 [trainer.py] => All params: 3846469
2024-10-23 15:23:16,112 [trainer.py] => Trainable params: 3846469
2024-10-23 15:23:16,113 [icarl.py] => Learning on 5-7
2024-10-23 15:23:18,114 [icarl.py] => Task 1, Epoch 1/150 => Loss 2.016, Train_accy 77.66, Test_accy 36.64
2024-10-23 15:23:24,295 [icarl.py] => Task 1, Epoch 6/150 => Loss 0.995, Train_accy 99.52, Test_accy 62.95
2024-10-23 15:23:30,209 [icarl.py] => Task 1, Epoch 11/150 => Loss 0.939, Train_accy 100.00, Test_accy 64.02
2024-10-23 15:23:36,033 [icarl.py] => Task 1, Epoch 16/150 => Loss 0.937, Train_accy 100.00, Test_accy 66.29
2024-10-23 15:23:42,116 [icarl.py] => Task 1, Epoch 21/150 => Loss 0.935, Train_accy 100.00, Test_accy 65.00
2024-10-23 15:23:48,545 [icarl.py] => Task 1, Epoch 26/150 => Loss 0.932, Train_accy 100.00, Test_accy 63.38
2024-10-23 15:23:55,122 [icarl.py] => Task 1, Epoch 31/150 => Loss 0.927, Train_accy 100.00, Test_accy 66.62
2024-10-23 15:24:02,372 [icarl.py] => Task 1, Epoch 36/150 => Loss 0.928, Train_accy 100.00, Test_accy 65.95
2024-10-23 15:24:09,775 [icarl.py] => Task 1, Epoch 41/150 => Loss 0.930, Train_accy 100.00, Test_accy 67.57
2024-10-23 15:24:17,588 [icarl.py] => Task 1, Epoch 46/150 => Loss 0.928, Train_accy 100.00, Test_accy 65.81
2024-10-23 15:24:26,281 [icarl.py] => Task 1, Epoch 51/150 => Loss 0.927, Train_accy 100.00, Test_accy 66.90
2024-10-23 15:24:35,662 [icarl.py] => Task 1, Epoch 56/150 => Loss 0.926, Train_accy 100.00, Test_accy 67.26
2024-10-23 15:24:44,683 [icarl.py] => Task 1, Epoch 61/150 => Loss 0.925, Train_accy 100.00, Test_accy 65.57
2024-10-23 15:24:53,446 [icarl.py] => Task 1, Epoch 66/150 => Loss 0.929, Train_accy 100.00, Test_accy 67.17
2024-10-23 15:25:02,174 [icarl.py] => Task 1, Epoch 71/150 => Loss 0.928, Train_accy 100.00, Test_accy 64.55
2024-10-23 15:25:11,958 [icarl.py] => Task 1, Epoch 76/150 => Loss 0.929, Train_accy 100.00, Test_accy 67.88
2024-10-23 15:25:21,136 [icarl.py] => Task 1, Epoch 81/150 => Loss 0.924, Train_accy 100.00, Test_accy 66.76
2024-10-23 15:25:30,652 [icarl.py] => Task 1, Epoch 86/150 => Loss 0.920, Train_accy 100.00, Test_accy 67.43
2024-10-23 15:25:40,243 [icarl.py] => Task 1, Epoch 91/150 => Loss 0.922, Train_accy 100.00, Test_accy 67.29
2024-10-23 15:25:49,276 [icarl.py] => Task 1, Epoch 96/150 => Loss 0.919, Train_accy 100.00, Test_accy 67.69
2024-10-23 15:25:58,255 [icarl.py] => Task 1, Epoch 101/150 => Loss 0.921, Train_accy 100.00, Test_accy 68.17
2024-10-23 15:26:07,329 [icarl.py] => Task 1, Epoch 106/150 => Loss 0.917, Train_accy 100.00, Test_accy 67.29
2024-10-23 15:26:16,202 [icarl.py] => Task 1, Epoch 111/150 => Loss 0.920, Train_accy 100.00, Test_accy 67.12
2024-10-23 15:26:24,695 [icarl.py] => Task 1, Epoch 116/150 => Loss 0.917, Train_accy 100.00, Test_accy 66.21
2024-10-23 15:26:33,038 [icarl.py] => Task 1, Epoch 121/150 => Loss 0.920, Train_accy 100.00, Test_accy 67.24
2024-10-23 15:26:41,984 [icarl.py] => Task 1, Epoch 126/150 => Loss 0.918, Train_accy 100.00, Test_accy 67.48
2024-10-23 15:26:51,405 [icarl.py] => Task 1, Epoch 131/150 => Loss 0.917, Train_accy 100.00, Test_accy 67.24
2024-10-23 15:27:00,652 [icarl.py] => Task 1, Epoch 136/150 => Loss 0.917, Train_accy 100.00, Test_accy 67.45
2024-10-23 15:27:09,917 [icarl.py] => Task 1, Epoch 141/150 => Loss 0.919, Train_accy 100.00, Test_accy 67.60
2024-10-23 15:27:18,755 [icarl.py] => Task 1, Epoch 146/150 => Loss 0.916, Train_accy 100.00, Test_accy 67.24
2024-10-23 15:27:24,622 [icarl.py] => Task 1, Epoch 150/150 => Loss 0.921, Train_accy 100.00
2024-10-23 15:27:24,623 [base.py] => Reducing exemplars...(57 per classes)
2024-10-23 15:27:26,481 [base.py] => Constructing exemplars...(57 per classes)
2024-10-23 15:27:29,803 [trainer.py] => All params: 3847495
2024-10-23 15:27:32,077 [icarl.py] => Exemplar size: 399
2024-10-23 15:27:32,077 [trainer.py] => CNN: {'total': 67.55, '00-04': 55.77, '05-06': 97.0, 'old': 55.77, 'new': 97.0}
2024-10-23 15:27:32,077 [trainer.py] => NME: {'total': 76.86, '00-04': 70.33, '05-06': 93.17, 'old': 70.33, 'new': 93.17}
2024-10-23 15:27:32,077 [trainer.py] => CNN top1 curve: [89.93, 67.55]
2024-10-23 15:27:32,078 [trainer.py] => CNN top5 curve: [100.0, 99.21]
2024-10-23 15:27:32,078 [trainer.py] => NME top1 curve: [90.0, 76.86]
2024-10-23 15:27:32,078 [trainer.py] => NME top5 curve: [100.0, 99.19]

2024-10-23 15:27:32,078 [trainer.py] => Average Accuracy (CNN): 78.74000000000001
2024-10-23 15:27:32,078 [trainer.py] => Average Accuracy (NME): 83.43
2024-10-23 15:27:32,078 [trainer.py] => All params: 3847495
2024-10-23 15:27:32,079 [trainer.py] => Trainable params: 3847495
2024-10-23 15:27:32,087 [icarl.py] => Learning on 7-9
2024-10-23 15:27:34,662 [icarl.py] => Task 2, Epoch 1/150 => Loss 3.140, Train_accy 66.77, Test_accy 11.44
2024-10-23 15:27:43,512 [icarl.py] => Task 2, Epoch 6/150 => Loss 1.768, Train_accy 89.93, Test_accy 29.52
2024-10-23 15:27:52,396 [icarl.py] => Task 2, Epoch 11/150 => Loss 1.508, Train_accy 92.73, Test_accy 31.17
2024-10-23 15:28:01,306 [icarl.py] => Task 2, Epoch 16/150 => Loss 1.288, Train_accy 97.04, Test_accy 42.06
2024-10-23 15:28:11,026 [icarl.py] => Task 2, Epoch 21/150 => Loss 1.181, Train_accy 98.86, Test_accy 51.35
2024-10-23 15:28:20,482 [icarl.py] => Task 2, Epoch 26/150 => Loss 1.071, Train_accy 100.00, Test_accy 58.06
2024-10-23 15:28:29,569 [icarl.py] => Task 2, Epoch 31/150 => Loss 1.045, Train_accy 100.00, Test_accy 60.39
2024-10-23 15:28:38,920 [icarl.py] => Task 2, Epoch 36/150 => Loss 1.035, Train_accy 100.00, Test_accy 60.15
2024-10-23 15:28:49,173 [icarl.py] => Task 2, Epoch 41/150 => Loss 1.030, Train_accy 100.00, Test_accy 58.65
2024-10-23 15:29:00,001 [icarl.py] => Task 2, Epoch 46/150 => Loss 1.132, Train_accy 99.14, Test_accy 48.87
2024-10-23 15:29:10,781 [icarl.py] => Task 2, Epoch 51/150 => Loss 1.038, Train_accy 100.00, Test_accy 60.31
2024-10-23 15:29:21,751 [icarl.py] => Task 2, Epoch 56/150 => Loss 1.024, Train_accy 100.00, Test_accy 60.48
2024-10-23 15:29:33,801 [icarl.py] => Task 2, Epoch 61/150 => Loss 1.023, Train_accy 100.00, Test_accy 61.24
2024-10-23 15:29:46,681 [icarl.py] => Task 2, Epoch 66/150 => Loss 1.022, Train_accy 100.00, Test_accy 58.46
2024-10-23 15:29:59,572 [icarl.py] => Task 2, Epoch 71/150 => Loss 1.022, Train_accy 100.00, Test_accy 59.87
2024-10-23 15:30:13,263 [icarl.py] => Task 2, Epoch 76/150 => Loss 1.022, Train_accy 100.00, Test_accy 58.43
2024-10-23 15:30:28,379 [icarl.py] => Task 2, Epoch 81/150 => Loss 1.015, Train_accy 100.00, Test_accy 61.48
2024-10-23 15:30:42,120 [icarl.py] => Task 2, Epoch 86/150 => Loss 1.015, Train_accy 100.00, Test_accy 60.20
2024-10-23 15:30:56,222 [icarl.py] => Task 2, Epoch 91/150 => Loss 1.013, Train_accy 100.00, Test_accy 61.00
2024-10-23 15:31:10,903 [icarl.py] => Task 2, Epoch 96/150 => Loss 1.011, Train_accy 100.00, Test_accy 60.94
2024-10-23 15:31:25,315 [icarl.py] => Task 2, Epoch 101/150 => Loss 1.013, Train_accy 100.00, Test_accy 60.35
2024-10-23 15:31:38,658 [icarl.py] => Task 2, Epoch 106/150 => Loss 1.016, Train_accy 100.00, Test_accy 59.28
2024-10-23 15:31:53,621 [icarl.py] => Task 2, Epoch 111/150 => Loss 1.013, Train_accy 100.00, Test_accy 60.02
2024-10-23 15:32:07,753 [icarl.py] => Task 2, Epoch 116/150 => Loss 1.011, Train_accy 100.00, Test_accy 60.22
2024-10-23 15:32:23,317 [icarl.py] => Task 2, Epoch 121/150 => Loss 1.013, Train_accy 100.00, Test_accy 60.37
2024-10-23 15:32:39,941 [icarl.py] => Task 2, Epoch 126/150 => Loss 1.009, Train_accy 100.00, Test_accy 60.26
2024-10-23 15:32:55,930 [icarl.py] => Task 2, Epoch 131/150 => Loss 1.008, Train_accy 100.00, Test_accy 59.52
2024-10-23 15:33:13,445 [icarl.py] => Task 2, Epoch 136/150 => Loss 1.012, Train_accy 100.00, Test_accy 60.56
2024-10-23 15:33:31,140 [icarl.py] => Task 2, Epoch 141/150 => Loss 1.012, Train_accy 100.00, Test_accy 60.57
2024-10-23 15:33:49,811 [icarl.py] => Task 2, Epoch 146/150 => Loss 1.012, Train_accy 100.00, Test_accy 60.78
2024-10-23 15:34:00,730 [icarl.py] => Task 2, Epoch 150/150 => Loss 1.012, Train_accy 100.00
2024-10-23 15:34:00,731 [base.py] => Reducing exemplars...(44 per classes)
2024-10-23 15:34:06,133 [base.py] => Constructing exemplars...(44 per classes)
2024-10-23 15:34:12,690 [trainer.py] => All params: 3848521
2024-10-23 15:34:18,787 [icarl.py] => Exemplar size: 396
2024-10-23 15:34:18,788 [trainer.py] => CNN: {'total': 60.04, '00-04': 43.93, '05-06': 63.92, '07-08': 96.42, 'old': 49.64, 'new': 96.42}
2024-10-23 15:34:18,788 [trainer.py] => NME: {'total': 68.59, '00-04': 58.4, '05-06': 74.08, '07-08': 88.58, 'old': 62.88, 'new': 88.58}
2024-10-23 15:34:18,788 [trainer.py] => CNN top1 curve: [89.93, 67.55, 60.04]
2024-10-23 15:34:18,788 [trainer.py] => CNN top5 curve: [100.0, 99.21, 95.24]
2024-10-23 15:34:18,788 [trainer.py] => NME top1 curve: [90.0, 76.86, 68.59]
2024-10-23 15:34:18,788 [trainer.py] => NME top5 curve: [100.0, 99.19, 96.78]

2024-10-23 15:34:18,788 [trainer.py] => Average Accuracy (CNN): 72.50666666666667
2024-10-23 15:34:18,789 [trainer.py] => Average Accuracy (NME): 78.48333333333333
2024-10-23 15:34:18,789 [trainer.py] => Forgetting (CNN): 39.540000000000006
