2024-08-30 17:15:30,707 [trainer.py] => config: ./exps/icarl.json
2024-08-30 17:15:30,707 [trainer.py] => prefix: cil
2024-08-30 17:15:30,707 [trainer.py] => dataset: hrrp9
2024-08-30 17:15:30,707 [trainer.py] => memory_size: 500
2024-08-30 17:15:30,707 [trainer.py] => memory_per_class: 20
2024-08-30 17:15:30,707 [trainer.py] => fixed_memory: False
2024-08-30 17:15:30,707 [trainer.py] => shuffle: True
2024-08-30 17:15:30,707 [trainer.py] => init_cls: 5
2024-08-30 17:15:30,707 [trainer.py] => increment: 2
2024-08-30 17:15:30,707 [trainer.py] => model_name: icarl
2024-08-30 17:15:30,707 [trainer.py] => convnet_type: resnet18
2024-08-30 17:15:30,707 [trainer.py] => device: [device(type='cuda', index=7)]
2024-08-30 17:15:30,707 [trainer.py] => init_train: False
2024-08-30 17:15:30,707 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 17:15:30,708 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 17:15:30,708 [trainer.py] => seed: 1993
2024-08-30 17:15:31,183 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 17:15:33,842 [trainer.py] => All params: 3843904
2024-08-30 17:15:33,843 [trainer.py] => Trainable params: 3843904
2024-08-30 17:15:33,845 [icarl.py] => Learning on 0-5
2024-08-30 17:15:36,352 [icarl.py] => init_train?---False
2024-08-30 17:15:40,865 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:15:40,865 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:15:54,634 [icarl.py] => Exemplar size: 500
2024-08-30 17:15:54,635 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-08-30 17:15:54,635 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-08-30 17:15:54,635 [trainer.py] => CNN top1 curve: [89.93]
2024-08-30 17:15:54,636 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 17:15:54,636 [trainer.py] => NME top1 curve: [90.0]
2024-08-30 17:15:54,636 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 17:15:54,636 [trainer.py] => Average Accuracy (CNN): 89.93
2024-08-30 17:15:54,637 [trainer.py] => Average Accuracy (NME): 90.0
2024-08-30 17:15:54,637 [trainer.py] => All params: 3846469
2024-08-30 17:15:54,638 [trainer.py] => Trainable params: 3846469
2024-08-30 17:15:54,639 [icarl.py] => Learning on 5-7
2024-08-30 17:15:58,785 [icarl.py] => Task 1, Epoch 1/300 => Loss 2.026, Train_accy 75.47, Test_accy 26.93
2024-08-30 17:16:10,578 [icarl.py] => Task 1, Epoch 6/300 => Loss 1.057, Train_accy 98.11, Test_accy 56.60
2024-08-30 17:16:21,664 [icarl.py] => Task 1, Epoch 11/300 => Loss 0.933, Train_accy 99.98, Test_accy 67.10
2024-08-30 17:16:32,806 [icarl.py] => Task 1, Epoch 16/300 => Loss 0.924, Train_accy 99.98, Test_accy 66.33
2024-08-30 17:16:44,157 [icarl.py] => Task 1, Epoch 21/300 => Loss 0.925, Train_accy 100.00, Test_accy 71.60
2024-08-30 17:16:55,803 [icarl.py] => Task 1, Epoch 26/300 => Loss 0.980, Train_accy 99.62, Test_accy 65.21
2024-08-30 17:17:07,108 [icarl.py] => Task 1, Epoch 31/300 => Loss 0.937, Train_accy 99.91, Test_accy 65.88
2024-08-30 17:17:18,389 [icarl.py] => Task 1, Epoch 36/300 => Loss 0.927, Train_accy 99.98, Test_accy 68.67
2024-08-30 17:17:29,727 [icarl.py] => Task 1, Epoch 41/300 => Loss 0.919, Train_accy 100.00, Test_accy 67.93
2024-08-30 17:17:41,158 [icarl.py] => Task 1, Epoch 46/300 => Loss 0.911, Train_accy 100.00, Test_accy 68.67
2024-08-30 17:17:52,374 [icarl.py] => Task 1, Epoch 51/300 => Loss 0.912, Train_accy 100.00, Test_accy 66.57
2024-08-30 17:18:03,110 [icarl.py] => Task 1, Epoch 56/300 => Loss 0.914, Train_accy 100.00, Test_accy 67.93
2024-08-30 17:18:14,101 [icarl.py] => Task 1, Epoch 61/300 => Loss 0.918, Train_accy 100.00, Test_accy 63.48
2024-08-30 17:18:25,364 [icarl.py] => Task 1, Epoch 66/300 => Loss 0.908, Train_accy 100.00, Test_accy 69.36
2024-08-30 17:18:36,502 [icarl.py] => Task 1, Epoch 71/300 => Loss 0.914, Train_accy 100.00, Test_accy 69.43
2024-08-30 17:18:47,914 [icarl.py] => Task 1, Epoch 76/300 => Loss 0.976, Train_accy 99.58, Test_accy 61.55
2024-08-30 17:18:59,283 [icarl.py] => Task 1, Epoch 81/300 => Loss 1.026, Train_accy 98.38, Test_accy 63.57
2024-08-30 17:19:10,557 [icarl.py] => Task 1, Epoch 86/300 => Loss 0.920, Train_accy 100.00, Test_accy 68.17
2024-08-30 17:19:21,731 [icarl.py] => Task 1, Epoch 91/300 => Loss 0.913, Train_accy 100.00, Test_accy 67.40
2024-08-30 17:19:32,994 [icarl.py] => Task 1, Epoch 96/300 => Loss 0.913, Train_accy 100.00, Test_accy 66.36
2024-08-30 17:19:43,987 [icarl.py] => Task 1, Epoch 101/300 => Loss 0.913, Train_accy 100.00, Test_accy 68.64
2024-08-30 17:19:54,885 [icarl.py] => Task 1, Epoch 106/300 => Loss 0.912, Train_accy 100.00, Test_accy 67.26
2024-08-30 17:20:05,686 [icarl.py] => Task 1, Epoch 111/300 => Loss 0.913, Train_accy 100.00, Test_accy 67.52
2024-08-30 17:20:16,548 [icarl.py] => Task 1, Epoch 116/300 => Loss 0.907, Train_accy 100.00, Test_accy 67.52
2024-08-30 17:20:27,696 [icarl.py] => Task 1, Epoch 121/300 => Loss 0.907, Train_accy 100.00, Test_accy 67.05
2024-08-30 17:20:38,788 [icarl.py] => Task 1, Epoch 126/300 => Loss 0.910, Train_accy 100.00, Test_accy 65.95
2024-08-30 17:20:49,829 [icarl.py] => Task 1, Epoch 131/300 => Loss 0.912, Train_accy 100.00, Test_accy 66.33
2024-08-30 17:21:00,855 [icarl.py] => Task 1, Epoch 136/300 => Loss 0.912, Train_accy 100.00, Test_accy 67.36
2024-08-30 17:21:12,167 [icarl.py] => Task 1, Epoch 141/300 => Loss 0.918, Train_accy 100.00, Test_accy 65.93
2024-08-30 17:21:23,410 [icarl.py] => Task 1, Epoch 146/300 => Loss 0.916, Train_accy 100.00, Test_accy 64.81
2024-08-30 17:21:34,650 [icarl.py] => Task 1, Epoch 151/300 => Loss 0.904, Train_accy 100.00, Test_accy 66.64
2024-08-30 17:21:46,178 [icarl.py] => Task 1, Epoch 156/300 => Loss 0.904, Train_accy 100.00, Test_accy 67.00
2024-08-30 17:21:57,310 [icarl.py] => Task 1, Epoch 161/300 => Loss 0.902, Train_accy 100.00, Test_accy 66.38
2024-08-30 17:22:08,622 [icarl.py] => Task 1, Epoch 166/300 => Loss 0.902, Train_accy 100.00, Test_accy 67.19
2024-08-30 17:22:20,705 [icarl.py] => Task 1, Epoch 171/300 => Loss 0.909, Train_accy 100.00, Test_accy 67.38
2024-08-30 17:22:32,059 [icarl.py] => Task 1, Epoch 176/300 => Loss 0.903, Train_accy 100.00, Test_accy 67.14
2024-08-30 17:22:43,127 [icarl.py] => Task 1, Epoch 181/300 => Loss 0.905, Train_accy 100.00, Test_accy 67.00
2024-08-30 17:22:54,671 [icarl.py] => Task 1, Epoch 186/300 => Loss 0.907, Train_accy 100.00, Test_accy 65.76
2024-08-30 17:23:05,951 [icarl.py] => Task 1, Epoch 191/300 => Loss 0.906, Train_accy 100.00, Test_accy 67.02
2024-08-30 17:23:17,411 [icarl.py] => Task 1, Epoch 196/300 => Loss 0.908, Train_accy 100.00, Test_accy 67.00
2024-08-30 17:23:28,234 [icarl.py] => Task 1, Epoch 201/300 => Loss 0.903, Train_accy 100.00, Test_accy 66.76
2024-08-30 17:23:40,813 [icarl.py] => Task 1, Epoch 206/300 => Loss 0.906, Train_accy 100.00, Test_accy 67.14
2024-08-30 17:23:52,535 [icarl.py] => Task 1, Epoch 211/300 => Loss 0.908, Train_accy 100.00, Test_accy 66.60
2024-08-30 17:24:03,899 [icarl.py] => Task 1, Epoch 216/300 => Loss 0.906, Train_accy 100.00, Test_accy 67.14
2024-08-30 17:24:15,334 [icarl.py] => Task 1, Epoch 221/300 => Loss 0.902, Train_accy 100.00, Test_accy 66.36
2024-08-30 17:24:26,492 [icarl.py] => Task 1, Epoch 226/300 => Loss 0.903, Train_accy 100.00, Test_accy 66.05
2024-08-30 17:24:38,024 [icarl.py] => Task 1, Epoch 231/300 => Loss 0.901, Train_accy 99.98, Test_accy 66.14
2024-08-30 17:24:49,524 [icarl.py] => Task 1, Epoch 236/300 => Loss 0.902, Train_accy 100.00, Test_accy 66.36
2024-08-30 17:25:00,297 [icarl.py] => Task 1, Epoch 241/300 => Loss 0.905, Train_accy 100.00, Test_accy 66.60
2024-08-30 17:25:10,892 [icarl.py] => Task 1, Epoch 246/300 => Loss 0.904, Train_accy 100.00, Test_accy 66.62
2024-08-30 17:25:21,903 [icarl.py] => Task 1, Epoch 251/300 => Loss 0.909, Train_accy 100.00, Test_accy 67.64
2024-08-30 17:25:32,449 [icarl.py] => Task 1, Epoch 256/300 => Loss 0.909, Train_accy 100.00, Test_accy 67.69
2024-08-30 17:25:43,309 [icarl.py] => Task 1, Epoch 261/300 => Loss 0.905, Train_accy 100.00, Test_accy 66.64
2024-08-30 17:25:53,700 [icarl.py] => Task 1, Epoch 266/300 => Loss 0.905, Train_accy 100.00, Test_accy 66.81
2024-08-30 17:26:04,188 [icarl.py] => Task 1, Epoch 271/300 => Loss 0.906, Train_accy 100.00, Test_accy 66.88
2024-08-30 17:26:14,712 [icarl.py] => Task 1, Epoch 276/300 => Loss 0.903, Train_accy 100.00, Test_accy 67.52
2024-08-30 17:26:26,247 [icarl.py] => Task 1, Epoch 281/300 => Loss 0.907, Train_accy 100.00, Test_accy 66.95
2024-08-30 17:26:38,159 [icarl.py] => Task 1, Epoch 286/300 => Loss 0.905, Train_accy 100.00, Test_accy 66.81
2024-08-30 17:26:49,573 [icarl.py] => Task 1, Epoch 291/300 => Loss 0.905, Train_accy 100.00, Test_accy 66.62
2024-08-30 17:26:59,959 [icarl.py] => Task 1, Epoch 296/300 => Loss 0.905, Train_accy 100.00, Test_accy 66.24
2024-08-30 17:27:07,246 [icarl.py] => Task 1, Epoch 300/300 => Loss 0.904, Train_accy 100.00
2024-08-30 17:27:07,248 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:27:10,919 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:27:17,529 [icarl.py] => Exemplar size: 497
2024-08-30 17:27:17,529 [trainer.py] => CNN: {'total': 66.57, '00-04': 54.13, '05-06': 97.67, 'old': 54.13, 'new': 97.67}
2024-08-30 17:27:17,529 [trainer.py] => NME: {'total': 77.29, '00-04': 70.57, '05-06': 94.08, 'old': 70.57, 'new': 94.08}
2024-08-30 17:27:17,529 [trainer.py] => CNN top1 curve: [89.93, 66.57]
2024-08-30 17:27:17,529 [trainer.py] => CNN top5 curve: [100.0, 98.57]
2024-08-30 17:27:17,529 [trainer.py] => NME top1 curve: [90.0, 77.29]
2024-08-30 17:27:17,529 [trainer.py] => NME top5 curve: [100.0, 98.88]

2024-08-30 17:27:17,529 [trainer.py] => Average Accuracy (CNN): 78.25
2024-08-30 17:27:17,530 [trainer.py] => Average Accuracy (NME): 83.64500000000001
2024-08-30 17:27:17,530 [trainer.py] => All params: 3847495
2024-08-30 17:27:17,530 [trainer.py] => Trainable params: 3847495
2024-08-30 17:27:17,531 [icarl.py] => Learning on 7-9
2024-08-30 17:27:21,010 [icarl.py] => Task 2, Epoch 1/300 => Loss 2.823, Train_accy 73.76, Test_accy 21.44
2024-08-30 17:27:31,769 [icarl.py] => Task 2, Epoch 6/300 => Loss 1.560, Train_accy 92.13, Test_accy 46.78
2024-08-30 17:27:42,918 [icarl.py] => Task 2, Epoch 11/300 => Loss 1.265, Train_accy 97.55, Test_accy 47.81
2024-08-30 17:27:53,176 [icarl.py] => Task 2, Epoch 16/300 => Loss 1.087, Train_accy 100.00, Test_accy 64.87
2024-08-30 17:28:04,002 [icarl.py] => Task 2, Epoch 21/300 => Loss 1.067, Train_accy 99.98, Test_accy 66.20
2024-08-30 17:28:15,426 [icarl.py] => Task 2, Epoch 26/300 => Loss 1.187, Train_accy 98.42, Test_accy 50.19
2024-08-30 17:28:26,224 [icarl.py] => Task 2, Epoch 31/300 => Loss 1.081, Train_accy 99.84, Test_accy 61.46
2024-08-30 17:28:37,619 [icarl.py] => Task 2, Epoch 36/300 => Loss 1.062, Train_accy 99.96, Test_accy 63.04
2024-08-30 17:28:48,330 [icarl.py] => Task 2, Epoch 41/300 => Loss 1.056, Train_accy 100.00, Test_accy 62.46
2024-08-30 17:28:59,009 [icarl.py] => Task 2, Epoch 46/300 => Loss 1.053, Train_accy 100.00, Test_accy 64.59
2024-08-30 17:29:10,155 [icarl.py] => Task 2, Epoch 51/300 => Loss 1.044, Train_accy 100.00, Test_accy 63.85
2024-08-30 17:29:21,291 [icarl.py] => Task 2, Epoch 56/300 => Loss 1.036, Train_accy 100.00, Test_accy 63.50
2024-08-30 17:29:32,277 [icarl.py] => Task 2, Epoch 61/300 => Loss 1.041, Train_accy 100.00, Test_accy 63.63
2024-08-30 17:29:43,576 [icarl.py] => Task 2, Epoch 66/300 => Loss 1.041, Train_accy 100.00, Test_accy 61.96
2024-08-30 17:29:55,104 [icarl.py] => Task 2, Epoch 71/300 => Loss 1.043, Train_accy 100.00, Test_accy 64.67
2024-08-30 17:30:06,362 [icarl.py] => Task 2, Epoch 76/300 => Loss 1.047, Train_accy 99.93, Test_accy 60.70
2024-08-30 17:30:17,798 [icarl.py] => Task 2, Epoch 81/300 => Loss 1.038, Train_accy 100.00, Test_accy 63.52
2024-08-30 17:30:28,729 [icarl.py] => Task 2, Epoch 86/300 => Loss 1.043, Train_accy 100.00, Test_accy 63.74
2024-08-30 17:30:40,435 [icarl.py] => Task 2, Epoch 91/300 => Loss 1.038, Train_accy 100.00, Test_accy 64.50
2024-08-30 17:30:51,670 [icarl.py] => Task 2, Epoch 96/300 => Loss 1.036, Train_accy 100.00, Test_accy 63.52
2024-08-30 17:31:02,985 [icarl.py] => Task 2, Epoch 101/300 => Loss 1.034, Train_accy 100.00, Test_accy 63.24
2024-08-30 17:31:14,310 [icarl.py] => Task 2, Epoch 106/300 => Loss 1.039, Train_accy 100.00, Test_accy 58.56
2024-08-30 17:31:25,912 [icarl.py] => Task 2, Epoch 111/300 => Loss 1.196, Train_accy 98.47, Test_accy 60.30
2024-08-30 17:31:37,089 [icarl.py] => Task 2, Epoch 116/300 => Loss 1.060, Train_accy 100.00, Test_accy 65.00
2024-08-30 17:31:48,531 [icarl.py] => Task 2, Epoch 121/300 => Loss 1.080, Train_accy 99.69, Test_accy 65.28
2024-08-30 17:31:59,921 [icarl.py] => Task 2, Epoch 126/300 => Loss 1.051, Train_accy 100.00, Test_accy 66.37
2024-08-30 17:32:11,867 [icarl.py] => Task 2, Epoch 131/300 => Loss 1.035, Train_accy 100.00, Test_accy 65.83
2024-08-30 17:32:23,570 [icarl.py] => Task 2, Epoch 136/300 => Loss 1.037, Train_accy 100.00, Test_accy 64.93
2024-08-30 17:32:35,069 [icarl.py] => Task 2, Epoch 141/300 => Loss 1.040, Train_accy 100.00, Test_accy 62.13
2024-08-30 17:32:46,228 [icarl.py] => Task 2, Epoch 146/300 => Loss 1.040, Train_accy 100.00, Test_accy 62.78
2024-08-30 17:32:57,522 [icarl.py] => Task 2, Epoch 151/300 => Loss 1.035, Train_accy 100.00, Test_accy 64.94
2024-08-30 17:33:09,057 [icarl.py] => Task 2, Epoch 156/300 => Loss 1.033, Train_accy 100.00, Test_accy 65.54
2024-08-30 17:33:20,721 [icarl.py] => Task 2, Epoch 161/300 => Loss 1.038, Train_accy 100.00, Test_accy 64.46
2024-08-30 17:33:31,980 [icarl.py] => Task 2, Epoch 166/300 => Loss 1.030, Train_accy 100.00, Test_accy 64.15
2024-08-30 17:33:42,885 [icarl.py] => Task 2, Epoch 171/300 => Loss 1.032, Train_accy 100.00, Test_accy 63.87
2024-08-30 17:33:54,330 [icarl.py] => Task 2, Epoch 176/300 => Loss 1.037, Train_accy 100.00, Test_accy 64.26
2024-08-30 17:34:05,258 [icarl.py] => Task 2, Epoch 181/300 => Loss 1.033, Train_accy 100.00, Test_accy 65.22
2024-08-30 17:34:16,288 [icarl.py] => Task 2, Epoch 186/300 => Loss 1.033, Train_accy 100.00, Test_accy 64.33
2024-08-30 17:34:27,324 [icarl.py] => Task 2, Epoch 191/300 => Loss 1.033, Train_accy 100.00, Test_accy 64.96
2024-08-30 17:34:38,646 [icarl.py] => Task 2, Epoch 196/300 => Loss 1.035, Train_accy 100.00, Test_accy 65.72
2024-08-30 17:34:49,747 [icarl.py] => Task 2, Epoch 201/300 => Loss 1.029, Train_accy 100.00, Test_accy 63.61
2024-08-30 17:35:00,839 [icarl.py] => Task 2, Epoch 206/300 => Loss 1.030, Train_accy 100.00, Test_accy 64.46
2024-08-30 17:35:12,288 [icarl.py] => Task 2, Epoch 211/300 => Loss 1.029, Train_accy 100.00, Test_accy 63.22
2024-08-30 17:35:23,496 [icarl.py] => Task 2, Epoch 216/300 => Loss 1.034, Train_accy 100.00, Test_accy 64.98
2024-08-30 17:35:34,634 [icarl.py] => Task 2, Epoch 221/300 => Loss 1.027, Train_accy 100.00, Test_accy 64.39
2024-08-30 17:35:45,710 [icarl.py] => Task 2, Epoch 226/300 => Loss 1.028, Train_accy 100.00, Test_accy 65.76
2024-08-30 17:35:56,772 [icarl.py] => Task 2, Epoch 231/300 => Loss 1.031, Train_accy 100.00, Test_accy 64.96
2024-08-30 17:36:07,854 [icarl.py] => Task 2, Epoch 236/300 => Loss 1.026, Train_accy 100.00, Test_accy 64.39
2024-08-30 17:36:19,254 [icarl.py] => Task 2, Epoch 241/300 => Loss 1.026, Train_accy 100.00, Test_accy 63.87
2024-08-30 17:36:30,328 [icarl.py] => Task 2, Epoch 246/300 => Loss 1.031, Train_accy 100.00, Test_accy 65.06
2024-08-30 17:36:41,188 [icarl.py] => Task 2, Epoch 251/300 => Loss 1.032, Train_accy 100.00, Test_accy 64.74
2024-08-30 17:36:52,340 [icarl.py] => Task 2, Epoch 256/300 => Loss 1.031, Train_accy 100.00, Test_accy 64.39
2024-08-30 17:37:04,164 [icarl.py] => Task 2, Epoch 261/300 => Loss 1.034, Train_accy 100.00, Test_accy 65.52
2024-08-30 17:37:15,662 [icarl.py] => Task 2, Epoch 266/300 => Loss 1.031, Train_accy 100.00, Test_accy 62.85
2024-08-30 17:37:26,678 [icarl.py] => Task 2, Epoch 271/300 => Loss 1.022, Train_accy 100.00, Test_accy 64.35
2024-08-30 17:37:38,419 [icarl.py] => Task 2, Epoch 276/300 => Loss 1.028, Train_accy 100.00, Test_accy 65.35
2024-08-30 17:37:49,744 [icarl.py] => Task 2, Epoch 281/300 => Loss 1.027, Train_accy 100.00, Test_accy 64.07
2024-08-30 17:38:01,132 [icarl.py] => Task 2, Epoch 286/300 => Loss 1.033, Train_accy 100.00, Test_accy 63.83
2024-08-30 17:38:12,875 [icarl.py] => Task 2, Epoch 291/300 => Loss 1.027, Train_accy 100.00, Test_accy 62.94
2024-08-30 17:38:24,045 [icarl.py] => Task 2, Epoch 296/300 => Loss 1.031, Train_accy 99.98, Test_accy 62.74
2024-08-30 17:38:31,908 [icarl.py] => Task 2, Epoch 300/300 => Loss 1.027, Train_accy 100.00
2024-08-30 17:38:31,909 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 17:38:37,320 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 17:38:44,441 [icarl.py] => Exemplar size: 495
2024-08-30 17:38:44,441 [trainer.py] => CNN: {'total': 64.63, '00-04': 49.53, '05-06': 69.0, '07-08': 98.0, 'old': 55.1, 'new': 98.0}
2024-08-30 17:38:44,441 [trainer.py] => NME: {'total': 72.24, '00-04': 61.3, '05-06': 78.58, '07-08': 93.25, 'old': 66.24, 'new': 93.25}
2024-08-30 17:38:44,441 [trainer.py] => CNN top1 curve: [89.93, 66.57, 64.63]
2024-08-30 17:38:44,441 [trainer.py] => CNN top5 curve: [100.0, 98.57, 95.09]
2024-08-30 17:38:44,441 [trainer.py] => NME top1 curve: [90.0, 77.29, 72.24]
2024-08-30 17:38:44,441 [trainer.py] => NME top5 curve: [100.0, 98.88, 97.07]

2024-08-30 17:38:44,441 [trainer.py] => Average Accuracy (CNN): 73.71
2024-08-30 17:38:44,441 [trainer.py] => Average Accuracy (NME): 79.84333333333335
2024-08-30 17:38:44,442 [trainer.py] => Forgetting (CNN): 34.535000000000004
