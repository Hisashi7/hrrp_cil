2024-10-25 16:26:03,620 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 16:26:03,620 [trainer.py] => prefix: cil
2024-10-25 16:26:03,620 [trainer.py] => dataset: hrrp9
2024-10-25 16:26:03,620 [trainer.py] => memory_size: 500
2024-10-25 16:26:03,620 [trainer.py] => memory_per_class: 20
2024-10-25 16:26:03,620 [trainer.py] => fixed_memory: False
2024-10-25 16:26:03,620 [trainer.py] => shuffle: True
2024-10-25 16:26:03,620 [trainer.py] => init_cls: 5
2024-10-25 16:26:03,620 [trainer.py] => increment: 2
2024-10-25 16:26:03,620 [trainer.py] => model_name: POD_foster
2024-10-25 16:26:03,620 [trainer.py] => convnet_type: resnet18
2024-10-25 16:26:03,620 [trainer.py] => init_train: False
2024-10-25 16:26:03,620 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 16:26:03,620 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 16:26:03,620 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 16:26:03,620 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 16:26:03,621 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-25 16:26:03,621 [trainer.py] => seed: 1993
2024-10-25 16:26:03,621 [trainer.py] => beta1: 0.96
2024-10-25 16:26:03,621 [trainer.py] => beta2: 0.97
2024-10-25 16:26:03,621 [trainer.py] => oofc: ft
2024-10-25 16:26:03,621 [trainer.py] => is_teacher_wa: True
2024-10-25 16:26:03,621 [trainer.py] => is_student_wa: False
2024-10-25 16:26:03,621 [trainer.py] => lambda_okd: 0
2024-10-25 16:26:03,621 [trainer.py] => wa_value: 1
2024-10-25 16:26:03,621 [trainer.py] => init_epochs: 0
2024-10-25 16:26:03,621 [trainer.py] => init_lr: 0.1
2024-10-25 16:26:03,621 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 16:26:03,621 [trainer.py] => boosting_epochs: 150
2024-10-25 16:26:03,621 [trainer.py] => compression_epochs: 120
2024-10-25 16:26:03,621 [trainer.py] => lr: 0.1
2024-10-25 16:26:03,621 [trainer.py] => batch_size: 128
2024-10-25 16:26:03,621 [trainer.py] => weight_decay: 0.0005
2024-10-25 16:26:03,621 [trainer.py] => num_workers: 8
2024-10-25 16:26:03,621 [trainer.py] => momentum: 0.9
2024-10-25 16:26:03,621 [trainer.py] => T: 2
2024-10-25 16:26:03,621 [trainer.py] => lambda_c_base: 0.9
2024-10-25 16:26:03,621 [trainer.py] => lambda_f_base: 1.0
2024-10-25 16:26:03,621 [trainer.py] => POD: c
2024-10-25 16:26:04,401 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 16:26:04,515 [trainer.py] => All params: 0
2024-10-25 16:26:04,515 [trainer.py] => Trainable params: 0
2024-10-25 16:26:05,774 [pod_foster.py] => Learning on 0-5
2024-10-25 16:26:05,775 [pod_foster.py] => All params: 3849034
2024-10-25 16:26:05,775 [pod_foster.py] => Trainable params: 3849034
2024-10-25 16:26:05,882 [pod_foster.py] => Adaptive factor: 0
2024-10-25 16:26:06,114 [pod_foster.py] => init_train?---False
2024-10-25 16:26:07,291 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 16:26:07,292 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 16:26:14,370 [trainer.py] => All params: 3849034
2024-10-25 16:26:15,831 [pod_foster.py] => Exemplar size: 500
2024-10-25 16:26:15,831 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 16:26:15,831 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 16:26:15,831 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 16:26:15,831 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 16:26:15,831 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 16:26:15,832 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 16:26:15,832 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 16:26:15,832 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 16:26:15,832 [trainer.py] => All params: 3849034
2024-10-25 16:26:15,833 [trainer.py] => Trainable params: 3849034
2024-10-25 16:26:15,876 [pod_foster.py] => Learning on 5-7
2024-10-25 16:26:15,878 [pod_foster.py] => All params: 7701139
2024-10-25 16:26:15,878 [pod_foster.py] => Trainable params: 3854670
2024-10-25 16:26:15,934 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 16:26:15,945 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 16:26:19,817 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.845, Loss_clf 0.703, Loss_fe 0.652, Loss_pod 0.265, Loss_flat 0.225, Train_accy 83.76, Test_accy 50.43
2024-10-25 16:26:31,162 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.343, Loss_clf 0.028, Loss_fe 0.052, Loss_pod 0.162, Loss_flat 0.101, Train_accy 99.58, Test_accy 70.33
2024-10-25 16:26:42,360 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.164, Loss_clf 0.008, Loss_fe 0.008, Loss_pod 0.096, Loss_flat 0.052, Train_accy 100.00, Test_accy 68.98
2024-10-25 16:26:53,387 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.135, Loss_clf 0.007, Loss_fe 0.007, Loss_pod 0.080, Loss_flat 0.042, Train_accy 99.98, Test_accy 70.26
2024-10-25 16:27:04,230 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.140, Loss_clf 0.010, Loss_fe 0.010, Loss_pod 0.079, Loss_flat 0.041, Train_accy 99.96, Test_accy 49.88
2024-10-25 16:27:15,193 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.473, Loss_clf 0.037, Loss_fe 0.090, Loss_pod 0.207, Loss_flat 0.139, Train_accy 98.98, Test_accy 71.79
2024-10-25 16:27:25,917 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.206, Loss_clf 0.008, Loss_fe 0.008, Loss_pod 0.122, Loss_flat 0.068, Train_accy 99.98, Test_accy 69.17
2024-10-25 16:27:36,724 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.150, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.094, Loss_flat 0.045, Train_accy 100.00, Test_accy 68.17
2024-10-25 16:27:47,558 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.126, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.080, Loss_flat 0.037, Train_accy 100.00, Test_accy 68.33
2024-10-25 16:27:58,497 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.117, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.075, Loss_flat 0.033, Train_accy 100.00, Test_accy 67.93
2024-10-25 16:28:09,434 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.110, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.071, Loss_flat 0.030, Train_accy 100.00, Test_accy 66.60
2024-10-25 16:28:20,559 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.101, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.065, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.86
2024-10-25 16:28:31,909 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.108, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.068, Loss_flat 0.030, Train_accy 100.00, Test_accy 67.05
2024-10-25 16:28:42,957 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.457, Loss_clf 0.067, Loss_fe 0.101, Loss_pod 0.188, Loss_flat 0.101, Train_accy 97.93, Test_accy 76.71
2024-10-25 16:28:54,063 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.146, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.092, Loss_flat 0.043, Train_accy 100.00, Test_accy 68.10
2024-10-25 16:29:05,222 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.109, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.069, Loss_flat 0.032, Train_accy 100.00, Test_accy 68.67
2024-10-25 16:29:16,429 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.097, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.061, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.69
2024-10-25 16:29:27,521 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.096, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.061, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.43
2024-10-25 16:29:38,747 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.089, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.056, Loss_flat 0.026, Train_accy 100.00, Test_accy 67.69
2024-10-25 16:29:49,576 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.103, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.061, Loss_flat 0.032, Train_accy 100.00, Test_accy 68.38
2024-10-25 16:30:00,655 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.082, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.049, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.45
2024-10-25 16:30:11,683 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.080, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.047, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.07
2024-10-25 16:30:22,769 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.078, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.045, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.19
2024-10-25 16:30:33,573 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.075, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.26
2024-10-25 16:30:44,678 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.071, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.040, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.14
2024-10-25 16:30:55,748 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.071, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.040, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.00
2024-10-25 16:31:06,824 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.069, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.038, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.52
2024-10-25 16:31:17,932 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.066, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.036, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.45
2024-10-25 16:31:29,107 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.066, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.035, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.64
2024-10-25 16:31:40,297 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.067, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.037, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.43
2024-10-25 16:31:48,088 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.067, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.036, Loss_flat 0.024, Train_accy 100.00
2024-10-25 16:31:48,090 [inc_net.py] => align weights, gamma = 0.5205428600311279 
2024-10-25 16:31:48,091 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 16:31:50,674 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.262,  Train_accy 69.56, Test_accy 66.10
2024-10-25 16:32:00,191 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.967,  Train_accy 94.36, Test_accy 76.81
2024-10-25 16:32:09,834 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.961,  Train_accy 95.42, Test_accy 76.64
2024-10-25 16:32:19,431 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.954,  Train_accy 95.67, Test_accy 77.74
2024-10-25 16:32:29,019 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.957,  Train_accy 95.89, Test_accy 78.38
2024-10-25 16:32:38,890 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.953,  Train_accy 95.87, Test_accy 78.62
2024-10-25 16:32:48,320 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.955,  Train_accy 96.07, Test_accy 78.50
2024-10-25 16:32:57,917 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.954,  Train_accy 95.76, Test_accy 77.79
2024-10-25 16:33:07,628 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.951,  Train_accy 96.07, Test_accy 78.24
2024-10-25 16:33:17,451 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.949,  Train_accy 96.09, Test_accy 78.62
2024-10-25 16:33:27,118 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.950,  Train_accy 96.27, Test_accy 78.26
2024-10-25 16:33:36,993 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.948,  Train_accy 96.02, Test_accy 78.64
2024-10-25 16:33:46,842 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.949,  Train_accy 96.13, Test_accy 78.07
2024-10-25 16:33:56,452 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.950,  Train_accy 96.42, Test_accy 77.88
2024-10-25 16:34:06,240 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.951,  Train_accy 96.38, Test_accy 79.36
2024-10-25 16:34:16,054 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.949,  Train_accy 96.33, Test_accy 79.24
2024-10-25 16:34:25,610 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.947,  Train_accy 96.09, Test_accy 79.07
2024-10-25 16:34:35,457 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.949,  Train_accy 96.60, Test_accy 79.14
2024-10-25 16:34:45,084 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.950,  Train_accy 96.40, Test_accy 79.05
2024-10-25 16:34:54,651 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.947,  Train_accy 96.71, Test_accy 78.55
2024-10-25 16:35:04,718 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.946,  Train_accy 96.53, Test_accy 79.10
2024-10-25 16:35:14,225 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.948,  Train_accy 96.22, Test_accy 79.10
2024-10-25 16:35:23,749 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.949,  Train_accy 96.60, Test_accy 79.38
2024-10-25 16:35:33,003 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.948,  Train_accy 96.47, Test_accy 78.79
2024-10-25 16:35:40,033 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.947,  Train_accy 96.71
2024-10-25 16:35:40,034 [pod_foster.py] => do not weight align student!
2024-10-25 16:35:40,767 [pod_foster.py] => darknet eval: 
2024-10-25 16:35:40,768 [pod_foster.py] => CNN top1 curve: 79.07
2024-10-25 16:35:40,768 [pod_foster.py] => CNN top5 curve: 98.71
2024-10-25 16:35:40,769 [pod_foster.py] => All params after compression: 3851086
2024-10-25 16:35:40,769 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 16:35:41,983 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 16:35:45,536 [trainer.py] => All params: 7701139
2024-10-25 16:35:47,813 [pod_foster.py] => Exemplar size: 497
2024-10-25 16:35:47,813 [trainer.py] => CNN: {'total': 80.71, '00-04': 78.8, '05-06': 85.5, 'old': 78.8, 'new': 85.5}
2024-10-25 16:35:47,814 [trainer.py] => NME: {'total': 73.6, '00-04': 78.43, '05-06': 61.5, 'old': 78.43, 'new': 61.5}
2024-10-25 16:35:47,814 [trainer.py] => CNN top1 curve: [89.93, 80.71]
2024-10-25 16:35:47,814 [trainer.py] => CNN top5 curve: [100.0, 98.79]
2024-10-25 16:35:47,814 [trainer.py] => NME top1 curve: [90.0, 73.6]
2024-10-25 16:35:47,814 [trainer.py] => NME top5 curve: [100.0, 98.93]

2024-10-25 16:35:47,814 [trainer.py] => Average Accuracy (CNN): 85.32
2024-10-25 16:35:47,814 [trainer.py] => Average Accuracy (NME): 81.8
2024-10-25 16:35:47,815 [trainer.py] => All params: 7701139
2024-10-25 16:35:47,816 [trainer.py] => Trainable params: 3854670
2024-10-25 16:35:47,876 [pod_foster.py] => Learning on 7-9
2024-10-25 16:35:47,878 [pod_foster.py] => All params: 7705241
2024-10-25 16:35:47,879 [pod_foster.py] => Trainable params: 3857746
2024-10-25 16:35:47,924 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 16:35:47,935 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 16:35:51,081 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.952, Loss_clf 0.710, Loss_fe 0.680, Loss_pod 0.327, Loss_flat 0.236, Train_accy 85.32, Test_accy 62.22
2024-10-25 16:36:02,022 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.266, Loss_clf 0.013, Loss_fe 0.035, Loss_pod 0.145, Loss_flat 0.074, Train_accy 99.93, Test_accy 61.20
2024-10-25 16:36:13,322 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.155, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.097, Loss_flat 0.042, Train_accy 100.00, Test_accy 63.91
2024-10-25 16:36:24,523 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.142, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.093, Loss_flat 0.038, Train_accy 100.00, Test_accy 65.33
2024-10-25 16:36:35,317 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.125, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.084, Loss_flat 0.031, Train_accy 100.00, Test_accy 66.37
2024-10-25 16:36:46,730 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 1.069, Loss_clf 0.141, Loss_fe 0.379, Loss_pod 0.343, Loss_flat 0.206, Train_accy 95.37, Test_accy 45.76
2024-10-25 16:36:57,906 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.348, Loss_clf 0.017, Loss_fe 0.044, Loss_pod 0.191, Loss_flat 0.096, Train_accy 99.73, Test_accy 69.87
2024-10-25 16:37:08,948 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.174, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.115, Loss_flat 0.048, Train_accy 100.00, Test_accy 65.96
2024-10-25 16:37:20,106 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.151, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.098, Loss_flat 0.044, Train_accy 100.00, Test_accy 66.33
2024-10-25 16:37:31,330 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.133, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.089, Loss_flat 0.036, Train_accy 100.00, Test_accy 67.91
2024-10-25 16:37:42,457 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.080, Loss_flat 0.031, Train_accy 100.00, Test_accy 64.50
2024-10-25 16:37:53,846 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.120, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.080, Loss_flat 0.029, Train_accy 100.00, Test_accy 49.46
2024-10-25 16:38:05,054 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.115, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.078, Loss_flat 0.029, Train_accy 100.00, Test_accy 64.91
2024-10-25 16:38:16,439 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.169, Loss_clf 0.008, Loss_fe 0.013, Loss_pod 0.101, Loss_flat 0.046, Train_accy 99.96, Test_accy 72.48
2024-10-25 16:38:27,825 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.121, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.078, Loss_flat 0.033, Train_accy 99.98, Test_accy 65.91
2024-10-25 16:38:39,029 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.102, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.066, Loss_flat 0.027, Train_accy 100.00, Test_accy 65.15
2024-10-25 16:38:50,257 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.153, Loss_clf 0.009, Loss_fe 0.013, Loss_pod 0.089, Loss_flat 0.042, Train_accy 99.89, Test_accy 63.04
2024-10-25 16:39:01,437 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.099, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.065, Loss_flat 0.026, Train_accy 100.00, Test_accy 66.83
2024-10-25 16:39:12,712 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.096, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.063, Loss_flat 0.025, Train_accy 100.00, Test_accy 64.63
2024-10-25 16:39:24,093 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.086, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.055, Loss_flat 0.024, Train_accy 100.00, Test_accy 63.89
2024-10-25 16:39:35,409 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.090, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.058, Loss_flat 0.024, Train_accy 100.00, Test_accy 64.54
2024-10-25 16:39:47,020 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.090, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.055, Loss_flat 0.027, Train_accy 100.00, Test_accy 64.70
2024-10-25 16:39:58,418 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.086, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.053, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.83
2024-10-25 16:40:09,658 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.086, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.052, Loss_flat 0.024, Train_accy 100.00, Test_accy 63.94
2024-10-25 16:40:20,819 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.076, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.046, Loss_flat 0.023, Train_accy 100.00, Test_accy 64.48
2024-10-25 16:40:32,180 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.075, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.045, Loss_flat 0.023, Train_accy 100.00, Test_accy 65.15
2024-10-25 16:40:43,662 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.072, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.042, Loss_flat 0.023, Train_accy 100.00, Test_accy 65.61
2024-10-25 16:40:55,302 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.069, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.040, Loss_flat 0.022, Train_accy 100.00, Test_accy 66.67
2024-10-25 16:41:06,668 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.067, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.039, Loss_flat 0.022, Train_accy 100.00, Test_accy 66.63
2024-10-25 16:41:17,740 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.072, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.040, Loss_flat 0.023, Train_accy 100.00, Test_accy 62.61
2024-10-25 16:41:25,766 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.066, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.037, Loss_flat 0.022, Train_accy 100.00
2024-10-25 16:41:25,769 [inc_net.py] => align weights, gamma = 0.5197262763977051 
2024-10-25 16:41:25,770 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 16:41:28,510 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.553,  Train_accy 74.65, Test_accy 60.63
2024-10-25 16:41:38,401 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.278,  Train_accy 97.29, Test_accy 72.61
2024-10-25 16:41:48,383 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.280,  Train_accy 97.40, Test_accy 72.57
2024-10-25 16:41:58,234 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.275,  Train_accy 97.73, Test_accy 74.19
2024-10-25 16:42:07,876 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.276,  Train_accy 97.55, Test_accy 74.54
2024-10-25 16:42:17,353 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.274,  Train_accy 98.04, Test_accy 75.20
2024-10-25 16:42:27,126 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.271,  Train_accy 98.09, Test_accy 75.70
2024-10-25 16:42:37,171 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.272,  Train_accy 97.71, Test_accy 76.07
2024-10-25 16:42:47,295 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.270,  Train_accy 98.04, Test_accy 75.98
2024-10-25 16:42:57,198 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.271,  Train_accy 98.31, Test_accy 75.52
2024-10-25 16:43:06,861 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.272,  Train_accy 98.02, Test_accy 75.28
2024-10-25 16:43:16,718 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.271,  Train_accy 98.02, Test_accy 75.94
2024-10-25 16:43:26,963 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.271,  Train_accy 98.15, Test_accy 75.65
2024-10-25 16:43:37,058 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.268,  Train_accy 98.18, Test_accy 76.30
2024-10-25 16:43:47,006 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.270,  Train_accy 98.33, Test_accy 76.65
2024-10-25 16:43:56,932 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.271,  Train_accy 98.22, Test_accy 76.31
2024-10-25 16:44:06,718 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.272,  Train_accy 98.24, Test_accy 76.43
2024-10-25 16:44:16,500 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.265,  Train_accy 98.31, Test_accy 75.69
2024-10-25 16:44:26,478 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.269,  Train_accy 98.27, Test_accy 76.44
2024-10-25 16:44:36,504 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.264,  Train_accy 98.44, Test_accy 76.41
2024-10-25 16:44:46,323 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.267,  Train_accy 98.33, Test_accy 76.37
2024-10-25 16:44:55,866 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.268,  Train_accy 98.40, Test_accy 76.65
2024-10-25 16:45:05,058 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.269,  Train_accy 98.22, Test_accy 76.80
2024-10-25 16:45:14,144 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.270,  Train_accy 98.42, Test_accy 76.15
2024-10-25 16:45:20,836 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.268,  Train_accy 98.42
2024-10-25 16:45:20,836 [pod_foster.py] => do not weight align student!
2024-10-25 16:45:21,549 [pod_foster.py] => darknet eval: 
2024-10-25 16:45:21,550 [pod_foster.py] => CNN top1 curve: 76.96
2024-10-25 16:45:21,550 [pod_foster.py] => CNN top5 curve: 96.69
2024-10-25 16:45:21,551 [pod_foster.py] => All params after compression: 3853138
2024-10-25 16:45:21,552 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 16:45:23,188 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 16:45:26,370 [trainer.py] => All params: 7705241
2024-10-25 16:45:28,710 [pod_foster.py] => Exemplar size: 495
2024-10-25 16:45:28,710 [trainer.py] => CNN: {'total': 75.28, '00-04': 63.83, '05-06': 87.58, '07-08': 91.58, 'old': 70.62, 'new': 91.58}
2024-10-25 16:45:28,710 [trainer.py] => NME: {'total': 71.59, '00-04': 66.73, '05-06': 74.67, '07-08': 80.67, 'old': 69.0, 'new': 80.67}
2024-10-25 16:45:28,710 [trainer.py] => CNN top1 curve: [89.93, 80.71, 75.28]
2024-10-25 16:45:28,710 [trainer.py] => CNN top5 curve: [100.0, 98.79, 96.26]
2024-10-25 16:45:28,710 [trainer.py] => NME top1 curve: [90.0, 73.6, 71.59]
2024-10-25 16:45:28,710 [trainer.py] => NME top5 curve: [100.0, 98.93, 96.33]

2024-10-25 16:45:28,711 [trainer.py] => Average Accuracy (CNN): 81.97333333333333
2024-10-25 16:45:28,711 [trainer.py] => Average Accuracy (NME): 78.39666666666666
2024-10-25 16:45:28,711 [trainer.py] => Forgetting (CNN): 13.050000000000004
