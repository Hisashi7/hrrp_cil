2024-10-25 16:48:21,296 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 16:48:21,296 [trainer.py] => prefix: cil
2024-10-25 16:48:21,296 [trainer.py] => dataset: hrrp9
2024-10-25 16:48:21,296 [trainer.py] => memory_size: 500
2024-10-25 16:48:21,296 [trainer.py] => memory_per_class: 20
2024-10-25 16:48:21,296 [trainer.py] => fixed_memory: False
2024-10-25 16:48:21,296 [trainer.py] => shuffle: True
2024-10-25 16:48:21,296 [trainer.py] => init_cls: 5
2024-10-25 16:48:21,296 [trainer.py] => increment: 2
2024-10-25 16:48:21,296 [trainer.py] => model_name: POD_foster
2024-10-25 16:48:21,296 [trainer.py] => convnet_type: resnet18
2024-10-25 16:48:21,296 [trainer.py] => init_train: False
2024-10-25 16:48:21,297 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 16:48:21,297 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 16:48:21,297 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 16:48:21,297 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 16:48:21,297 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-25 16:48:21,297 [trainer.py] => seed: 1993
2024-10-25 16:48:21,297 [trainer.py] => beta1: 0.96
2024-10-25 16:48:21,297 [trainer.py] => beta2: 0.97
2024-10-25 16:48:21,297 [trainer.py] => oofc: ft
2024-10-25 16:48:21,297 [trainer.py] => is_teacher_wa: True
2024-10-25 16:48:21,297 [trainer.py] => is_student_wa: False
2024-10-25 16:48:21,297 [trainer.py] => lambda_okd: 0
2024-10-25 16:48:21,297 [trainer.py] => wa_value: 1
2024-10-25 16:48:21,297 [trainer.py] => init_epochs: 0
2024-10-25 16:48:21,297 [trainer.py] => init_lr: 0.1
2024-10-25 16:48:21,297 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 16:48:21,297 [trainer.py] => boosting_epochs: 150
2024-10-25 16:48:21,297 [trainer.py] => compression_epochs: 120
2024-10-25 16:48:21,297 [trainer.py] => lr: 0.1
2024-10-25 16:48:21,298 [trainer.py] => batch_size: 128
2024-10-25 16:48:21,298 [trainer.py] => weight_decay: 0.0005
2024-10-25 16:48:21,298 [trainer.py] => num_workers: 8
2024-10-25 16:48:21,298 [trainer.py] => momentum: 0.9
2024-10-25 16:48:21,298 [trainer.py] => T: 2
2024-10-25 16:48:21,298 [trainer.py] => lambda_c_base: 1.0
2024-10-25 16:48:21,298 [trainer.py] => lambda_f_base: 1.0
2024-10-25 16:48:21,298 [trainer.py] => POD: c+w
2024-10-25 16:48:22,043 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 16:48:22,091 [trainer.py] => All params: 0
2024-10-25 16:48:22,092 [trainer.py] => Trainable params: 0
2024-10-25 16:48:23,259 [pod_foster.py] => Learning on 0-5
2024-10-25 16:48:23,260 [pod_foster.py] => All params: 3849034
2024-10-25 16:48:23,260 [pod_foster.py] => Trainable params: 3849034
2024-10-25 16:48:23,331 [pod_foster.py] => Adaptive factor: 0
2024-10-25 16:48:23,538 [pod_foster.py] => init_train?---False
2024-10-25 16:48:24,575 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 16:48:24,576 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 16:48:31,055 [trainer.py] => All params: 3849034
2024-10-25 16:48:32,319 [pod_foster.py] => Exemplar size: 500
2024-10-25 16:48:32,319 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 16:48:32,319 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 16:48:32,319 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 16:48:32,319 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 16:48:32,319 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 16:48:32,319 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 16:48:32,320 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 16:48:32,320 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 16:48:32,320 [trainer.py] => All params: 3849034
2024-10-25 16:48:32,320 [trainer.py] => Trainable params: 3849034
2024-10-25 16:48:32,366 [pod_foster.py] => Learning on 5-7
2024-10-25 16:48:32,367 [pod_foster.py] => All params: 7701139
2024-10-25 16:48:32,368 [pod_foster.py] => Trainable params: 3854670
2024-10-25 16:48:32,396 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 16:48:32,405 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 16:48:35,964 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.927, Loss_clf 0.707, Loss_fe 0.650, Loss_pod 0.352, Loss_flat 0.217, Train_accy 83.73, Test_accy 52.21
2024-10-25 16:48:46,691 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.336, Loss_clf 0.022, Loss_fe 0.038, Loss_pod 0.190, Loss_flat 0.087, Train_accy 99.58, Test_accy 69.05
2024-10-25 16:48:57,628 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.196, Loss_clf 0.008, Loss_fe 0.008, Loss_pod 0.132, Loss_flat 0.048, Train_accy 100.00, Test_accy 69.67
2024-10-25 16:49:08,648 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.168, Loss_clf 0.007, Loss_fe 0.007, Loss_pod 0.117, Loss_flat 0.038, Train_accy 100.00, Test_accy 69.31
2024-10-25 16:49:19,074 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.172, Loss_clf 0.009, Loss_fe 0.009, Loss_pod 0.116, Loss_flat 0.037, Train_accy 99.98, Test_accy 60.86
2024-10-25 16:49:29,703 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.458, Loss_clf 0.033, Loss_fe 0.062, Loss_pod 0.247, Loss_flat 0.115, Train_accy 99.40, Test_accy 71.64
2024-10-25 16:49:40,071 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.232, Loss_clf 0.010, Loss_fe 0.009, Loss_pod 0.155, Loss_flat 0.058, Train_accy 99.96, Test_accy 68.33
2024-10-25 16:49:50,262 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.168, Loss_clf 0.006, Loss_fe 0.005, Loss_pod 0.121, Loss_flat 0.036, Train_accy 100.00, Test_accy 66.10
2024-10-25 16:50:00,688 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.150, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.108, Loss_flat 0.032, Train_accy 100.00, Test_accy 67.26
2024-10-25 16:50:11,103 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.139, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.102, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.38
2024-10-25 16:50:21,445 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.134, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.099, Loss_flat 0.026, Train_accy 100.00, Test_accy 66.64
2024-10-25 16:50:31,842 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.126, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.093, Loss_flat 0.025, Train_accy 100.00, Test_accy 68.64
2024-10-25 16:50:41,984 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.135, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.097, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.60
2024-10-25 16:50:52,263 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.520, Loss_clf 0.052, Loss_fe 0.092, Loss_pod 0.265, Loss_flat 0.111, Train_accy 98.31, Test_accy 69.62
2024-10-25 16:51:02,433 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.164, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.118, Loss_flat 0.035, Train_accy 100.00, Test_accy 67.86
2024-10-25 16:51:12,718 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.136, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.099, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.57
2024-10-25 16:51:23,038 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.127, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.093, Loss_flat 0.026, Train_accy 100.00, Test_accy 67.48
2024-10-25 16:51:33,192 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.121, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.087, Loss_flat 0.025, Train_accy 100.00, Test_accy 66.33
2024-10-25 16:51:43,430 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.115, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.083, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.05
2024-10-25 16:51:53,857 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.114, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.080, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.93
2024-10-25 16:52:03,946 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.104, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.074, Loss_flat 0.022, Train_accy 100.00, Test_accy 66.81
2024-10-25 16:52:14,290 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.103, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.073, Loss_flat 0.022, Train_accy 100.00, Test_accy 67.31
2024-10-25 16:52:24,645 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.102, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.071, Loss_flat 0.022, Train_accy 100.00, Test_accy 66.69
2024-10-25 16:52:34,849 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.099, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.069, Loss_flat 0.021, Train_accy 100.00, Test_accy 67.69
2024-10-25 16:52:45,324 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.093, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.064, Loss_flat 0.021, Train_accy 100.00, Test_accy 67.00
2024-10-25 16:52:55,391 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.093, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.064, Loss_flat 0.021, Train_accy 100.00, Test_accy 66.64
2024-10-25 16:53:05,576 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.092, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.063, Loss_flat 0.021, Train_accy 100.00, Test_accy 66.74
2024-10-25 16:53:15,890 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.087, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.059, Loss_flat 0.020, Train_accy 100.00, Test_accy 66.93
2024-10-25 16:53:26,036 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.087, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.058, Loss_flat 0.021, Train_accy 100.00, Test_accy 67.12
2024-10-25 16:53:36,585 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.090, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.062, Loss_flat 0.020, Train_accy 100.00, Test_accy 67.60
2024-10-25 16:53:44,131 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.088, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.059, Loss_flat 0.021, Train_accy 100.00
2024-10-25 16:53:44,132 [inc_net.py] => align weights, gamma = 0.5193051099777222 
2024-10-25 16:53:44,134 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 16:53:46,540 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.259,  Train_accy 68.80, Test_accy 67.26
2024-10-25 16:53:55,558 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.974,  Train_accy 93.00, Test_accy 76.81
2024-10-25 16:54:04,329 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.968,  Train_accy 94.09, Test_accy 76.43
2024-10-25 16:54:13,157 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.960,  Train_accy 94.29, Test_accy 77.40
2024-10-25 16:54:22,160 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.963,  Train_accy 94.56, Test_accy 77.79
2024-10-25 16:54:31,097 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.959,  Train_accy 94.69, Test_accy 78.52
2024-10-25 16:54:40,130 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.962,  Train_accy 94.58, Test_accy 77.95
2024-10-25 16:54:48,911 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.960,  Train_accy 94.51, Test_accy 77.60
2024-10-25 16:54:57,771 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.957,  Train_accy 94.69, Test_accy 78.19
2024-10-25 16:55:06,676 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.955,  Train_accy 94.71, Test_accy 78.45
2024-10-25 16:55:15,235 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.957,  Train_accy 94.71, Test_accy 78.00
2024-10-25 16:55:24,284 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.955,  Train_accy 94.73, Test_accy 78.57
2024-10-25 16:55:33,256 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.956,  Train_accy 94.84, Test_accy 78.19
2024-10-25 16:55:43,509 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.956,  Train_accy 95.40, Test_accy 77.45
2024-10-25 16:55:52,950 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.957,  Train_accy 94.80, Test_accy 78.88
2024-10-25 16:56:02,012 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.956,  Train_accy 94.96, Test_accy 79.05
2024-10-25 16:56:11,181 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.953,  Train_accy 94.87, Test_accy 79.05
2024-10-25 16:56:20,222 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.955,  Train_accy 95.11, Test_accy 78.79
2024-10-25 16:56:29,318 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.956,  Train_accy 95.07, Test_accy 78.98
2024-10-25 16:56:38,945 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.954,  Train_accy 95.49, Test_accy 78.36
2024-10-25 16:56:48,223 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.952,  Train_accy 94.98, Test_accy 78.81
2024-10-25 16:56:57,341 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.954,  Train_accy 94.80, Test_accy 78.76
2024-10-25 16:57:06,544 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.956,  Train_accy 95.04, Test_accy 79.26
2024-10-25 16:57:15,951 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.954,  Train_accy 95.40, Test_accy 78.40
2024-10-25 16:57:22,582 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.953,  Train_accy 95.24
2024-10-25 16:57:22,583 [pod_foster.py] => do not weight align student!
2024-10-25 16:57:23,292 [pod_foster.py] => darknet eval: 
2024-10-25 16:57:23,293 [pod_foster.py] => CNN top1 curve: 78.83
2024-10-25 16:57:23,293 [pod_foster.py] => CNN top5 curve: 98.67
2024-10-25 16:57:23,294 [pod_foster.py] => All params after compression: 3851086
2024-10-25 16:57:23,294 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 16:57:24,700 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 16:57:28,460 [trainer.py] => All params: 7701139
2024-10-25 16:57:30,398 [pod_foster.py] => Exemplar size: 497
2024-10-25 16:57:30,399 [trainer.py] => CNN: {'total': 79.69, '00-04': 78.23, '05-06': 83.33, 'old': 78.23, 'new': 83.33}
2024-10-25 16:57:30,399 [trainer.py] => NME: {'total': 72.86, '00-04': 78.37, '05-06': 59.08, 'old': 78.37, 'new': 59.08}
2024-10-25 16:57:30,399 [trainer.py] => CNN top1 curve: [89.93, 79.69]
2024-10-25 16:57:30,399 [trainer.py] => CNN top5 curve: [100.0, 98.69]
2024-10-25 16:57:30,399 [trainer.py] => NME top1 curve: [90.0, 72.86]
2024-10-25 16:57:30,399 [trainer.py] => NME top5 curve: [100.0, 98.93]

2024-10-25 16:57:30,399 [trainer.py] => Average Accuracy (CNN): 84.81
2024-10-25 16:57:30,399 [trainer.py] => Average Accuracy (NME): 81.43
2024-10-25 16:57:30,400 [trainer.py] => All params: 7701139
2024-10-25 16:57:30,401 [trainer.py] => Trainable params: 3854670
2024-10-25 16:57:30,451 [pod_foster.py] => Learning on 7-9
2024-10-25 16:57:30,452 [pod_foster.py] => All params: 7705241
2024-10-25 16:57:30,453 [pod_foster.py] => Trainable params: 3857746
2024-10-25 16:57:30,502 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 16:57:30,513 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 16:57:34,088 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.002, Loss_clf 0.699, Loss_fe 0.657, Loss_pod 0.426, Loss_flat 0.220, Train_accy 85.59, Test_accy 56.52
2024-10-25 16:57:45,452 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.326, Loss_clf 0.013, Loss_fe 0.039, Loss_pod 0.201, Loss_flat 0.072, Train_accy 99.93, Test_accy 64.06
2024-10-25 16:57:56,451 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.191, Loss_clf 0.007, Loss_fe 0.010, Loss_pod 0.136, Loss_flat 0.038, Train_accy 100.00, Test_accy 63.31
2024-10-25 16:58:07,527 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.189, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.139, Loss_flat 0.037, Train_accy 100.00, Test_accy 65.00
2024-10-25 16:58:18,040 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.162, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.123, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.20
2024-10-25 16:58:28,944 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.160, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.120, Loss_flat 0.029, Train_accy 100.00, Test_accy 63.69
2024-10-25 16:58:39,859 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.962, Loss_clf 0.104, Loss_fe 0.267, Loss_pod 0.429, Loss_flat 0.163, Train_accy 96.75, Test_accy 57.35
2024-10-25 16:58:50,554 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.219, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.163, Loss_flat 0.044, Train_accy 100.00, Test_accy 67.17
2024-10-25 16:59:01,354 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.199, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.145, Loss_flat 0.043, Train_accy 100.00, Test_accy 65.28
2024-10-25 16:59:11,992 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.168, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.127, Loss_flat 0.032, Train_accy 100.00, Test_accy 68.81
2024-10-25 16:59:22,864 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.154, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.116, Loss_flat 0.028, Train_accy 100.00, Test_accy 65.78
2024-10-25 16:59:33,707 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.149, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.113, Loss_flat 0.026, Train_accy 100.00, Test_accy 61.44
2024-10-25 16:59:44,781 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.149, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.112, Loss_flat 0.028, Train_accy 100.00, Test_accy 64.41
2024-10-25 16:59:55,441 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.171, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.122, Loss_flat 0.035, Train_accy 99.98, Test_accy 70.44
2024-10-25 17:00:06,338 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.155, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.113, Loss_flat 0.029, Train_accy 99.96, Test_accy 66.00
2024-10-25 17:00:17,138 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.131, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.098, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.93
2024-10-25 17:00:27,793 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.156, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.111, Loss_flat 0.031, Train_accy 99.96, Test_accy 65.80
2024-10-25 17:00:38,367 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.127, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.097, Loss_flat 0.022, Train_accy 100.00, Test_accy 63.59
2024-10-25 17:00:49,051 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.122, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.092, Loss_flat 0.022, Train_accy 100.00, Test_accy 64.81
2024-10-25 17:00:59,861 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.115, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.086, Loss_flat 0.021, Train_accy 100.00, Test_accy 65.43
2024-10-25 17:01:10,439 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.090, Loss_flat 0.022, Train_accy 100.00, Test_accy 63.91
2024-10-25 17:01:21,168 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.130, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.092, Loss_flat 0.027, Train_accy 100.00, Test_accy 65.06
2024-10-25 17:01:31,906 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.115, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.083, Loss_flat 0.023, Train_accy 100.00, Test_accy 65.26
2024-10-25 17:01:43,163 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.115, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.082, Loss_flat 0.022, Train_accy 100.00, Test_accy 63.61
2024-10-25 17:01:53,913 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.103, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.075, Loss_flat 0.020, Train_accy 100.00, Test_accy 63.61
2024-10-25 17:02:04,588 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.104, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.076, Loss_flat 0.021, Train_accy 100.00, Test_accy 65.65
2024-10-25 17:02:15,417 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.098, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.069, Loss_flat 0.021, Train_accy 100.00, Test_accy 65.39
2024-10-25 17:02:26,290 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.095, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.068, Loss_flat 0.020, Train_accy 100.00, Test_accy 66.22
2024-10-25 17:02:37,114 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.092, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.065, Loss_flat 0.019, Train_accy 100.00, Test_accy 65.89
2024-10-25 17:02:47,696 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.097, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.067, Loss_flat 0.021, Train_accy 100.00, Test_accy 62.37
2024-10-25 17:02:55,636 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.090, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.063, Loss_flat 0.019, Train_accy 100.00
2024-10-25 17:02:55,638 [inc_net.py] => align weights, gamma = 0.5145048499107361 
2024-10-25 17:02:55,639 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 17:02:58,339 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.557,  Train_accy 73.98, Test_accy 62.02
2024-10-25 17:03:07,755 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.294,  Train_accy 95.77, Test_accy 72.02
2024-10-25 17:03:17,095 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.295,  Train_accy 96.53, Test_accy 72.74
2024-10-25 17:03:26,505 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.290,  Train_accy 96.71, Test_accy 73.91
2024-10-25 17:03:36,086 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.292,  Train_accy 96.22, Test_accy 73.85
2024-10-25 17:03:45,473 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.290,  Train_accy 96.91, Test_accy 75.02
2024-10-25 17:03:54,932 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.286,  Train_accy 96.80, Test_accy 75.33
2024-10-25 17:04:04,293 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.287,  Train_accy 96.82, Test_accy 75.89
2024-10-25 17:04:13,804 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.285,  Train_accy 97.04, Test_accy 76.19
2024-10-25 17:04:23,056 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.286,  Train_accy 97.24, Test_accy 75.52
2024-10-25 17:04:32,780 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.286,  Train_accy 96.95, Test_accy 75.35
2024-10-25 17:04:42,278 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.286,  Train_accy 96.82, Test_accy 75.94
2024-10-25 17:04:51,404 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.286,  Train_accy 97.13, Test_accy 75.65
2024-10-25 17:05:00,883 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.283,  Train_accy 97.13, Test_accy 75.91
2024-10-25 17:05:10,261 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.286,  Train_accy 97.35, Test_accy 76.30
2024-10-25 17:05:19,767 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.286,  Train_accy 97.06, Test_accy 76.20
2024-10-25 17:05:29,358 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.288,  Train_accy 97.29, Test_accy 75.98
2024-10-25 17:05:38,818 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.280,  Train_accy 97.24, Test_accy 75.81
2024-10-25 17:05:48,044 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.284,  Train_accy 97.20, Test_accy 76.15
2024-10-25 17:05:57,284 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.280,  Train_accy 97.33, Test_accy 76.22
2024-10-25 17:06:06,554 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.281,  Train_accy 97.29, Test_accy 76.39
2024-10-25 17:06:15,907 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.284,  Train_accy 97.24, Test_accy 76.46
2024-10-25 17:06:25,349 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.284,  Train_accy 97.24, Test_accy 76.37
2024-10-25 17:06:34,493 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.286,  Train_accy 97.18, Test_accy 75.52
2024-10-25 17:06:41,542 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.283,  Train_accy 97.38
2024-10-25 17:06:41,542 [pod_foster.py] => do not weight align student!
2024-10-25 17:06:42,384 [pod_foster.py] => darknet eval: 
2024-10-25 17:06:42,384 [pod_foster.py] => CNN top1 curve: 76.63
2024-10-25 17:06:42,385 [pod_foster.py] => CNN top5 curve: 96.28
2024-10-25 17:06:42,386 [pod_foster.py] => All params after compression: 3853138
2024-10-25 17:06:42,386 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 17:06:44,092 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 17:06:47,491 [trainer.py] => All params: 7705241
2024-10-25 17:06:49,707 [pod_foster.py] => Exemplar size: 495
2024-10-25 17:06:49,707 [trainer.py] => CNN: {'total': 75.7, '00-04': 65.07, '05-06': 88.08, '07-08': 89.92, 'old': 71.64, 'new': 89.92}
2024-10-25 17:06:49,707 [trainer.py] => NME: {'total': 71.61, '00-04': 67.93, '05-06': 73.92, '07-08': 78.5, 'old': 69.64, 'new': 78.5}
2024-10-25 17:06:49,707 [trainer.py] => CNN top1 curve: [89.93, 79.69, 75.7]
2024-10-25 17:06:49,707 [trainer.py] => CNN top5 curve: [100.0, 98.69, 96.3]
2024-10-25 17:06:49,708 [trainer.py] => NME top1 curve: [90.0, 72.86, 71.61]
2024-10-25 17:06:49,708 [trainer.py] => NME top5 curve: [100.0, 98.93, 96.39]

2024-10-25 17:06:49,708 [trainer.py] => Average Accuracy (CNN): 81.77333333333333
2024-10-25 17:06:49,708 [trainer.py] => Average Accuracy (NME): 78.15666666666668
2024-10-25 17:06:49,708 [trainer.py] => Forgetting (CNN): 12.430000000000007
