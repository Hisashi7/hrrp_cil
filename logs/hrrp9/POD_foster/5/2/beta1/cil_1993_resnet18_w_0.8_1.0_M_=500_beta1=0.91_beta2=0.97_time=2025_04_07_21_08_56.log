2025-04-07 21:08:56,142 [trainer.py] => config: ./exps/POD_foster.json
2025-04-07 21:08:56,142 [trainer.py] => prefix: cil
2025-04-07 21:08:56,143 [trainer.py] => dataset: hrrp9
2025-04-07 21:08:56,143 [trainer.py] => memory_size: 500
2025-04-07 21:08:56,143 [trainer.py] => memory_per_class: 20
2025-04-07 21:08:56,144 [trainer.py] => fixed_memory: False
2025-04-07 21:08:56,144 [trainer.py] => shuffle: True
2025-04-07 21:08:56,144 [trainer.py] => init_cls: 5
2025-04-07 21:08:56,144 [trainer.py] => increment: 2
2025-04-07 21:08:56,145 [trainer.py] => model_name: POD_foster
2025-04-07 21:08:56,145 [trainer.py] => convnet_type: resnet18
2025-04-07 21:08:56,145 [trainer.py] => init_train: False
2025-04-07 21:08:56,146 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2025-04-07 21:08:56,146 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2025-04-07 21:08:56,146 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2025-04-07 21:08:56,147 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2025-04-07 21:08:56,147 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42871.pth
2025-04-07 21:08:56,147 [trainer.py] => fc_path2: checkpoints/init_train/fc_42871.pth
2025-04-07 21:08:56,148 [trainer.py] => device: [device(type='cuda', index=6)]
2025-04-07 21:08:56,148 [trainer.py] => seed: 1993
2025-04-07 21:08:56,149 [trainer.py] => beta1: 0.91
2025-04-07 21:08:56,149 [trainer.py] => beta2: 0.97
2025-04-07 21:08:56,149 [trainer.py] => oofc: ft
2025-04-07 21:08:56,150 [trainer.py] => is_teacher_wa: True
2025-04-07 21:08:56,150 [trainer.py] => is_student_wa: False
2025-04-07 21:08:56,150 [trainer.py] => is_teacher_la: True
2025-04-07 21:08:56,151 [trainer.py] => is_student_la: True
2025-04-07 21:08:56,151 [trainer.py] => lambda_okd: 0
2025-04-07 21:08:56,151 [trainer.py] => wa_value: 1
2025-04-07 21:08:56,151 [trainer.py] => init_epochs: 0
2025-04-07 21:08:56,152 [trainer.py] => init_lr: 0.1
2025-04-07 21:08:56,152 [trainer.py] => init_weight_decay: 0.0005
2025-04-07 21:08:56,153 [trainer.py] => boosting_epochs: 150
2025-04-07 21:08:56,153 [trainer.py] => compression_epochs: 120
2025-04-07 21:08:56,154 [trainer.py] => lr: 0.1
2025-04-07 21:08:56,154 [trainer.py] => batch_size: 128
2025-04-07 21:08:56,154 [trainer.py] => weight_decay: 0.0005
2025-04-07 21:08:56,155 [trainer.py] => num_workers: 8
2025-04-07 21:08:56,155 [trainer.py] => momentum: 0.9
2025-04-07 21:08:56,155 [trainer.py] => T: 2
2025-04-07 21:08:56,156 [trainer.py] => lambda_c_base: 0.8
2025-04-07 21:08:56,156 [trainer.py] => lambda_f_base: 1.0
2025-04-07 21:08:56,156 [trainer.py] => POD: w
2025-04-07 21:08:56,677 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2025-04-07 21:08:56,710 [trainer.py] => All params: 0
2025-04-07 21:08:56,711 [trainer.py] => Trainable params: 0
2025-04-07 21:08:58,981 [pod_foster.py] => Learning on 0-5
2025-04-07 21:08:58,981 [pod_foster.py] => All params: 3849034
2025-04-07 21:08:58,982 [pod_foster.py] => Trainable params: 3849034
2025-04-07 21:08:59,030 [pod_foster.py] => Adaptive factor: 0
2025-04-07 21:09:00,833 [pod_foster.py] => init_train?---False
2025-04-07 21:09:04,250 [base.py] => Reducing exemplars...(100 per classes)
2025-04-07 21:09:04,251 [base.py] => Constructing exemplars...(100 per classes)
2025-04-07 21:09:11,564 [trainer.py] => task:0 training time:14.85s
2025-04-07 21:09:11,565 [trainer.py] => All params: 3849034
2025-04-07 21:09:13,278 [pod_foster.py] => Exemplar size: 500
2025-04-07 21:09:13,278 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2025-04-07 21:09:13,278 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2025-04-07 21:09:13,278 [trainer.py] => CNN top1 curve: [89.93]
2025-04-07 21:09:13,278 [trainer.py] => CNN top5 curve: [100.0]
2025-04-07 21:09:13,278 [trainer.py] => NME top1 curve: [90.0]
2025-04-07 21:09:13,279 [trainer.py] => NME top5 curve: [100.0]

2025-04-07 21:09:13,279 [trainer.py] => Average Accuracy (CNN): 89.93
2025-04-07 21:09:13,279 [trainer.py] => Average Accuracy (NME): 90.0
2025-04-07 21:09:13,279 [trainer.py] => All params: 3849034
2025-04-07 21:09:13,279 [trainer.py] => Trainable params: 3849034
2025-04-07 21:09:13,312 [pod_foster.py] => Learning on 5-7
2025-04-07 21:09:13,313 [pod_foster.py] => All params: 7701139
2025-04-07 21:09:13,313 [pod_foster.py] => Trainable params: 3854670
2025-04-07 21:09:13,327 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2025-04-07 21:09:13,346 [pod_foster.py] => per cls weights : [1.00002291 1.00002291 1.00002291 1.00002291 1.00002291 0.99994272
 0.99994272]
2025-04-07 21:09:17,688 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.178, Loss_clf 0.768, Loss_fe 0.000, Loss_pod 0.296, Loss_flat 0.114, Train_accy 81.71, Test_accy 64.76
2025-04-07 21:09:31,095 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.210, Loss_clf 0.022, Loss_fe 0.000, Loss_pod 0.156, Loss_flat 0.032, Train_accy 99.96, Test_accy 68.55
2025-04-07 21:09:44,736 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.202, Loss_clf 0.020, Loss_fe 0.000, Loss_pod 0.156, Loss_flat 0.026, Train_accy 99.78, Test_accy 67.67
2025-04-07 21:09:58,528 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.166, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.136, Loss_flat 0.020, Train_accy 100.00, Test_accy 67.26
2025-04-07 21:10:12,732 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.169, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.137, Loss_flat 0.019, Train_accy 100.00, Test_accy 68.24
2025-04-07 21:10:26,618 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.295, Loss_clf 0.031, Loss_fe 0.000, Loss_pod 0.213, Loss_flat 0.051, Train_accy 99.47, Test_accy 69.31
2025-04-07 21:10:40,295 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.181, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.145, Loss_flat 0.024, Train_accy 100.00, Test_accy 69.60
2025-04-07 21:10:53,396 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.153, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.127, Loss_flat 0.017, Train_accy 100.00, Test_accy 65.21
2025-04-07 21:11:06,909 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.142, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.119, Loss_flat 0.015, Train_accy 100.00, Test_accy 65.29
2025-04-07 21:11:19,880 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.143, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.120, Loss_flat 0.015, Train_accy 100.00, Test_accy 67.88
2025-04-07 21:11:33,141 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.144, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.122, Loss_flat 0.015, Train_accy 100.00, Test_accy 65.24
2025-04-07 21:11:47,171 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.134, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.113, Loss_flat 0.014, Train_accy 100.00, Test_accy 65.48
2025-04-07 21:12:01,304 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.158, Loss_clf 0.012, Loss_fe 0.000, Loss_pod 0.127, Loss_flat 0.019, Train_accy 99.89, Test_accy 63.31
2025-04-07 21:12:13,946 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.142, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.119, Loss_flat 0.015, Train_accy 100.00, Test_accy 66.64
2025-04-07 21:12:27,803 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.135, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.113, Loss_flat 0.014, Train_accy 100.00, Test_accy 66.81
2025-04-07 21:12:41,175 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.126, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.106, Loss_flat 0.013, Train_accy 100.00, Test_accy 64.98
2025-04-07 21:12:53,566 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.121, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.102, Loss_flat 0.012, Train_accy 100.00, Test_accy 68.19
2025-04-07 21:13:06,467 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.124, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.104, Loss_flat 0.013, Train_accy 100.00, Test_accy 65.31
2025-04-07 21:13:18,994 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.115, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.097, Loss_flat 0.012, Train_accy 100.00, Test_accy 65.10
2025-04-07 21:13:32,125 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.112, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.093, Loss_flat 0.012, Train_accy 100.00, Test_accy 67.38
2025-04-07 21:13:44,807 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.107, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.089, Loss_flat 0.011, Train_accy 100.00, Test_accy 66.57
2025-04-07 21:13:57,216 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.106, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.088, Loss_flat 0.011, Train_accy 100.00, Test_accy 66.81
2025-04-07 21:14:09,729 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.105, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.087, Loss_flat 0.011, Train_accy 100.00, Test_accy 64.90
2025-04-07 21:14:22,739 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.101, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.084, Loss_flat 0.010, Train_accy 100.00, Test_accy 66.69
2025-04-07 21:14:36,020 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.095, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.078, Loss_flat 0.010, Train_accy 100.00, Test_accy 65.50
2025-04-07 21:14:49,088 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.094, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.078, Loss_flat 0.010, Train_accy 100.00, Test_accy 65.02
2025-04-07 21:15:02,361 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.093, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.077, Loss_flat 0.010, Train_accy 100.00, Test_accy 64.52
2025-04-07 21:15:15,520 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.087, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.072, Loss_flat 0.009, Train_accy 100.00, Test_accy 65.98
2025-04-07 21:15:28,005 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.087, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.070, Loss_flat 0.010, Train_accy 100.00, Test_accy 65.33
2025-04-07 21:15:41,282 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.091, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.075, Loss_flat 0.009, Train_accy 100.00, Test_accy 66.19
2025-04-07 21:15:51,060 [pod_foster.py] => Task 1, time 2.32s, Epoch 150/150 => Loss 0.088, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.072, Loss_flat 0.010, Train_accy 100.00
2025-04-07 21:15:51,061 [pod_foster.py] => 100 epoches training time:268.01s
2025-04-07 21:15:51,061 [pod_foster.py] => Average training time of single epoch:2.40s
2025-04-07 21:15:51,063 [inc_net.py] => align weights, gamma = 0.4771954119205475 
2025-04-07 21:15:51,068 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2025-04-07 21:15:54,326 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.343,  Train_accy 70.31, Test_accy 62.48
2025-04-07 21:16:04,808 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.063,  Train_accy 96.89, Test_accy 75.45
2025-04-07 21:16:16,004 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.053,  Train_accy 97.49, Test_accy 74.67
2025-04-07 21:16:26,500 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.047,  Train_accy 97.47, Test_accy 78.36
2025-04-07 21:16:38,028 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.048,  Train_accy 97.67, Test_accy 78.74
2025-04-07 21:16:50,043 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.044,  Train_accy 97.89, Test_accy 77.31
2025-04-07 21:17:01,047 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.046,  Train_accy 97.78, Test_accy 78.55
2025-04-07 21:17:11,887 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.047,  Train_accy 98.09, Test_accy 78.05
2025-04-07 21:17:23,572 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.040,  Train_accy 98.16, Test_accy 78.17
2025-04-07 21:17:35,233 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.043,  Train_accy 98.24, Test_accy 77.48
2025-04-07 21:17:46,052 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.041,  Train_accy 98.20, Test_accy 78.67
2025-04-07 21:17:57,975 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.040,  Train_accy 98.18, Test_accy 79.05
2025-04-07 21:18:08,743 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.041,  Train_accy 97.96, Test_accy 78.19
2025-04-07 21:18:20,046 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.042,  Train_accy 98.18, Test_accy 77.45
2025-04-07 21:18:31,368 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.042,  Train_accy 98.16, Test_accy 79.14
2025-04-07 21:18:42,437 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.041,  Train_accy 98.13, Test_accy 78.29
2025-04-07 21:18:54,137 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.038,  Train_accy 98.33, Test_accy 78.60
2025-04-07 21:19:04,881 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.041,  Train_accy 98.42, Test_accy 78.10
2025-04-07 21:19:15,745 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.042,  Train_accy 98.53, Test_accy 78.86
2025-04-07 21:19:27,272 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.039,  Train_accy 98.36, Test_accy 77.79
2025-04-07 21:19:38,043 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.039,  Train_accy 98.31, Test_accy 78.76
2025-04-07 21:19:49,508 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.039,  Train_accy 98.33, Test_accy 78.64
2025-04-07 21:20:01,316 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.040,  Train_accy 98.40, Test_accy 79.00
2025-04-07 21:20:11,992 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.040,  Train_accy 98.36, Test_accy 78.55
2025-04-07 21:20:20,207 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.039,  Train_accy 98.33
2025-04-07 21:20:20,207 [pod_foster.py] => do not weight align student!
2025-04-07 21:20:21,323 [pod_foster.py] => darknet eval: 
2025-04-07 21:20:21,324 [pod_foster.py] => CNN top1 curve: 78.57
2025-04-07 21:20:21,324 [pod_foster.py] => CNN top5 curve: 98.76
2025-04-07 21:20:21,324 [pod_foster.py] => CNN: {'total': 78.57, '00-04': 74.07, '05-06': 89.83, 'old': 74.07, 'new': 89.83}
2025-04-07 21:20:21,325 [pod_foster.py] => All params after compression: 3851086
2025-04-07 21:20:21,325 [base.py] => Reducing exemplars...(71 per classes)
2025-04-07 21:20:23,118 [base.py] => Constructing exemplars...(71 per classes)
2025-04-07 21:20:26,760 [trainer.py] => task:1 training time:673.48s
2025-04-07 21:20:26,761 [trainer.py] => All params: 7701139
2025-04-07 21:20:29,370 [pod_foster.py] => Exemplar size: 497
2025-04-07 21:20:29,371 [trainer.py] => CNN: {'total': 79.71, '00-04': 77.2, '05-06': 86.0, 'old': 77.2, 'new': 86.0}
2025-04-07 21:20:29,371 [trainer.py] => NME: {'total': 74.24, '00-04': 75.03, '05-06': 72.25, 'old': 75.03, 'new': 72.25}
2025-04-07 21:20:29,371 [trainer.py] => CNN top1 curve: [89.93, 79.71]
2025-04-07 21:20:29,371 [trainer.py] => CNN top5 curve: [100.0, 98.86]
2025-04-07 21:20:29,371 [trainer.py] => NME top1 curve: [90.0, 74.24]
2025-04-07 21:20:29,371 [trainer.py] => NME top5 curve: [100.0, 99.1]

2025-04-07 21:20:29,371 [trainer.py] => Average Accuracy (CNN): 84.82
2025-04-07 21:20:29,371 [trainer.py] => Average Accuracy (NME): 82.12
2025-04-07 21:20:29,371 [trainer.py] => All params: 7701139
2025-04-07 21:20:29,372 [trainer.py] => Trainable params: 3854670
2025-04-07 21:20:29,418 [pod_foster.py] => Learning on 7-9
2025-04-07 21:20:29,419 [pod_foster.py] => All params: 7705241
2025-04-07 21:20:29,420 [pod_foster.py] => Trainable params: 3857746
2025-04-07 21:20:29,446 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2025-04-07 21:20:29,456 [pod_foster.py] => per cls weights : [1.00027469 1.00027469 1.00027469 1.00027469 1.00027469 1.00027469
 1.00027469 0.99903858 0.99903858]
2025-04-07 21:20:33,860 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.149, Loss_clf 0.615, Loss_fe 0.000, Loss_pod 0.394, Loss_flat 0.139, Train_accy 85.12, Test_accy 65.85
2025-04-07 21:20:45,938 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.233, Loss_clf 0.017, Loss_fe 0.000, Loss_pod 0.188, Loss_flat 0.028, Train_accy 99.98, Test_accy 62.69
2025-04-07 21:20:58,394 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.210, Loss_clf 0.014, Loss_fe 0.000, Loss_pod 0.173, Loss_flat 0.024, Train_accy 99.98, Test_accy 57.46
2025-04-07 21:21:10,655 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.207, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.173, Loss_flat 0.023, Train_accy 99.96, Test_accy 63.30
2025-04-07 21:21:23,902 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.189, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.162, Loss_flat 0.019, Train_accy 100.00, Test_accy 65.85
2025-04-07 21:21:36,678 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.187, Loss_clf 0.010, Loss_fe 0.000, Loss_pod 0.158, Loss_flat 0.019, Train_accy 100.00, Test_accy 56.04
2025-04-07 21:21:49,992 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.236, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.192, Loss_flat 0.031, Train_accy 99.89, Test_accy 67.22
2025-04-07 21:22:03,305 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.183, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.157, Loss_flat 0.018, Train_accy 100.00, Test_accy 62.67
2025-04-07 21:22:17,291 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.171, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.144, Loss_flat 0.018, Train_accy 100.00, Test_accy 63.24
2025-04-07 21:22:30,052 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.172, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.148, Loss_flat 0.017, Train_accy 100.00, Test_accy 64.48
2025-04-07 21:22:43,809 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.164, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.141, Loss_flat 0.015, Train_accy 100.00, Test_accy 63.59
2025-04-07 21:22:57,916 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.161, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.137, Loss_flat 0.015, Train_accy 100.00, Test_accy 63.52
2025-04-07 21:23:10,495 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.159, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.136, Loss_flat 0.016, Train_accy 100.00, Test_accy 65.26
2025-04-07 21:23:21,920 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.172, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.146, Loss_flat 0.018, Train_accy 100.00, Test_accy 66.39
2025-04-07 21:23:35,225 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.187, Loss_clf 0.012, Loss_fe 0.000, Loss_pod 0.154, Loss_flat 0.021, Train_accy 99.93, Test_accy 64.70
2025-04-07 21:23:48,001 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.148, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.127, Loss_flat 0.014, Train_accy 100.00, Test_accy 62.44
2025-04-07 21:24:01,660 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.145, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.123, Loss_flat 0.014, Train_accy 99.98, Test_accy 61.56
2025-04-07 21:24:14,099 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.144, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.124, Loss_flat 0.013, Train_accy 100.00, Test_accy 61.17
2025-04-07 21:24:27,908 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.136, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.116, Loss_flat 0.012, Train_accy 100.00, Test_accy 64.04
2025-04-07 21:24:40,043 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.130, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.112, Loss_flat 0.011, Train_accy 100.00, Test_accy 60.85
2025-04-07 21:24:53,460 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.137, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.117, Loss_flat 0.012, Train_accy 100.00, Test_accy 61.11
2025-04-07 21:25:05,806 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.130, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.109, Loss_flat 0.013, Train_accy 100.00, Test_accy 61.56
2025-04-07 21:25:18,896 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.133, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.112, Loss_flat 0.013, Train_accy 99.98, Test_accy 63.67
2025-04-07 21:25:32,323 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.126, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.106, Loss_flat 0.012, Train_accy 100.00, Test_accy 60.07
2025-04-07 21:25:45,265 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.115, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.098, Loss_flat 0.011, Train_accy 100.00, Test_accy 63.63
2025-04-07 21:25:58,384 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.115, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.098, Loss_flat 0.011, Train_accy 100.00, Test_accy 63.70
2025-04-07 21:26:11,792 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.107, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.090, Loss_flat 0.010, Train_accy 100.00, Test_accy 63.35
2025-04-07 21:26:25,710 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.104, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.088, Loss_flat 0.010, Train_accy 100.00, Test_accy 64.31
2025-04-07 21:26:38,797 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.099, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.083, Loss_flat 0.009, Train_accy 100.00, Test_accy 63.78
2025-04-07 21:26:51,546 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.103, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.086, Loss_flat 0.010, Train_accy 100.00, Test_accy 61.19
2025-04-07 21:27:00,310 [pod_foster.py] => Task 2, time 2.30s, Epoch 150/150 => Loss 0.096, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.080, Loss_flat 0.009, Train_accy 100.00
2025-04-07 21:27:00,310 [pod_foster.py] => 100 epoches training time:260.10s
2025-04-07 21:27:00,310 [pod_foster.py] => Average training time of single epoch:2.31s
2025-04-07 21:27:00,312 [inc_net.py] => align weights, gamma = 0.4895928204059601 
2025-04-07 21:27:00,313 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2025-04-07 21:27:03,930 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.610,  Train_accy 76.78, Test_accy 49.81
2025-04-07 21:27:15,024 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.352,  Train_accy 98.22, Test_accy 72.06
2025-04-07 21:27:26,395 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.345,  Train_accy 98.73, Test_accy 74.52
2025-04-07 21:27:37,227 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.344,  Train_accy 98.84, Test_accy 74.54
2025-04-07 21:27:48,454 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.342,  Train_accy 98.91, Test_accy 75.35
2025-04-07 21:28:00,107 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.343,  Train_accy 99.02, Test_accy 76.39
2025-04-07 21:28:11,315 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.339,  Train_accy 99.00, Test_accy 75.74
2025-04-07 21:28:22,452 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.340,  Train_accy 99.20, Test_accy 77.30
2025-04-07 21:28:33,925 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.338,  Train_accy 99.13, Test_accy 76.15
2025-04-07 21:28:46,568 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.338,  Train_accy 99.29, Test_accy 75.74
2025-04-07 21:28:57,471 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.339,  Train_accy 99.02, Test_accy 75.63
2025-04-07 21:29:08,406 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.339,  Train_accy 99.24, Test_accy 76.06
2025-04-07 21:29:19,618 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.338,  Train_accy 99.24, Test_accy 75.50
2025-04-07 21:29:30,897 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.339,  Train_accy 99.40, Test_accy 75.72
2025-04-07 21:29:42,763 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.339,  Train_accy 99.33, Test_accy 76.11
2025-04-07 21:29:54,283 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.338,  Train_accy 99.29, Test_accy 76.07
2025-04-07 21:30:04,846 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.339,  Train_accy 99.36, Test_accy 75.65
2025-04-07 21:30:17,158 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.334,  Train_accy 99.44, Test_accy 75.52
2025-04-07 21:30:28,566 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.337,  Train_accy 99.20, Test_accy 76.09
2025-04-07 21:30:39,794 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.335,  Train_accy 99.42, Test_accy 75.37
2025-04-07 21:30:51,527 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.335,  Train_accy 99.27, Test_accy 75.63
2025-04-07 21:31:02,359 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.336,  Train_accy 99.24, Test_accy 75.78
2025-04-07 21:31:13,381 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.336,  Train_accy 99.29, Test_accy 76.30
2025-04-07 21:31:24,256 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.338,  Train_accy 99.15, Test_accy 75.33
2025-04-07 21:31:32,503 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.336,  Train_accy 99.20
2025-04-07 21:31:32,503 [pod_foster.py] => do not weight align student!
2025-04-07 21:31:33,531 [pod_foster.py] => darknet eval: 
2025-04-07 21:31:33,531 [pod_foster.py] => CNN top1 curve: 76.57
2025-04-07 21:31:33,531 [pod_foster.py] => CNN top5 curve: 97.35
2025-04-07 21:31:33,531 [pod_foster.py] => CNN: {'total': 76.57, '00-04': 65.67, '05-06': 85.58, '07-08': 94.83, 'old': 71.36, 'new': 94.83}
2025-04-07 21:31:33,532 [pod_foster.py] => All params after compression: 3853138
2025-04-07 21:31:33,532 [base.py] => Reducing exemplars...(55 per classes)
2025-04-07 21:31:36,083 [base.py] => Constructing exemplars...(55 per classes)
2025-04-07 21:31:39,249 [trainer.py] => task:2 training time:669.88s
2025-04-07 21:31:39,250 [trainer.py] => All params: 7705241
2025-04-07 21:31:42,019 [pod_foster.py] => Exemplar size: 495
2025-04-07 21:31:42,019 [trainer.py] => CNN: {'total': 73.52, '00-04': 59.73, '05-06': 91.5, '07-08': 90.0, 'old': 68.81, 'new': 90.0}
2025-04-07 21:31:42,019 [trainer.py] => NME: {'total': 72.65, '00-04': 61.3, '05-06': 82.42, '07-08': 91.25, 'old': 67.33, 'new': 91.25}
2025-04-07 21:31:42,019 [trainer.py] => CNN top1 curve: [89.93, 79.71, 73.52]
2025-04-07 21:31:42,020 [trainer.py] => CNN top5 curve: [100.0, 98.86, 97.04]
2025-04-07 21:31:42,020 [trainer.py] => NME top1 curve: [90.0, 74.24, 72.65]
2025-04-07 21:31:42,020 [trainer.py] => NME top5 curve: [100.0, 99.1, 96.89]

2025-04-07 21:31:42,020 [trainer.py] => Average Accuracy (CNN): 81.05333333333333
2025-04-07 21:31:42,020 [trainer.py] => Average Accuracy (NME): 78.96333333333334
2025-04-07 21:31:42,020 [trainer.py] => Time consumed in all training process:1365.31s
2025-04-07 21:31:42,020 [trainer.py] => Average Time consumed in single task:452.74s
2025-04-07 21:31:42,020 [trainer.py] => Forgetting (CNN): 15.100000000000005
