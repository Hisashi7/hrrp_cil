2024-10-30 10:06:45,706 [trainer.py] => config: ./exps/POD_foster.json
2024-10-30 10:06:45,706 [trainer.py] => prefix: cil
2024-10-30 10:06:45,706 [trainer.py] => dataset: hrrp9
2024-10-30 10:06:45,706 [trainer.py] => memory_size: 500
2024-10-30 10:06:45,706 [trainer.py] => memory_per_class: 20
2024-10-30 10:06:45,706 [trainer.py] => fixed_memory: False
2024-10-30 10:06:45,707 [trainer.py] => shuffle: True
2024-10-30 10:06:45,707 [trainer.py] => init_cls: 5
2024-10-30 10:06:45,707 [trainer.py] => increment: 2
2024-10-30 10:06:45,707 [trainer.py] => model_name: POD_foster
2024-10-30 10:06:45,707 [trainer.py] => convnet_type: resnet18
2024-10-30 10:06:45,707 [trainer.py] => init_train: False
2024-10-30 10:06:45,707 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_35172.pth
2024-10-30 10:06:45,707 [trainer.py] => fc_path: checkpoints/init_train/fc_35172.pth
2024-10-30 10:06:45,707 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42503.pth
2024-10-30 10:06:45,707 [trainer.py] => fc_path2: checkpoints/init_train/fc_42503.pth
2024-10-30 10:06:45,707 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42871.pth
2024-10-30 10:06:45,707 [trainer.py] => fc_path1: checkpoints/init_train/fc_42871.pth
2024-10-30 10:06:45,707 [trainer.py] => device: [device(type='cuda', index=2)]
2024-10-30 10:06:45,707 [trainer.py] => seed: 2001
2024-10-30 10:06:45,707 [trainer.py] => beta1: 0.96
2024-10-30 10:06:45,707 [trainer.py] => beta2: 0.97
2024-10-30 10:06:45,707 [trainer.py] => oofc: ft
2024-10-30 10:06:45,707 [trainer.py] => is_teacher_wa: True
2024-10-30 10:06:45,707 [trainer.py] => is_student_wa: False
2024-10-30 10:06:45,708 [trainer.py] => is_teacher_la: True
2024-10-30 10:06:45,708 [trainer.py] => is_student_la: True
2024-10-30 10:06:45,708 [trainer.py] => lambda_okd: 0
2024-10-30 10:06:45,708 [trainer.py] => wa_value: 1
2024-10-30 10:06:45,708 [trainer.py] => init_epochs: 0
2024-10-30 10:06:45,708 [trainer.py] => init_lr: 0.1
2024-10-30 10:06:45,708 [trainer.py] => init_weight_decay: 0.0005
2024-10-30 10:06:45,708 [trainer.py] => boosting_epochs: 150
2024-10-30 10:06:45,708 [trainer.py] => compression_epochs: 120
2024-10-30 10:06:45,708 [trainer.py] => lr: 0.1
2024-10-30 10:06:45,708 [trainer.py] => batch_size: 128
2024-10-30 10:06:45,708 [trainer.py] => weight_decay: 0.0005
2024-10-30 10:06:45,708 [trainer.py] => num_workers: 8
2024-10-30 10:06:45,708 [trainer.py] => momentum: 0.9
2024-10-30 10:06:45,708 [trainer.py] => T: 2
2024-10-30 10:06:45,708 [trainer.py] => lambda_c_base: 0.6
2024-10-30 10:06:45,708 [trainer.py] => lambda_f_base: 1.0
2024-10-30 10:06:45,708 [trainer.py] => POD: c
2024-10-30 10:06:46,533 [data_manager.py] => [3, 5, 1, 7, 2, 8, 6, 4, 0]
2024-10-30 10:06:46,602 [trainer.py] => All params: 0
2024-10-30 10:06:46,602 [trainer.py] => Trainable params: 0
2024-10-30 10:06:47,299 [pod_foster.py] => Learning on 0-5
2024-10-30 10:06:47,301 [pod_foster.py] => All params: 3849034
2024-10-30 10:06:47,302 [pod_foster.py] => Trainable params: 3849034
2024-10-30 10:06:47,415 [pod_foster.py] => Adaptive factor: 0
2024-10-30 10:06:47,769 [pod_foster.py] => init_train?---False
2024-10-30 10:06:49,138 [base.py] => Reducing exemplars...(100 per classes)
2024-10-30 10:06:49,138 [base.py] => Constructing exemplars...(100 per classes)
2024-10-30 10:06:58,693 [trainer.py] => All params: 3849034
2024-10-30 10:07:00,765 [pod_foster.py] => Exemplar size: 500
2024-10-30 10:07:00,766 [trainer.py] => CNN: {'total': 90.13, '00-04': 90.13, 'old': 0, 'new': 90.13}
2024-10-30 10:07:00,766 [trainer.py] => NME: {'total': 89.53, '00-04': 89.53, 'old': 0, 'new': 89.53}
2024-10-30 10:07:00,767 [trainer.py] => CNN top1 curve: [90.13]
2024-10-30 10:07:00,767 [trainer.py] => CNN top5 curve: [100.0]
2024-10-30 10:07:00,767 [trainer.py] => NME top1 curve: [89.53]
2024-10-30 10:07:00,768 [trainer.py] => NME top5 curve: [100.0]

2024-10-30 10:07:00,768 [trainer.py] => Average Accuracy (CNN): 90.13
2024-10-30 10:07:00,769 [trainer.py] => Average Accuracy (NME): 89.53
2024-10-30 10:07:00,769 [trainer.py] => All params: 3849034
2024-10-30 10:07:00,770 [trainer.py] => Trainable params: 3849034
2024-10-30 10:07:00,862 [pod_foster.py] => Learning on 5-7
2024-10-30 10:07:00,864 [pod_foster.py] => All params: 7701139
2024-10-30 10:07:00,865 [pod_foster.py] => Trainable params: 3854670
2024-10-30 10:07:00,931 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-30 10:07:00,941 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-30 10:07:05,314 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.810, Loss_clf 0.618, Loss_fe 0.676, Loss_pod 0.239, Loss_flat 0.277, Train_accy 81.71, Test_accy 70.38
2024-10-30 10:07:19,171 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.228, Loss_clf 0.010, Loss_fe 0.019, Loss_pod 0.109, Loss_flat 0.090, Train_accy 100.00, Test_accy 78.98
2024-10-30 10:07:34,286 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.142, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.077, Loss_flat 0.052, Train_accy 100.00, Test_accy 72.50
2024-10-30 10:07:48,716 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.119, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.066, Loss_flat 0.043, Train_accy 100.00, Test_accy 70.43
2024-10-30 10:08:03,225 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.879, Loss_clf 0.094, Loss_fe 0.299, Loss_pod 0.248, Loss_flat 0.238, Train_accy 96.84, Test_accy 47.45
2024-10-30 10:08:16,872 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.351, Loss_clf 0.013, Loss_fe 0.039, Loss_pod 0.160, Loss_flat 0.139, Train_accy 99.82, Test_accy 74.12
2024-10-30 10:08:30,968 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.212, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.112, Loss_flat 0.081, Train_accy 99.98, Test_accy 73.00
2024-10-30 10:08:44,960 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.167, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.096, Loss_flat 0.060, Train_accy 100.00, Test_accy 80.50
2024-10-30 10:08:58,361 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.129, Loss_clf 0.005, Loss_fe 0.004, Loss_pod 0.077, Loss_flat 0.043, Train_accy 100.00, Test_accy 78.55
2024-10-30 10:09:11,690 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.132, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.077, Loss_flat 0.045, Train_accy 100.00, Test_accy 78.31
2024-10-30 10:09:26,296 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.107, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.063, Loss_flat 0.036, Train_accy 100.00, Test_accy 78.36
2024-10-30 10:09:40,654 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.103, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.061, Loss_flat 0.034, Train_accy 100.00, Test_accy 79.10
2024-10-30 10:09:54,392 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.101, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.061, Loss_flat 0.032, Train_accy 100.00, Test_accy 78.83
2024-10-30 10:10:08,099 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.095, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.056, Loss_flat 0.031, Train_accy 100.00, Test_accy 77.83
2024-10-30 10:10:22,561 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.093, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.056, Loss_flat 0.029, Train_accy 100.00, Test_accy 76.90
2024-10-30 10:10:36,437 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.088, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.052, Loss_flat 0.029, Train_accy 100.00, Test_accy 77.69
2024-10-30 10:10:48,404 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.086, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.051, Loss_flat 0.028, Train_accy 100.00, Test_accy 79.43
2024-10-30 10:10:59,770 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.086, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.051, Loss_flat 0.028, Train_accy 100.00, Test_accy 77.36
2024-10-30 10:11:11,143 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.079, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.045, Loss_flat 0.026, Train_accy 100.00, Test_accy 77.95
2024-10-30 10:11:22,721 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.131, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.068, Loss_flat 0.047, Train_accy 99.96, Test_accy 76.43
2024-10-30 10:11:34,419 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.080, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.027, Train_accy 100.00, Test_accy 78.45
2024-10-30 10:11:46,382 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.074, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.041, Loss_flat 0.026, Train_accy 100.00, Test_accy 78.93
2024-10-30 10:11:57,833 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.070, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.037, Loss_flat 0.025, Train_accy 100.00, Test_accy 78.02
2024-10-30 10:12:09,365 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.068, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.035, Loss_flat 0.025, Train_accy 100.00, Test_accy 78.60
2024-10-30 10:12:20,897 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.066, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.034, Loss_flat 0.024, Train_accy 100.00, Test_accy 78.36
2024-10-30 10:12:32,510 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.065, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.033, Loss_flat 0.024, Train_accy 100.00, Test_accy 77.50
2024-10-30 10:12:44,302 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.063, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.031, Loss_flat 0.025, Train_accy 100.00, Test_accy 76.86
2024-10-30 10:12:56,029 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.060, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.029, Loss_flat 0.023, Train_accy 100.00, Test_accy 78.24
2024-10-30 10:13:07,521 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.060, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.030, Loss_flat 0.024, Train_accy 100.00, Test_accy 78.05
2024-10-30 10:13:18,965 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.061, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.030, Loss_flat 0.024, Train_accy 100.00, Test_accy 77.76
2024-10-30 10:13:27,211 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.059, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.029, Loss_flat 0.024, Train_accy 100.00
2024-10-30 10:13:27,215 [inc_net.py] => align weights, gamma = 0.4781961441040039 
2024-10-30 10:13:27,216 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-30 10:13:30,151 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.339,  Train_accy 69.58, Test_accy 69.14
2024-10-30 10:13:40,070 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.069,  Train_accy 95.51, Test_accy 81.33
2024-10-30 10:13:50,027 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.063,  Train_accy 96.11, Test_accy 80.60
2024-10-30 10:13:59,793 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.058,  Train_accy 96.22, Test_accy 81.76
2024-10-30 10:14:09,559 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.060,  Train_accy 96.09, Test_accy 82.21
2024-10-30 10:14:19,692 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.056,  Train_accy 96.24, Test_accy 82.45
2024-10-30 10:14:29,787 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.058,  Train_accy 96.53, Test_accy 82.10
2024-10-30 10:14:39,869 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.056,  Train_accy 96.04, Test_accy 83.07
2024-10-30 10:14:49,982 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.055,  Train_accy 96.67, Test_accy 82.90
2024-10-30 10:15:00,325 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.052,  Train_accy 96.60, Test_accy 83.24
2024-10-30 10:15:10,524 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.054,  Train_accy 96.62, Test_accy 83.43
2024-10-30 10:15:21,386 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.051,  Train_accy 96.64, Test_accy 82.83
2024-10-30 10:15:34,333 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.051,  Train_accy 96.51, Test_accy 83.33
2024-10-30 10:15:46,858 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.055,  Train_accy 97.04, Test_accy 82.98
2024-10-30 10:15:58,625 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.055,  Train_accy 96.73, Test_accy 83.64
2024-10-30 10:16:11,445 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.052,  Train_accy 96.67, Test_accy 83.64
2024-10-30 10:16:24,042 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.053,  Train_accy 96.53, Test_accy 83.67
2024-10-30 10:16:37,518 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.051,  Train_accy 96.76, Test_accy 83.55
2024-10-30 10:16:51,054 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.053,  Train_accy 97.07, Test_accy 83.64
2024-10-30 10:17:03,841 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.051,  Train_accy 96.78, Test_accy 83.64
2024-10-30 10:17:16,286 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.050,  Train_accy 96.60, Test_accy 83.98
2024-10-30 10:17:28,820 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.052,  Train_accy 96.93, Test_accy 83.86
2024-10-30 10:17:41,425 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.053,  Train_accy 96.73, Test_accy 83.90
2024-10-30 10:17:54,029 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.050,  Train_accy 96.62, Test_accy 83.90
2024-10-30 10:18:03,168 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.050,  Train_accy 96.64
2024-10-30 10:18:03,168 [pod_foster.py] => do not weight align student!
2024-10-30 10:18:04,184 [pod_foster.py] => darknet eval: 
2024-10-30 10:18:04,185 [pod_foster.py] => CNN top1 curve: 83.71
2024-10-30 10:18:04,187 [pod_foster.py] => CNN top5 curve: 99.55
2024-10-30 10:18:04,188 [pod_foster.py] => CNN: {'total': 83.71, '00-04': 82.03, '05-06': 87.92, 'old': 82.03, 'new': 87.92}
2024-10-30 10:18:04,192 [pod_foster.py] => All params after compression: 3851086
2024-10-30 10:18:04,195 [base.py] => Reducing exemplars...(71 per classes)
2024-10-30 10:18:06,415 [base.py] => Constructing exemplars...(71 per classes)
2024-10-30 10:18:11,187 [trainer.py] => All params: 7701139
2024-10-30 10:18:13,729 [pod_foster.py] => Exemplar size: 497
2024-10-30 10:18:13,730 [trainer.py] => CNN: {'total': 85.14, '00-04': 85.17, '05-06': 85.08, 'old': 85.17, 'new': 85.08}
2024-10-30 10:18:13,730 [trainer.py] => NME: {'total': 75.76, '00-04': 82.13, '05-06': 59.83, 'old': 82.13, 'new': 59.83}
2024-10-30 10:18:13,731 [trainer.py] => CNN top1 curve: [90.13, 85.14]
2024-10-30 10:18:13,731 [trainer.py] => CNN top5 curve: [100.0, 99.67]
2024-10-30 10:18:13,731 [trainer.py] => NME top1 curve: [89.53, 75.76]
2024-10-30 10:18:13,732 [trainer.py] => NME top5 curve: [100.0, 99.62]

2024-10-30 10:18:13,732 [trainer.py] => Average Accuracy (CNN): 87.63499999999999
2024-10-30 10:18:13,732 [trainer.py] => Average Accuracy (NME): 82.64500000000001
2024-10-30 10:18:13,733 [trainer.py] => All params: 7701139
2024-10-30 10:18:13,734 [trainer.py] => Trainable params: 3854670
2024-10-30 10:18:13,875 [pod_foster.py] => Learning on 7-9
2024-10-30 10:18:13,877 [pod_foster.py] => All params: 7705241
2024-10-30 10:18:13,878 [pod_foster.py] => Trainable params: 3857746
2024-10-30 10:18:13,994 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-30 10:18:14,003 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-30 10:18:18,092 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.045, Loss_clf 0.752, Loss_fe 0.759, Loss_pod 0.266, Loss_flat 0.268, Train_accy 81.34, Test_accy 56.93
2024-10-30 10:18:32,077 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.371, Loss_clf 0.026, Loss_fe 0.066, Loss_pod 0.150, Loss_flat 0.129, Train_accy 99.51, Test_accy 56.22
2024-10-30 10:18:47,363 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.177, Loss_clf 0.009, Loss_fe 0.011, Loss_pod 0.092, Loss_flat 0.065, Train_accy 99.98, Test_accy 62.63
2024-10-30 10:19:01,784 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.344, Loss_clf 0.030, Loss_fe 0.044, Loss_pod 0.157, Loss_flat 0.113, Train_accy 99.29, Test_accy 73.22
2024-10-30 10:19:16,057 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.154, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.087, Loss_flat 0.056, Train_accy 100.00, Test_accy 64.20
2024-10-30 10:19:29,817 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.211, Loss_clf 0.012, Loss_fe 0.015, Loss_pod 0.108, Loss_flat 0.077, Train_accy 99.87, Test_accy 61.70
2024-10-30 10:19:42,311 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.133, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.077, Loss_flat 0.047, Train_accy 100.00, Test_accy 65.39
2024-10-30 10:19:54,232 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.116, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.069, Loss_flat 0.039, Train_accy 100.00, Test_accy 62.70
2024-10-30 10:20:06,163 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.323, Loss_clf 0.019, Loss_fe 0.038, Loss_pod 0.151, Loss_flat 0.115, Train_accy 99.60, Test_accy 71.93
2024-10-30 10:20:17,966 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.126, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.074, Loss_flat 0.044, Train_accy 100.00, Test_accy 63.59
2024-10-30 10:20:29,946 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.108, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.062, Loss_flat 0.036, Train_accy 100.00, Test_accy 61.33
2024-10-30 10:20:41,815 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.100, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.057, Loss_flat 0.034, Train_accy 100.00, Test_accy 62.15
2024-10-30 10:20:53,639 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.100, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.058, Loss_flat 0.034, Train_accy 100.00, Test_accy 61.46
2024-10-30 10:21:05,810 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.177, Loss_clf 0.011, Loss_fe 0.015, Loss_pod 0.092, Loss_flat 0.059, Train_accy 99.84, Test_accy 66.04
2024-10-30 10:21:17,874 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.094, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.053, Loss_flat 0.033, Train_accy 100.00, Test_accy 63.13
2024-10-30 10:21:29,651 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.087, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.049, Loss_flat 0.030, Train_accy 100.00, Test_accy 61.41
2024-10-30 10:21:41,723 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.085, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.047, Loss_flat 0.031, Train_accy 100.00, Test_accy 60.87
2024-10-30 10:21:54,421 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.085, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.048, Loss_flat 0.029, Train_accy 100.00, Test_accy 62.22
2024-10-30 10:22:06,463 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.082, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.046, Loss_flat 0.029, Train_accy 100.00, Test_accy 61.85
2024-10-30 10:22:18,384 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.077, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.042, Loss_flat 0.027, Train_accy 100.00, Test_accy 62.37
2024-10-30 10:22:31,897 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.076, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.041, Loss_flat 0.027, Train_accy 100.00, Test_accy 61.80
2024-10-30 10:22:46,942 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.084, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.044, Loss_flat 0.030, Train_accy 100.00, Test_accy 61.13
2024-10-30 10:23:01,477 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.074, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.038, Loss_flat 0.027, Train_accy 100.00, Test_accy 63.33
2024-10-30 10:23:16,046 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.072, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.036, Loss_flat 0.027, Train_accy 100.00, Test_accy 63.04
2024-10-30 10:23:30,435 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.069, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.035, Loss_flat 0.026, Train_accy 100.00, Test_accy 62.93
2024-10-30 10:23:45,558 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.064, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.032, Loss_flat 0.025, Train_accy 100.00, Test_accy 62.91
2024-10-30 10:24:00,492 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.066, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.032, Loss_flat 0.026, Train_accy 100.00, Test_accy 61.94
2024-10-30 10:24:15,216 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.064, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.032, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.44
2024-10-30 10:24:30,004 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.062, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.030, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.39
2024-10-30 10:24:44,520 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.061, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.029, Loss_flat 0.025, Train_accy 100.00, Test_accy 60.44
2024-10-30 10:24:54,523 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.063, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.030, Loss_flat 0.026, Train_accy 100.00
2024-10-30 10:24:54,525 [inc_net.py] => align weights, gamma = 0.4702742397785187 
2024-10-30 10:24:54,527 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-30 10:24:58,139 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.611,  Train_accy 72.27, Test_accy 65.30
2024-10-30 10:25:11,133 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.376,  Train_accy 95.84, Test_accy 73.70
2024-10-30 10:25:25,615 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.378,  Train_accy 96.75, Test_accy 76.31
2024-10-30 10:25:39,526 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.372,  Train_accy 97.18, Test_accy 76.19
2024-10-30 10:25:52,918 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.370,  Train_accy 97.18, Test_accy 75.96
2024-10-30 10:26:06,222 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.371,  Train_accy 97.46, Test_accy 76.22
2024-10-30 10:26:19,282 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.370,  Train_accy 97.15, Test_accy 76.89
2024-10-30 10:26:32,365 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.370,  Train_accy 97.22, Test_accy 76.61
2024-10-30 10:26:44,057 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.367,  Train_accy 97.40, Test_accy 76.81
2024-10-30 10:26:54,306 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.366,  Train_accy 97.26, Test_accy 76.85
2024-10-30 10:27:05,083 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.367,  Train_accy 97.44, Test_accy 77.26
2024-10-30 10:27:15,969 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.369,  Train_accy 97.60, Test_accy 77.22
2024-10-30 10:27:26,537 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.368,  Train_accy 97.55, Test_accy 77.06
2024-10-30 10:27:36,957 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.365,  Train_accy 97.58, Test_accy 76.87
2024-10-30 10:27:47,473 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.366,  Train_accy 97.75, Test_accy 77.37
2024-10-30 10:27:57,805 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.367,  Train_accy 97.67, Test_accy 77.20
2024-10-30 10:28:07,979 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.368,  Train_accy 97.64, Test_accy 77.41
2024-10-30 10:28:18,372 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.361,  Train_accy 97.67, Test_accy 76.74
2024-10-30 10:28:28,683 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.365,  Train_accy 97.82, Test_accy 77.57
2024-10-30 10:28:39,197 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.361,  Train_accy 98.11, Test_accy 76.78
2024-10-30 10:28:49,631 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.362,  Train_accy 97.78, Test_accy 77.33
2024-10-30 10:29:00,212 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.366,  Train_accy 97.87, Test_accy 77.41
2024-10-30 10:29:11,329 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.366,  Train_accy 97.64, Test_accy 77.33
2024-10-30 10:29:23,154 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.367,  Train_accy 97.71, Test_accy 77.63
2024-10-30 10:29:31,834 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.365,  Train_accy 97.80
2024-10-30 10:29:31,835 [pod_foster.py] => do not weight align student!
2024-10-30 10:29:32,876 [pod_foster.py] => darknet eval: 
2024-10-30 10:29:32,876 [pod_foster.py] => CNN top1 curve: 77.52
2024-10-30 10:29:32,876 [pod_foster.py] => CNN top5 curve: 97.72
2024-10-30 10:29:32,876 [pod_foster.py] => CNN: {'total': 77.52, '00-04': 77.2, '05-06': 74.08, '07-08': 81.75, 'old': 76.31, 'new': 81.75}
2024-10-30 10:29:32,878 [pod_foster.py] => All params after compression: 3853138
2024-10-30 10:29:32,878 [base.py] => Reducing exemplars...(55 per classes)
2024-10-30 10:29:35,338 [base.py] => Constructing exemplars...(55 per classes)
2024-10-30 10:29:39,294 [trainer.py] => All params: 7705241
2024-10-30 10:29:42,301 [pod_foster.py] => Exemplar size: 495
2024-10-30 10:29:42,302 [trainer.py] => CNN: {'total': 79.24, '00-04': 77.6, '05-06': 80.0, '07-08': 82.58, 'old': 78.29, 'new': 82.58}
2024-10-30 10:29:42,303 [trainer.py] => NME: {'total': 67.96, '00-04': 76.07, '05-06': 60.25, '07-08': 55.42, 'old': 71.55, 'new': 55.42}
2024-10-30 10:29:42,303 [trainer.py] => CNN top1 curve: [90.13, 85.14, 79.24]
2024-10-30 10:29:42,303 [trainer.py] => CNN top5 curve: [100.0, 99.67, 97.59]
2024-10-30 10:29:42,303 [trainer.py] => NME top1 curve: [89.53, 75.76, 67.96]
2024-10-30 10:29:42,304 [trainer.py] => NME top5 curve: [100.0, 99.62, 97.41]

2024-10-30 10:29:42,304 [trainer.py] => Average Accuracy (CNN): 84.83666666666666
2024-10-30 10:29:42,305 [trainer.py] => Average Accuracy (NME): 77.75
2024-10-30 10:29:42,306 [trainer.py] => Forgetting (CNN): 8.805
