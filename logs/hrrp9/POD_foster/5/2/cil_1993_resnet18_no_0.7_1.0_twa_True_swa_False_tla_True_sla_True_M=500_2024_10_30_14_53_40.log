2024-10-30 14:53:40,480 [trainer.py] => config: ./exps/POD_foster.json
2024-10-30 14:53:40,480 [trainer.py] => prefix: cil
2024-10-30 14:53:40,481 [trainer.py] => dataset: hrrp9
2024-10-30 14:53:40,481 [trainer.py] => memory_size: 500
2024-10-30 14:53:40,482 [trainer.py] => memory_per_class: 20
2024-10-30 14:53:40,482 [trainer.py] => fixed_memory: False
2024-10-30 14:53:40,482 [trainer.py] => shuffle: True
2024-10-30 14:53:40,482 [trainer.py] => init_cls: 5
2024-10-30 14:53:40,483 [trainer.py] => increment: 2
2024-10-30 14:53:40,483 [trainer.py] => model_name: POD_foster
2024-10-30 14:53:40,483 [trainer.py] => convnet_type: resnet18
2024-10-30 14:53:40,483 [trainer.py] => init_train: False
2024-10-30 14:53:40,483 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-30 14:53:40,483 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-30 14:53:40,483 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-30 14:53:40,483 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-30 14:53:40,483 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42871.pth
2024-10-30 14:53:40,483 [trainer.py] => fc_path2: checkpoints/init_train/fc_42871.pth
2024-10-30 14:53:40,483 [trainer.py] => device: [device(type='cuda', index=2)]
2024-10-30 14:53:40,484 [trainer.py] => seed: 1993
2024-10-30 14:53:40,484 [trainer.py] => beta1: 0.96
2024-10-30 14:53:40,484 [trainer.py] => beta2: 0.97
2024-10-30 14:53:40,484 [trainer.py] => oofc: ft
2024-10-30 14:53:40,484 [trainer.py] => is_teacher_wa: True
2024-10-30 14:53:40,484 [trainer.py] => is_student_wa: False
2024-10-30 14:53:40,484 [trainer.py] => is_teacher_la: True
2024-10-30 14:53:40,484 [trainer.py] => is_student_la: True
2024-10-30 14:53:40,484 [trainer.py] => lambda_okd: 0
2024-10-30 14:53:40,484 [trainer.py] => wa_value: 1
2024-10-30 14:53:40,485 [trainer.py] => init_epochs: 0
2024-10-30 14:53:40,485 [trainer.py] => init_lr: 0.1
2024-10-30 14:53:40,485 [trainer.py] => init_weight_decay: 0.0005
2024-10-30 14:53:40,485 [trainer.py] => boosting_epochs: 150
2024-10-30 14:53:40,485 [trainer.py] => compression_epochs: 120
2024-10-30 14:53:40,485 [trainer.py] => lr: 0.1
2024-10-30 14:53:40,485 [trainer.py] => batch_size: 128
2024-10-30 14:53:40,486 [trainer.py] => weight_decay: 0.0005
2024-10-30 14:53:40,486 [trainer.py] => num_workers: 8
2024-10-30 14:53:40,486 [trainer.py] => momentum: 0.9
2024-10-30 14:53:40,486 [trainer.py] => T: 2
2024-10-30 14:53:40,486 [trainer.py] => lambda_c_base: 0.7
2024-10-30 14:53:40,486 [trainer.py] => lambda_f_base: 1.0
2024-10-30 14:53:40,487 [trainer.py] => POD: no
2024-10-30 14:53:41,619 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-30 14:53:41,682 [trainer.py] => All params: 0
2024-10-30 14:53:41,683 [trainer.py] => Trainable params: 0
2024-10-30 14:53:42,421 [pod_foster.py] => Learning on 0-5
2024-10-30 14:53:42,423 [pod_foster.py] => All params: 3849034
2024-10-30 14:53:42,424 [pod_foster.py] => Trainable params: 3849034
2024-10-30 14:53:42,601 [pod_foster.py] => Adaptive factor: 0
2024-10-30 14:53:42,953 [pod_foster.py] => init_train?---False
2024-10-30 14:53:45,131 [base.py] => Reducing exemplars...(100 per classes)
2024-10-30 14:53:45,131 [base.py] => Constructing exemplars...(100 per classes)
2024-10-30 14:53:57,370 [trainer.py] => All params: 3849034
2024-10-30 14:54:00,117 [pod_foster.py] => Exemplar size: 500
2024-10-30 14:54:00,118 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-30 14:54:00,119 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-30 14:54:00,119 [trainer.py] => CNN top1 curve: [89.93]
2024-10-30 14:54:00,119 [trainer.py] => CNN top5 curve: [100.0]
2024-10-30 14:54:00,120 [trainer.py] => NME top1 curve: [90.0]
2024-10-30 14:54:00,125 [trainer.py] => NME top5 curve: [100.0]

2024-10-30 14:54:00,137 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-30 14:54:00,137 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-30 14:54:00,138 [trainer.py] => All params: 3849034
2024-10-30 14:54:00,139 [trainer.py] => Trainable params: 3849034
2024-10-30 14:54:00,205 [pod_foster.py] => Learning on 5-7
2024-10-30 14:54:00,207 [pod_foster.py] => All params: 7701139
2024-10-30 14:54:00,208 [pod_foster.py] => Trainable params: 3854670
2024-10-30 14:54:00,296 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-30 14:54:00,309 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-30 14:54:05,932 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.691, Loss_clf 0.708, Loss_fe 0.692, Loss_pod 0.037, Loss_flat 0.253, Train_accy 82.96, Test_accy 62.86
2024-10-30 14:54:24,250 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.272, Loss_clf 0.034, Loss_fe 0.085, Loss_pod 0.036, Loss_flat 0.117, Train_accy 99.27, Test_accy 69.10
2024-10-30 14:54:41,991 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.103, Loss_clf 0.007, Loss_fe 0.010, Loss_pod 0.023, Loss_flat 0.062, Train_accy 99.98, Test_accy 67.83
2024-10-30 14:54:58,334 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.073, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.018, Loss_flat 0.044, Train_accy 100.00, Test_accy 69.98
2024-10-30 14:55:12,612 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.148, Loss_clf 0.027, Loss_fe 0.032, Loss_pod 0.023, Loss_flat 0.065, Train_accy 99.58, Test_accy 25.76
2024-10-30 14:55:27,481 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.470, Loss_clf 0.066, Loss_fe 0.174, Loss_pod 0.057, Loss_flat 0.174, Train_accy 97.96, Test_accy 64.74
2024-10-30 14:55:43,189 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.257, Loss_clf 0.026, Loss_fe 0.056, Loss_pod 0.049, Loss_flat 0.125, Train_accy 99.47, Test_accy 72.64
2024-10-30 14:55:57,786 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.135, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.043, Loss_flat 0.080, Train_accy 100.00, Test_accy 71.76
2024-10-30 14:56:12,504 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.113, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.038, Loss_flat 0.064, Train_accy 100.00, Test_accy 69.14
2024-10-30 14:56:28,872 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.089, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.034, Loss_flat 0.046, Train_accy 100.00, Test_accy 68.76
2024-10-30 14:56:44,216 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.081, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.032, Loss_flat 0.040, Train_accy 100.00, Test_accy 70.02
2024-10-30 14:56:57,895 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.071, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.029, Loss_flat 0.033, Train_accy 100.00, Test_accy 69.29
2024-10-30 14:57:09,286 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.071, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.028, Loss_flat 0.034, Train_accy 100.00, Test_accy 69.31
2024-10-30 14:57:20,918 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.344, Loss_clf 0.051, Loss_fe 0.109, Loss_pod 0.050, Loss_flat 0.134, Train_accy 98.36, Test_accy 64.19
2024-10-30 14:57:32,595 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.101, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.035, Loss_flat 0.056, Train_accy 100.00, Test_accy 70.62
2024-10-30 14:57:44,126 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.079, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.030, Loss_flat 0.041, Train_accy 100.00, Test_accy 70.02
2024-10-30 14:57:57,497 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.070, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.028, Loss_flat 0.034, Train_accy 100.00, Test_accy 69.60
2024-10-30 14:58:11,064 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.066, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.027, Loss_flat 0.032, Train_accy 100.00, Test_accy 69.21
2024-10-30 14:58:25,932 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.062, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.026, Loss_flat 0.029, Train_accy 100.00, Test_accy 69.14
2024-10-30 14:58:38,601 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.067, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.025, Loss_flat 0.033, Train_accy 100.00, Test_accy 69.55
2024-10-30 14:58:52,283 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.059, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.024, Loss_flat 0.027, Train_accy 100.00, Test_accy 68.83
2024-10-30 14:59:07,331 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.058, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.024, Loss_flat 0.026, Train_accy 100.00, Test_accy 69.14
2024-10-30 14:59:20,958 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.058, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.023, Loss_flat 0.026, Train_accy 100.00, Test_accy 68.31
2024-10-30 14:59:33,237 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.056, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.023, Loss_flat 0.024, Train_accy 100.00, Test_accy 69.29
2024-10-30 14:59:45,297 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.054, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.023, Loss_flat 0.024, Train_accy 100.00, Test_accy 68.83
2024-10-30 14:59:57,226 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.054, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.023, Loss_flat 0.024, Train_accy 100.00, Test_accy 68.45
2024-10-30 15:00:08,804 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.054, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.023, Loss_flat 0.025, Train_accy 100.00, Test_accy 68.86
2024-10-30 15:00:20,641 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.053, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.022, Loss_flat 0.024, Train_accy 100.00, Test_accy 68.88
2024-10-30 15:00:34,769 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.054, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.022, Loss_flat 0.024, Train_accy 100.00, Test_accy 68.81
2024-10-30 15:00:50,440 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.053, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.022, Loss_flat 0.024, Train_accy 100.00, Test_accy 69.52
2024-10-30 15:01:02,055 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.054, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.022, Loss_flat 0.024, Train_accy 100.00
2024-10-30 15:01:02,056 [inc_net.py] => align weights, gamma = 0.5676273107528687 
2024-10-30 15:01:02,058 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-30 15:01:05,746 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.217,  Train_accy 71.69, Test_accy 62.55
2024-10-30 15:01:18,499 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.882,  Train_accy 96.62, Test_accy 74.40
2024-10-30 15:01:34,713 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.875,  Train_accy 97.31, Test_accy 75.26
2024-10-30 15:01:47,967 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.868,  Train_accy 97.20, Test_accy 76.57
2024-10-30 15:01:59,719 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.869,  Train_accy 97.58, Test_accy 76.90
2024-10-30 15:02:10,003 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.865,  Train_accy 97.56, Test_accy 76.57
2024-10-30 15:02:20,488 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.869,  Train_accy 97.96, Test_accy 77.05
2024-10-30 15:02:30,902 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.868,  Train_accy 97.64, Test_accy 76.05
2024-10-30 15:02:42,023 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.864,  Train_accy 98.04, Test_accy 76.48
2024-10-30 15:02:54,223 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.862,  Train_accy 97.67, Test_accy 76.60
2024-10-30 15:03:09,530 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.863,  Train_accy 98.04, Test_accy 76.79
2024-10-30 15:03:25,327 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.863,  Train_accy 97.73, Test_accy 76.81
2024-10-30 15:03:40,053 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.863,  Train_accy 97.91, Test_accy 76.36
2024-10-30 15:03:53,940 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.863,  Train_accy 98.18, Test_accy 76.12
2024-10-30 15:04:08,690 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.863,  Train_accy 98.00, Test_accy 78.14
2024-10-30 15:04:21,779 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.863,  Train_accy 98.02, Test_accy 77.71
2024-10-30 15:04:33,034 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.860,  Train_accy 97.96, Test_accy 77.60
2024-10-30 15:04:44,219 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.862,  Train_accy 98.31, Test_accy 77.43
2024-10-30 15:04:55,158 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.863,  Train_accy 98.58, Test_accy 77.86
2024-10-30 15:05:06,143 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.860,  Train_accy 98.29, Test_accy 76.86
2024-10-30 15:05:18,600 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.859,  Train_accy 98.31, Test_accy 77.38
2024-10-30 15:05:32,661 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.861,  Train_accy 98.04, Test_accy 77.38
2024-10-30 15:05:47,337 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.862,  Train_accy 98.22, Test_accy 78.31
2024-10-30 15:06:01,834 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.860,  Train_accy 98.18, Test_accy 77.19
2024-10-30 15:06:12,216 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.860,  Train_accy 98.16
2024-10-30 15:06:12,217 [pod_foster.py] => do not weight align student!
2024-10-30 15:06:13,806 [pod_foster.py] => darknet eval: 
2024-10-30 15:06:13,807 [pod_foster.py] => CNN top1 curve: 77.6
2024-10-30 15:06:13,807 [pod_foster.py] => CNN top5 curve: 98.55
2024-10-30 15:06:13,807 [pod_foster.py] => CNN: {'total': 77.6, '00-04': 73.2, '05-06': 88.58, 'old': 73.2, 'new': 88.58}
2024-10-30 15:06:13,809 [pod_foster.py] => All params after compression: 3851086
2024-10-30 15:06:13,811 [base.py] => Reducing exemplars...(71 per classes)
2024-10-30 15:06:16,932 [base.py] => Constructing exemplars...(71 per classes)
2024-10-30 15:06:23,951 [trainer.py] => All params: 7701139
2024-10-30 15:06:28,531 [pod_foster.py] => Exemplar size: 497
2024-10-30 15:06:28,532 [trainer.py] => CNN: {'total': 79.83, '00-04': 76.93, '05-06': 87.08, 'old': 76.93, 'new': 87.08}
2024-10-30 15:06:28,532 [trainer.py] => NME: {'total': 73.02, '00-04': 76.47, '05-06': 64.42, 'old': 76.47, 'new': 64.42}
2024-10-30 15:06:28,533 [trainer.py] => CNN top1 curve: [89.93, 79.83]
2024-10-30 15:06:28,533 [trainer.py] => CNN top5 curve: [100.0, 98.64]
2024-10-30 15:06:28,533 [trainer.py] => NME top1 curve: [90.0, 73.02]
2024-10-30 15:06:28,534 [trainer.py] => NME top5 curve: [100.0, 98.71]

2024-10-30 15:06:28,534 [trainer.py] => Average Accuracy (CNN): 84.88
2024-10-30 15:06:28,535 [trainer.py] => Average Accuracy (NME): 81.50999999999999
2024-10-30 15:06:28,536 [trainer.py] => All params: 7701139
2024-10-30 15:06:28,537 [trainer.py] => Trainable params: 3854670
2024-10-30 15:06:28,601 [pod_foster.py] => Learning on 7-9
2024-10-30 15:06:28,603 [pod_foster.py] => All params: 7705241
2024-10-30 15:06:28,605 [pod_foster.py] => Trainable params: 3857746
2024-10-30 15:06:28,719 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-30 15:06:28,732 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-30 15:06:33,366 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.714, Loss_clf 0.711, Loss_fe 0.695, Loss_pod 0.047, Loss_flat 0.261, Train_accy 85.59, Test_accy 66.65
2024-10-30 15:06:49,270 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.233, Loss_clf 0.017, Loss_fe 0.084, Loss_pod 0.038, Loss_flat 0.094, Train_accy 99.76, Test_accy 62.70
2024-10-30 15:07:01,397 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.097, Loss_clf 0.007, Loss_fe 0.012, Loss_pod 0.026, Loss_flat 0.052, Train_accy 99.98, Test_accy 67.83
2024-10-30 15:07:13,968 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.075, Loss_clf 0.005, Loss_fe 0.008, Loss_pod 0.021, Loss_flat 0.041, Train_accy 100.00, Test_accy 67.65
2024-10-30 15:07:26,656 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.056, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.016, Loss_flat 0.030, Train_accy 100.00, Test_accy 67.80
2024-10-30 15:07:39,129 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.676, Loss_clf 0.112, Loss_fe 0.318, Loss_pod 0.065, Loss_flat 0.181, Train_accy 96.40, Test_accy 34.35
2024-10-30 15:07:52,278 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.220, Loss_clf 0.019, Loss_fe 0.047, Loss_pod 0.050, Loss_flat 0.105, Train_accy 99.58, Test_accy 64.81
2024-10-30 15:08:07,538 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.085, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.030, Loss_flat 0.046, Train_accy 100.00, Test_accy 68.06
2024-10-30 15:08:22,104 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.100, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.032, Loss_flat 0.057, Train_accy 100.00, Test_accy 68.09
2024-10-30 15:08:38,308 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.075, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.025, Loss_flat 0.041, Train_accy 100.00, Test_accy 67.94
2024-10-30 15:08:54,453 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.061, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.021, Loss_flat 0.031, Train_accy 100.00, Test_accy 66.87
2024-10-30 15:09:10,828 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.057, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.019, Loss_flat 0.028, Train_accy 100.00, Test_accy 60.98
2024-10-30 15:09:25,361 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.062, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.020, Loss_flat 0.032, Train_accy 100.00, Test_accy 67.19
2024-10-30 15:09:37,721 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.071, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.020, Loss_flat 0.039, Train_accy 100.00, Test_accy 72.72
2024-10-30 15:09:50,073 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.065, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.019, Loss_flat 0.035, Train_accy 99.98, Test_accy 68.63
2024-10-30 15:10:02,833 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.049, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.016, Loss_flat 0.026, Train_accy 100.00, Test_accy 67.13
2024-10-30 15:10:16,643 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.064, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.017, Loss_flat 0.035, Train_accy 100.00, Test_accy 68.54
2024-10-30 15:10:33,686 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.046, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.015, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.50
2024-10-30 15:10:51,419 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.045, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.014, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.17
2024-10-30 15:11:10,891 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.043, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.013, Loss_flat 0.022, Train_accy 100.00, Test_accy 67.07
2024-10-30 15:11:31,292 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.043, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.013, Loss_flat 0.022, Train_accy 100.00, Test_accy 66.59
2024-10-30 15:11:50,363 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.097, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.031, Loss_flat 0.049, Train_accy 99.98, Test_accy 65.41
2024-10-30 15:12:07,410 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.055, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.018, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.74
2024-10-30 15:12:21,574 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.055, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.016, Loss_flat 0.027, Train_accy 100.00, Test_accy 65.98
2024-10-30 15:12:36,029 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.047, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.015, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.00
2024-10-30 15:12:50,131 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.046, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.015, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.80
2024-10-30 15:13:04,003 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.046, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.014, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.81
2024-10-30 15:13:17,720 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.044, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.014, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.80
2024-10-30 15:13:33,085 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.044, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.014, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.28
2024-10-30 15:13:49,757 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.047, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.014, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.24
2024-10-30 15:14:02,936 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.043, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.014, Loss_flat 0.023, Train_accy 100.00
2024-10-30 15:14:02,937 [inc_net.py] => align weights, gamma = 0.5541638731956482 
2024-10-30 15:14:02,939 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-30 15:14:08,455 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.519,  Train_accy 75.45, Test_accy 58.19
2024-10-30 15:14:27,560 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.212,  Train_accy 98.09, Test_accy 72.94
2024-10-30 15:14:45,716 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.214,  Train_accy 98.20, Test_accy 72.70
2024-10-30 15:15:03,877 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.209,  Train_accy 98.71, Test_accy 73.94
2024-10-30 15:15:19,533 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.211,  Train_accy 98.53, Test_accy 73.54
2024-10-30 15:15:31,100 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.208,  Train_accy 98.71, Test_accy 75.43
2024-10-30 15:15:42,540 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.205,  Train_accy 98.67, Test_accy 75.41
2024-10-30 15:15:54,937 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.205,  Train_accy 98.69, Test_accy 76.19
2024-10-30 15:16:06,518 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.203,  Train_accy 98.62, Test_accy 76.67
2024-10-30 15:16:18,235 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.204,  Train_accy 98.84, Test_accy 76.02
2024-10-30 15:16:30,508 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.204,  Train_accy 98.80, Test_accy 75.59
2024-10-30 15:16:46,716 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.204,  Train_accy 98.95, Test_accy 76.48
2024-10-30 15:17:02,472 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.205,  Train_accy 98.93, Test_accy 75.81
2024-10-30 15:17:19,954 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.201,  Train_accy 99.15, Test_accy 76.52
2024-10-30 15:17:34,550 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.203,  Train_accy 98.89, Test_accy 76.41
2024-10-30 15:17:50,933 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.204,  Train_accy 98.93, Test_accy 76.35
2024-10-30 15:18:06,946 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.205,  Train_accy 99.09, Test_accy 76.28
2024-10-30 15:18:22,370 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.199,  Train_accy 98.98, Test_accy 76.30
2024-10-30 15:18:35,538 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.203,  Train_accy 99.00, Test_accy 76.35
2024-10-30 15:18:47,671 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.200,  Train_accy 99.11, Test_accy 76.52
2024-10-30 15:18:59,655 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.201,  Train_accy 99.04, Test_accy 76.44
2024-10-30 15:19:11,049 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.202,  Train_accy 99.07, Test_accy 76.44
2024-10-30 15:19:21,482 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.202,  Train_accy 98.91, Test_accy 76.65
2024-10-30 15:19:34,122 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.204,  Train_accy 99.11, Test_accy 76.30
2024-10-30 15:19:44,397 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.201,  Train_accy 99.11
2024-10-30 15:19:44,398 [pod_foster.py] => do not weight align student!
2024-10-30 15:19:45,957 [pod_foster.py] => darknet eval: 
2024-10-30 15:19:45,957 [pod_foster.py] => CNN top1 curve: 76.8
2024-10-30 15:19:45,958 [pod_foster.py] => CNN top5 curve: 96.65
2024-10-30 15:19:45,958 [pod_foster.py] => CNN: {'total': 76.8, '00-04': 66.47, '05-06': 85.67, '07-08': 93.75, 'old': 71.95, 'new': 93.75}
2024-10-30 15:19:45,959 [pod_foster.py] => All params after compression: 3853138
2024-10-30 15:19:45,959 [base.py] => Reducing exemplars...(55 per classes)
2024-10-30 15:19:49,462 [base.py] => Constructing exemplars...(55 per classes)
2024-10-30 15:19:55,298 [trainer.py] => All params: 7705241
2024-10-30 15:19:59,223 [pod_foster.py] => Exemplar size: 495
2024-10-30 15:19:59,223 [trainer.py] => CNN: {'total': 76.09, '00-04': 64.4, '05-06': 89.25, '07-08': 92.17, 'old': 71.5, 'new': 92.17}
2024-10-30 15:19:59,224 [trainer.py] => NME: {'total': 72.26, '00-04': 66.37, '05-06': 77.25, '07-08': 82.0, 'old': 69.48, 'new': 82.0}
2024-10-30 15:19:59,224 [trainer.py] => CNN top1 curve: [89.93, 79.83, 76.09]
2024-10-30 15:19:59,224 [trainer.py] => CNN top5 curve: [100.0, 98.64, 96.22]
2024-10-30 15:19:59,224 [trainer.py] => NME top1 curve: [90.0, 73.02, 72.26]
2024-10-30 15:19:59,224 [trainer.py] => NME top5 curve: [100.0, 98.71, 95.96]

2024-10-30 15:19:59,224 [trainer.py] => Average Accuracy (CNN): 81.95
2024-10-30 15:19:59,224 [trainer.py] => Average Accuracy (NME): 78.42666666666666
2024-10-30 15:19:59,225 [trainer.py] => Forgetting (CNN): 12.765
