2024-10-26 16:29:23,047 [trainer.py] => config: ./exps/POD_foster.json
2024-10-26 16:29:23,047 [trainer.py] => prefix: cil
2024-10-26 16:29:23,047 [trainer.py] => dataset: hrrp9
2024-10-26 16:29:23,047 [trainer.py] => memory_size: 500
2024-10-26 16:29:23,047 [trainer.py] => memory_per_class: 20
2024-10-26 16:29:23,047 [trainer.py] => fixed_memory: False
2024-10-26 16:29:23,047 [trainer.py] => shuffle: True
2024-10-26 16:29:23,047 [trainer.py] => init_cls: 5
2024-10-26 16:29:23,047 [trainer.py] => increment: 2
2024-10-26 16:29:23,047 [trainer.py] => model_name: POD_foster
2024-10-26 16:29:23,047 [trainer.py] => convnet_type: resnet18
2024-10-26 16:29:23,047 [trainer.py] => init_train: False
2024-10-26 16:29:23,047 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-26 16:29:23,047 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-26 16:29:23,047 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-26 16:29:23,048 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-26 16:29:23,048 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-26 16:29:23,048 [trainer.py] => seed: 1993
2024-10-26 16:29:23,048 [trainer.py] => beta1: 0.96
2024-10-26 16:29:23,048 [trainer.py] => beta2: 0.97
2024-10-26 16:29:23,048 [trainer.py] => oofc: ft
2024-10-26 16:29:23,048 [trainer.py] => is_teacher_wa: True
2024-10-26 16:29:23,048 [trainer.py] => is_student_wa: False
2024-10-26 16:29:23,048 [trainer.py] => is_teacher_la: False
2024-10-26 16:29:23,048 [trainer.py] => is_student_la: True
2024-10-26 16:29:23,048 [trainer.py] => lambda_okd: 0
2024-10-26 16:29:23,048 [trainer.py] => wa_value: 1
2024-10-26 16:29:23,048 [trainer.py] => init_epochs: 0
2024-10-26 16:29:23,048 [trainer.py] => init_lr: 0.1
2024-10-26 16:29:23,048 [trainer.py] => init_weight_decay: 0.0005
2024-10-26 16:29:23,048 [trainer.py] => boosting_epochs: 150
2024-10-26 16:29:23,048 [trainer.py] => compression_epochs: 120
2024-10-26 16:29:23,048 [trainer.py] => lr: 0.1
2024-10-26 16:29:23,048 [trainer.py] => batch_size: 128
2024-10-26 16:29:23,048 [trainer.py] => weight_decay: 0.0005
2024-10-26 16:29:23,049 [trainer.py] => num_workers: 8
2024-10-26 16:29:23,049 [trainer.py] => momentum: 0.9
2024-10-26 16:29:23,049 [trainer.py] => T: 2
2024-10-26 16:29:23,049 [trainer.py] => lambda_c_base: 0.8
2024-10-26 16:29:23,049 [trainer.py] => lambda_f_base: 1.0
2024-10-26 16:29:23,049 [trainer.py] => POD: w
2024-10-26 16:29:23,780 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-26 16:29:23,823 [trainer.py] => All params: 0
2024-10-26 16:29:23,823 [trainer.py] => Trainable params: 0
2024-10-26 16:29:24,987 [pod_foster.py] => Learning on 0-5
2024-10-26 16:29:24,988 [pod_foster.py] => All params: 3849034
2024-10-26 16:29:24,989 [pod_foster.py] => Trainable params: 3849034
2024-10-26 16:29:25,125 [pod_foster.py] => Adaptive factor: 0
2024-10-26 16:29:25,336 [pod_foster.py] => init_train?---False
2024-10-26 16:29:26,307 [base.py] => Reducing exemplars...(100 per classes)
2024-10-26 16:29:26,307 [base.py] => Constructing exemplars...(100 per classes)
2024-10-26 16:29:32,660 [trainer.py] => All params: 3849034
2024-10-26 16:29:33,943 [pod_foster.py] => Exemplar size: 500
2024-10-26 16:29:33,944 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-26 16:29:33,944 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-26 16:29:33,944 [trainer.py] => CNN top1 curve: [89.93]
2024-10-26 16:29:33,944 [trainer.py] => CNN top5 curve: [100.0]
2024-10-26 16:29:33,944 [trainer.py] => NME top1 curve: [90.0]
2024-10-26 16:29:33,944 [trainer.py] => NME top5 curve: [100.0]

2024-10-26 16:29:33,944 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-26 16:29:33,944 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-26 16:29:33,945 [trainer.py] => All params: 3849034
2024-10-26 16:29:33,945 [trainer.py] => Trainable params: 3849034
2024-10-26 16:29:33,988 [pod_foster.py] => Learning on 5-7
2024-10-26 16:29:33,989 [pod_foster.py] => All params: 7701139
2024-10-26 16:29:33,989 [pod_foster.py] => Trainable params: 3854670
2024-10-26 16:29:34,038 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-26 16:29:34,050 [pod_foster.py] => per cls weights : tensor([1., 1., 1., 1., 1., 1., 1.])
2024-10-26 16:29:37,295 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.973, Loss_clf 0.711, Loss_fe 0.661, Loss_pod 0.402, Loss_flat 0.198, Train_accy 83.18, Test_accy 54.00
2024-10-26 16:29:47,703 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.371, Loss_clf 0.021, Loss_fe 0.045, Loss_pod 0.232, Loss_flat 0.073, Train_accy 99.60, Test_accy 69.98
2024-10-26 16:29:57,521 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.294, Loss_clf 0.018, Loss_fe 0.026, Loss_pod 0.199, Loss_flat 0.051, Train_accy 99.62, Test_accy 69.83
2024-10-26 16:30:07,256 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.198, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.152, Loss_flat 0.032, Train_accy 100.00, Test_accy 69.71
2024-10-26 16:30:17,205 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.192, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.145, Loss_flat 0.028, Train_accy 100.00, Test_accy 63.93
2024-10-26 16:30:26,890 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.436, Loss_clf 0.023, Loss_fe 0.042, Loss_pod 0.284, Loss_flat 0.088, Train_accy 99.56, Test_accy 72.60
2024-10-26 16:30:36,503 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.241, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.182, Loss_flat 0.041, Train_accy 100.00, Test_accy 69.19
2024-10-26 16:30:46,369 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.191, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.150, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.60
2024-10-26 16:30:56,199 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.183, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.142, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.45
2024-10-26 16:31:06,066 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.164, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.131, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.31
2024-10-26 16:31:15,848 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.162, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.130, Loss_flat 0.021, Train_accy 100.00, Test_accy 67.10
2024-10-26 16:31:25,770 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.149, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.118, Loss_flat 0.020, Train_accy 100.00, Test_accy 68.24
2024-10-26 16:31:35,557 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.228, Loss_clf 0.013, Loss_fe 0.020, Loss_pod 0.160, Loss_flat 0.036, Train_accy 99.89, Test_accy 67.21
2024-10-26 16:31:45,618 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.169, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.133, Loss_flat 0.024, Train_accy 100.00, Test_accy 68.76
2024-10-26 16:31:55,436 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.149, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.119, Loss_flat 0.020, Train_accy 100.00, Test_accy 68.00
2024-10-26 16:32:05,149 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.140, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.112, Loss_flat 0.019, Train_accy 100.00, Test_accy 66.29
2024-10-26 16:32:15,075 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.138, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.110, Loss_flat 0.019, Train_accy 100.00, Test_accy 68.48
2024-10-26 16:32:24,881 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.137, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.108, Loss_flat 0.019, Train_accy 100.00, Test_accy 67.02
2024-10-26 16:32:34,882 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.103, Loss_flat 0.018, Train_accy 100.00, Test_accy 66.57
2024-10-26 16:32:44,860 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.135, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.104, Loss_flat 0.020, Train_accy 100.00, Test_accy 68.62
2024-10-26 16:32:54,895 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.122, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.095, Loss_flat 0.017, Train_accy 100.00, Test_accy 66.69
2024-10-26 16:33:04,900 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.121, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.095, Loss_flat 0.017, Train_accy 100.00, Test_accy 67.60
2024-10-26 16:33:14,805 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.121, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.094, Loss_flat 0.018, Train_accy 100.00, Test_accy 67.38
2024-10-26 16:33:24,904 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.117, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.091, Loss_flat 0.016, Train_accy 100.00, Test_accy 68.52
2024-10-26 16:33:34,827 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.111, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.086, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.19
2024-10-26 16:33:44,771 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.111, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.086, Loss_flat 0.017, Train_accy 100.00, Test_accy 66.64
2024-10-26 16:33:54,547 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.110, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.085, Loss_flat 0.017, Train_accy 100.00, Test_accy 66.55
2024-10-26 16:34:04,442 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.104, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.080, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.55
2024-10-26 16:34:14,449 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.105, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.079, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.45
2024-10-26 16:34:24,415 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.108, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.083, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.86
2024-10-26 16:34:31,594 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.106, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.081, Loss_flat 0.017, Train_accy 100.00
2024-10-26 16:34:31,595 [inc_net.py] => align weights, gamma = 0.5036171674728394 
2024-10-26 16:34:31,597 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-26 16:34:33,950 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.271,  Train_accy 68.56, Test_accy 65.26
2024-10-26 16:34:42,488 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.995,  Train_accy 93.36, Test_accy 75.76
2024-10-26 16:34:51,332 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.987,  Train_accy 94.18, Test_accy 75.21
2024-10-26 16:35:00,049 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.981,  Train_accy 94.22, Test_accy 77.07
2024-10-26 16:35:08,891 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.982,  Train_accy 94.78, Test_accy 77.69
2024-10-26 16:35:17,684 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.978,  Train_accy 94.73, Test_accy 77.43
2024-10-26 16:35:26,527 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.981,  Train_accy 94.58, Test_accy 77.79
2024-10-26 16:35:35,354 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.980,  Train_accy 94.71, Test_accy 76.86
2024-10-26 16:35:44,298 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.976,  Train_accy 94.93, Test_accy 78.12
2024-10-26 16:35:53,133 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.974,  Train_accy 94.89, Test_accy 77.31
2024-10-26 16:36:01,919 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.976,  Train_accy 95.02, Test_accy 77.52
2024-10-26 16:36:10,606 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.975,  Train_accy 94.80, Test_accy 78.50
2024-10-26 16:36:19,577 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.975,  Train_accy 94.78, Test_accy 78.00
2024-10-26 16:36:28,329 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.976,  Train_accy 95.47, Test_accy 77.55
2024-10-26 16:36:37,130 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.976,  Train_accy 95.11, Test_accy 79.14
2024-10-26 16:36:45,940 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.975,  Train_accy 95.33, Test_accy 79.02
2024-10-26 16:36:54,924 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.973,  Train_accy 95.11, Test_accy 78.62
2024-10-26 16:37:03,774 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.974,  Train_accy 95.16, Test_accy 78.69
2024-10-26 16:37:12,549 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.975,  Train_accy 95.44, Test_accy 79.10
2024-10-26 16:37:21,508 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.972,  Train_accy 95.51, Test_accy 77.95
2024-10-26 16:37:30,157 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.972,  Train_accy 95.24, Test_accy 79.02
2024-10-26 16:37:38,763 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.973,  Train_accy 95.24, Test_accy 78.81
2024-10-26 16:37:47,413 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.974,  Train_accy 95.47, Test_accy 79.90
2024-10-26 16:37:56,242 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.972,  Train_accy 95.31, Test_accy 78.45
2024-10-26 16:38:02,668 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.972,  Train_accy 95.38
2024-10-26 16:38:02,668 [pod_foster.py] => do not weight align student!
2024-10-26 16:38:03,302 [pod_foster.py] => darknet eval: 
2024-10-26 16:38:03,303 [pod_foster.py] => CNN top1 curve: 78.9
2024-10-26 16:38:03,303 [pod_foster.py] => CNN top5 curve: 98.57
2024-10-26 16:38:03,303 [pod_foster.py] => CNN: {'total': 78.9, '00-04': 76.13, '05-06': 85.83, 'old': 76.13, 'new': 85.83}
2024-10-26 16:38:03,304 [pod_foster.py] => All params after compression: 3851086
2024-10-26 16:38:03,305 [base.py] => Reducing exemplars...(71 per classes)
2024-10-26 16:38:04,589 [base.py] => Constructing exemplars...(71 per classes)
2024-10-26 16:38:07,901 [trainer.py] => All params: 7701139
2024-10-26 16:38:09,929 [pod_foster.py] => Exemplar size: 497
2024-10-26 16:38:09,929 [trainer.py] => CNN: {'total': 80.57, '00-04': 79.5, '05-06': 83.25, 'old': 79.5, 'new': 83.25}
2024-10-26 16:38:09,929 [trainer.py] => NME: {'total': 73.36, '00-04': 78.0, '05-06': 61.75, 'old': 78.0, 'new': 61.75}
2024-10-26 16:38:09,929 [trainer.py] => CNN top1 curve: [89.93, 80.57]
2024-10-26 16:38:09,929 [trainer.py] => CNN top5 curve: [100.0, 98.48]
2024-10-26 16:38:09,929 [trainer.py] => NME top1 curve: [90.0, 73.36]
2024-10-26 16:38:09,929 [trainer.py] => NME top5 curve: [100.0, 98.93]

2024-10-26 16:38:09,929 [trainer.py] => Average Accuracy (CNN): 85.25
2024-10-26 16:38:09,930 [trainer.py] => Average Accuracy (NME): 81.68
2024-10-26 16:38:09,930 [trainer.py] => All params: 7701139
2024-10-26 16:38:09,931 [trainer.py] => Trainable params: 3854670
2024-10-26 16:38:09,971 [pod_foster.py] => Learning on 7-9
2024-10-26 16:38:09,972 [pod_foster.py] => All params: 7705241
2024-10-26 16:38:09,972 [pod_foster.py] => Trainable params: 3857746
2024-10-26 16:38:10,006 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-26 16:38:10,019 [pod_foster.py] => per cls weights : tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])
2024-10-26 16:38:12,959 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.066, Loss_clf 0.682, Loss_fe 0.660, Loss_pod 0.516, Loss_flat 0.209, Train_accy 85.48, Test_accy 67.80
2024-10-26 16:38:22,740 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.353, Loss_clf 0.011, Loss_fe 0.049, Loss_pod 0.238, Loss_flat 0.055, Train_accy 99.98, Test_accy 66.19
2024-10-26 16:38:32,889 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.231, Loss_clf 0.008, Loss_fe 0.014, Loss_pod 0.177, Loss_flat 0.031, Train_accy 99.98, Test_accy 66.44
2024-10-26 16:38:43,068 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.333, Loss_clf 0.011, Loss_fe 0.018, Loss_pod 0.251, Loss_flat 0.054, Train_accy 99.91, Test_accy 67.93
2024-10-26 16:38:52,977 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.217, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.176, Loss_flat 0.028, Train_accy 100.00, Test_accy 68.09
2024-10-26 16:39:03,076 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.209, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.169, Loss_flat 0.026, Train_accy 100.00, Test_accy 65.87
2024-10-26 16:39:13,054 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.549, Loss_clf 0.042, Loss_fe 0.078, Loss_pod 0.343, Loss_flat 0.085, Train_accy 98.87, Test_accy 62.43
2024-10-26 16:39:23,002 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.204, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.167, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.57
2024-10-26 16:39:33,055 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.207, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.167, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.06
2024-10-26 16:39:43,186 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.192, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.158, Loss_flat 0.024, Train_accy 100.00, Test_accy 68.87
2024-10-26 16:39:53,565 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.179, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.147, Loss_flat 0.021, Train_accy 100.00, Test_accy 67.15
2024-10-26 16:40:03,714 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.175, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.142, Loss_flat 0.020, Train_accy 100.00, Test_accy 61.41
2024-10-26 16:40:13,661 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.169, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.138, Loss_flat 0.020, Train_accy 100.00, Test_accy 66.35
2024-10-26 16:40:23,753 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.183, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.147, Loss_flat 0.023, Train_accy 100.00, Test_accy 73.11
2024-10-26 16:40:33,842 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.251, Loss_clf 0.010, Loss_fe 0.013, Loss_pod 0.191, Loss_flat 0.037, Train_accy 99.87, Test_accy 70.26
2024-10-26 16:40:43,882 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.163, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.132, Loss_flat 0.019, Train_accy 100.00, Test_accy 66.57
2024-10-26 16:40:54,153 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.154, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.126, Loss_flat 0.019, Train_accy 100.00, Test_accy 66.78
2024-10-26 16:41:04,362 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.158, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.130, Loss_flat 0.018, Train_accy 100.00, Test_accy 64.04
2024-10-26 16:41:14,205 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.150, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.122, Loss_flat 0.018, Train_accy 100.00, Test_accy 65.52
2024-10-26 16:41:24,299 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.141, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.115, Loss_flat 0.017, Train_accy 100.00, Test_accy 65.28
2024-10-26 16:41:34,484 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.148, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.121, Loss_flat 0.017, Train_accy 100.00, Test_accy 64.54
2024-10-26 16:41:44,681 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.147, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.116, Loss_flat 0.020, Train_accy 100.00, Test_accy 65.83
2024-10-26 16:41:54,816 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.141, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.113, Loss_flat 0.017, Train_accy 100.00, Test_accy 66.72
2024-10-26 16:42:04,947 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.142, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.113, Loss_flat 0.018, Train_accy 100.00, Test_accy 64.37
2024-10-26 16:42:15,079 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.105, Loss_flat 0.016, Train_accy 100.00, Test_accy 65.43
2024-10-26 16:42:25,168 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.105, Loss_flat 0.016, Train_accy 100.00, Test_accy 66.28
2024-10-26 16:42:35,131 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.123, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.097, Loss_flat 0.016, Train_accy 100.00, Test_accy 66.67
2024-10-26 16:42:45,458 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.119, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.095, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.24
2024-10-26 16:42:55,503 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.117, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.092, Loss_flat 0.016, Train_accy 100.00, Test_accy 66.81
2024-10-26 16:43:05,545 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.121, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.095, Loss_flat 0.016, Train_accy 100.00, Test_accy 64.02
2024-10-26 16:43:12,823 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.113, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.089, Loss_flat 0.015, Train_accy 100.00
2024-10-26 16:43:12,825 [inc_net.py] => align weights, gamma = 0.5200619697570801 
2024-10-26 16:43:12,826 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-26 16:43:15,351 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.524,  Train_accy 75.23, Test_accy 59.00
2024-10-26 16:43:23,955 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.247,  Train_accy 97.46, Test_accy 73.19
2024-10-26 16:43:32,989 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.244,  Train_accy 97.91, Test_accy 73.44
2024-10-26 16:43:41,665 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.240,  Train_accy 98.35, Test_accy 75.04
2024-10-26 16:43:50,222 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.240,  Train_accy 97.93, Test_accy 74.96
2024-10-26 16:43:58,761 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.238,  Train_accy 98.40, Test_accy 76.98
2024-10-26 16:44:07,673 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.236,  Train_accy 98.38, Test_accy 76.65
2024-10-26 16:44:16,381 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.237,  Train_accy 98.40, Test_accy 77.56
2024-10-26 16:44:24,989 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.234,  Train_accy 98.31, Test_accy 77.89
2024-10-26 16:44:33,567 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.235,  Train_accy 98.69, Test_accy 77.19
2024-10-26 16:44:42,200 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.235,  Train_accy 98.51, Test_accy 76.70
2024-10-26 16:44:50,701 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.235,  Train_accy 98.58, Test_accy 77.44
2024-10-26 16:44:59,348 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.235,  Train_accy 98.55, Test_accy 76.91
2024-10-26 16:45:07,980 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.233,  Train_accy 98.58, Test_accy 77.37
2024-10-26 16:45:16,704 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.234,  Train_accy 98.55, Test_accy 77.72
2024-10-26 16:45:25,701 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.235,  Train_accy 98.62, Test_accy 77.46
2024-10-26 16:45:34,615 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.236,  Train_accy 98.82, Test_accy 77.06
2024-10-26 16:45:43,470 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.230,  Train_accy 98.73, Test_accy 77.06
2024-10-26 16:45:52,252 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.233,  Train_accy 98.67, Test_accy 77.48
2024-10-26 16:46:01,169 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.230,  Train_accy 98.87, Test_accy 77.37
2024-10-26 16:46:09,761 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.231,  Train_accy 98.84, Test_accy 77.19
2024-10-26 16:46:18,528 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.233,  Train_accy 98.73, Test_accy 77.31
2024-10-26 16:46:27,208 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.233,  Train_accy 98.62, Test_accy 77.70
2024-10-26 16:46:35,865 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.234,  Train_accy 98.80, Test_accy 77.00
2024-10-26 16:46:42,646 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.232,  Train_accy 98.71
2024-10-26 16:46:42,646 [pod_foster.py] => do not weight align student!
2024-10-26 16:46:43,321 [pod_foster.py] => darknet eval: 
2024-10-26 16:46:43,322 [pod_foster.py] => CNN top1 curve: 77.93
2024-10-26 16:46:43,322 [pod_foster.py] => CNN top5 curve: 96.67
2024-10-26 16:46:43,322 [pod_foster.py] => CNN: {'total': 77.93, '00-04': 68.5, '05-06': 85.83, '07-08': 93.58, 'old': 73.45, 'new': 93.58}
2024-10-26 16:46:43,324 [pod_foster.py] => All params after compression: 3853138
2024-10-26 16:46:43,325 [base.py] => Reducing exemplars...(55 per classes)
2024-10-26 16:46:44,694 [base.py] => Constructing exemplars...(55 per classes)
2024-10-26 16:46:47,384 [trainer.py] => All params: 7705241
2024-10-26 16:46:49,575 [pod_foster.py] => Exemplar size: 495
2024-10-26 16:46:49,575 [trainer.py] => CNN: {'total': 76.74, '00-04': 65.73, '05-06': 89.92, '07-08': 91.08, 'old': 72.64, 'new': 91.08}
2024-10-26 16:46:49,575 [trainer.py] => NME: {'total': 73.44, '00-04': 68.2, '05-06': 77.17, '07-08': 82.83, 'old': 70.76, 'new': 82.83}
2024-10-26 16:46:49,575 [trainer.py] => CNN top1 curve: [89.93, 80.57, 76.74]
2024-10-26 16:46:49,576 [trainer.py] => CNN top5 curve: [100.0, 98.48, 96.44]
2024-10-26 16:46:49,576 [trainer.py] => NME top1 curve: [90.0, 73.36, 73.44]
2024-10-26 16:46:49,576 [trainer.py] => NME top5 curve: [100.0, 98.93, 96.19]

2024-10-26 16:46:49,576 [trainer.py] => Average Accuracy (CNN): 82.41333333333334
2024-10-26 16:46:49,576 [trainer.py] => Average Accuracy (NME): 78.93333333333334
2024-10-26 16:46:49,577 [trainer.py] => Forgetting (CNN): 12.100000000000001
