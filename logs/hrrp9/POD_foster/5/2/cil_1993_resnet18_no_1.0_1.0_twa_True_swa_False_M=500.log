2024-10-25 15:46:49,900 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 15:46:49,901 [trainer.py] => prefix: cil
2024-10-25 15:46:49,901 [trainer.py] => dataset: hrrp9
2024-10-25 15:46:49,901 [trainer.py] => memory_size: 500
2024-10-25 15:46:49,901 [trainer.py] => memory_per_class: 20
2024-10-25 15:46:49,901 [trainer.py] => fixed_memory: False
2024-10-25 15:46:49,901 [trainer.py] => shuffle: True
2024-10-25 15:46:49,901 [trainer.py] => init_cls: 5
2024-10-25 15:46:49,901 [trainer.py] => increment: 2
2024-10-25 15:46:49,901 [trainer.py] => model_name: POD_foster
2024-10-25 15:46:49,901 [trainer.py] => convnet_type: resnet18
2024-10-25 15:46:49,901 [trainer.py] => init_train: False
2024-10-25 15:46:49,901 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 15:46:49,901 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 15:46:49,901 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 15:46:49,901 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 15:46:49,901 [trainer.py] => device: [device(type='cuda', index=4)]
2024-10-25 15:46:49,901 [trainer.py] => seed: 1993
2024-10-25 15:46:49,901 [trainer.py] => beta1: 0.96
2024-10-25 15:46:49,902 [trainer.py] => beta2: 0.97
2024-10-25 15:46:49,902 [trainer.py] => oofc: ft
2024-10-25 15:46:49,902 [trainer.py] => is_teacher_wa: True
2024-10-25 15:46:49,902 [trainer.py] => is_student_wa: False
2024-10-25 15:46:49,902 [trainer.py] => lambda_okd: 0
2024-10-25 15:46:49,902 [trainer.py] => wa_value: 1
2024-10-25 15:46:49,902 [trainer.py] => init_epochs: 0
2024-10-25 15:46:49,902 [trainer.py] => init_lr: 0.1
2024-10-25 15:46:49,902 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 15:46:49,902 [trainer.py] => boosting_epochs: 150
2024-10-25 15:46:49,902 [trainer.py] => compression_epochs: 120
2024-10-25 15:46:49,902 [trainer.py] => lr: 0.1
2024-10-25 15:46:49,902 [trainer.py] => batch_size: 128
2024-10-25 15:46:49,902 [trainer.py] => weight_decay: 0.0005
2024-10-25 15:46:49,902 [trainer.py] => num_workers: 8
2024-10-25 15:46:49,902 [trainer.py] => momentum: 0.9
2024-10-25 15:46:49,902 [trainer.py] => T: 2
2024-10-25 15:46:49,902 [trainer.py] => lambda_c_base: 1.0
2024-10-25 15:46:49,902 [trainer.py] => lambda_f_base: 1.0
2024-10-25 15:46:49,902 [trainer.py] => POD: c
2024-10-25 15:46:50,632 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 15:46:50,687 [trainer.py] => All params: 0
2024-10-25 15:46:50,688 [trainer.py] => Trainable params: 0
2024-10-25 15:46:51,861 [pod_foster.py] => Learning on 0-5
2024-10-25 15:46:51,862 [pod_foster.py] => All params: 3849034
2024-10-25 15:46:51,862 [pod_foster.py] => Trainable params: 3849034
2024-10-25 15:46:51,939 [pod_foster.py] => Adaptive factor: 0
2024-10-25 15:46:52,197 [pod_foster.py] => init_train?---False
2024-10-25 15:46:53,256 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 15:46:53,256 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 15:46:59,762 [trainer.py] => All params: 3849034
2024-10-25 15:47:00,975 [pod_foster.py] => Exemplar size: 500
2024-10-25 15:47:00,975 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 15:47:00,975 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 15:47:00,975 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 15:47:00,975 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 15:47:00,975 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 15:47:00,975 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 15:47:00,975 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 15:47:00,975 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 15:47:00,976 [trainer.py] => All params: 3849034
2024-10-25 15:47:00,976 [trainer.py] => Trainable params: 3849034
2024-10-25 15:47:01,006 [pod_foster.py] => Learning on 5-7
2024-10-25 15:47:01,007 [pod_foster.py] => All params: 7701139
2024-10-25 15:47:01,008 [pod_foster.py] => Trainable params: 3854670
2024-10-25 15:47:01,050 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 15:47:01,059 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 15:47:04,096 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 2.068, Loss_clf 0.704, Loss_fe 0.663, Loss_pod 0.486, Loss_flat 0.215, Train_accy 83.13, Test_accy 53.31
2024-10-25 15:47:13,369 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.450, Loss_clf 0.020, Loss_fe 0.038, Loss_pod 0.318, Loss_flat 0.074, Train_accy 99.89, Test_accy 67.81
2024-10-25 15:47:22,925 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.367, Loss_clf 0.016, Loss_fe 0.022, Loss_pod 0.271, Loss_flat 0.058, Train_accy 99.87, Test_accy 71.52
2024-10-25 15:47:33,078 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.321, Loss_clf 0.010, Loss_fe 0.013, Loss_pod 0.253, Loss_flat 0.045, Train_accy 100.00, Test_accy 66.67
2024-10-25 15:47:43,478 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.315, Loss_clf 0.014, Loss_fe 0.017, Loss_pod 0.236, Loss_flat 0.047, Train_accy 99.98, Test_accy 66.21
2024-10-25 15:47:54,238 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.589, Loss_clf 0.027, Loss_fe 0.052, Loss_pod 0.403, Loss_flat 0.108, Train_accy 99.62, Test_accy 69.93
2024-10-25 15:48:04,894 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.377, Loss_clf 0.012, Loss_fe 0.013, Loss_pod 0.297, Loss_flat 0.056, Train_accy 100.00, Test_accy 71.17
2024-10-25 15:48:15,004 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.354, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.291, Loss_flat 0.043, Train_accy 100.00, Test_accy 67.19
2024-10-25 15:48:25,700 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.312, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.256, Loss_flat 0.038, Train_accy 100.00, Test_accy 67.62
2024-10-25 15:48:36,907 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.275, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.223, Loss_flat 0.035, Train_accy 100.00, Test_accy 67.88
2024-10-25 15:48:47,396 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.278, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.228, Loss_flat 0.034, Train_accy 100.00, Test_accy 65.19
2024-10-25 15:48:57,539 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.266, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.217, Loss_flat 0.032, Train_accy 100.00, Test_accy 68.38
2024-10-25 15:49:07,801 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.301, Loss_clf 0.012, Loss_fe 0.014, Loss_pod 0.231, Loss_flat 0.044, Train_accy 99.93, Test_accy 66.83
2024-10-25 15:49:18,022 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.726, Loss_clf 0.076, Loss_fe 0.127, Loss_pod 0.406, Loss_flat 0.117, Train_accy 97.64, Test_accy 71.67
2024-10-25 15:49:27,619 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.295, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.237, Loss_flat 0.041, Train_accy 100.00, Test_accy 68.88
2024-10-25 15:49:37,879 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.246, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.196, Loss_flat 0.034, Train_accy 100.00, Test_accy 68.60
2024-10-25 15:49:47,797 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.233, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.186, Loss_flat 0.032, Train_accy 100.00, Test_accy 67.43
2024-10-25 15:49:57,678 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.221, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.174, Loss_flat 0.031, Train_accy 100.00, Test_accy 65.45
2024-10-25 15:50:07,682 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.216, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.170, Loss_flat 0.030, Train_accy 100.00, Test_accy 68.62
2024-10-25 15:50:17,811 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.211, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.164, Loss_flat 0.031, Train_accy 100.00, Test_accy 67.81
2024-10-25 15:50:27,840 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.206, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.162, Loss_flat 0.029, Train_accy 100.00, Test_accy 66.83
2024-10-25 15:50:38,260 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.190, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.146, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.19
2024-10-25 15:50:48,907 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.190, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.145, Loss_flat 0.029, Train_accy 100.00, Test_accy 66.55
2024-10-25 15:50:59,480 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.183, Loss_clf 0.008, Loss_fe 0.008, Loss_pod 0.139, Loss_flat 0.028, Train_accy 100.00, Test_accy 68.38
2024-10-25 15:51:09,885 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.171, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.128, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.83
2024-10-25 15:51:20,405 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.174, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.132, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.52
2024-10-25 15:51:30,830 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.167, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.125, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.38
2024-10-25 15:51:41,618 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.157, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.115, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.36
2024-10-25 15:51:51,930 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.154, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.111, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.52
2024-10-25 15:52:02,483 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.161, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.119, Loss_flat 0.027, Train_accy 100.00, Test_accy 68.05
2024-10-25 15:52:10,234 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.152, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.110, Loss_flat 0.028, Train_accy 100.00
2024-10-25 15:52:10,236 [inc_net.py] => align weights, gamma = 0.4757048189640045 
2024-10-25 15:52:10,237 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 15:52:12,756 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.363,  Train_accy 64.96, Test_accy 70.69
2024-10-25 15:52:22,427 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.141,  Train_accy 90.62, Test_accy 77.64
2024-10-25 15:52:32,072 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.137,  Train_accy 92.51, Test_accy 77.76
2024-10-25 15:52:41,732 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.130,  Train_accy 92.47, Test_accy 78.52
2024-10-25 15:52:50,936 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.133,  Train_accy 92.53, Test_accy 78.74
2024-10-25 15:53:00,082 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.130,  Train_accy 93.22, Test_accy 79.26
2024-10-25 15:53:09,449 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.131,  Train_accy 92.16, Test_accy 79.02
2024-10-25 15:53:18,871 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.130,  Train_accy 92.44, Test_accy 78.98
2024-10-25 15:53:28,520 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.128,  Train_accy 93.07, Test_accy 79.31
2024-10-25 15:53:37,906 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.126,  Train_accy 93.20, Test_accy 79.02
2024-10-25 15:53:47,184 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.127,  Train_accy 93.09, Test_accy 78.71
2024-10-25 15:53:56,876 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.125,  Train_accy 92.93, Test_accy 79.14
2024-10-25 15:54:06,330 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.127,  Train_accy 93.07, Test_accy 79.55
2024-10-25 15:54:15,856 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.127,  Train_accy 93.51, Test_accy 78.90
2024-10-25 15:54:25,374 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.128,  Train_accy 93.00, Test_accy 79.69
2024-10-25 15:54:35,033 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.127,  Train_accy 93.13, Test_accy 79.45
2024-10-25 15:54:44,642 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.125,  Train_accy 92.89, Test_accy 79.76
2024-10-25 15:54:54,357 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.126,  Train_accy 93.20, Test_accy 79.60
2024-10-25 15:55:03,933 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.127,  Train_accy 93.11, Test_accy 79.90
2024-10-25 15:55:13,498 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.125,  Train_accy 93.31, Test_accy 79.45
2024-10-25 15:55:23,052 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.123,  Train_accy 93.27, Test_accy 79.69
2024-10-25 15:55:32,501 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.125,  Train_accy 93.16, Test_accy 79.76
2024-10-25 15:55:42,225 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.126,  Train_accy 93.44, Test_accy 79.95
2024-10-25 15:55:51,983 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.125,  Train_accy 93.49, Test_accy 79.57
2024-10-25 15:55:59,021 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.123,  Train_accy 93.31
2024-10-25 15:55:59,021 [pod_foster.py] => do not weight align student!
2024-10-25 15:55:59,701 [pod_foster.py] => darknet eval: 
2024-10-25 15:55:59,701 [pod_foster.py] => CNN top1 curve: 79.81
2024-10-25 15:55:59,701 [pod_foster.py] => CNN top5 curve: 98.69
2024-10-25 15:55:59,702 [pod_foster.py] => All params after compression: 3851086
2024-10-25 15:55:59,703 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 15:56:00,975 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 15:56:07,615 [trainer.py] => All params: 7701139
2024-10-25 15:56:11,522 [pod_foster.py] => Exemplar size: 497
2024-10-25 15:56:11,523 [trainer.py] => CNN: {'total': 80.24, '00-04': 80.17, '05-06': 80.42, 'old': 80.17, 'new': 80.42}
2024-10-25 15:56:11,523 [trainer.py] => NME: {'total': 73.48, '00-04': 79.37, '05-06': 58.75, 'old': 79.37, 'new': 58.75}
2024-10-25 15:56:11,523 [trainer.py] => CNN top1 curve: [89.93, 80.24]
2024-10-25 15:56:11,523 [trainer.py] => CNN top5 curve: [100.0, 98.48]
2024-10-25 15:56:11,523 [trainer.py] => NME top1 curve: [90.0, 73.48]
2024-10-25 15:56:11,523 [trainer.py] => NME top5 curve: [100.0, 98.88]

2024-10-25 15:56:11,523 [trainer.py] => Average Accuracy (CNN): 85.08500000000001
2024-10-25 15:56:11,523 [trainer.py] => Average Accuracy (NME): 81.74000000000001
2024-10-25 15:56:11,524 [trainer.py] => All params: 7701139
2024-10-25 15:56:11,525 [trainer.py] => Trainable params: 3854670
2024-10-25 15:56:11,573 [pod_foster.py] => Learning on 7-9
2024-10-25 15:56:11,575 [pod_foster.py] => All params: 7705241
2024-10-25 15:56:11,575 [pod_foster.py] => Trainable params: 3857746
2024-10-25 15:56:11,615 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 15:56:11,628 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 15:56:14,865 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.113, Loss_clf 0.657, Loss_fe 0.665, Loss_pod 0.587, Loss_flat 0.203, Train_accy 85.28, Test_accy 60.59
2024-10-25 15:56:25,470 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.503, Loss_clf 0.016, Loss_fe 0.057, Loss_pod 0.368, Loss_flat 0.061, Train_accy 100.00, Test_accy 61.57
2024-10-25 15:56:36,005 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.377, Loss_clf 0.013, Loss_fe 0.020, Loss_pod 0.303, Loss_flat 0.041, Train_accy 100.00, Test_accy 66.98
2024-10-25 15:56:47,068 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.348, Loss_clf 0.011, Loss_fe 0.015, Loss_pod 0.284, Loss_flat 0.039, Train_accy 99.98, Test_accy 66.13
2024-10-25 15:56:57,621 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.317, Loss_clf 0.009, Loss_fe 0.012, Loss_pod 0.264, Loss_flat 0.032, Train_accy 100.00, Test_accy 68.70
2024-10-25 15:57:08,200 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.599, Loss_clf 0.038, Loss_fe 0.072, Loss_pod 0.410, Loss_flat 0.079, Train_accy 99.11, Test_accy 53.76
2024-10-25 15:57:18,793 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.400, Loss_clf 0.009, Loss_fe 0.012, Loss_pod 0.334, Loss_flat 0.045, Train_accy 100.00, Test_accy 67.35
2024-10-25 15:57:29,617 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.330, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.282, Loss_flat 0.031, Train_accy 100.00, Test_accy 65.19
2024-10-25 15:57:40,293 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.324, Loss_clf 0.009, Loss_fe 0.011, Loss_pod 0.268, Loss_flat 0.036, Train_accy 100.00, Test_accy 65.35
2024-10-25 15:57:50,896 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.309, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.262, Loss_flat 0.031, Train_accy 100.00, Test_accy 68.22
2024-10-25 15:58:01,782 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.286, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.241, Loss_flat 0.028, Train_accy 100.00, Test_accy 65.26
2024-10-25 15:58:12,783 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.291, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.246, Loss_flat 0.027, Train_accy 100.00, Test_accy 64.72
2024-10-25 15:58:23,234 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.265, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.220, Loss_flat 0.029, Train_accy 100.00, Test_accy 63.74
2024-10-25 15:58:33,645 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.328, Loss_clf 0.011, Loss_fe 0.016, Loss_pod 0.257, Loss_flat 0.043, Train_accy 99.98, Test_accy 72.07
2024-10-25 15:58:44,102 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.293, Loss_clf 0.010, Loss_fe 0.012, Loss_pod 0.234, Loss_flat 0.037, Train_accy 99.93, Test_accy 67.13
2024-10-25 15:58:54,548 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.281, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.230, Loss_flat 0.032, Train_accy 100.00, Test_accy 64.11
2024-10-25 15:59:05,137 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.262, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.215, Loss_flat 0.030, Train_accy 100.00, Test_accy 64.43
2024-10-25 15:59:15,444 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.228, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.187, Loss_flat 0.026, Train_accy 100.00, Test_accy 62.56
2024-10-25 15:59:26,011 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.221, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.181, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.44
2024-10-25 15:59:36,205 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.213, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.174, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.50
2024-10-25 15:59:46,818 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.210, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.169, Loss_flat 0.025, Train_accy 100.00, Test_accy 62.26
2024-10-25 15:59:56,962 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.251, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.197, Loss_flat 0.035, Train_accy 100.00, Test_accy 65.17
2024-10-25 16:00:06,648 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.208, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.165, Loss_flat 0.027, Train_accy 100.00, Test_accy 65.93
2024-10-25 16:00:16,837 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.204, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.158, Loss_flat 0.027, Train_accy 100.00, Test_accy 63.13
2024-10-25 16:00:27,352 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.175, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.136, Loss_flat 0.024, Train_accy 100.00, Test_accy 63.98
2024-10-25 16:00:37,400 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.174, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.135, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.65
2024-10-25 16:00:47,629 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.167, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.127, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.67
2024-10-25 16:00:57,516 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.158, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.120, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.33
2024-10-25 16:01:07,410 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.148, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.110, Loss_flat 0.024, Train_accy 100.00, Test_accy 64.78
2024-10-25 16:01:17,278 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.157, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.117, Loss_flat 0.025, Train_accy 100.00, Test_accy 61.69
2024-10-25 16:01:24,346 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.145, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.108, Loss_flat 0.024, Train_accy 100.00
2024-10-25 16:01:24,347 [inc_net.py] => align weights, gamma = 0.4685855209827423 
2024-10-25 16:01:24,348 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 16:01:26,854 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.658,  Train_accy 69.27, Test_accy 67.59
2024-10-25 16:01:36,236 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.475,  Train_accy 91.33, Test_accy 74.56
2024-10-25 16:01:45,651 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.475,  Train_accy 92.84, Test_accy 74.31
2024-10-25 16:01:54,772 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.470,  Train_accy 93.28, Test_accy 74.78
2024-10-25 16:02:04,050 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.471,  Train_accy 92.97, Test_accy 74.94
2024-10-25 16:02:13,215 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.470,  Train_accy 93.71, Test_accy 76.26
2024-10-25 16:02:22,247 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.466,  Train_accy 93.15, Test_accy 76.65
2024-10-25 16:02:31,674 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.468,  Train_accy 93.53, Test_accy 76.91
2024-10-25 16:02:40,456 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.466,  Train_accy 93.55, Test_accy 77.50
2024-10-25 16:02:49,622 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.467,  Train_accy 93.95, Test_accy 77.15
2024-10-25 16:02:58,541 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.467,  Train_accy 93.13, Test_accy 76.46
2024-10-25 16:03:07,102 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.467,  Train_accy 93.42, Test_accy 77.15
2024-10-25 16:03:15,951 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.467,  Train_accy 93.37, Test_accy 77.33
2024-10-25 16:03:25,127 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.463,  Train_accy 93.73, Test_accy 77.61
2024-10-25 16:03:33,957 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.467,  Train_accy 93.73, Test_accy 77.20
2024-10-25 16:03:43,245 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.467,  Train_accy 93.44, Test_accy 76.94
2024-10-25 16:03:52,557 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.468,  Train_accy 93.77, Test_accy 77.04
2024-10-25 16:04:01,903 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.462,  Train_accy 93.82, Test_accy 76.78
2024-10-25 16:04:11,110 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.465,  Train_accy 93.77, Test_accy 77.24
2024-10-25 16:04:20,444 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.460,  Train_accy 93.77, Test_accy 77.67
2024-10-25 16:04:29,731 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.463,  Train_accy 93.75, Test_accy 77.43
2024-10-25 16:04:38,884 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.464,  Train_accy 93.75, Test_accy 77.33
2024-10-25 16:04:48,061 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.466,  Train_accy 93.91, Test_accy 77.46
2024-10-25 16:04:57,348 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.467,  Train_accy 93.66, Test_accy 77.04
2024-10-25 16:05:04,125 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.464,  Train_accy 93.88
2024-10-25 16:05:04,126 [pod_foster.py] => do not weight align student!
2024-10-25 16:05:04,896 [pod_foster.py] => darknet eval: 
2024-10-25 16:05:04,896 [pod_foster.py] => CNN top1 curve: 77.63
2024-10-25 16:05:04,896 [pod_foster.py] => CNN top5 curve: 96.3
2024-10-25 16:05:04,897 [pod_foster.py] => All params after compression: 3853138
2024-10-25 16:05:04,898 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 16:05:06,462 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 16:05:09,517 [trainer.py] => All params: 7705241
2024-10-25 16:05:11,908 [pod_foster.py] => Exemplar size: 495
2024-10-25 16:05:11,909 [trainer.py] => CNN: {'total': 75.5, '00-04': 66.4, '05-06': 88.0, '07-08': 85.75, 'old': 72.57, 'new': 85.75}
2024-10-25 16:05:11,909 [trainer.py] => NME: {'total': 73.41, '00-04': 70.3, '05-06': 75.25, '07-08': 79.33, 'old': 71.71, 'new': 79.33}
2024-10-25 16:05:11,909 [trainer.py] => CNN top1 curve: [89.93, 80.24, 75.5]
2024-10-25 16:05:11,909 [trainer.py] => CNN top5 curve: [100.0, 98.48, 96.46]
2024-10-25 16:05:11,909 [trainer.py] => NME top1 curve: [90.0, 73.48, 73.41]
2024-10-25 16:05:11,909 [trainer.py] => NME top5 curve: [100.0, 98.88, 96.61]

2024-10-25 16:05:11,910 [trainer.py] => Average Accuracy (CNN): 81.89
2024-10-25 16:05:11,910 [trainer.py] => Average Accuracy (NME): 78.96333333333334
2024-10-25 16:05:11,911 [trainer.py] => Forgetting (CNN): 11.765
