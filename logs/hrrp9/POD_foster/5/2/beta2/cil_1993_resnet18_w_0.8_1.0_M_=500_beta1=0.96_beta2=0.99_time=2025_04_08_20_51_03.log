2025-04-08 20:51:03,943 [trainer.py] => config: ./exps/POD_foster.json
2025-04-08 20:51:03,944 [trainer.py] => prefix: cil
2025-04-08 20:51:03,944 [trainer.py] => dataset: hrrp9
2025-04-08 20:51:03,944 [trainer.py] => memory_size: 500
2025-04-08 20:51:03,944 [trainer.py] => memory_per_class: 20
2025-04-08 20:51:03,944 [trainer.py] => fixed_memory: False
2025-04-08 20:51:03,944 [trainer.py] => shuffle: True
2025-04-08 20:51:03,944 [trainer.py] => init_cls: 5
2025-04-08 20:51:03,944 [trainer.py] => increment: 2
2025-04-08 20:51:03,944 [trainer.py] => model_name: POD_foster
2025-04-08 20:51:03,944 [trainer.py] => convnet_type: resnet18
2025-04-08 20:51:03,944 [trainer.py] => init_train: False
2025-04-08 20:51:03,944 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2025-04-08 20:51:03,944 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2025-04-08 20:51:03,944 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2025-04-08 20:51:03,944 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2025-04-08 20:51:03,944 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42871.pth
2025-04-08 20:51:03,944 [trainer.py] => fc_path2: checkpoints/init_train/fc_42871.pth
2025-04-08 20:51:03,944 [trainer.py] => device: [device(type='cuda', index=7)]
2025-04-08 20:51:03,944 [trainer.py] => seed: 1993
2025-04-08 20:51:03,944 [trainer.py] => beta1: 0.96
2025-04-08 20:51:03,944 [trainer.py] => beta2: 0.99
2025-04-08 20:51:03,945 [trainer.py] => oofc: ft
2025-04-08 20:51:03,945 [trainer.py] => is_teacher_wa: True
2025-04-08 20:51:03,945 [trainer.py] => is_student_wa: False
2025-04-08 20:51:03,945 [trainer.py] => is_teacher_la: True
2025-04-08 20:51:03,945 [trainer.py] => is_student_la: True
2025-04-08 20:51:03,945 [trainer.py] => lambda_okd: 0
2025-04-08 20:51:03,945 [trainer.py] => wa_value: 1
2025-04-08 20:51:03,945 [trainer.py] => init_epochs: 0
2025-04-08 20:51:03,945 [trainer.py] => init_lr: 0.1
2025-04-08 20:51:03,945 [trainer.py] => init_weight_decay: 0.0005
2025-04-08 20:51:03,945 [trainer.py] => boosting_epochs: 150
2025-04-08 20:51:03,945 [trainer.py] => compression_epochs: 120
2025-04-08 20:51:03,945 [trainer.py] => lr: 0.1
2025-04-08 20:51:03,945 [trainer.py] => batch_size: 128
2025-04-08 20:51:03,945 [trainer.py] => weight_decay: 0.0005
2025-04-08 20:51:03,945 [trainer.py] => num_workers: 8
2025-04-08 20:51:03,945 [trainer.py] => momentum: 0.9
2025-04-08 20:51:03,945 [trainer.py] => T: 2
2025-04-08 20:51:03,945 [trainer.py] => lambda_c_base: 0.8
2025-04-08 20:51:03,945 [trainer.py] => lambda_f_base: 1.0
2025-04-08 20:51:03,945 [trainer.py] => POD: w
2025-04-08 20:51:04,465 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2025-04-08 20:51:04,509 [trainer.py] => All params: 0
2025-04-08 20:51:04,510 [trainer.py] => Trainable params: 0
2025-04-08 20:51:07,589 [pod_foster.py] => Learning on 0-5
2025-04-08 20:51:07,589 [pod_foster.py] => All params: 3849034
2025-04-08 20:51:07,589 [pod_foster.py] => Trainable params: 3849034
2025-04-08 20:51:07,632 [pod_foster.py] => Adaptive factor: 0
2025-04-08 20:51:09,683 [pod_foster.py] => init_train?---False
2025-04-08 20:51:13,666 [base.py] => Reducing exemplars...(100 per classes)
2025-04-08 20:51:13,667 [base.py] => Constructing exemplars...(100 per classes)
2025-04-08 20:51:23,308 [trainer.py] => task:0 training time:18.80s
2025-04-08 20:51:23,309 [trainer.py] => All params: 3849034
2025-04-08 20:51:25,723 [pod_foster.py] => Exemplar size: 500
2025-04-08 20:51:25,724 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2025-04-08 20:51:25,724 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2025-04-08 20:51:25,724 [trainer.py] => CNN top1 curve: [89.93]
2025-04-08 20:51:25,724 [trainer.py] => CNN top5 curve: [100.0]
2025-04-08 20:51:25,724 [trainer.py] => NME top1 curve: [90.0]
2025-04-08 20:51:25,724 [trainer.py] => NME top5 curve: [100.0]

2025-04-08 20:51:25,724 [trainer.py] => Average Accuracy (CNN): 89.93
2025-04-08 20:51:25,724 [trainer.py] => Average Accuracy (NME): 90.0
2025-04-08 20:51:25,725 [trainer.py] => All params: 3849034
2025-04-08 20:51:25,725 [trainer.py] => Trainable params: 3849034
2025-04-08 20:51:25,957 [pod_foster.py] => Learning on 5-7
2025-04-08 20:51:25,958 [pod_foster.py] => All params: 7701139
2025-04-08 20:51:25,959 [pod_foster.py] => Trainable params: 3854670
2025-04-08 20:51:25,975 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2025-04-08 20:51:26,090 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2025-04-08 20:51:31,004 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.174, Loss_clf 0.764, Loss_fe 0.000, Loss_pod 0.295, Loss_flat 0.114, Train_accy 81.80, Test_accy 65.31
2025-04-08 20:51:47,042 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.210, Loss_clf 0.022, Loss_fe 0.000, Loss_pod 0.156, Loss_flat 0.032, Train_accy 99.96, Test_accy 68.55
2025-04-08 20:52:02,494 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.213, Loss_clf 0.023, Loss_fe 0.000, Loss_pod 0.162, Loss_flat 0.027, Train_accy 99.67, Test_accy 70.90
2025-04-08 20:52:19,465 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.165, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.134, Loss_flat 0.019, Train_accy 100.00, Test_accy 68.17
2025-04-08 20:52:36,267 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.168, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.136, Loss_flat 0.019, Train_accy 100.00, Test_accy 68.71
2025-04-08 20:52:51,869 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.235, Loss_clf 0.015, Loss_fe 0.000, Loss_pod 0.182, Loss_flat 0.038, Train_accy 99.98, Test_accy 69.33
2025-04-08 20:53:08,017 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.188, Loss_clf 0.017, Loss_fe 0.000, Loss_pod 0.146, Loss_flat 0.025, Train_accy 99.80, Test_accy 70.05
2025-04-08 20:53:25,617 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.149, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.124, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.07
2025-04-08 20:53:42,543 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.139, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.116, Loss_flat 0.014, Train_accy 100.00, Test_accy 66.71
2025-04-08 20:53:58,388 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.140, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.118, Loss_flat 0.014, Train_accy 100.00, Test_accy 68.12
2025-04-08 20:54:13,990 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.142, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.120, Loss_flat 0.014, Train_accy 100.00, Test_accy 65.60
2025-04-08 20:54:30,289 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.132, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.111, Loss_flat 0.014, Train_accy 100.00, Test_accy 66.14
2025-04-08 20:54:46,551 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.152, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.124, Loss_flat 0.018, Train_accy 99.93, Test_accy 65.52
2025-04-08 20:55:02,960 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.142, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.119, Loss_flat 0.015, Train_accy 100.00, Test_accy 67.67
2025-04-08 20:55:18,623 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.134, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.113, Loss_flat 0.014, Train_accy 100.00, Test_accy 66.69
2025-04-08 20:55:34,896 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.125, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.105, Loss_flat 0.013, Train_accy 100.00, Test_accy 66.00
2025-04-08 20:55:50,827 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.121, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.102, Loss_flat 0.012, Train_accy 100.00, Test_accy 68.86
2025-04-08 20:56:06,743 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.122, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.103, Loss_flat 0.013, Train_accy 100.00, Test_accy 65.12
2025-04-08 20:56:22,459 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.114, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.096, Loss_flat 0.012, Train_accy 100.00, Test_accy 66.07
2025-04-08 20:56:38,855 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.111, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.093, Loss_flat 0.012, Train_accy 100.00, Test_accy 67.17
2025-04-08 20:56:54,616 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.106, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.088, Loss_flat 0.011, Train_accy 100.00, Test_accy 67.26
2025-04-08 20:57:10,466 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.105, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.087, Loss_flat 0.011, Train_accy 100.00, Test_accy 67.50
2025-04-08 20:57:26,529 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.104, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.086, Loss_flat 0.011, Train_accy 100.00, Test_accy 66.14
2025-04-08 20:57:42,719 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.100, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.083, Loss_flat 0.010, Train_accy 100.00, Test_accy 67.48
2025-04-08 20:57:58,692 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.094, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.077, Loss_flat 0.010, Train_accy 100.00, Test_accy 66.21
2025-04-08 20:58:14,874 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.094, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.077, Loss_flat 0.010, Train_accy 100.00, Test_accy 66.19
2025-04-08 20:58:31,014 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.092, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.076, Loss_flat 0.010, Train_accy 100.00, Test_accy 65.17
2025-04-08 20:58:46,851 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.086, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.071, Loss_flat 0.009, Train_accy 100.00, Test_accy 67.05
2025-04-08 20:59:03,128 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.086, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.070, Loss_flat 0.010, Train_accy 100.00, Test_accy 66.33
2025-04-08 20:59:19,696 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.090, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.074, Loss_flat 0.009, Train_accy 100.00, Test_accy 67.33
2025-04-08 20:59:31,089 [pod_foster.py] => Task 1, time 2.69s, Epoch 150/150 => Loss 0.087, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.071, Loss_flat 0.010, Train_accy 100.00
2025-04-08 20:59:31,090 [pod_foster.py] => 100 epoches training time:324.12s
2025-04-08 20:59:31,090 [pod_foster.py] => Average training time of single epoch:2.88s
2025-04-08 20:59:31,090 [inc_net.py] => align weights, gamma = 0.46779006719589233 
2025-04-08 20:59:31,091 [pod_foster.py] => per cls weights : [1.11679519 1.11679519 1.11679519 1.11679519 1.11679519 0.70801203
 0.70801203]
2025-04-08 20:59:35,398 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.401,  Train_accy 66.47, Test_accy 68.71
2025-04-08 20:59:50,656 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.176,  Train_accy 89.53, Test_accy 78.02
2025-04-08 21:00:05,548 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.166,  Train_accy 90.56, Test_accy 78.10
2025-04-08 21:00:20,099 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.160,  Train_accy 91.44, Test_accy 80.52
2025-04-08 21:00:33,912 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.160,  Train_accy 91.58, Test_accy 80.07
2025-04-08 21:00:48,503 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.157,  Train_accy 91.64, Test_accy 80.31
2025-04-08 21:01:03,448 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.158,  Train_accy 91.73, Test_accy 80.52
2025-04-08 21:01:18,494 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.158,  Train_accy 91.58, Test_accy 81.05
2025-04-08 21:01:32,445 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.152,  Train_accy 92.00, Test_accy 80.24
2025-04-08 21:01:48,018 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.154,  Train_accy 91.78, Test_accy 80.88
2025-04-08 21:02:03,264 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.153,  Train_accy 92.18, Test_accy 80.43
2025-04-08 21:02:17,500 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.151,  Train_accy 92.56, Test_accy 81.45
2025-04-08 21:02:31,300 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.153,  Train_accy 91.89, Test_accy 80.71
2025-04-08 21:02:45,722 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.154,  Train_accy 92.58, Test_accy 79.95
2025-04-08 21:02:59,868 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.154,  Train_accy 92.27, Test_accy 81.50
2025-04-08 21:03:14,676 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.152,  Train_accy 92.31, Test_accy 81.07
2025-04-08 21:03:29,204 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.150,  Train_accy 92.27, Test_accy 80.67
2025-04-08 21:03:43,604 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.152,  Train_accy 92.27, Test_accy 80.69
2025-04-08 21:03:58,531 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.153,  Train_accy 92.60, Test_accy 80.83
2025-04-08 21:04:13,660 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.150,  Train_accy 92.40, Test_accy 80.33
2025-04-08 21:04:28,163 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.150,  Train_accy 92.33, Test_accy 80.98
2025-04-08 21:04:43,045 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.151,  Train_accy 92.29, Test_accy 81.00
2025-04-08 21:04:57,096 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.152,  Train_accy 92.58, Test_accy 81.19
2025-04-08 21:05:12,014 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.151,  Train_accy 92.29, Test_accy 80.71
2025-04-08 21:05:22,196 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.150,  Train_accy 92.44
2025-04-08 21:05:22,196 [pod_foster.py] => do not weight align student!
2025-04-08 21:05:23,753 [pod_foster.py] => darknet eval: 
2025-04-08 21:05:23,754 [pod_foster.py] => CNN top1 curve: 80.86
2025-04-08 21:05:23,754 [pod_foster.py] => CNN top5 curve: 98.79
2025-04-08 21:05:23,754 [pod_foster.py] => CNN: {'total': 80.86, '00-04': 80.4, '05-06': 82.0, 'old': 80.4, 'new': 82.0}
2025-04-08 21:05:23,755 [pod_foster.py] => All params after compression: 3851086
2025-04-08 21:05:23,756 [base.py] => Reducing exemplars...(71 per classes)
2025-04-08 21:05:27,231 [base.py] => Constructing exemplars...(71 per classes)
2025-04-08 21:05:33,022 [trainer.py] => task:1 training time:847.30s
2025-04-08 21:05:33,023 [trainer.py] => All params: 7701139
2025-04-08 21:05:37,049 [pod_foster.py] => Exemplar size: 497
2025-04-08 21:05:37,049 [trainer.py] => CNN: {'total': 79.74, '00-04': 78.0, '05-06': 84.08, 'old': 78.0, 'new': 84.08}
2025-04-08 21:05:37,049 [trainer.py] => NME: {'total': 74.0, '00-04': 75.1, '05-06': 71.25, 'old': 75.1, 'new': 71.25}
2025-04-08 21:05:37,049 [trainer.py] => CNN top1 curve: [89.93, 79.74]
2025-04-08 21:05:37,049 [trainer.py] => CNN top5 curve: [100.0, 98.9]
2025-04-08 21:05:37,049 [trainer.py] => NME top1 curve: [90.0, 74.0]
2025-04-08 21:05:37,049 [trainer.py] => NME top5 curve: [100.0, 99.07]

2025-04-08 21:05:37,049 [trainer.py] => Average Accuracy (CNN): 84.83500000000001
2025-04-08 21:05:37,049 [trainer.py] => Average Accuracy (NME): 82.0
2025-04-08 21:05:37,050 [trainer.py] => All params: 7701139
2025-04-08 21:05:37,051 [trainer.py] => Trainable params: 3854670
2025-04-08 21:05:37,234 [pod_foster.py] => Learning on 7-9
2025-04-08 21:05:37,236 [pod_foster.py] => All params: 7705241
2025-04-08 21:05:37,236 [pod_foster.py] => Trainable params: 3857746
2025-04-08 21:05:37,277 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2025-04-08 21:05:37,289 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2025-04-08 21:05:42,820 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.107, Loss_clf 0.599, Loss_fe 0.000, Loss_pod 0.378, Loss_flat 0.130, Train_accy 85.35, Test_accy 67.15
2025-04-08 21:05:59,840 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.231, Loss_clf 0.017, Loss_fe 0.000, Loss_pod 0.187, Loss_flat 0.026, Train_accy 99.96, Test_accy 62.46
2025-04-08 21:06:16,307 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.206, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.172, Loss_flat 0.022, Train_accy 100.00, Test_accy 59.69
2025-04-08 21:06:33,099 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.206, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.173, Loss_flat 0.022, Train_accy 100.00, Test_accy 62.00
2025-04-08 21:06:49,735 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.186, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.160, Loss_flat 0.018, Train_accy 100.00, Test_accy 65.41
2025-04-08 21:07:06,199 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.199, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.166, Loss_flat 0.021, Train_accy 100.00, Test_accy 59.07
2025-04-08 21:07:22,934 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.185, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.160, Loss_flat 0.018, Train_accy 100.00, Test_accy 65.43
2025-04-08 21:07:39,026 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.182, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.156, Loss_flat 0.018, Train_accy 100.00, Test_accy 65.24
2025-04-08 21:07:55,468 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.172, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.145, Loss_flat 0.019, Train_accy 100.00, Test_accy 63.74
2025-04-08 21:08:12,376 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.175, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.150, Loss_flat 0.017, Train_accy 100.00, Test_accy 65.65
2025-04-08 21:08:29,383 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.165, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.142, Loss_flat 0.015, Train_accy 100.00, Test_accy 64.72
2025-04-08 21:08:46,185 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.160, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.137, Loss_flat 0.015, Train_accy 100.00, Test_accy 60.09
2025-04-08 21:09:02,542 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.156, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.134, Loss_flat 0.015, Train_accy 100.00, Test_accy 65.57
2025-04-08 21:09:18,135 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.169, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.143, Loss_flat 0.017, Train_accy 100.00, Test_accy 67.52
2025-04-08 21:09:34,652 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.175, Loss_clf 0.010, Loss_fe 0.000, Loss_pod 0.146, Loss_flat 0.018, Train_accy 99.93, Test_accy 66.09
2025-04-08 21:09:51,750 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.164, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.139, Loss_flat 0.017, Train_accy 100.00, Test_accy 65.35
2025-04-08 21:10:08,921 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.160, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.133, Loss_flat 0.017, Train_accy 99.93, Test_accy 63.85
2025-04-08 21:10:25,711 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.147, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.127, Loss_flat 0.013, Train_accy 100.00, Test_accy 62.06
2025-04-08 21:10:41,867 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.137, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.117, Loss_flat 0.012, Train_accy 100.00, Test_accy 64.28
2025-04-08 21:10:57,789 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.130, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.111, Loss_flat 0.011, Train_accy 100.00, Test_accy 61.22
2025-04-08 21:11:13,466 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.137, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.117, Loss_flat 0.012, Train_accy 100.00, Test_accy 63.26
2025-04-08 21:11:30,671 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.132, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.111, Loss_flat 0.013, Train_accy 100.00, Test_accy 62.54
2025-04-08 21:11:47,636 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.126, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.107, Loss_flat 0.011, Train_accy 100.00, Test_accy 63.80
2025-04-08 21:12:04,486 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.124, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.105, Loss_flat 0.011, Train_accy 100.00, Test_accy 61.56
2025-04-08 21:12:21,123 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.114, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.096, Loss_flat 0.010, Train_accy 100.00, Test_accy 64.17
2025-04-08 21:12:37,840 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.116, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.099, Loss_flat 0.010, Train_accy 100.00, Test_accy 64.19
2025-04-08 21:12:53,810 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.108, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.091, Loss_flat 0.010, Train_accy 100.00, Test_accy 64.09
2025-04-08 21:13:09,972 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.105, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.088, Loss_flat 0.010, Train_accy 100.00, Test_accy 65.15
2025-04-08 21:13:25,604 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.100, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.084, Loss_flat 0.009, Train_accy 100.00, Test_accy 64.13
2025-04-08 21:13:42,043 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.104, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.087, Loss_flat 0.010, Train_accy 100.00, Test_accy 62.30
2025-04-08 21:13:53,705 [pod_foster.py] => Task 2, time 2.87s, Epoch 150/150 => Loss 0.097, Loss_clf 0.006, Loss_fe 0.000, Loss_pod 0.081, Loss_flat 0.009, Train_accy 100.00
2025-04-08 21:13:53,705 [pod_foster.py] => 100 epoches training time:331.27s
2025-04-08 21:13:53,705 [pod_foster.py] => Average training time of single epoch:2.91s
2025-04-08 21:13:53,706 [inc_net.py] => align weights, gamma = 0.48876240849494934 
2025-04-08 21:13:53,707 [pod_foster.py] => per cls weights : [1.12216381 1.12216381 1.12216381 1.12216381 1.12216381 1.12216381
 1.12216381 0.57242667 0.57242667]
2025-04-08 21:13:58,035 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.722,  Train_accy 68.91, Test_accy 67.52
2025-04-08 21:14:12,304 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.549,  Train_accy 87.81, Test_accy 74.19
2025-04-08 21:14:26,324 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.544,  Train_accy 88.08, Test_accy 74.46
2025-04-08 21:14:41,875 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.540,  Train_accy 88.88, Test_accy 74.56
2025-04-08 21:14:56,110 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.541,  Train_accy 88.57, Test_accy 75.30
2025-04-08 21:15:10,782 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.541,  Train_accy 89.10, Test_accy 74.78
2025-04-08 21:15:24,780 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.536,  Train_accy 88.99, Test_accy 76.39
2025-04-08 21:15:39,111 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.537,  Train_accy 89.44, Test_accy 76.67
2025-04-08 21:15:53,604 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.535,  Train_accy 89.26, Test_accy 75.69
2025-04-08 21:16:07,892 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.537,  Train_accy 89.62, Test_accy 75.89
2025-04-08 21:16:22,069 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.536,  Train_accy 89.17, Test_accy 75.96
2025-04-08 21:16:36,441 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.537,  Train_accy 89.04, Test_accy 75.72
2025-04-08 21:16:50,923 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.536,  Train_accy 89.70, Test_accy 76.44
2025-04-08 21:17:04,114 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.535,  Train_accy 89.88, Test_accy 76.61
2025-04-08 21:17:17,690 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.538,  Train_accy 89.64, Test_accy 75.35
2025-04-08 21:17:31,573 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.536,  Train_accy 89.17, Test_accy 75.74
2025-04-08 21:17:45,743 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.536,  Train_accy 89.55, Test_accy 75.44
2025-04-08 21:17:59,804 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.532,  Train_accy 89.22, Test_accy 76.22
2025-04-08 21:18:13,217 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.534,  Train_accy 89.62, Test_accy 76.33
2025-04-08 21:18:26,672 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.533,  Train_accy 90.30, Test_accy 76.56
2025-04-08 21:18:41,007 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.533,  Train_accy 89.62, Test_accy 76.35
2025-04-08 21:18:54,815 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.534,  Train_accy 89.17, Test_accy 75.70
2025-04-08 21:19:09,001 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.535,  Train_accy 89.66, Test_accy 76.31
2025-04-08 21:19:22,649 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.535,  Train_accy 89.68, Test_accy 75.52
2025-04-08 21:19:31,875 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.534,  Train_accy 89.75
2025-04-08 21:19:31,875 [pod_foster.py] => do not weight align student!
2025-04-08 21:19:33,311 [pod_foster.py] => darknet eval: 
2025-04-08 21:19:33,311 [pod_foster.py] => CNN top1 curve: 76.76
2025-04-08 21:19:33,311 [pod_foster.py] => CNN top5 curve: 97.83
2025-04-08 21:19:33,311 [pod_foster.py] => CNN: {'total': 76.76, '00-04': 69.67, '05-06': 91.83, '07-08': 79.42, 'old': 76.0, 'new': 79.42}
2025-04-08 21:19:33,312 [pod_foster.py] => All params after compression: 3853138
2025-04-08 21:19:33,313 [base.py] => Reducing exemplars...(55 per classes)
2025-04-08 21:19:37,217 [base.py] => Constructing exemplars...(55 per classes)
2025-04-08 21:19:40,992 [trainer.py] => task:2 training time:843.94s
2025-04-08 21:19:40,994 [trainer.py] => All params: 7705241
2025-04-08 21:19:44,155 [pod_foster.py] => Exemplar size: 495
2025-04-08 21:19:44,155 [trainer.py] => CNN: {'total': 73.67, '00-04': 60.23, '05-06': 91.92, '07-08': 89.0, 'old': 69.29, 'new': 89.0}
2025-04-08 21:19:44,155 [trainer.py] => NME: {'total': 72.5, '00-04': 62.0, '05-06': 81.67, '07-08': 89.58, 'old': 67.62, 'new': 89.58}
2025-04-08 21:19:44,155 [trainer.py] => CNN top1 curve: [89.93, 79.74, 73.67]
2025-04-08 21:19:44,156 [trainer.py] => CNN top5 curve: [100.0, 98.9, 97.02]
2025-04-08 21:19:44,156 [trainer.py] => NME top1 curve: [90.0, 74.0, 72.5]
2025-04-08 21:19:44,156 [trainer.py] => NME top5 curve: [100.0, 99.07, 97.26]

2025-04-08 21:19:44,156 [trainer.py] => Average Accuracy (CNN): 81.11333333333334
2025-04-08 21:19:44,156 [trainer.py] => Average Accuracy (NME): 78.83333333333333
2025-04-08 21:19:44,156 [trainer.py] => Time consumed in all training process:1719.65s
2025-04-08 21:19:44,156 [trainer.py] => Average Time consumed in single task:570.01s
2025-04-08 21:19:44,156 [trainer.py] => Forgetting (CNN): 14.850000000000005
