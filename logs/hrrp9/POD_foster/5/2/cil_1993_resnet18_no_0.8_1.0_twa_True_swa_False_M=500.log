2024-10-25 15:47:38,207 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 15:47:38,208 [trainer.py] => prefix: cil
2024-10-25 15:47:38,208 [trainer.py] => dataset: hrrp9
2024-10-25 15:47:38,208 [trainer.py] => memory_size: 500
2024-10-25 15:47:38,208 [trainer.py] => memory_per_class: 20
2024-10-25 15:47:38,208 [trainer.py] => fixed_memory: False
2024-10-25 15:47:38,208 [trainer.py] => shuffle: True
2024-10-25 15:47:38,208 [trainer.py] => init_cls: 5
2024-10-25 15:47:38,208 [trainer.py] => increment: 2
2024-10-25 15:47:38,208 [trainer.py] => model_name: POD_foster
2024-10-25 15:47:38,208 [trainer.py] => convnet_type: resnet18
2024-10-25 15:47:38,208 [trainer.py] => init_train: False
2024-10-25 15:47:38,208 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 15:47:38,208 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 15:47:38,209 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 15:47:38,209 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 15:47:38,209 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-25 15:47:38,209 [trainer.py] => seed: 1993
2024-10-25 15:47:38,209 [trainer.py] => beta1: 0.96
2024-10-25 15:47:38,209 [trainer.py] => beta2: 0.97
2024-10-25 15:47:38,209 [trainer.py] => oofc: ft
2024-10-25 15:47:38,209 [trainer.py] => is_teacher_wa: True
2024-10-25 15:47:38,209 [trainer.py] => is_student_wa: False
2024-10-25 15:47:38,209 [trainer.py] => lambda_okd: 0
2024-10-25 15:47:38,209 [trainer.py] => wa_value: 1
2024-10-25 15:47:38,209 [trainer.py] => init_epochs: 0
2024-10-25 15:47:38,209 [trainer.py] => init_lr: 0.1
2024-10-25 15:47:38,209 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 15:47:38,209 [trainer.py] => boosting_epochs: 150
2024-10-25 15:47:38,209 [trainer.py] => compression_epochs: 120
2024-10-25 15:47:38,209 [trainer.py] => lr: 0.1
2024-10-25 15:47:38,210 [trainer.py] => batch_size: 128
2024-10-25 15:47:38,210 [trainer.py] => weight_decay: 0.0005
2024-10-25 15:47:38,210 [trainer.py] => num_workers: 8
2024-10-25 15:47:38,210 [trainer.py] => momentum: 0.9
2024-10-25 15:47:38,210 [trainer.py] => T: 2
2024-10-25 15:47:38,210 [trainer.py] => lambda_c_base: 0.8
2024-10-25 15:47:38,210 [trainer.py] => lambda_f_base: 1.0
2024-10-25 15:47:38,210 [trainer.py] => POD: c
2024-10-25 15:47:38,969 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 15:47:39,029 [trainer.py] => All params: 0
2024-10-25 15:47:39,029 [trainer.py] => Trainable params: 0
2024-10-25 15:47:40,243 [pod_foster.py] => Learning on 0-5
2024-10-25 15:47:40,244 [pod_foster.py] => All params: 3849034
2024-10-25 15:47:40,244 [pod_foster.py] => Trainable params: 3849034
2024-10-25 15:47:40,374 [pod_foster.py] => Adaptive factor: 0
2024-10-25 15:47:40,611 [pod_foster.py] => init_train?---False
2024-10-25 15:47:41,685 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 15:47:41,686 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 15:47:48,560 [trainer.py] => All params: 3849034
2024-10-25 15:47:50,009 [pod_foster.py] => Exemplar size: 500
2024-10-25 15:47:50,009 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 15:47:50,009 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 15:47:50,009 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 15:47:50,009 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 15:47:50,010 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 15:47:50,010 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 15:47:50,010 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 15:47:50,010 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 15:47:50,010 [trainer.py] => All params: 3849034
2024-10-25 15:47:50,011 [trainer.py] => Trainable params: 3849034
2024-10-25 15:47:50,064 [pod_foster.py] => Learning on 5-7
2024-10-25 15:47:50,066 [pod_foster.py] => All params: 7701139
2024-10-25 15:47:50,066 [pod_foster.py] => Trainable params: 3854670
2024-10-25 15:47:50,106 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 15:47:50,118 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 15:47:53,680 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.968, Loss_clf 0.702, Loss_fe 0.664, Loss_pod 0.380, Loss_flat 0.223, Train_accy 83.24, Test_accy 46.95
2024-10-25 15:48:04,441 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.415, Loss_clf 0.023, Loss_fe 0.049, Loss_pod 0.258, Loss_flat 0.085, Train_accy 99.69, Test_accy 67.29
2024-10-25 15:48:15,326 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.286, Loss_clf 0.011, Loss_fe 0.016, Loss_pod 0.205, Loss_flat 0.053, Train_accy 100.00, Test_accy 72.12
2024-10-25 15:48:26,542 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.270, Loss_clf 0.010, Loss_fe 0.013, Loss_pod 0.199, Loss_flat 0.048, Train_accy 100.00, Test_accy 67.69
2024-10-25 15:48:38,831 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.280, Loss_clf 0.017, Loss_fe 0.019, Loss_pod 0.194, Loss_flat 0.050, Train_accy 99.89, Test_accy 61.86
2024-10-25 15:48:49,694 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.889, Loss_clf 0.071, Loss_fe 0.201, Loss_pod 0.439, Loss_flat 0.178, Train_accy 97.82, Test_accy 68.83
2024-10-25 15:49:00,272 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.643, Loss_clf 0.036, Loss_fe 0.068, Loss_pod 0.410, Loss_flat 0.130, Train_accy 99.07, Test_accy 68.86
2024-10-25 15:49:10,720 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.420, Loss_clf 0.010, Loss_fe 0.012, Loss_pod 0.327, Loss_flat 0.071, Train_accy 100.00, Test_accy 67.62
2024-10-25 15:49:21,239 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.441, Loss_clf 0.014, Loss_fe 0.020, Loss_pod 0.324, Loss_flat 0.083, Train_accy 99.93, Test_accy 68.88
2024-10-25 15:49:31,801 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.326, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.256, Loss_flat 0.053, Train_accy 100.00, Test_accy 67.95
2024-10-25 15:49:42,621 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.315, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.252, Loss_flat 0.046, Train_accy 100.00, Test_accy 66.90
2024-10-25 15:49:52,942 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.300, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.243, Loss_flat 0.042, Train_accy 100.00, Test_accy 68.69
2024-10-25 15:50:03,073 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.300, Loss_clf 0.011, Loss_fe 0.012, Loss_pod 0.230, Loss_flat 0.047, Train_accy 99.96, Test_accy 66.81
2024-10-25 15:50:13,564 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.295, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.232, Loss_flat 0.045, Train_accy 100.00, Test_accy 67.40
2024-10-25 15:50:24,011 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.254, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.203, Loss_flat 0.035, Train_accy 100.00, Test_accy 69.00
2024-10-25 15:50:34,375 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.232, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.184, Loss_flat 0.034, Train_accy 100.00, Test_accy 67.98
2024-10-25 15:50:45,039 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.219, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.172, Loss_flat 0.032, Train_accy 100.00, Test_accy 67.21
2024-10-25 15:50:55,525 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.228, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.180, Loss_flat 0.033, Train_accy 100.00, Test_accy 66.24
2024-10-25 15:51:05,782 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.210, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.164, Loss_flat 0.031, Train_accy 100.00, Test_accy 68.50
2024-10-25 15:51:15,982 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.280, Loss_clf 0.017, Loss_fe 0.020, Loss_pod 0.194, Loss_flat 0.050, Train_accy 99.71, Test_accy 73.45
2024-10-25 15:51:26,338 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.208, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.162, Loss_flat 0.031, Train_accy 100.00, Test_accy 67.05
2024-10-25 15:51:36,751 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.200, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.154, Loss_flat 0.031, Train_accy 100.00, Test_accy 67.00
2024-10-25 15:51:47,614 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.196, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.150, Loss_flat 0.031, Train_accy 100.00, Test_accy 66.40
2024-10-25 15:51:58,089 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.190, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.146, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.79
2024-10-25 15:52:08,530 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.180, Loss_clf 0.007, Loss_fe 0.007, Loss_pod 0.137, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.17
2024-10-25 15:52:18,749 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.182, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.139, Loss_flat 0.029, Train_accy 100.00, Test_accy 66.12
2024-10-25 15:52:29,260 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.176, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.134, Loss_flat 0.029, Train_accy 100.00, Test_accy 66.12
2024-10-25 15:52:39,855 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.169, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.127, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.02
2024-10-25 15:52:50,374 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.167, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.124, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.88
2024-10-25 15:53:00,859 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.172, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.130, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.81
2024-10-25 15:53:08,294 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.166, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.123, Loss_flat 0.029, Train_accy 100.00
2024-10-25 15:53:08,297 [inc_net.py] => align weights, gamma = 0.48156458139419556 
2024-10-25 15:53:08,298 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 15:53:10,878 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.359,  Train_accy 65.20, Test_accy 70.33
2024-10-25 15:53:20,380 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.131,  Train_accy 91.42, Test_accy 77.90
2024-10-25 15:53:30,035 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.127,  Train_accy 93.09, Test_accy 77.60
2024-10-25 15:53:39,932 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.120,  Train_accy 93.11, Test_accy 78.55
2024-10-25 15:53:49,458 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.122,  Train_accy 93.29, Test_accy 78.62
2024-10-25 15:53:59,091 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.120,  Train_accy 93.58, Test_accy 79.17
2024-10-25 15:54:08,615 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.121,  Train_accy 93.16, Test_accy 78.90
2024-10-25 15:54:18,166 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.119,  Train_accy 93.22, Test_accy 78.69
2024-10-25 15:54:27,565 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.117,  Train_accy 93.67, Test_accy 78.90
2024-10-25 15:54:37,384 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.116,  Train_accy 93.58, Test_accy 78.93
2024-10-25 15:54:46,736 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.117,  Train_accy 93.71, Test_accy 78.86
2024-10-25 15:54:56,266 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.115,  Train_accy 93.58, Test_accy 79.48
2024-10-25 15:55:05,828 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.116,  Train_accy 93.69, Test_accy 79.38
2024-10-25 15:55:15,394 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.117,  Train_accy 94.02, Test_accy 78.79
2024-10-25 15:55:24,845 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.118,  Train_accy 93.71, Test_accy 79.69
2024-10-25 15:55:34,659 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.116,  Train_accy 93.91, Test_accy 79.64
2024-10-25 15:55:44,520 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.115,  Train_accy 93.22, Test_accy 79.67
2024-10-25 15:55:54,036 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.116,  Train_accy 93.78, Test_accy 79.67
2024-10-25 15:56:03,659 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.117,  Train_accy 93.60, Test_accy 79.71
2024-10-25 15:56:13,523 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.114,  Train_accy 93.93, Test_accy 79.50
2024-10-25 15:56:23,027 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.113,  Train_accy 93.69, Test_accy 79.64
2024-10-25 15:56:32,674 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.115,  Train_accy 93.89, Test_accy 79.76
2024-10-25 15:56:42,570 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.116,  Train_accy 94.00, Test_accy 80.10
2024-10-25 15:56:52,401 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.114,  Train_accy 93.82, Test_accy 79.43
2024-10-25 15:56:59,495 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.113,  Train_accy 93.96
2024-10-25 15:56:59,495 [pod_foster.py] => do not weight align student!
2024-10-25 15:57:00,326 [pod_foster.py] => darknet eval: 
2024-10-25 15:57:00,326 [pod_foster.py] => CNN top1 curve: 79.71
2024-10-25 15:57:00,326 [pod_foster.py] => CNN top5 curve: 98.79
2024-10-25 15:57:00,328 [pod_foster.py] => All params after compression: 3851086
2024-10-25 15:57:00,328 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 15:57:01,707 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 15:57:05,447 [trainer.py] => All params: 7701139
2024-10-25 15:57:07,904 [pod_foster.py] => Exemplar size: 497
2024-10-25 15:57:07,905 [trainer.py] => CNN: {'total': 80.31, '00-04': 80.0, '05-06': 81.08, 'old': 80.0, 'new': 81.08}
2024-10-25 15:57:07,905 [trainer.py] => NME: {'total': 73.57, '00-04': 78.53, '05-06': 61.17, 'old': 78.53, 'new': 61.17}
2024-10-25 15:57:07,905 [trainer.py] => CNN top1 curve: [89.93, 80.31]
2024-10-25 15:57:07,906 [trainer.py] => CNN top5 curve: [100.0, 98.55]
2024-10-25 15:57:07,906 [trainer.py] => NME top1 curve: [90.0, 73.57]
2024-10-25 15:57:07,906 [trainer.py] => NME top5 curve: [100.0, 98.9]

2024-10-25 15:57:07,906 [trainer.py] => Average Accuracy (CNN): 85.12
2024-10-25 15:57:07,906 [trainer.py] => Average Accuracy (NME): 81.785
2024-10-25 15:57:07,907 [trainer.py] => All params: 7701139
2024-10-25 15:57:07,908 [trainer.py] => Trainable params: 3854670
2024-10-25 15:57:07,967 [pod_foster.py] => Learning on 7-9
2024-10-25 15:57:07,969 [pod_foster.py] => All params: 7705241
2024-10-25 15:57:07,970 [pod_foster.py] => Trainable params: 3857746
2024-10-25 15:57:08,021 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 15:57:08,032 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 15:57:11,180 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.998, Loss_clf 0.646, Loss_fe 0.658, Loss_pod 0.480, Loss_flat 0.214, Train_accy 85.50, Test_accy 65.31
2024-10-25 15:57:21,572 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.396, Loss_clf 0.015, Loss_fe 0.051, Loss_pod 0.269, Loss_flat 0.060, Train_accy 99.98, Test_accy 61.69
2024-10-25 15:57:32,368 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.319, Loss_clf 0.013, Loss_fe 0.020, Loss_pod 0.240, Loss_flat 0.045, Train_accy 100.00, Test_accy 67.87
2024-10-25 15:57:43,553 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.286, Loss_clf 0.010, Loss_fe 0.013, Loss_pod 0.224, Loss_flat 0.039, Train_accy 100.00, Test_accy 66.22
2024-10-25 15:57:54,302 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.272, Loss_clf 0.008, Loss_fe 0.011, Loss_pod 0.219, Loss_flat 0.033, Train_accy 100.00, Test_accy 69.17
2024-10-25 15:58:05,040 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.489, Loss_clf 0.034, Loss_fe 0.061, Loss_pod 0.315, Loss_flat 0.079, Train_accy 99.44, Test_accy 14.04
2024-10-25 15:58:16,003 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.425, Loss_clf 0.010, Loss_fe 0.015, Loss_pod 0.337, Loss_flat 0.063, Train_accy 99.98, Test_accy 67.15
2024-10-25 15:58:26,884 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.373, Loss_clf 0.009, Loss_fe 0.013, Loss_pod 0.300, Loss_flat 0.052, Train_accy 100.00, Test_accy 66.13
2024-10-25 15:58:37,900 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.297, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.241, Loss_flat 0.039, Train_accy 100.00, Test_accy 68.19
2024-10-25 15:58:48,631 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.280, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.231, Loss_flat 0.033, Train_accy 100.00, Test_accy 68.35
2024-10-25 15:58:59,568 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.263, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.217, Loss_flat 0.030, Train_accy 100.00, Test_accy 63.33
2024-10-25 15:59:10,455 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.262, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.215, Loss_flat 0.029, Train_accy 100.00, Test_accy 64.74
2024-10-25 15:59:21,034 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.253, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.203, Loss_flat 0.032, Train_accy 100.00, Test_accy 66.33
2024-10-25 15:59:31,960 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.321, Loss_clf 0.011, Loss_fe 0.015, Loss_pod 0.245, Loss_flat 0.049, Train_accy 99.98, Test_accy 72.19
2024-10-25 15:59:42,685 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.256, Loss_clf 0.010, Loss_fe 0.011, Loss_pod 0.199, Loss_flat 0.036, Train_accy 99.98, Test_accy 66.44
2024-10-25 15:59:53,525 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.233, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.189, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.44
2024-10-25 16:00:04,005 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.233, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.186, Loss_flat 0.031, Train_accy 100.00, Test_accy 64.24
2024-10-25 16:00:14,901 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.206, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.165, Loss_flat 0.027, Train_accy 100.00, Test_accy 63.50
2024-10-25 16:00:25,796 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.207, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.166, Loss_flat 0.026, Train_accy 100.00, Test_accy 64.22
2024-10-25 16:00:36,561 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.193, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.152, Loss_flat 0.026, Train_accy 100.00, Test_accy 63.46
2024-10-25 16:00:47,488 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.194, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.153, Loss_flat 0.026, Train_accy 100.00, Test_accy 62.46
2024-10-25 16:00:58,054 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.251, Loss_clf 0.009, Loss_fe 0.011, Loss_pod 0.192, Loss_flat 0.040, Train_accy 100.00, Test_accy 66.76
2024-10-25 16:01:08,870 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.193, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.148, Loss_flat 0.029, Train_accy 100.00, Test_accy 66.57
2024-10-25 16:01:19,326 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.192, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.147, Loss_flat 0.028, Train_accy 100.00, Test_accy 63.59
2024-10-25 16:01:29,767 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.167, Loss_clf 0.007, Loss_fe 0.007, Loss_pod 0.128, Loss_flat 0.025, Train_accy 100.00, Test_accy 65.09
2024-10-25 16:01:40,457 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.169, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.130, Loss_flat 0.026, Train_accy 100.00, Test_accy 64.94
2024-10-25 16:01:51,059 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.164, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.124, Loss_flat 0.026, Train_accy 100.00, Test_accy 64.98
2024-10-25 16:02:01,541 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.158, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.120, Loss_flat 0.025, Train_accy 100.00, Test_accy 66.94
2024-10-25 16:02:11,921 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.149, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.110, Loss_flat 0.025, Train_accy 100.00, Test_accy 66.24
2024-10-25 16:02:22,234 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.158, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.117, Loss_flat 0.026, Train_accy 100.00, Test_accy 62.41
2024-10-25 16:02:29,770 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.150, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.111, Loss_flat 0.025, Train_accy 100.00
2024-10-25 16:02:29,771 [inc_net.py] => align weights, gamma = 0.47675278782844543 
2024-10-25 16:02:29,772 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 16:02:32,449 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.649,  Train_accy 69.82, Test_accy 67.00
2024-10-25 16:02:41,991 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.459,  Train_accy 92.55, Test_accy 74.54
2024-10-25 16:02:51,473 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.461,  Train_accy 93.75, Test_accy 74.74
2024-10-25 16:03:01,020 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.456,  Train_accy 94.40, Test_accy 75.07
2024-10-25 16:03:10,483 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.457,  Train_accy 93.93, Test_accy 75.22
2024-10-25 16:03:20,026 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.456,  Train_accy 94.37, Test_accy 76.46
2024-10-25 16:03:29,675 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.451,  Train_accy 93.88, Test_accy 76.63
2024-10-25 16:03:39,357 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.453,  Train_accy 94.46, Test_accy 77.06
2024-10-25 16:03:49,243 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.452,  Train_accy 94.42, Test_accy 77.87
2024-10-25 16:03:58,763 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.453,  Train_accy 95.09, Test_accy 77.09
2024-10-25 16:04:08,443 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.453,  Train_accy 94.02, Test_accy 75.98
2024-10-25 16:04:18,004 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.453,  Train_accy 94.26, Test_accy 77.19
2024-10-25 16:04:27,657 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.453,  Train_accy 94.24, Test_accy 77.02
2024-10-25 16:04:37,264 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.449,  Train_accy 94.62, Test_accy 77.37
2024-10-25 16:04:47,010 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.453,  Train_accy 94.69, Test_accy 77.39
2024-10-25 16:04:56,558 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.453,  Train_accy 94.44, Test_accy 77.15
2024-10-25 16:05:06,293 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.454,  Train_accy 94.51, Test_accy 77.11
2024-10-25 16:05:15,912 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.447,  Train_accy 94.57, Test_accy 76.94
2024-10-25 16:05:25,577 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.450,  Train_accy 94.80, Test_accy 77.56
2024-10-25 16:05:35,243 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.445,  Train_accy 94.86, Test_accy 77.54
2024-10-25 16:05:45,071 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.449,  Train_accy 94.82, Test_accy 77.35
2024-10-25 16:05:54,742 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.450,  Train_accy 94.51, Test_accy 77.69
2024-10-25 16:06:04,086 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.451,  Train_accy 94.77, Test_accy 77.65
2024-10-25 16:06:13,287 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.452,  Train_accy 94.46, Test_accy 76.87
2024-10-25 16:06:20,126 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.450,  Train_accy 94.49
2024-10-25 16:06:20,127 [pod_foster.py] => do not weight align student!
2024-10-25 16:06:20,966 [pod_foster.py] => darknet eval: 
2024-10-25 16:06:20,966 [pod_foster.py] => CNN top1 curve: 78.06
2024-10-25 16:06:20,966 [pod_foster.py] => CNN top5 curve: 96.57
2024-10-25 16:06:20,968 [pod_foster.py] => All params after compression: 3853138
2024-10-25 16:06:20,968 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 16:06:22,512 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 16:06:25,686 [trainer.py] => All params: 7705241
2024-10-25 16:06:27,914 [pod_foster.py] => Exemplar size: 495
2024-10-25 16:06:27,914 [trainer.py] => CNN: {'total': 76.33, '00-04': 67.33, '05-06': 89.17, '07-08': 86.0, 'old': 73.57, 'new': 86.0}
2024-10-25 16:06:27,914 [trainer.py] => NME: {'total': 73.37, '00-04': 69.3, '05-06': 75.83, '07-08': 81.08, 'old': 71.17, 'new': 81.08}
2024-10-25 16:06:27,914 [trainer.py] => CNN top1 curve: [89.93, 80.31, 76.33]
2024-10-25 16:06:27,914 [trainer.py] => CNN top5 curve: [100.0, 98.55, 96.37]
2024-10-25 16:06:27,914 [trainer.py] => NME top1 curve: [90.0, 73.57, 73.37]
2024-10-25 16:06:27,914 [trainer.py] => NME top5 curve: [100.0, 98.9, 96.52]

2024-10-25 16:06:27,914 [trainer.py] => Average Accuracy (CNN): 82.19
2024-10-25 16:06:27,915 [trainer.py] => Average Accuracy (NME): 78.98
2024-10-25 16:06:27,915 [trainer.py] => Forgetting (CNN): 11.300000000000004
