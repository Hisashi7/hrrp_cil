2024-10-23 20:28:56,567 [trainer.py] => config: ./exps/bic.json
2024-10-23 20:28:56,567 [trainer.py] => prefix: cil
2024-10-23 20:28:56,567 [trainer.py] => dataset: hrrp9
2024-10-23 20:28:56,567 [trainer.py] => memory_size: 100
2024-10-23 20:28:56,567 [trainer.py] => memory_per_class: 20
2024-10-23 20:28:56,567 [trainer.py] => fixed_memory: False
2024-10-23 20:28:56,567 [trainer.py] => shuffle: True
2024-10-23 20:28:56,568 [trainer.py] => init_cls: 5
2024-10-23 20:28:56,568 [trainer.py] => increment: 2
2024-10-23 20:28:56,568 [trainer.py] => model_name: bic
2024-10-23 20:28:56,568 [trainer.py] => convnet_type: resnet18
2024-10-23 20:28:56,568 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-23 20:28:56,568 [trainer.py] => init_train: False
2024-10-23 20:28:56,568 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-23 20:28:56,568 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-23 20:28:56,568 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-23 20:28:56,568 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-23 20:28:56,568 [trainer.py] => seed: 1993
2024-10-23 20:28:56,568 [trainer.py] => init_epochs: 0
2024-10-23 20:28:56,568 [trainer.py] => epochs: 150
2024-10-23 20:28:56,568 [trainer.py] => lrate: 0.1
2024-10-23 20:28:56,568 [trainer.py] => milestones: [50, 80, 120]
2024-10-23 20:28:56,569 [trainer.py] => lrate_decay: 0.1
2024-10-23 20:28:56,569 [trainer.py] => momentum: 0.9
2024-10-23 20:28:56,569 [trainer.py] => batch_size: 128
2024-10-23 20:28:56,569 [trainer.py] => split_ratio: 0.1
2024-10-23 20:28:56,569 [trainer.py] => weight_decay: 0.0002
2024-10-23 20:28:56,569 [trainer.py] => num_workers: 0
2024-10-23 20:28:56,569 [trainer.py] => T: 2
2024-10-23 20:28:56,569 [trainer.py] => bc_lrate: 0.001
2024-10-23 20:28:56,569 [trainer.py] => bc_epochs: [100, 100, 10]
2024-10-23 20:28:57,298 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-23 20:28:57,868 [trainer.py] => All params: 3843904
2024-10-23 20:28:57,869 [trainer.py] => Trainable params: 3843904
2024-10-23 20:28:57,872 [bic.py] => Learning on 0-5
2024-10-23 20:28:57,918 [bic.py] => Parameters of bias layer:
2024-10-23 20:28:57,919 [bic.py] => 0 => 1.000, 0.000
2024-10-23 20:28:58,386 [base.py] => Reducing exemplars...(20 per classes)
2024-10-23 20:28:58,386 [base.py] => Constructing exemplars...(20 per classes)
2024-10-23 20:29:02,713 [bic.py] => Parameters of bias layer:
2024-10-23 20:29:02,714 [bic.py] => 0 => 1.000, 0.000
2024-10-23 20:29:02,714 [trainer.py] => All params: 3846471
2024-10-23 20:29:03,312 [bic.py] => Exemplar size: 100
2024-10-23 20:29:03,312 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-23 20:29:03,312 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-23 20:29:03,313 [trainer.py] => CNN top1 curve: [89.93]
2024-10-23 20:29:03,313 [trainer.py] => CNN top5 curve: [100.0]
2024-10-23 20:29:03,313 [trainer.py] => NME top1 curve: [90.0]
2024-10-23 20:29:03,313 [trainer.py] => NME top5 curve: [100.0]

2024-10-23 20:29:03,313 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-23 20:29:03,313 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-23 20:29:03,313 [trainer.py] => All params: 3846471
2024-10-23 20:29:03,314 [trainer.py] => Trainable params: 3846471
2024-10-23 20:29:03,318 [bic.py] => Learning on 5-7
2024-10-23 20:29:03,332 [bic.py] => Stage1 dset: 4086, Stage2 dset: 14
2024-10-23 20:29:03,332 [bic.py] => Lambda: 0.714
2024-10-23 20:29:03,343 [bic.py] => Parameters of bias layer:
2024-10-23 20:29:03,343 [bic.py] => 0 => 1.000, 0.000
2024-10-23 20:29:03,343 [bic.py] => 1 => 1.000, 0.000
2024-10-23 20:29:04,902 [bic.py] => training => Task 1, Epoch 1/150 => Loss 1.067, Train_accy 93.050, Test_accy 27.290
2024-10-23 20:29:06,194 [bic.py] => training => Task 1, Epoch 2/150 => Loss 0.809, Train_accy 96.480, Test_accy 27.830
2024-10-23 20:29:07,423 [bic.py] => training => Task 1, Epoch 3/150 => Loss 0.751, Train_accy 98.700, Test_accy 33.330
2024-10-23 20:29:08,662 [bic.py] => training => Task 1, Epoch 4/150 => Loss 0.729, Train_accy 99.530, Test_accy 39.570
2024-10-23 20:29:09,921 [bic.py] => training => Task 1, Epoch 5/150 => Loss 0.717, Train_accy 99.580, Test_accy 41.050
2024-10-23 20:29:11,197 [bic.py] => training => Task 1, Epoch 6/150 => Loss 0.712, Train_accy 99.730, Test_accy 41.170
2024-10-23 20:29:12,523 [bic.py] => training => Task 1, Epoch 7/150 => Loss 0.711, Train_accy 99.980, Test_accy 43.240
2024-10-23 20:29:13,693 [bic.py] => training => Task 1, Epoch 8/150 => Loss 0.708, Train_accy 100.000, Test_accy 45.190
2024-10-23 20:29:14,872 [bic.py] => training => Task 1, Epoch 9/150 => Loss 0.705, Train_accy 100.000, Test_accy 44.360
2024-10-23 20:29:16,127 [bic.py] => training => Task 1, Epoch 10/150 => Loss 0.705, Train_accy 100.000, Test_accy 44.050
2024-10-23 20:29:17,328 [bic.py] => training => Task 1, Epoch 11/150 => Loss 0.703, Train_accy 100.000, Test_accy 43.760
2024-10-23 20:29:18,542 [bic.py] => training => Task 1, Epoch 12/150 => Loss 0.703, Train_accy 100.000, Test_accy 42.950
2024-10-23 20:29:19,770 [bic.py] => training => Task 1, Epoch 13/150 => Loss 0.702, Train_accy 99.980, Test_accy 46.830
2024-10-23 20:29:21,054 [bic.py] => training => Task 1, Epoch 14/150 => Loss 0.702, Train_accy 100.000, Test_accy 45.600
2024-10-23 20:29:22,332 [bic.py] => training => Task 1, Epoch 15/150 => Loss 0.701, Train_accy 100.000, Test_accy 45.670
2024-10-23 20:29:23,526 [bic.py] => training => Task 1, Epoch 16/150 => Loss 0.701, Train_accy 100.000, Test_accy 44.980
2024-10-23 20:29:24,753 [bic.py] => training => Task 1, Epoch 17/150 => Loss 0.701, Train_accy 100.000, Test_accy 44.620
2024-10-23 20:29:25,982 [bic.py] => training => Task 1, Epoch 18/150 => Loss 0.701, Train_accy 100.000, Test_accy 44.860
2024-10-23 20:29:27,204 [bic.py] => training => Task 1, Epoch 19/150 => Loss 0.701, Train_accy 100.000, Test_accy 45.790
2024-10-23 20:29:28,483 [bic.py] => training => Task 1, Epoch 20/150 => Loss 0.701, Train_accy 100.000, Test_accy 46.000
2024-10-23 20:29:29,761 [bic.py] => training => Task 1, Epoch 21/150 => Loss 0.700, Train_accy 100.000, Test_accy 44.000
2024-10-23 20:29:31,083 [bic.py] => training => Task 1, Epoch 22/150 => Loss 0.701, Train_accy 100.000, Test_accy 46.310
2024-10-23 20:29:32,365 [bic.py] => training => Task 1, Epoch 23/150 => Loss 0.701, Train_accy 100.000, Test_accy 46.400
2024-10-23 20:29:33,549 [bic.py] => training => Task 1, Epoch 24/150 => Loss 0.699, Train_accy 100.000, Test_accy 45.640
2024-10-23 20:29:34,838 [bic.py] => training => Task 1, Epoch 25/150 => Loss 0.700, Train_accy 100.000, Test_accy 43.670
2024-10-23 20:29:36,067 [bic.py] => training => Task 1, Epoch 26/150 => Loss 0.701, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:29:37,277 [bic.py] => training => Task 1, Epoch 27/150 => Loss 0.699, Train_accy 100.000, Test_accy 44.740
2024-10-23 20:29:38,525 [bic.py] => training => Task 1, Epoch 28/150 => Loss 0.699, Train_accy 100.000, Test_accy 45.380
2024-10-23 20:29:39,783 [bic.py] => training => Task 1, Epoch 29/150 => Loss 0.699, Train_accy 100.000, Test_accy 45.740
2024-10-23 20:29:41,097 [bic.py] => training => Task 1, Epoch 30/150 => Loss 0.699, Train_accy 100.000, Test_accy 46.050
2024-10-23 20:29:42,347 [bic.py] => training => Task 1, Epoch 31/150 => Loss 0.699, Train_accy 100.000, Test_accy 45.190
2024-10-23 20:29:43,746 [bic.py] => training => Task 1, Epoch 32/150 => Loss 0.699, Train_accy 100.000, Test_accy 47.790
2024-10-23 20:29:45,096 [bic.py] => training => Task 1, Epoch 33/150 => Loss 0.700, Train_accy 100.000, Test_accy 45.600
2024-10-23 20:29:46,447 [bic.py] => training => Task 1, Epoch 34/150 => Loss 0.699, Train_accy 100.000, Test_accy 47.600
2024-10-23 20:29:47,706 [bic.py] => training => Task 1, Epoch 35/150 => Loss 0.699, Train_accy 100.000, Test_accy 44.860
2024-10-23 20:29:49,101 [bic.py] => training => Task 1, Epoch 36/150 => Loss 0.699, Train_accy 100.000, Test_accy 45.570
2024-10-23 20:29:50,509 [bic.py] => training => Task 1, Epoch 37/150 => Loss 0.699, Train_accy 100.000, Test_accy 43.570
2024-10-23 20:29:51,775 [bic.py] => training => Task 1, Epoch 38/150 => Loss 0.698, Train_accy 100.000, Test_accy 46.740
2024-10-23 20:29:53,169 [bic.py] => training => Task 1, Epoch 39/150 => Loss 0.698, Train_accy 100.000, Test_accy 47.810
2024-10-23 20:29:54,555 [bic.py] => training => Task 1, Epoch 40/150 => Loss 0.699, Train_accy 100.000, Test_accy 46.360
2024-10-23 20:29:55,773 [bic.py] => training => Task 1, Epoch 41/150 => Loss 0.698, Train_accy 100.000, Test_accy 47.450
2024-10-23 20:29:57,022 [bic.py] => training => Task 1, Epoch 42/150 => Loss 0.699, Train_accy 100.000, Test_accy 46.640
2024-10-23 20:29:58,384 [bic.py] => training => Task 1, Epoch 43/150 => Loss 0.699, Train_accy 100.000, Test_accy 47.190
2024-10-23 20:29:59,653 [bic.py] => training => Task 1, Epoch 44/150 => Loss 0.698, Train_accy 100.000, Test_accy 48.480
2024-10-23 20:30:00,865 [bic.py] => training => Task 1, Epoch 45/150 => Loss 0.700, Train_accy 100.000, Test_accy 47.360
2024-10-23 20:30:02,138 [bic.py] => training => Task 1, Epoch 46/150 => Loss 0.701, Train_accy 100.000, Test_accy 45.670
2024-10-23 20:30:03,571 [bic.py] => training => Task 1, Epoch 47/150 => Loss 0.699, Train_accy 100.000, Test_accy 45.620
2024-10-23 20:30:04,850 [bic.py] => training => Task 1, Epoch 48/150 => Loss 0.699, Train_accy 100.000, Test_accy 47.900
2024-10-23 20:30:06,106 [bic.py] => training => Task 1, Epoch 49/150 => Loss 0.701, Train_accy 100.000, Test_accy 47.070
2024-10-23 20:30:07,318 [bic.py] => training => Task 1, Epoch 50/150 => Loss 0.699, Train_accy 100.000, Test_accy 43.120
2024-10-23 20:30:08,741 [bic.py] => training => Task 1, Epoch 51/150 => Loss 0.697, Train_accy 100.000, Test_accy 47.830
2024-10-23 20:30:10,011 [bic.py] => training => Task 1, Epoch 52/150 => Loss 0.696, Train_accy 100.000, Test_accy 47.400
2024-10-23 20:30:11,282 [bic.py] => training => Task 1, Epoch 53/150 => Loss 0.696, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:30:12,605 [bic.py] => training => Task 1, Epoch 54/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.620
2024-10-23 20:30:13,841 [bic.py] => training => Task 1, Epoch 55/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:30:15,032 [bic.py] => training => Task 1, Epoch 56/150 => Loss 0.696, Train_accy 100.000, Test_accy 48.120
2024-10-23 20:30:16,315 [bic.py] => training => Task 1, Epoch 57/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.240
2024-10-23 20:30:17,528 [bic.py] => training => Task 1, Epoch 58/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.190
2024-10-23 20:30:18,681 [bic.py] => training => Task 1, Epoch 59/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.400
2024-10-23 20:30:19,939 [bic.py] => training => Task 1, Epoch 60/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.740
2024-10-23 20:30:21,091 [bic.py] => training => Task 1, Epoch 61/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.620
2024-10-23 20:30:22,292 [bic.py] => training => Task 1, Epoch 62/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.670
2024-10-23 20:30:23,360 [bic.py] => training => Task 1, Epoch 63/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.900
2024-10-23 20:30:24,433 [bic.py] => training => Task 1, Epoch 64/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.830
2024-10-23 20:30:25,464 [bic.py] => training => Task 1, Epoch 65/150 => Loss 0.696, Train_accy 100.000, Test_accy 47.210
2024-10-23 20:30:26,507 [bic.py] => training => Task 1, Epoch 66/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:30:27,514 [bic.py] => training => Task 1, Epoch 67/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.240
2024-10-23 20:30:28,554 [bic.py] => training => Task 1, Epoch 68/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.670
2024-10-23 20:30:29,782 [bic.py] => training => Task 1, Epoch 69/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.210
2024-10-23 20:30:31,004 [bic.py] => training => Task 1, Epoch 70/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.600
2024-10-23 20:30:32,102 [bic.py] => training => Task 1, Epoch 71/150 => Loss 0.694, Train_accy 100.000, Test_accy 46.980
2024-10-23 20:30:33,242 [bic.py] => training => Task 1, Epoch 72/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:30:34,388 [bic.py] => training => Task 1, Epoch 73/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.930
2024-10-23 20:30:35,582 [bic.py] => training => Task 1, Epoch 74/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.880
2024-10-23 20:30:36,769 [bic.py] => training => Task 1, Epoch 75/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.900
2024-10-23 20:30:37,945 [bic.py] => training => Task 1, Epoch 76/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.810
2024-10-23 20:30:39,046 [bic.py] => training => Task 1, Epoch 77/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.790
2024-10-23 20:30:40,179 [bic.py] => training => Task 1, Epoch 78/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:30:41,250 [bic.py] => training => Task 1, Epoch 79/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.760
2024-10-23 20:30:42,500 [bic.py] => training => Task 1, Epoch 80/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.260
2024-10-23 20:30:43,663 [bic.py] => training => Task 1, Epoch 81/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.260
2024-10-23 20:30:44,801 [bic.py] => training => Task 1, Epoch 82/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.880
2024-10-23 20:30:45,986 [bic.py] => training => Task 1, Epoch 83/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:30:47,108 [bic.py] => training => Task 1, Epoch 84/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:30:48,156 [bic.py] => training => Task 1, Epoch 85/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:30:49,169 [bic.py] => training => Task 1, Epoch 86/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.260
2024-10-23 20:30:50,265 [bic.py] => training => Task 1, Epoch 87/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.860
2024-10-23 20:30:51,406 [bic.py] => training => Task 1, Epoch 88/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.710
2024-10-23 20:30:52,483 [bic.py] => training => Task 1, Epoch 89/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:30:53,654 [bic.py] => training => Task 1, Epoch 90/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:30:54,762 [bic.py] => training => Task 1, Epoch 91/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.070
2024-10-23 20:30:55,835 [bic.py] => training => Task 1, Epoch 92/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.900
2024-10-23 20:30:56,877 [bic.py] => training => Task 1, Epoch 93/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.690
2024-10-23 20:30:58,148 [bic.py] => training => Task 1, Epoch 94/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.140
2024-10-23 20:30:59,329 [bic.py] => training => Task 1, Epoch 95/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.140
2024-10-23 20:31:00,472 [bic.py] => training => Task 1, Epoch 96/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.310
2024-10-23 20:31:01,846 [bic.py] => training => Task 1, Epoch 97/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.190
2024-10-23 20:31:03,149 [bic.py] => training => Task 1, Epoch 98/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.880
2024-10-23 20:31:04,297 [bic.py] => training => Task 1, Epoch 99/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.880
2024-10-23 20:31:05,428 [bic.py] => training => Task 1, Epoch 100/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.120
2024-10-23 20:31:06,619 [bic.py] => training => Task 1, Epoch 101/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.070
2024-10-23 20:31:07,673 [bic.py] => training => Task 1, Epoch 102/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.810
2024-10-23 20:31:08,733 [bic.py] => training => Task 1, Epoch 103/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.190
2024-10-23 20:31:09,853 [bic.py] => training => Task 1, Epoch 104/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.120
2024-10-23 20:31:10,942 [bic.py] => training => Task 1, Epoch 105/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.830
2024-10-23 20:31:12,111 [bic.py] => training => Task 1, Epoch 106/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.070
2024-10-23 20:31:13,246 [bic.py] => training => Task 1, Epoch 107/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.190
2024-10-23 20:31:14,331 [bic.py] => training => Task 1, Epoch 108/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.260
2024-10-23 20:31:15,499 [bic.py] => training => Task 1, Epoch 109/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:31:16,778 [bic.py] => training => Task 1, Epoch 110/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:31:17,887 [bic.py] => training => Task 1, Epoch 111/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.790
2024-10-23 20:31:18,964 [bic.py] => training => Task 1, Epoch 112/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.070
2024-10-23 20:31:20,039 [bic.py] => training => Task 1, Epoch 113/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.050
2024-10-23 20:31:21,175 [bic.py] => training => Task 1, Epoch 114/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.050
2024-10-23 20:31:22,248 [bic.py] => training => Task 1, Epoch 115/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.240
2024-10-23 20:31:23,295 [bic.py] => training => Task 1, Epoch 116/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.760
2024-10-23 20:31:24,404 [bic.py] => training => Task 1, Epoch 117/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.070
2024-10-23 20:31:25,506 [bic.py] => training => Task 1, Epoch 118/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.020
2024-10-23 20:31:26,638 [bic.py] => training => Task 1, Epoch 119/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.310
2024-10-23 20:31:27,764 [bic.py] => training => Task 1, Epoch 120/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.360
2024-10-23 20:31:28,917 [bic.py] => training => Task 1, Epoch 121/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.140
2024-10-23 20:31:29,993 [bic.py] => training => Task 1, Epoch 122/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.640
2024-10-23 20:31:31,099 [bic.py] => training => Task 1, Epoch 123/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.810
2024-10-23 20:31:32,247 [bic.py] => training => Task 1, Epoch 124/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.380
2024-10-23 20:31:33,384 [bic.py] => training => Task 1, Epoch 125/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.880
2024-10-23 20:31:34,430 [bic.py] => training => Task 1, Epoch 126/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.190
2024-10-23 20:31:35,554 [bic.py] => training => Task 1, Epoch 127/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.050
2024-10-23 20:31:36,666 [bic.py] => training => Task 1, Epoch 128/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.710
2024-10-23 20:31:37,821 [bic.py] => training => Task 1, Epoch 129/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.240
2024-10-23 20:31:38,937 [bic.py] => training => Task 1, Epoch 130/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.310
2024-10-23 20:31:40,120 [bic.py] => training => Task 1, Epoch 131/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.860
2024-10-23 20:31:41,185 [bic.py] => training => Task 1, Epoch 132/150 => Loss 0.695, Train_accy 100.000, Test_accy 47.760
2024-10-23 20:31:42,305 [bic.py] => training => Task 1, Epoch 133/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:31:43,360 [bic.py] => training => Task 1, Epoch 134/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.140
2024-10-23 20:31:44,372 [bic.py] => training => Task 1, Epoch 135/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:31:45,391 [bic.py] => training => Task 1, Epoch 136/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:31:46,429 [bic.py] => training => Task 1, Epoch 137/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.140
2024-10-23 20:31:47,521 [bic.py] => training => Task 1, Epoch 138/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:31:48,694 [bic.py] => training => Task 1, Epoch 139/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.000
2024-10-23 20:31:49,900 [bic.py] => training => Task 1, Epoch 140/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:31:51,065 [bic.py] => training => Task 1, Epoch 141/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.050
2024-10-23 20:31:52,260 [bic.py] => training => Task 1, Epoch 142/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.640
2024-10-23 20:31:53,373 [bic.py] => training => Task 1, Epoch 143/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.170
2024-10-23 20:31:54,481 [bic.py] => training => Task 1, Epoch 144/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.190
2024-10-23 20:31:55,649 [bic.py] => training => Task 1, Epoch 145/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.100
2024-10-23 20:31:56,890 [bic.py] => training => Task 1, Epoch 146/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:31:58,148 [bic.py] => training => Task 1, Epoch 147/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:31:59,402 [bic.py] => training => Task 1, Epoch 148/150 => Loss 0.694, Train_accy 100.000, Test_accy 48.170
2024-10-23 20:32:00,741 [bic.py] => training => Task 1, Epoch 149/150 => Loss 0.695, Train_accy 100.000, Test_accy 48.310
2024-10-23 20:32:01,806 [bic.py] => training => Task 1, Epoch 150/150 => Loss 0.694, Train_accy 100.000, Test_accy 47.980
2024-10-23 20:32:02,105 [bic.py] => bias_correction => Task 1, Epoch 1/100 => Loss 1.745, Train_accy 35.710, Test_accy 48.550
2024-10-23 20:32:02,316 [bic.py] => bias_correction => Task 1, Epoch 2/100 => Loss 1.733, Train_accy 35.710, Test_accy 50.860
2024-10-23 20:32:02,548 [bic.py] => bias_correction => Task 1, Epoch 3/100 => Loss 1.705, Train_accy 64.290, Test_accy 54.760
2024-10-23 20:32:02,754 [bic.py] => bias_correction => Task 1, Epoch 4/100 => Loss 1.648, Train_accy 85.710, Test_accy 63.690
2024-10-23 20:32:02,959 [bic.py] => bias_correction => Task 1, Epoch 5/100 => Loss 1.537, Train_accy 92.860, Test_accy 72.930
2024-10-23 20:32:03,182 [bic.py] => bias_correction => Task 1, Epoch 6/100 => Loss 1.392, Train_accy 78.570, Test_accy 69.950
2024-10-23 20:32:03,433 [bic.py] => bias_correction => Task 1, Epoch 7/100 => Loss 1.400, Train_accy 71.430, Test_accy 63.900
2024-10-23 20:32:03,714 [bic.py] => bias_correction => Task 1, Epoch 8/100 => Loss 1.448, Train_accy 71.430, Test_accy 62.740
2024-10-23 20:32:03,914 [bic.py] => bias_correction => Task 1, Epoch 9/100 => Loss 1.458, Train_accy 71.430, Test_accy 62.670
2024-10-23 20:32:04,116 [bic.py] => bias_correction => Task 1, Epoch 10/100 => Loss 1.457, Train_accy 71.430, Test_accy 62.550
2024-10-23 20:32:04,316 [bic.py] => bias_correction => Task 1, Epoch 11/100 => Loss 1.456, Train_accy 71.430, Test_accy 62.380
2024-10-23 20:32:04,523 [bic.py] => bias_correction => Task 1, Epoch 12/100 => Loss 1.454, Train_accy 71.430, Test_accy 62.360
2024-10-23 20:32:04,750 [bic.py] => bias_correction => Task 1, Epoch 13/100 => Loss 1.453, Train_accy 71.430, Test_accy 62.290
2024-10-23 20:32:04,950 [bic.py] => bias_correction => Task 1, Epoch 14/100 => Loss 1.452, Train_accy 71.430, Test_accy 62.100
2024-10-23 20:32:05,146 [bic.py] => bias_correction => Task 1, Epoch 15/100 => Loss 1.452, Train_accy 71.430, Test_accy 62.000
2024-10-23 20:32:05,355 [bic.py] => bias_correction => Task 1, Epoch 16/100 => Loss 1.451, Train_accy 71.430, Test_accy 61.830
2024-10-23 20:32:05,551 [bic.py] => bias_correction => Task 1, Epoch 17/100 => Loss 1.451, Train_accy 71.430, Test_accy 61.810
2024-10-23 20:32:05,738 [bic.py] => bias_correction => Task 1, Epoch 18/100 => Loss 1.450, Train_accy 71.430, Test_accy 61.760
2024-10-23 20:32:05,930 [bic.py] => bias_correction => Task 1, Epoch 19/100 => Loss 1.450, Train_accy 71.430, Test_accy 61.690
2024-10-23 20:32:06,134 [bic.py] => bias_correction => Task 1, Epoch 20/100 => Loss 1.450, Train_accy 71.430, Test_accy 61.620
2024-10-23 20:32:06,331 [bic.py] => bias_correction => Task 1, Epoch 21/100 => Loss 1.450, Train_accy 71.430, Test_accy 61.550
2024-10-23 20:32:06,542 [bic.py] => bias_correction => Task 1, Epoch 22/100 => Loss 1.450, Train_accy 71.430, Test_accy 61.430
2024-10-23 20:32:06,743 [bic.py] => bias_correction => Task 1, Epoch 23/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.400
2024-10-23 20:32:06,936 [bic.py] => bias_correction => Task 1, Epoch 24/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.290
2024-10-23 20:32:07,143 [bic.py] => bias_correction => Task 1, Epoch 25/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.210
2024-10-23 20:32:07,333 [bic.py] => bias_correction => Task 1, Epoch 26/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.190
2024-10-23 20:32:07,528 [bic.py] => bias_correction => Task 1, Epoch 27/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.170
2024-10-23 20:32:07,724 [bic.py] => bias_correction => Task 1, Epoch 28/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.100
2024-10-23 20:32:07,922 [bic.py] => bias_correction => Task 1, Epoch 29/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.100
2024-10-23 20:32:08,116 [bic.py] => bias_correction => Task 1, Epoch 30/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.070
2024-10-23 20:32:08,318 [bic.py] => bias_correction => Task 1, Epoch 31/100 => Loss 1.449, Train_accy 71.430, Test_accy 61.000
2024-10-23 20:32:08,520 [bic.py] => bias_correction => Task 1, Epoch 32/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.980
2024-10-23 20:32:08,733 [bic.py] => bias_correction => Task 1, Epoch 33/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.980
2024-10-23 20:32:08,928 [bic.py] => bias_correction => Task 1, Epoch 34/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.980
2024-10-23 20:32:09,135 [bic.py] => bias_correction => Task 1, Epoch 35/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.980
2024-10-23 20:32:09,325 [bic.py] => bias_correction => Task 1, Epoch 36/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.980
2024-10-23 20:32:09,522 [bic.py] => bias_correction => Task 1, Epoch 37/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.980
2024-10-23 20:32:09,726 [bic.py] => bias_correction => Task 1, Epoch 38/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.930
2024-10-23 20:32:09,929 [bic.py] => bias_correction => Task 1, Epoch 39/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.900
2024-10-23 20:32:10,125 [bic.py] => bias_correction => Task 1, Epoch 40/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.880
2024-10-23 20:32:10,319 [bic.py] => bias_correction => Task 1, Epoch 41/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.880
2024-10-23 20:32:10,521 [bic.py] => bias_correction => Task 1, Epoch 42/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.880
2024-10-23 20:32:10,711 [bic.py] => bias_correction => Task 1, Epoch 43/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.860
2024-10-23 20:32:10,910 [bic.py] => bias_correction => Task 1, Epoch 44/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.860
2024-10-23 20:32:11,110 [bic.py] => bias_correction => Task 1, Epoch 45/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.830
2024-10-23 20:32:11,321 [bic.py] => bias_correction => Task 1, Epoch 46/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.810
2024-10-23 20:32:11,574 [bic.py] => bias_correction => Task 1, Epoch 47/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.790
2024-10-23 20:32:11,783 [bic.py] => bias_correction => Task 1, Epoch 48/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.810
2024-10-23 20:32:11,996 [bic.py] => bias_correction => Task 1, Epoch 49/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.790
2024-10-23 20:32:12,231 [bic.py] => bias_correction => Task 1, Epoch 50/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.790
2024-10-23 20:32:12,444 [bic.py] => bias_correction => Task 1, Epoch 51/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.790
2024-10-23 20:32:12,689 [bic.py] => bias_correction => Task 1, Epoch 52/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:12,920 [bic.py] => bias_correction => Task 1, Epoch 53/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:13,125 [bic.py] => bias_correction => Task 1, Epoch 54/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:13,328 [bic.py] => bias_correction => Task 1, Epoch 55/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:13,530 [bic.py] => bias_correction => Task 1, Epoch 56/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:13,734 [bic.py] => bias_correction => Task 1, Epoch 57/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:13,943 [bic.py] => bias_correction => Task 1, Epoch 58/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:14,131 [bic.py] => bias_correction => Task 1, Epoch 59/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:14,336 [bic.py] => bias_correction => Task 1, Epoch 60/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:14,524 [bic.py] => bias_correction => Task 1, Epoch 61/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:14,722 [bic.py] => bias_correction => Task 1, Epoch 62/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:14,940 [bic.py] => bias_correction => Task 1, Epoch 63/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.760
2024-10-23 20:32:15,178 [bic.py] => bias_correction => Task 1, Epoch 64/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:15,377 [bic.py] => bias_correction => Task 1, Epoch 65/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:15,582 [bic.py] => bias_correction => Task 1, Epoch 66/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:15,798 [bic.py] => bias_correction => Task 1, Epoch 67/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:16,001 [bic.py] => bias_correction => Task 1, Epoch 68/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:16,198 [bic.py] => bias_correction => Task 1, Epoch 69/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:16,396 [bic.py] => bias_correction => Task 1, Epoch 70/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:16,597 [bic.py] => bias_correction => Task 1, Epoch 71/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:16,798 [bic.py] => bias_correction => Task 1, Epoch 72/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:16,997 [bic.py] => bias_correction => Task 1, Epoch 73/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:17,213 [bic.py] => bias_correction => Task 1, Epoch 74/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:17,439 [bic.py] => bias_correction => Task 1, Epoch 75/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:17,666 [bic.py] => bias_correction => Task 1, Epoch 76/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:17,887 [bic.py] => bias_correction => Task 1, Epoch 77/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:18,097 [bic.py] => bias_correction => Task 1, Epoch 78/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:18,304 [bic.py] => bias_correction => Task 1, Epoch 79/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:18,519 [bic.py] => bias_correction => Task 1, Epoch 80/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:18,720 [bic.py] => bias_correction => Task 1, Epoch 81/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:18,922 [bic.py] => bias_correction => Task 1, Epoch 82/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:19,128 [bic.py] => bias_correction => Task 1, Epoch 83/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:19,332 [bic.py] => bias_correction => Task 1, Epoch 84/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:19,552 [bic.py] => bias_correction => Task 1, Epoch 85/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:19,759 [bic.py] => bias_correction => Task 1, Epoch 86/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:19,977 [bic.py] => bias_correction => Task 1, Epoch 87/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:20,184 [bic.py] => bias_correction => Task 1, Epoch 88/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:20,388 [bic.py] => bias_correction => Task 1, Epoch 89/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:20,589 [bic.py] => bias_correction => Task 1, Epoch 90/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:20,794 [bic.py] => bias_correction => Task 1, Epoch 91/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:20,994 [bic.py] => bias_correction => Task 1, Epoch 92/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:21,203 [bic.py] => bias_correction => Task 1, Epoch 93/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:21,446 [bic.py] => bias_correction => Task 1, Epoch 94/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:21,701 [bic.py] => bias_correction => Task 1, Epoch 95/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:21,927 [bic.py] => bias_correction => Task 1, Epoch 96/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:22,130 [bic.py] => bias_correction => Task 1, Epoch 97/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:22,341 [bic.py] => bias_correction => Task 1, Epoch 98/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:22,546 [bic.py] => bias_correction => Task 1, Epoch 99/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:22,787 [bic.py] => bias_correction => Task 1, Epoch 100/100 => Loss 1.449, Train_accy 71.430, Test_accy 60.740
2024-10-23 20:32:22,787 [base.py] => Reducing exemplars...(14 per classes)
2024-10-23 20:32:23,903 [base.py] => Constructing exemplars...(14 per classes)
2024-10-23 20:32:25,351 [bic.py] => Parameters of bias layer:
2024-10-23 20:32:25,351 [bic.py] => 0 => 1.000, 0.000
2024-10-23 20:32:25,351 [bic.py] => 1 => -1.507, -0.550
2024-10-23 20:32:25,352 [trainer.py] => All params: 3847499
2024-10-23 20:32:25,825 [bic.py] => Exemplar size: 98
2024-10-23 20:32:25,825 [trainer.py] => CNN: {'total': 60.74, '00-04': 85.03, '05-06': 0.0, 'old': 85.03, 'new': 0.0}
2024-10-23 20:32:25,826 [trainer.py] => NME: {'total': 73.1, '00-04': 66.7, '05-06': 89.08, 'old': 66.7, 'new': 89.08}
2024-10-23 20:32:25,826 [trainer.py] => CNN top1 curve: [89.93, 60.74]
2024-10-23 20:32:25,826 [trainer.py] => CNN top5 curve: [100.0, 71.21]
2024-10-23 20:32:25,826 [trainer.py] => NME top1 curve: [90.0, 73.1]
2024-10-23 20:32:25,826 [trainer.py] => NME top5 curve: [100.0, 98.98]

2024-10-23 20:32:25,826 [trainer.py] => Average Accuracy (CNN): 75.33500000000001
2024-10-23 20:32:25,826 [trainer.py] => Average Accuracy (NME): 81.55
2024-10-23 20:32:25,826 [trainer.py] => All params: 3847499
2024-10-23 20:32:25,827 [trainer.py] => Trainable params: 3847499
2024-10-23 20:32:25,828 [bic.py] => Learning on 7-9
2024-10-23 20:32:25,844 [bic.py] => Stage1 dset: 4089, Stage2 dset: 9
2024-10-23 20:32:25,844 [bic.py] => Lambda: 0.778
2024-10-23 20:32:25,852 [bic.py] => Parameters of bias layer:
2024-10-23 20:32:25,853 [bic.py] => 0 => 1.000, 0.000
2024-10-23 20:32:25,853 [bic.py] => 1 => -1.507, -0.550
2024-10-23 20:32:25,853 [bic.py] => 2 => 1.000, 0.000
2024-10-23 20:32:27,051 [bic.py] => training => Task 2, Epoch 1/150 => Loss 1.083, Train_accy 96.090, Test_accy 22.220
2024-10-23 20:32:28,262 [bic.py] => training => Task 2, Epoch 2/150 => Loss 0.959, Train_accy 97.920, Test_accy 24.200
2024-10-23 20:32:29,445 [bic.py] => training => Task 2, Epoch 3/150 => Loss 0.939, Train_accy 98.750, Test_accy 29.760
2024-10-23 20:32:30,679 [bic.py] => training => Task 2, Epoch 4/150 => Loss 0.925, Train_accy 98.880, Test_accy 29.690
2024-10-23 20:32:31,890 [bic.py] => training => Task 2, Epoch 5/150 => Loss 0.918, Train_accy 99.190, Test_accy 28.350
2024-10-23 20:32:33,166 [bic.py] => training => Task 2, Epoch 6/150 => Loss 0.913, Train_accy 99.240, Test_accy 28.800
2024-10-23 20:32:34,302 [bic.py] => training => Task 2, Epoch 7/150 => Loss 0.913, Train_accy 99.320, Test_accy 33.350
2024-10-23 20:32:35,439 [bic.py] => training => Task 2, Epoch 8/150 => Loss 0.912, Train_accy 99.360, Test_accy 30.280
2024-10-23 20:32:36,632 [bic.py] => training => Task 2, Epoch 9/150 => Loss 0.908, Train_accy 99.360, Test_accy 31.310
2024-10-23 20:32:37,725 [bic.py] => training => Task 2, Epoch 10/150 => Loss 0.906, Train_accy 99.710, Test_accy 30.300
2024-10-23 20:32:38,942 [bic.py] => training => Task 2, Epoch 11/150 => Loss 0.904, Train_accy 99.830, Test_accy 29.610
2024-10-23 20:32:40,114 [bic.py] => training => Task 2, Epoch 12/150 => Loss 0.904, Train_accy 99.830, Test_accy 29.020
2024-10-23 20:32:41,228 [bic.py] => training => Task 2, Epoch 13/150 => Loss 0.903, Train_accy 99.800, Test_accy 31.300
2024-10-23 20:32:42,388 [bic.py] => training => Task 2, Epoch 14/150 => Loss 0.903, Train_accy 99.900, Test_accy 31.460
2024-10-23 20:32:43,413 [bic.py] => training => Task 2, Epoch 15/150 => Loss 0.902, Train_accy 99.850, Test_accy 32.480
2024-10-23 20:32:44,455 [bic.py] => training => Task 2, Epoch 16/150 => Loss 0.903, Train_accy 99.780, Test_accy 28.200
2024-10-23 20:32:45,584 [bic.py] => training => Task 2, Epoch 17/150 => Loss 0.901, Train_accy 99.800, Test_accy 30.260
2024-10-23 20:32:46,743 [bic.py] => training => Task 2, Epoch 18/150 => Loss 0.902, Train_accy 99.880, Test_accy 32.430
2024-10-23 20:32:47,890 [bic.py] => training => Task 2, Epoch 19/150 => Loss 0.901, Train_accy 99.830, Test_accy 28.540
2024-10-23 20:32:49,048 [bic.py] => training => Task 2, Epoch 20/150 => Loss 0.902, Train_accy 99.830, Test_accy 31.370
2024-10-23 20:32:50,137 [bic.py] => training => Task 2, Epoch 21/150 => Loss 0.901, Train_accy 99.950, Test_accy 30.040
2024-10-23 20:32:51,253 [bic.py] => training => Task 2, Epoch 22/150 => Loss 0.900, Train_accy 99.800, Test_accy 30.260
2024-10-23 20:32:52,322 [bic.py] => training => Task 2, Epoch 23/150 => Loss 0.900, Train_accy 99.880, Test_accy 30.170
2024-10-23 20:32:53,502 [bic.py] => training => Task 2, Epoch 24/150 => Loss 0.901, Train_accy 99.800, Test_accy 28.220
2024-10-23 20:32:54,676 [bic.py] => training => Task 2, Epoch 25/150 => Loss 0.902, Train_accy 99.760, Test_accy 29.540
2024-10-23 20:32:55,876 [bic.py] => training => Task 2, Epoch 26/150 => Loss 0.900, Train_accy 99.780, Test_accy 29.500
2024-10-23 20:32:57,059 [bic.py] => training => Task 2, Epoch 27/150 => Loss 0.900, Train_accy 99.930, Test_accy 30.890
2024-10-23 20:32:58,174 [bic.py] => training => Task 2, Epoch 28/150 => Loss 0.900, Train_accy 99.900, Test_accy 32.720
2024-10-23 20:32:59,335 [bic.py] => training => Task 2, Epoch 29/150 => Loss 0.899, Train_accy 99.900, Test_accy 33.870
2024-10-23 20:33:00,520 [bic.py] => training => Task 2, Epoch 30/150 => Loss 0.900, Train_accy 99.930, Test_accy 33.690
2024-10-23 20:33:01,657 [bic.py] => training => Task 2, Epoch 31/150 => Loss 0.901, Train_accy 99.980, Test_accy 31.590
2024-10-23 20:33:02,931 [bic.py] => training => Task 2, Epoch 32/150 => Loss 0.901, Train_accy 99.850, Test_accy 29.220
2024-10-23 20:33:04,070 [bic.py] => training => Task 2, Epoch 33/150 => Loss 0.900, Train_accy 99.830, Test_accy 31.350
2024-10-23 20:33:05,176 [bic.py] => training => Task 2, Epoch 34/150 => Loss 0.900, Train_accy 99.880, Test_accy 29.370
2024-10-23 20:33:06,253 [bic.py] => training => Task 2, Epoch 35/150 => Loss 0.900, Train_accy 99.630, Test_accy 28.430
2024-10-23 20:33:07,425 [bic.py] => training => Task 2, Epoch 36/150 => Loss 0.899, Train_accy 99.830, Test_accy 30.110
2024-10-23 20:33:08,564 [bic.py] => training => Task 2, Epoch 37/150 => Loss 0.900, Train_accy 99.980, Test_accy 31.260
2024-10-23 20:33:09,721 [bic.py] => training => Task 2, Epoch 38/150 => Loss 0.901, Train_accy 99.730, Test_accy 30.390
2024-10-23 20:33:10,784 [bic.py] => training => Task 2, Epoch 39/150 => Loss 0.899, Train_accy 99.900, Test_accy 31.540
2024-10-23 20:33:11,936 [bic.py] => training => Task 2, Epoch 40/150 => Loss 0.900, Train_accy 99.760, Test_accy 29.000
2024-10-23 20:33:13,098 [bic.py] => training => Task 2, Epoch 41/150 => Loss 0.900, Train_accy 99.830, Test_accy 30.170
2024-10-23 20:33:14,287 [bic.py] => training => Task 2, Epoch 42/150 => Loss 0.899, Train_accy 99.930, Test_accy 29.480
2024-10-23 20:33:15,459 [bic.py] => training => Task 2, Epoch 43/150 => Loss 0.899, Train_accy 99.760, Test_accy 28.200
2024-10-23 20:33:16,554 [bic.py] => training => Task 2, Epoch 44/150 => Loss 0.899, Train_accy 99.950, Test_accy 34.500
2024-10-23 20:33:17,610 [bic.py] => training => Task 2, Epoch 45/150 => Loss 0.899, Train_accy 99.900, Test_accy 30.000
2024-10-23 20:33:18,814 [bic.py] => training => Task 2, Epoch 46/150 => Loss 0.900, Train_accy 99.850, Test_accy 28.440
2024-10-23 20:33:19,953 [bic.py] => training => Task 2, Epoch 47/150 => Loss 0.899, Train_accy 99.880, Test_accy 29.810
2024-10-23 20:33:21,085 [bic.py] => training => Task 2, Epoch 48/150 => Loss 0.899, Train_accy 99.880, Test_accy 29.280
2024-10-23 20:33:22,195 [bic.py] => training => Task 2, Epoch 49/150 => Loss 0.898, Train_accy 99.850, Test_accy 30.940
2024-10-23 20:33:23,317 [bic.py] => training => Task 2, Epoch 50/150 => Loss 0.899, Train_accy 99.780, Test_accy 31.430
2024-10-23 20:33:24,443 [bic.py] => training => Task 2, Epoch 51/150 => Loss 0.898, Train_accy 99.830, Test_accy 29.670
2024-10-23 20:33:25,551 [bic.py] => training => Task 2, Epoch 52/150 => Loss 0.897, Train_accy 99.900, Test_accy 30.260
2024-10-23 20:33:26,657 [bic.py] => training => Task 2, Epoch 53/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.570
2024-10-23 20:33:27,779 [bic.py] => training => Task 2, Epoch 54/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.520
2024-10-23 20:33:28,790 [bic.py] => training => Task 2, Epoch 55/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.780
2024-10-23 20:33:29,791 [bic.py] => training => Task 2, Epoch 56/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.440
2024-10-23 20:33:30,803 [bic.py] => training => Task 2, Epoch 57/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.930
2024-10-23 20:33:31,880 [bic.py] => training => Task 2, Epoch 58/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.330
2024-10-23 20:33:33,014 [bic.py] => training => Task 2, Epoch 59/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.960
2024-10-23 20:33:34,101 [bic.py] => training => Task 2, Epoch 60/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.690
2024-10-23 20:33:35,232 [bic.py] => training => Task 2, Epoch 61/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.410
2024-10-23 20:33:36,368 [bic.py] => training => Task 2, Epoch 62/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.540
2024-10-23 20:33:37,479 [bic.py] => training => Task 2, Epoch 63/150 => Loss 0.896, Train_accy 99.830, Test_accy 30.300
2024-10-23 20:33:38,538 [bic.py] => training => Task 2, Epoch 64/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.170
2024-10-23 20:33:39,537 [bic.py] => training => Task 2, Epoch 65/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.370
2024-10-23 20:33:40,647 [bic.py] => training => Task 2, Epoch 66/150 => Loss 0.895, Train_accy 99.930, Test_accy 31.300
2024-10-23 20:33:41,773 [bic.py] => training => Task 2, Epoch 67/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.670
2024-10-23 20:33:42,989 [bic.py] => training => Task 2, Epoch 68/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.260
2024-10-23 20:33:44,213 [bic.py] => training => Task 2, Epoch 69/150 => Loss 0.896, Train_accy 99.830, Test_accy 30.090
2024-10-23 20:33:45,411 [bic.py] => training => Task 2, Epoch 70/150 => Loss 0.897, Train_accy 99.850, Test_accy 30.350
2024-10-23 20:33:46,599 [bic.py] => training => Task 2, Epoch 71/150 => Loss 0.896, Train_accy 99.950, Test_accy 30.940
2024-10-23 20:33:47,673 [bic.py] => training => Task 2, Epoch 72/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.520
2024-10-23 20:33:48,741 [bic.py] => training => Task 2, Epoch 73/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.150
2024-10-23 20:33:49,969 [bic.py] => training => Task 2, Epoch 74/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.440
2024-10-23 20:33:51,128 [bic.py] => training => Task 2, Epoch 75/150 => Loss 0.895, Train_accy 99.850, Test_accy 29.980
2024-10-23 20:33:52,344 [bic.py] => training => Task 2, Epoch 76/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.910
2024-10-23 20:33:53,438 [bic.py] => training => Task 2, Epoch 77/150 => Loss 0.896, Train_accy 99.830, Test_accy 29.690
2024-10-23 20:33:54,492 [bic.py] => training => Task 2, Epoch 78/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.560
2024-10-23 20:33:55,519 [bic.py] => training => Task 2, Epoch 79/150 => Loss 0.896, Train_accy 99.830, Test_accy 29.760
2024-10-23 20:33:56,655 [bic.py] => training => Task 2, Epoch 80/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.890
2024-10-23 20:33:57,757 [bic.py] => training => Task 2, Epoch 81/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.690
2024-10-23 20:33:58,974 [bic.py] => training => Task 2, Epoch 82/150 => Loss 0.895, Train_accy 99.830, Test_accy 29.800
2024-10-23 20:34:00,178 [bic.py] => training => Task 2, Epoch 83/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.110
2024-10-23 20:34:01,333 [bic.py] => training => Task 2, Epoch 84/150 => Loss 0.896, Train_accy 99.830, Test_accy 30.040
2024-10-23 20:34:02,444 [bic.py] => training => Task 2, Epoch 85/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.280
2024-10-23 20:34:03,542 [bic.py] => training => Task 2, Epoch 86/150 => Loss 0.896, Train_accy 99.830, Test_accy 30.190
2024-10-23 20:34:04,702 [bic.py] => training => Task 2, Epoch 87/150 => Loss 0.897, Train_accy 99.850, Test_accy 30.090
2024-10-23 20:34:05,937 [bic.py] => training => Task 2, Epoch 88/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.410
2024-10-23 20:34:07,136 [bic.py] => training => Task 2, Epoch 89/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.910
2024-10-23 20:34:08,232 [bic.py] => training => Task 2, Epoch 90/150 => Loss 0.895, Train_accy 99.880, Test_accy 30.300
2024-10-23 20:34:09,343 [bic.py] => training => Task 2, Epoch 91/150 => Loss 0.895, Train_accy 99.930, Test_accy 30.890
2024-10-23 20:34:10,460 [bic.py] => training => Task 2, Epoch 92/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.800
2024-10-23 20:34:11,600 [bic.py] => training => Task 2, Epoch 93/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.830
2024-10-23 20:34:12,707 [bic.py] => training => Task 2, Epoch 94/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.440
2024-10-23 20:34:13,782 [bic.py] => training => Task 2, Epoch 95/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.110
2024-10-23 20:34:14,814 [bic.py] => training => Task 2, Epoch 96/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.760
2024-10-23 20:34:15,834 [bic.py] => training => Task 2, Epoch 97/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.460
2024-10-23 20:34:16,898 [bic.py] => training => Task 2, Epoch 98/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.410
2024-10-23 20:34:17,943 [bic.py] => training => Task 2, Epoch 99/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.650
2024-10-23 20:34:19,069 [bic.py] => training => Task 2, Epoch 100/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.630
2024-10-23 20:34:20,122 [bic.py] => training => Task 2, Epoch 101/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.110
2024-10-23 20:34:21,222 [bic.py] => training => Task 2, Epoch 102/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.000
2024-10-23 20:34:22,321 [bic.py] => training => Task 2, Epoch 103/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.870
2024-10-23 20:34:23,356 [bic.py] => training => Task 2, Epoch 104/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.870
2024-10-23 20:34:24,374 [bic.py] => training => Task 2, Epoch 105/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.310
2024-10-23 20:34:25,431 [bic.py] => training => Task 2, Epoch 106/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.170
2024-10-23 20:34:26,441 [bic.py] => training => Task 2, Epoch 107/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.810
2024-10-23 20:34:27,511 [bic.py] => training => Task 2, Epoch 108/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.960
2024-10-23 20:34:28,568 [bic.py] => training => Task 2, Epoch 109/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.190
2024-10-23 20:34:29,584 [bic.py] => training => Task 2, Epoch 110/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.060
2024-10-23 20:34:30,700 [bic.py] => training => Task 2, Epoch 111/150 => Loss 0.895, Train_accy 99.900, Test_accy 30.740
2024-10-23 20:34:31,830 [bic.py] => training => Task 2, Epoch 112/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.190
2024-10-23 20:34:32,953 [bic.py] => training => Task 2, Epoch 113/150 => Loss 0.896, Train_accy 99.930, Test_accy 30.960
2024-10-23 20:34:33,950 [bic.py] => training => Task 2, Epoch 114/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.260
2024-10-23 20:34:34,961 [bic.py] => training => Task 2, Epoch 115/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.020
2024-10-23 20:34:35,969 [bic.py] => training => Task 2, Epoch 116/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.440
2024-10-23 20:34:37,004 [bic.py] => training => Task 2, Epoch 117/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.940
2024-10-23 20:34:38,047 [bic.py] => training => Task 2, Epoch 118/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.040
2024-10-23 20:34:39,072 [bic.py] => training => Task 2, Epoch 119/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.410
2024-10-23 20:34:40,107 [bic.py] => training => Task 2, Epoch 120/150 => Loss 0.896, Train_accy 99.900, Test_accy 30.540
2024-10-23 20:34:41,160 [bic.py] => training => Task 2, Epoch 121/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.500
2024-10-23 20:34:42,167 [bic.py] => training => Task 2, Epoch 122/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.850
2024-10-23 20:34:43,191 [bic.py] => training => Task 2, Epoch 123/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.930
2024-10-23 20:34:44,202 [bic.py] => training => Task 2, Epoch 124/150 => Loss 0.896, Train_accy 99.830, Test_accy 29.630
2024-10-23 20:34:45,260 [bic.py] => training => Task 2, Epoch 125/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.130
2024-10-23 20:34:46,275 [bic.py] => training => Task 2, Epoch 126/150 => Loss 0.895, Train_accy 99.880, Test_accy 30.310
2024-10-23 20:34:47,317 [bic.py] => training => Task 2, Epoch 127/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.330
2024-10-23 20:34:48,352 [bic.py] => training => Task 2, Epoch 128/150 => Loss 0.896, Train_accy 99.850, Test_accy 29.870
2024-10-23 20:34:49,409 [bic.py] => training => Task 2, Epoch 129/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.150
2024-10-23 20:34:50,426 [bic.py] => training => Task 2, Epoch 130/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.310
2024-10-23 20:34:51,435 [bic.py] => training => Task 2, Epoch 131/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.020
2024-10-23 20:34:52,498 [bic.py] => training => Task 2, Epoch 132/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.500
2024-10-23 20:34:53,562 [bic.py] => training => Task 2, Epoch 133/150 => Loss 0.895, Train_accy 99.830, Test_accy 29.720
2024-10-23 20:34:54,550 [bic.py] => training => Task 2, Epoch 134/150 => Loss 0.896, Train_accy 99.830, Test_accy 29.830
2024-10-23 20:34:55,594 [bic.py] => training => Task 2, Epoch 135/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.350
2024-10-23 20:34:56,611 [bic.py] => training => Task 2, Epoch 136/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.500
2024-10-23 20:34:57,649 [bic.py] => training => Task 2, Epoch 137/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.520
2024-10-23 20:34:58,727 [bic.py] => training => Task 2, Epoch 138/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.330
2024-10-23 20:34:59,745 [bic.py] => training => Task 2, Epoch 139/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.190
2024-10-23 20:35:00,777 [bic.py] => training => Task 2, Epoch 140/150 => Loss 0.895, Train_accy 99.880, Test_accy 30.590
2024-10-23 20:35:01,893 [bic.py] => training => Task 2, Epoch 141/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.460
2024-10-23 20:35:02,941 [bic.py] => training => Task 2, Epoch 142/150 => Loss 0.895, Train_accy 99.850, Test_accy 29.960
2024-10-23 20:35:03,965 [bic.py] => training => Task 2, Epoch 143/150 => Loss 0.895, Train_accy 99.830, Test_accy 29.810
2024-10-23 20:35:04,978 [bic.py] => training => Task 2, Epoch 144/150 => Loss 0.896, Train_accy 99.850, Test_accy 30.280
2024-10-23 20:35:05,996 [bic.py] => training => Task 2, Epoch 145/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.350
2024-10-23 20:35:06,984 [bic.py] => training => Task 2, Epoch 146/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.020
2024-10-23 20:35:08,025 [bic.py] => training => Task 2, Epoch 147/150 => Loss 0.896, Train_accy 99.880, Test_accy 30.500
2024-10-23 20:35:09,045 [bic.py] => training => Task 2, Epoch 148/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.110
2024-10-23 20:35:10,054 [bic.py] => training => Task 2, Epoch 149/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.430
2024-10-23 20:35:11,046 [bic.py] => training => Task 2, Epoch 150/150 => Loss 0.895, Train_accy 99.850, Test_accy 30.350
2024-10-23 20:35:11,290 [bic.py] => bias_correction => Task 2, Epoch 1/10 => Loss 2.095, Train_accy 33.330, Test_accy 29.370
2024-10-23 20:35:11,498 [bic.py] => bias_correction => Task 2, Epoch 2/10 => Loss 2.093, Train_accy 33.330, Test_accy 28.800
2024-10-23 20:35:11,708 [bic.py] => bias_correction => Task 2, Epoch 3/10 => Loss 2.089, Train_accy 33.330, Test_accy 28.700
2024-10-23 20:35:11,917 [bic.py] => bias_correction => Task 2, Epoch 4/10 => Loss 2.083, Train_accy 33.330, Test_accy 28.810
2024-10-23 20:35:12,119 [bic.py] => bias_correction => Task 2, Epoch 5/10 => Loss 2.073, Train_accy 44.440, Test_accy 29.300
2024-10-23 20:35:12,328 [bic.py] => bias_correction => Task 2, Epoch 6/10 => Loss 2.058, Train_accy 44.440, Test_accy 30.390
2024-10-23 20:35:12,535 [bic.py] => bias_correction => Task 2, Epoch 7/10 => Loss 2.035, Train_accy 44.440, Test_accy 32.300
2024-10-23 20:35:12,741 [bic.py] => bias_correction => Task 2, Epoch 8/10 => Loss 1.999, Train_accy 55.560, Test_accy 36.300
2024-10-23 20:35:12,943 [bic.py] => bias_correction => Task 2, Epoch 9/10 => Loss 1.933, Train_accy 77.780, Test_accy 43.890
2024-10-23 20:35:13,144 [bic.py] => bias_correction => Task 2, Epoch 10/10 => Loss 1.832, Train_accy 66.670, Test_accy 45.610
2024-10-23 20:35:13,145 [base.py] => Reducing exemplars...(11 per classes)
2024-10-23 20:35:14,582 [base.py] => Constructing exemplars...(11 per classes)
2024-10-23 20:35:15,970 [bic.py] => Parameters of bias layer:
2024-10-23 20:35:15,971 [bic.py] => 0 => 1.000, 0.000
2024-10-23 20:35:15,971 [bic.py] => 1 => -1.507, -0.550
2024-10-23 20:35:15,971 [bic.py] => 2 => 0.116, -0.212
2024-10-23 20:35:15,972 [trainer.py] => All params: 3848527
2024-10-23 20:35:16,457 [bic.py] => Exemplar size: 99
2024-10-23 20:35:16,458 [trainer.py] => CNN: {'total': 45.61, '00-04': 65.53, '05-06': 0.0, '07-08': 41.42, 'old': 46.81, 'new': 41.42}
2024-10-23 20:35:16,458 [trainer.py] => NME: {'total': 47.46, '00-04': 46.4, '05-06': 1.75, '07-08': 95.83, 'old': 33.64, 'new': 95.83}
2024-10-23 20:35:16,458 [trainer.py] => CNN top1 curve: [89.93, 60.74, 45.61]
2024-10-23 20:35:16,458 [trainer.py] => CNN top5 curve: [100.0, 71.21, 75.31]
2024-10-23 20:35:16,458 [trainer.py] => NME top1 curve: [90.0, 73.1, 47.46]
2024-10-23 20:35:16,458 [trainer.py] => NME top5 curve: [100.0, 98.98, 80.24]

2024-10-23 20:35:16,458 [trainer.py] => Average Accuracy (CNN): 65.42666666666668
2024-10-23 20:35:16,458 [trainer.py] => Average Accuracy (NME): 70.18666666666667
2024-10-23 20:35:16,459 [trainer.py] => Forgetting (CNN): 12.200000000000003
