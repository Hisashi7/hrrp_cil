2024-09-02 15:32:25,835 [trainer.py] => config: ./exps/foster.json
2024-09-02 15:32:25,835 [trainer.py] => prefix: cil
2024-09-02 15:32:25,835 [trainer.py] => dataset: hrrp9
2024-09-02 15:32:25,835 [trainer.py] => memory_size: 500
2024-09-02 15:32:25,835 [trainer.py] => memory_per_class: 20
2024-09-02 15:32:25,835 [trainer.py] => fixed_memory: False
2024-09-02 15:32:25,835 [trainer.py] => shuffle: True
2024-09-02 15:32:25,835 [trainer.py] => init_cls: 5
2024-09-02 15:32:25,836 [trainer.py] => increment: 2
2024-09-02 15:32:25,836 [trainer.py] => model_name: foster
2024-09-02 15:32:25,836 [trainer.py] => convnet_type: resnet18
2024-09-02 15:32:25,836 [trainer.py] => init_train: False
2024-09-02 15:32:25,836 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-09-02 15:32:25,836 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-09-02 15:32:25,836 [trainer.py] => device: [device(type='cuda', index=2)]
2024-09-02 15:32:25,836 [trainer.py] => seed: 1993
2024-09-02 15:32:25,836 [trainer.py] => beta1: 0.96
2024-09-02 15:32:25,836 [trainer.py] => beta2: 0.97
2024-09-02 15:32:25,836 [trainer.py] => oofc: ft
2024-09-02 15:32:25,836 [trainer.py] => is_teacher_wa: True
2024-09-02 15:32:25,836 [trainer.py] => is_student_wa: True
2024-09-02 15:32:25,836 [trainer.py] => lambda_okd: 0
2024-09-02 15:32:25,836 [trainer.py] => wa_value: 1
2024-09-02 15:32:25,836 [trainer.py] => init_epochs: 0
2024-09-02 15:32:25,836 [trainer.py] => init_lr: 0.1
2024-09-02 15:32:25,836 [trainer.py] => init_weight_decay: 0.0005
2024-09-02 15:32:25,836 [trainer.py] => boosting_epochs: 120
2024-09-02 15:32:25,836 [trainer.py] => compression_epochs: 80
2024-09-02 15:32:25,836 [trainer.py] => lr: 0.1
2024-09-02 15:32:25,836 [trainer.py] => batch_size: 128
2024-09-02 15:32:25,836 [trainer.py] => weight_decay: 0.0005
2024-09-02 15:32:25,836 [trainer.py] => num_workers: 8
2024-09-02 15:32:25,836 [trainer.py] => T: 2
2024-09-02 15:32:26,342 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-09-02 15:32:26,382 [trainer.py] => All params: 0
2024-09-02 15:32:26,383 [trainer.py] => Trainable params: 0
2024-09-02 15:32:28,573 [foster.py] => Learning on 0-5
2024-09-02 15:32:28,574 [foster.py] => All params: 3849034
2024-09-02 15:32:28,574 [foster.py] => Trainable params: 3849034
2024-09-02 15:32:30,376 [foster.py] => init_train?---False
2024-09-02 15:32:33,974 [base.py] => Reducing exemplars...(100 per classes)
2024-09-02 15:32:33,974 [base.py] => Constructing exemplars...(100 per classes)
2024-09-02 15:32:48,494 [foster.py] => Exemplar size: 500
2024-09-02 15:32:48,494 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-09-02 15:32:48,494 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-09-02 15:32:48,495 [trainer.py] => CNN top1 curve: [89.93]
2024-09-02 15:32:48,495 [trainer.py] => CNN top5 curve: [100.0]
2024-09-02 15:32:48,495 [trainer.py] => NME top1 curve: [90.0]
2024-09-02 15:32:48,495 [trainer.py] => NME top5 curve: [100.0]

2024-09-02 15:32:48,495 [trainer.py] => Average Accuracy (CNN): 89.93
2024-09-02 15:32:48,495 [trainer.py] => Average Accuracy (NME): 90.0
2024-09-02 15:32:48,495 [trainer.py] => All params: 3849034
2024-09-02 15:32:48,496 [trainer.py] => Trainable params: 3849034
2024-09-02 15:32:48,540 [foster.py] => Learning on 5-7
2024-09-02 15:32:48,542 [foster.py] => All params: 7701139
2024-09-02 15:32:48,543 [foster.py] => Trainable params: 3854670
2024-09-02 15:32:48,575 [foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-09-02 15:32:52,502 [foster.py] => Task 1, Epoch 1/120 => Loss 1.482, Loss_clf 0.721, Loss_fe 0.761, Loss_kd 0.000, Train_accy 81.60, Test_accy 29.14
2024-09-02 15:33:03,520 [foster.py] => Task 1, Epoch 6/120 => Loss 0.257, Loss_clf 0.056, Loss_fe 0.201, Loss_kd 0.000, Train_accy 98.13, Test_accy 68.33
2024-09-02 15:33:15,006 [foster.py] => Task 1, Epoch 11/120 => Loss 0.192, Loss_clf 0.050, Loss_fe 0.141, Loss_kd 0.000, Train_accy 98.31, Test_accy 63.43
2024-09-02 15:33:26,477 [foster.py] => Task 1, Epoch 16/120 => Loss 0.083, Loss_clf 0.026, Loss_fe 0.057, Loss_kd 0.000, Train_accy 99.33, Test_accy 66.55
2024-09-02 15:33:37,841 [foster.py] => Task 1, Epoch 21/120 => Loss 0.144, Loss_clf 0.041, Loss_fe 0.104, Loss_kd 0.000, Train_accy 99.02, Test_accy 60.40
2024-09-02 15:33:48,861 [foster.py] => Task 1, Epoch 26/120 => Loss 0.024, Loss_clf 0.007, Loss_fe 0.017, Loss_kd 0.000, Train_accy 99.96, Test_accy 69.26
2024-09-02 15:34:00,242 [foster.py] => Task 1, Epoch 31/120 => Loss 0.005, Loss_clf 0.002, Loss_fe 0.003, Loss_kd 0.000, Train_accy 100.00, Test_accy 72.33
2024-09-02 15:34:11,655 [foster.py] => Task 1, Epoch 36/120 => Loss 0.008, Loss_clf 0.003, Loss_fe 0.005, Loss_kd 0.000, Train_accy 99.96, Test_accy 71.10
2024-09-02 15:34:22,528 [foster.py] => Task 1, Epoch 41/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 71.14
2024-09-02 15:34:33,299 [foster.py] => Task 1, Epoch 46/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 71.14
2024-09-02 15:34:45,524 [foster.py] => Task 1, Epoch 51/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 71.19
2024-09-02 15:34:56,815 [foster.py] => Task 1, Epoch 56/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 71.24
2024-09-02 15:35:08,060 [foster.py] => Task 1, Epoch 61/120 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 71.67
2024-09-02 15:35:18,936 [foster.py] => Task 1, Epoch 66/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.83
2024-09-02 15:35:30,425 [foster.py] => Task 1, Epoch 71/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.31
2024-09-02 15:35:42,136 [foster.py] => Task 1, Epoch 76/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.93
2024-09-02 15:35:52,690 [foster.py] => Task 1, Epoch 81/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.43
2024-09-02 15:36:04,387 [foster.py] => Task 1, Epoch 86/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.38
2024-09-02 15:36:14,974 [foster.py] => Task 1, Epoch 91/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.14
2024-09-02 15:36:26,460 [foster.py] => Task 1, Epoch 96/120 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.26
2024-09-02 15:36:38,342 [foster.py] => Task 1, Epoch 101/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.36
2024-09-02 15:36:49,324 [foster.py] => Task 1, Epoch 106/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.50
2024-09-02 15:37:00,216 [foster.py] => Task 1, Epoch 111/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.57
2024-09-02 15:37:11,343 [foster.py] => Task 1, Epoch 116/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 70.00
2024-09-02 15:37:19,615 [foster.py] => Task 1, Epoch 120/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00
2024-09-02 15:37:19,616 [inc_net.py] => align weights, gamma = 0.7654335498809814 
2024-09-02 15:37:19,617 [foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-09-02 15:37:23,213 [foster.py] => SNet: Task 1, Epoch 1/80 => Loss 1.060,  Train_accy 71.38, Test_accy 34.64
2024-09-02 15:37:35,431 [foster.py] => SNet: Task 1, Epoch 6/80 => Loss 0.490,  Train_accy 99.98, Test_accy 70.05
2024-09-02 15:37:47,188 [foster.py] => SNet: Task 1, Epoch 11/80 => Loss 0.478,  Train_accy 100.00, Test_accy 71.60
2024-09-02 15:37:58,788 [foster.py] => SNet: Task 1, Epoch 16/80 => Loss 0.476,  Train_accy 100.00, Test_accy 73.31
2024-09-02 15:38:10,121 [foster.py] => SNet: Task 1, Epoch 21/80 => Loss 0.475,  Train_accy 100.00, Test_accy 72.50
2024-09-02 15:38:21,999 [foster.py] => SNet: Task 1, Epoch 26/80 => Loss 0.473,  Train_accy 100.00, Test_accy 73.12
2024-09-02 15:38:33,699 [foster.py] => SNet: Task 1, Epoch 31/80 => Loss 0.473,  Train_accy 100.00, Test_accy 72.33
2024-09-02 15:38:45,475 [foster.py] => SNet: Task 1, Epoch 36/80 => Loss 0.472,  Train_accy 100.00, Test_accy 72.60
2024-09-02 15:38:57,363 [foster.py] => SNet: Task 1, Epoch 41/80 => Loss 0.473,  Train_accy 100.00, Test_accy 74.10
2024-09-02 15:39:09,097 [foster.py] => SNet: Task 1, Epoch 46/80 => Loss 0.471,  Train_accy 100.00, Test_accy 73.33
2024-09-02 15:39:20,551 [foster.py] => SNet: Task 1, Epoch 51/80 => Loss 0.472,  Train_accy 100.00, Test_accy 72.98
2024-09-02 15:39:32,178 [foster.py] => SNet: Task 1, Epoch 56/80 => Loss 0.470,  Train_accy 100.00, Test_accy 73.45
2024-09-02 15:39:43,996 [foster.py] => SNet: Task 1, Epoch 61/80 => Loss 0.471,  Train_accy 100.00, Test_accy 73.36
2024-09-02 15:39:56,085 [foster.py] => SNet: Task 1, Epoch 66/80 => Loss 0.474,  Train_accy 100.00, Test_accy 72.95
2024-09-02 15:40:08,270 [foster.py] => SNet: Task 1, Epoch 71/80 => Loss 0.471,  Train_accy 100.00, Test_accy 72.86
2024-09-02 15:40:20,093 [foster.py] => SNet: Task 1, Epoch 76/80 => Loss 0.472,  Train_accy 100.00, Test_accy 73.05
2024-09-02 15:40:28,678 [foster.py] => SNet: Task 1, Epoch 80/80 => Loss 0.471,  Train_accy 100.00
2024-09-02 15:40:28,679 [inc_net.py] => align weights, gamma = 1.1253845691680908 
2024-09-02 15:40:30,185 [foster.py] => darknet eval: 
2024-09-02 15:40:30,186 [foster.py] => CNN top1 curve: 71.57
2024-09-02 15:40:30,186 [foster.py] => CNN top5 curve: 98.64
2024-09-02 15:40:30,187 [base.py] => Reducing exemplars...(71 per classes)
2024-09-02 15:40:33,817 [base.py] => Constructing exemplars...(71 per classes)
2024-09-02 15:40:41,544 [foster.py] => Exemplar size: 497
2024-09-02 15:40:41,544 [trainer.py] => CNN: {'total': 73.26, '00-04': 64.03, '05-06': 96.33, 'old': 64.03, 'new': 96.33}
2024-09-02 15:40:41,544 [trainer.py] => NME: {'total': 74.31, '00-04': 66.3, '05-06': 94.33, 'old': 66.3, 'new': 94.33}
2024-09-02 15:40:41,545 [trainer.py] => CNN top1 curve: [89.93, 73.26]
2024-09-02 15:40:41,545 [trainer.py] => CNN top5 curve: [100.0, 98.79]
2024-09-02 15:40:41,545 [trainer.py] => NME top1 curve: [90.0, 74.31]
2024-09-02 15:40:41,545 [trainer.py] => NME top5 curve: [100.0, 98.86]

2024-09-02 15:40:41,545 [trainer.py] => Average Accuracy (CNN): 81.595
2024-09-02 15:40:41,545 [trainer.py] => Average Accuracy (NME): 82.155
2024-09-02 15:40:41,546 [trainer.py] => All params: 7701139
2024-09-02 15:40:41,546 [trainer.py] => Trainable params: 3854670
2024-09-02 15:40:41,603 [foster.py] => Learning on 7-9
2024-09-02 15:40:41,604 [foster.py] => All params: 7705241
2024-09-02 15:40:41,604 [foster.py] => Trainable params: 3857746
2024-09-02 15:40:41,630 [foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-09-02 15:40:45,426 [foster.py] => Task 2, Epoch 1/120 => Loss 1.341, Loss_clf 0.594, Loss_fe 0.746, Loss_kd 0.000, Train_accy 85.77, Test_accy 12.93
2024-09-02 15:40:56,884 [foster.py] => Task 2, Epoch 6/120 => Loss 0.294, Loss_clf 0.036, Loss_fe 0.258, Loss_kd 0.000, Train_accy 99.00, Test_accy 67.37
2024-09-02 15:41:08,698 [foster.py] => Task 2, Epoch 11/120 => Loss 0.159, Loss_clf 0.019, Loss_fe 0.140, Loss_kd 0.000, Train_accy 99.56, Test_accy 64.54
2024-09-02 15:41:20,623 [foster.py] => Task 2, Epoch 16/120 => Loss 0.057, Loss_clf 0.008, Loss_fe 0.049, Loss_kd 0.000, Train_accy 99.89, Test_accy 66.07
2024-09-02 15:41:32,420 [foster.py] => Task 2, Epoch 21/120 => Loss 0.153, Loss_clf 0.024, Loss_fe 0.128, Loss_kd 0.000, Train_accy 99.40, Test_accy 67.19
2024-09-02 15:41:44,537 [foster.py] => Task 2, Epoch 26/120 => Loss 0.046, Loss_clf 0.011, Loss_fe 0.035, Loss_kd 0.000, Train_accy 99.73, Test_accy 65.04
2024-09-02 15:41:56,545 [foster.py] => Task 2, Epoch 31/120 => Loss 0.060, Loss_clf 0.015, Loss_fe 0.045, Loss_kd 0.000, Train_accy 99.71, Test_accy 64.54
2024-09-02 15:42:08,337 [foster.py] => Task 2, Epoch 36/120 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.02
2024-09-02 15:42:20,042 [foster.py] => Task 2, Epoch 41/120 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.59
2024-09-02 15:42:32,285 [foster.py] => Task 2, Epoch 46/120 => Loss 0.008, Loss_clf 0.004, Loss_fe 0.005, Loss_kd 0.000, Train_accy 99.98, Test_accy 54.93
2024-09-02 15:42:44,056 [foster.py] => Task 2, Epoch 51/120 => Loss 0.045, Loss_clf 0.012, Loss_fe 0.033, Loss_kd 0.000, Train_accy 99.73, Test_accy 67.46
2024-09-02 15:42:55,577 [foster.py] => Task 2, Epoch 56/120 => Loss 0.007, Loss_clf 0.002, Loss_fe 0.004, Loss_kd 0.000, Train_accy 100.00, Test_accy 66.59
2024-09-02 15:43:07,588 [foster.py] => Task 2, Epoch 61/120 => Loss 0.007, Loss_clf 0.002, Loss_fe 0.005, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.93
2024-09-02 15:43:19,148 [foster.py] => Task 2, Epoch 66/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.89
2024-09-02 15:43:30,759 [foster.py] => Task 2, Epoch 71/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.83
2024-09-02 15:43:42,634 [foster.py] => Task 2, Epoch 76/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.61
2024-09-02 15:43:53,732 [foster.py] => Task 2, Epoch 81/120 => Loss 0.004, Loss_clf 0.002, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.00
2024-09-02 15:44:05,125 [foster.py] => Task 2, Epoch 86/120 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.57
2024-09-02 15:44:16,629 [foster.py] => Task 2, Epoch 91/120 => Loss 0.004, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.72
2024-09-02 15:44:28,368 [foster.py] => Task 2, Epoch 96/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.59
2024-09-02 15:44:40,280 [foster.py] => Task 2, Epoch 101/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.28
2024-09-02 15:44:51,889 [foster.py] => Task 2, Epoch 106/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.48
2024-09-02 15:45:03,832 [foster.py] => Task 2, Epoch 111/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.06
2024-09-02 15:45:16,066 [foster.py] => Task 2, Epoch 116/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.19
2024-09-02 15:45:24,597 [foster.py] => Task 2, Epoch 120/120 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00
2024-09-02 15:45:24,599 [inc_net.py] => align weights, gamma = 0.7391167879104614 
2024-09-02 15:45:24,601 [foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-09-02 15:45:27,979 [foster.py] => SNet: Task 2, Epoch 1/80 => Loss 1.235,  Train_accy 76.52, Test_accy 36.11
2024-09-02 15:45:39,617 [foster.py] => SNet: Task 2, Epoch 6/80 => Loss 0.671,  Train_accy 99.96, Test_accy 68.26
2024-09-02 15:45:51,802 [foster.py] => SNet: Task 2, Epoch 11/80 => Loss 0.667,  Train_accy 99.98, Test_accy 70.83
2024-09-02 15:46:04,412 [foster.py] => SNet: Task 2, Epoch 16/80 => Loss 0.660,  Train_accy 100.00, Test_accy 71.48
2024-09-02 15:46:16,958 [foster.py] => SNet: Task 2, Epoch 21/80 => Loss 0.660,  Train_accy 100.00, Test_accy 71.96
2024-09-02 15:46:28,131 [foster.py] => SNet: Task 2, Epoch 26/80 => Loss 0.657,  Train_accy 100.00, Test_accy 72.80
2024-09-02 15:46:40,342 [foster.py] => SNet: Task 2, Epoch 31/80 => Loss 0.657,  Train_accy 100.00, Test_accy 73.28
2024-09-02 15:46:52,230 [foster.py] => SNet: Task 2, Epoch 36/80 => Loss 0.657,  Train_accy 100.00, Test_accy 73.26
2024-09-02 15:47:03,817 [foster.py] => SNet: Task 2, Epoch 41/80 => Loss 0.655,  Train_accy 100.00, Test_accy 73.74
2024-09-02 15:47:16,225 [foster.py] => SNet: Task 2, Epoch 46/80 => Loss 0.653,  Train_accy 100.00, Test_accy 73.91
2024-09-02 15:47:28,476 [foster.py] => SNet: Task 2, Epoch 51/80 => Loss 0.651,  Train_accy 100.00, Test_accy 74.15
2024-09-02 15:47:40,559 [foster.py] => SNet: Task 2, Epoch 56/80 => Loss 0.654,  Train_accy 100.00, Test_accy 73.04
2024-09-02 15:47:52,759 [foster.py] => SNet: Task 2, Epoch 61/80 => Loss 0.653,  Train_accy 100.00, Test_accy 73.76
2024-09-02 15:48:04,298 [foster.py] => SNet: Task 2, Epoch 66/80 => Loss 0.655,  Train_accy 100.00, Test_accy 73.37
2024-09-02 15:48:16,628 [foster.py] => SNet: Task 2, Epoch 71/80 => Loss 0.654,  Train_accy 100.00, Test_accy 73.44
2024-09-02 15:48:28,264 [foster.py] => SNet: Task 2, Epoch 76/80 => Loss 0.652,  Train_accy 100.00, Test_accy 73.37
2024-09-02 15:48:36,763 [foster.py] => SNet: Task 2, Epoch 80/80 => Loss 0.652,  Train_accy 100.00
2024-09-02 15:48:36,764 [inc_net.py] => align weights, gamma = 1.1320948600769043 
2024-09-02 15:48:38,263 [foster.py] => darknet eval: 
2024-09-02 15:48:38,263 [foster.py] => CNN top1 curve: 72.57
2024-09-02 15:48:38,263 [foster.py] => CNN top5 curve: 97.61
2024-09-02 15:48:38,264 [base.py] => Reducing exemplars...(55 per classes)
2024-09-02 15:48:43,265 [base.py] => Constructing exemplars...(55 per classes)
2024-09-02 15:48:50,606 [foster.py] => Exemplar size: 495
2024-09-02 15:48:50,607 [trainer.py] => CNN: {'total': 71.15, '00-04': 57.3, '05-06': 81.42, '07-08': 95.5, 'old': 64.19, 'new': 95.5}
2024-09-02 15:48:50,607 [trainer.py] => NME: {'total': 70.33, '00-04': 58.0, '05-06': 79.42, '07-08': 92.08, 'old': 64.12, 'new': 92.08}
2024-09-02 15:48:50,607 [trainer.py] => CNN top1 curve: [89.93, 73.26, 71.15]
2024-09-02 15:48:50,607 [trainer.py] => CNN top5 curve: [100.0, 98.79, 97.69]
2024-09-02 15:48:50,607 [trainer.py] => NME top1 curve: [90.0, 74.31, 70.33]
2024-09-02 15:48:50,607 [trainer.py] => NME top5 curve: [100.0, 98.86, 96.85]

2024-09-02 15:48:50,607 [trainer.py] => Average Accuracy (CNN): 78.11333333333333
2024-09-02 15:48:50,607 [trainer.py] => Average Accuracy (NME): 78.21333333333332
2024-09-02 15:48:50,608 [trainer.py] => Forgetting (CNN): 23.770000000000003
