2024-10-23 11:07:43,270 [trainer.py] => config: ./exps/foster.json
2024-10-23 11:07:43,270 [trainer.py] => prefix: cil
2024-10-23 11:07:43,270 [trainer.py] => dataset: hrrp9
2024-10-23 11:07:43,270 [trainer.py] => memory_size: 500
2024-10-23 11:07:43,270 [trainer.py] => memory_per_class: 20
2024-10-23 11:07:43,270 [trainer.py] => fixed_memory: False
2024-10-23 11:07:43,270 [trainer.py] => shuffle: True
2024-10-23 11:07:43,271 [trainer.py] => init_cls: 5
2024-10-23 11:07:43,271 [trainer.py] => increment: 2
2024-10-23 11:07:43,271 [trainer.py] => model_name: foster
2024-10-23 11:07:43,271 [trainer.py] => convnet_type: resnet18
2024-10-23 11:07:43,271 [trainer.py] => init_train: False
2024-10-23 11:07:43,271 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-23 11:07:43,271 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-23 11:07:43,271 [trainer.py] => device: [device(type='cuda', index=2)]
2024-10-23 11:07:43,271 [trainer.py] => seed: 1993
2024-10-23 11:07:43,271 [trainer.py] => beta1: 0.96
2024-10-23 11:07:43,271 [trainer.py] => beta2: 0.97
2024-10-23 11:07:43,271 [trainer.py] => oofc: ft
2024-10-23 11:07:43,271 [trainer.py] => is_teacher_wa: True
2024-10-23 11:07:43,271 [trainer.py] => is_student_wa: True
2024-10-23 11:07:43,271 [trainer.py] => lambda_okd: 2
2024-10-23 11:07:43,271 [trainer.py] => wa_value: 1
2024-10-23 11:07:43,271 [trainer.py] => init_epochs: 0
2024-10-23 11:07:43,271 [trainer.py] => init_lr: 0.1
2024-10-23 11:07:43,271 [trainer.py] => init_weight_decay: 0.0005
2024-10-23 11:07:43,271 [trainer.py] => boosting_epochs: 120
2024-10-23 11:07:43,271 [trainer.py] => compression_epochs: 100
2024-10-23 11:07:43,271 [trainer.py] => lr: 0.1
2024-10-23 11:07:43,272 [trainer.py] => batch_size: 128
2024-10-23 11:07:43,272 [trainer.py] => weight_decay: 0.0005
2024-10-23 11:07:43,272 [trainer.py] => num_workers: 8
2024-10-23 11:07:43,272 [trainer.py] => T: 2
2024-10-23 11:07:43,945 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-23 11:07:43,985 [trainer.py] => All params: 0
2024-10-23 11:07:43,985 [trainer.py] => Trainable params: 0
2024-10-23 11:07:44,518 [foster.py] => Learning on 0-5
2024-10-23 11:07:44,519 [foster.py] => All params: 3849034
2024-10-23 11:07:44,519 [foster.py] => Trainable params: 3849034
2024-10-23 11:07:44,824 [foster.py] => init_train?---False
2024-10-23 11:07:45,884 [base.py] => Reducing exemplars...(100 per classes)
2024-10-23 11:07:45,885 [base.py] => Constructing exemplars...(100 per classes)
2024-10-23 11:07:51,811 [trainer.py] => All params: 3849034
2024-10-23 11:07:53,079 [foster.py] => Exemplar size: 500
2024-10-23 11:07:53,079 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-23 11:07:53,079 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-23 11:07:53,079 [trainer.py] => CNN top1 curve: [89.93]
2024-10-23 11:07:53,079 [trainer.py] => CNN top5 curve: [100.0]
2024-10-23 11:07:53,079 [trainer.py] => NME top1 curve: [90.0]
2024-10-23 11:07:53,079 [trainer.py] => NME top5 curve: [100.0]

2024-10-23 11:07:53,080 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-23 11:07:53,080 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-23 11:07:53,080 [trainer.py] => All params: 3849034
2024-10-23 11:07:53,080 [trainer.py] => Trainable params: 3849034
2024-10-23 11:07:53,165 [foster.py] => Learning on 5-7
2024-10-23 11:07:53,166 [foster.py] => All params: 7701139
2024-10-23 11:07:53,167 [foster.py] => Trainable params: 3854670
2024-10-23 11:07:53,194 [foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-23 11:07:55,829 [foster.py] => Task 1, Epoch 1/120 => Loss 3.273, Loss_clf 0.658, Loss_fe 0.773, Loss_kd 1.316, Train_accy 81.58, Test_accy 25.33
2024-10-23 11:08:03,847 [foster.py] => Task 1, Epoch 6/120 => Loss 2.050, Loss_clf 0.054, Loss_fe 0.194, Loss_kd 1.287, Train_accy 98.27, Test_accy 62.69
2024-10-23 11:08:12,013 [foster.py] => Task 1, Epoch 11/120 => Loss 1.969, Loss_clf 0.048, Loss_fe 0.123, Loss_kd 1.284, Train_accy 98.53, Test_accy 61.02
2024-10-23 11:08:20,107 [foster.py] => Task 1, Epoch 16/120 => Loss 1.854, Loss_clf 0.015, Loss_fe 0.037, Loss_kd 1.288, Train_accy 99.71, Test_accy 70.60
2024-10-23 11:08:28,198 [foster.py] => Task 1, Epoch 21/120 => Loss 1.996, Loss_clf 0.064, Loss_fe 0.136, Loss_kd 1.283, Train_accy 98.11, Test_accy 55.67
2024-10-23 11:08:36,050 [foster.py] => Task 1, Epoch 26/120 => Loss 1.852, Loss_clf 0.013, Loss_fe 0.031, Loss_kd 1.291, Train_accy 99.78, Test_accy 70.90
2024-10-23 11:08:44,033 [foster.py] => Task 1, Epoch 31/120 => Loss 1.855, Loss_clf 0.019, Loss_fe 0.036, Loss_kd 1.286, Train_accy 99.51, Test_accy 65.48
2024-10-23 11:08:52,121 [foster.py] => Task 1, Epoch 36/120 => Loss 1.802, Loss_clf 0.002, Loss_fe 0.003, Loss_kd 1.284, Train_accy 99.98, Test_accy 73.14
2024-10-23 11:09:00,216 [foster.py] => Task 1, Epoch 41/120 => Loss 1.792, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.279, Train_accy 100.00, Test_accy 73.24
2024-10-23 11:09:08,236 [foster.py] => Task 1, Epoch 46/120 => Loss 1.802, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.286, Train_accy 100.00, Test_accy 74.19
2024-10-23 11:09:16,055 [foster.py] => Task 1, Epoch 51/120 => Loss 1.796, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.282, Train_accy 100.00, Test_accy 74.07
2024-10-23 11:09:23,806 [foster.py] => Task 1, Epoch 56/120 => Loss 1.804, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.287, Train_accy 100.00, Test_accy 75.02
2024-10-23 11:09:31,810 [foster.py] => Task 1, Epoch 61/120 => Loss 1.799, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.283, Train_accy 100.00, Test_accy 74.33
2024-10-23 11:09:39,831 [foster.py] => Task 1, Epoch 66/120 => Loss 1.804, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.287, Train_accy 100.00, Test_accy 73.05
2024-10-23 11:09:47,836 [foster.py] => Task 1, Epoch 71/120 => Loss 1.797, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 1.282, Train_accy 100.00, Test_accy 73.19
2024-10-23 11:09:55,716 [foster.py] => Task 1, Epoch 76/120 => Loss 1.801, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.285, Train_accy 100.00, Test_accy 74.05
2024-10-23 11:10:03,901 [foster.py] => Task 1, Epoch 81/120 => Loss 1.805, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.288, Train_accy 100.00, Test_accy 73.40
2024-10-23 11:10:11,887 [foster.py] => Task 1, Epoch 86/120 => Loss 1.806, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.289, Train_accy 100.00, Test_accy 72.98
2024-10-23 11:10:19,786 [foster.py] => Task 1, Epoch 91/120 => Loss 1.801, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.285, Train_accy 100.00, Test_accy 73.40
2024-10-23 11:10:27,756 [foster.py] => Task 1, Epoch 96/120 => Loss 1.799, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.283, Train_accy 100.00, Test_accy 72.98
2024-10-23 11:10:35,960 [foster.py] => Task 1, Epoch 101/120 => Loss 1.800, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.284, Train_accy 100.00, Test_accy 73.50
2024-10-23 11:10:44,144 [foster.py] => Task 1, Epoch 106/120 => Loss 1.804, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.287, Train_accy 100.00, Test_accy 73.02
2024-10-23 11:10:52,212 [foster.py] => Task 1, Epoch 111/120 => Loss 1.797, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.282, Train_accy 100.00, Test_accy 72.50
2024-10-23 11:11:00,359 [foster.py] => Task 1, Epoch 116/120 => Loss 1.802, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.285, Train_accy 100.00, Test_accy 74.60
2024-10-23 11:11:06,153 [foster.py] => Task 1, Epoch 120/120 => Loss 1.805, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.288, Train_accy 100.00
2024-10-23 11:11:06,198 [inc_net.py] => align weights, gamma = 1.029930591583252 
2024-10-23 11:11:06,199 [foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-23 11:11:08,778 [foster.py] => SNet: Task 1, Epoch 1/100 => Loss 0.891,  Train_accy 71.73, Test_accy 31.50
2024-10-23 11:11:17,604 [foster.py] => SNet: Task 1, Epoch 6/100 => Loss 0.203,  Train_accy 99.93, Test_accy 67.81
2024-10-23 11:11:26,520 [foster.py] => SNet: Task 1, Epoch 11/100 => Loss 0.186,  Train_accy 100.00, Test_accy 69.60
2024-10-23 11:11:35,628 [foster.py] => SNet: Task 1, Epoch 16/100 => Loss 0.186,  Train_accy 100.00, Test_accy 71.93
2024-10-23 11:11:44,625 [foster.py] => SNet: Task 1, Epoch 21/100 => Loss 0.183,  Train_accy 100.00, Test_accy 71.05
2024-10-23 11:11:53,652 [foster.py] => SNet: Task 1, Epoch 26/100 => Loss 0.184,  Train_accy 100.00, Test_accy 72.40
2024-10-23 11:12:02,605 [foster.py] => SNet: Task 1, Epoch 31/100 => Loss 0.182,  Train_accy 100.00, Test_accy 70.81
2024-10-23 11:12:11,775 [foster.py] => SNet: Task 1, Epoch 36/100 => Loss 0.182,  Train_accy 100.00, Test_accy 71.60
2024-10-23 11:12:21,533 [foster.py] => SNet: Task 1, Epoch 41/100 => Loss 0.183,  Train_accy 100.00, Test_accy 72.57
2024-10-23 11:12:32,461 [foster.py] => SNet: Task 1, Epoch 46/100 => Loss 0.183,  Train_accy 100.00, Test_accy 70.98
2024-10-23 11:12:41,608 [foster.py] => SNet: Task 1, Epoch 51/100 => Loss 0.182,  Train_accy 100.00, Test_accy 71.86
2024-10-23 11:12:50,661 [foster.py] => SNet: Task 1, Epoch 56/100 => Loss 0.181,  Train_accy 100.00, Test_accy 71.83
2024-10-23 11:13:00,096 [foster.py] => SNet: Task 1, Epoch 61/100 => Loss 0.182,  Train_accy 100.00, Test_accy 71.71
2024-10-23 11:13:09,318 [foster.py] => SNet: Task 1, Epoch 66/100 => Loss 0.183,  Train_accy 100.00, Test_accy 70.50
2024-10-23 11:13:18,321 [foster.py] => SNet: Task 1, Epoch 71/100 => Loss 0.182,  Train_accy 100.00, Test_accy 69.98
2024-10-23 11:13:27,447 [foster.py] => SNet: Task 1, Epoch 76/100 => Loss 0.182,  Train_accy 100.00, Test_accy 70.93
2024-10-23 11:13:36,545 [foster.py] => SNet: Task 1, Epoch 81/100 => Loss 0.181,  Train_accy 100.00, Test_accy 70.71
2024-10-23 11:13:45,499 [foster.py] => SNet: Task 1, Epoch 86/100 => Loss 0.182,  Train_accy 100.00, Test_accy 70.52
2024-10-23 11:13:54,567 [foster.py] => SNet: Task 1, Epoch 91/100 => Loss 0.182,  Train_accy 100.00, Test_accy 70.90
2024-10-23 11:14:05,280 [foster.py] => SNet: Task 1, Epoch 96/100 => Loss 0.180,  Train_accy 100.00, Test_accy 70.76
2024-10-23 11:14:13,007 [foster.py] => SNet: Task 1, Epoch 100/100 => Loss 0.182,  Train_accy 100.00
2024-10-23 11:14:13,010 [inc_net.py] => align weights, gamma = 0.9847676753997803 
2024-10-23 11:14:13,687 [foster.py] => darknet eval: 
2024-10-23 11:14:13,688 [foster.py] => CNN top1 curve: 70.67
2024-10-23 11:14:13,688 [foster.py] => CNN top5 curve: 98.88
2024-10-23 11:14:13,689 [base.py] => Reducing exemplars...(71 per classes)
2024-10-23 11:14:14,735 [base.py] => Constructing exemplars...(71 per classes)
2024-10-23 11:14:17,823 [trainer.py] => All params: 7701139
2024-10-23 11:14:19,527 [foster.py] => Exemplar size: 497
2024-10-23 11:14:19,527 [trainer.py] => CNN: {'total': 72.9, '00-04': 63.23, '05-06': 97.08, 'old': 63.23, 'new': 97.08}
2024-10-23 11:14:19,527 [trainer.py] => NME: {'total': 75.57, '00-04': 68.4, '05-06': 93.5, 'old': 68.4, 'new': 93.5}
2024-10-23 11:14:19,527 [trainer.py] => CNN top1 curve: [89.93, 72.9]
2024-10-23 11:14:19,527 [trainer.py] => CNN top5 curve: [100.0, 99.07]
2024-10-23 11:14:19,528 [trainer.py] => NME top1 curve: [90.0, 75.57]
2024-10-23 11:14:19,528 [trainer.py] => NME top5 curve: [100.0, 99.02]

2024-10-23 11:14:19,528 [trainer.py] => Average Accuracy (CNN): 81.415
2024-10-23 11:14:19,528 [trainer.py] => Average Accuracy (NME): 82.785
2024-10-23 11:14:19,529 [trainer.py] => All params: 7701139
2024-10-23 11:14:19,529 [trainer.py] => Trainable params: 3854670
2024-10-23 11:14:19,650 [foster.py] => Learning on 7-9
2024-10-23 11:14:19,652 [foster.py] => All params: 7705241
2024-10-23 11:14:19,652 [foster.py] => Trainable params: 3857746
2024-10-23 11:14:19,693 [foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-23 11:14:22,494 [foster.py] => Task 2, Epoch 1/120 => Loss 3.343, Loss_clf 0.540, Loss_fe 0.702, Loss_kd 1.634, Train_accy 85.90, Test_accy 17.89
2024-10-23 11:14:30,890 [foster.py] => Task 2, Epoch 6/120 => Loss 2.315, Loss_clf 0.033, Loss_fe 0.234, Loss_kd 1.593, Train_accy 99.22, Test_accy 66.09
2024-10-23 11:14:39,051 [foster.py] => Task 2, Epoch 11/120 => Loss 2.595, Loss_clf 0.130, Loss_fe 0.417, Loss_kd 1.592, Train_accy 95.53, Test_accy 64.11
2024-10-23 11:14:47,541 [foster.py] => Task 2, Epoch 16/120 => Loss 2.133, Loss_clf 0.011, Loss_fe 0.074, Loss_kd 1.593, Train_accy 99.78, Test_accy 62.76
2024-10-23 11:14:55,895 [foster.py] => Task 2, Epoch 21/120 => Loss 2.114, Loss_clf 0.014, Loss_fe 0.059, Loss_kd 1.587, Train_accy 99.53, Test_accy 60.54
2024-10-23 11:15:04,427 [foster.py] => Task 2, Epoch 26/120 => Loss 2.078, Loss_clf 0.010, Loss_fe 0.022, Loss_kd 1.592, Train_accy 99.89, Test_accy 24.00
2024-10-23 11:15:12,804 [foster.py] => Task 2, Epoch 31/120 => Loss 2.086, Loss_clf 0.007, Loss_fe 0.025, Loss_kd 1.597, Train_accy 99.87, Test_accy 64.70
2024-10-23 11:15:20,999 [foster.py] => Task 2, Epoch 36/120 => Loss 2.050, Loss_clf 0.004, Loss_fe 0.007, Loss_kd 1.585, Train_accy 99.93, Test_accy 66.43
2024-10-23 11:15:29,530 [foster.py] => Task 2, Epoch 41/120 => Loss 2.043, Loss_clf 0.001, Loss_fe 0.005, Loss_kd 1.583, Train_accy 100.00, Test_accy 67.87
2024-10-23 11:15:38,043 [foster.py] => Task 2, Epoch 46/120 => Loss 2.040, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.585, Train_accy 100.00, Test_accy 65.98
2024-10-23 11:15:46,225 [foster.py] => Task 2, Epoch 51/120 => Loss 2.054, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.596, Train_accy 100.00, Test_accy 67.28
2024-10-23 11:15:54,544 [foster.py] => Task 2, Epoch 56/120 => Loss 2.053, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.595, Train_accy 100.00, Test_accy 66.89
2024-10-23 11:16:02,952 [foster.py] => Task 2, Epoch 61/120 => Loss 2.047, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 1.590, Train_accy 100.00, Test_accy 65.63
2024-10-23 11:16:11,397 [foster.py] => Task 2, Epoch 66/120 => Loss 2.053, Loss_clf 0.002, Loss_fe 0.002, Loss_kd 1.594, Train_accy 100.00, Test_accy 64.81
2024-10-23 11:16:19,955 [foster.py] => Task 2, Epoch 71/120 => Loss 2.040, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 1.585, Train_accy 100.00, Test_accy 64.04
2024-10-23 11:16:28,262 [foster.py] => Task 2, Epoch 76/120 => Loss 2.046, Loss_clf 0.002, Loss_fe 0.002, Loss_kd 1.589, Train_accy 100.00, Test_accy 66.31
2024-10-23 11:16:36,597 [foster.py] => Task 2, Epoch 81/120 => Loss 2.052, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.594, Train_accy 100.00, Test_accy 66.43
2024-10-23 11:16:45,313 [foster.py] => Task 2, Epoch 86/120 => Loss 2.051, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.594, Train_accy 100.00, Test_accy 66.19
2024-10-23 11:16:53,861 [foster.py] => Task 2, Epoch 91/120 => Loss 2.039, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 1.584, Train_accy 100.00, Test_accy 66.13
2024-10-23 11:17:01,994 [foster.py] => Task 2, Epoch 96/120 => Loss 2.039, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.584, Train_accy 100.00, Test_accy 65.39
2024-10-23 11:17:10,703 [foster.py] => Task 2, Epoch 101/120 => Loss 2.045, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.589, Train_accy 100.00, Test_accy 65.22
2024-10-23 11:17:19,038 [foster.py] => Task 2, Epoch 106/120 => Loss 2.047, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.590, Train_accy 100.00, Test_accy 66.15
2024-10-23 11:17:27,646 [foster.py] => Task 2, Epoch 111/120 => Loss 2.047, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.590, Train_accy 100.00, Test_accy 65.50
2024-10-23 11:17:35,954 [foster.py] => Task 2, Epoch 116/120 => Loss 2.041, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 1.585, Train_accy 100.00, Test_accy 65.37
2024-10-23 11:17:41,941 [foster.py] => Task 2, Epoch 120/120 => Loss 2.051, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 1.593, Train_accy 100.00
2024-10-23 11:17:41,943 [inc_net.py] => align weights, gamma = 1.055593729019165 
2024-10-23 11:17:41,944 [foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-23 11:17:44,413 [foster.py] => SNet: Task 2, Epoch 1/100 => Loss 0.978,  Train_accy 73.16, Test_accy 26.02
2024-10-23 11:17:53,314 [foster.py] => SNet: Task 2, Epoch 6/100 => Loss 0.220,  Train_accy 99.91, Test_accy 66.30
2024-10-23 11:18:02,361 [foster.py] => SNet: Task 2, Epoch 11/100 => Loss 0.208,  Train_accy 100.00, Test_accy 70.69
2024-10-23 11:18:11,358 [foster.py] => SNet: Task 2, Epoch 16/100 => Loss 0.215,  Train_accy 99.78, Test_accy 67.30
2024-10-23 11:18:20,253 [foster.py] => SNet: Task 2, Epoch 21/100 => Loss 0.203,  Train_accy 100.00, Test_accy 70.74
2024-10-23 11:18:29,591 [foster.py] => SNet: Task 2, Epoch 26/100 => Loss 0.202,  Train_accy 100.00, Test_accy 69.26
2024-10-23 11:18:39,068 [foster.py] => SNet: Task 2, Epoch 31/100 => Loss 0.199,  Train_accy 100.00, Test_accy 71.74
2024-10-23 11:18:48,672 [foster.py] => SNet: Task 2, Epoch 36/100 => Loss 0.200,  Train_accy 100.00, Test_accy 69.33
2024-10-23 11:18:58,048 [foster.py] => SNet: Task 2, Epoch 41/100 => Loss 0.201,  Train_accy 99.98, Test_accy 70.85
2024-10-23 11:19:07,248 [foster.py] => SNet: Task 2, Epoch 46/100 => Loss 0.201,  Train_accy 100.00, Test_accy 70.87
2024-10-23 11:19:16,685 [foster.py] => SNet: Task 2, Epoch 51/100 => Loss 0.199,  Train_accy 100.00, Test_accy 72.15
2024-10-23 11:19:25,853 [foster.py] => SNet: Task 2, Epoch 56/100 => Loss 0.201,  Train_accy 100.00, Test_accy 70.24
2024-10-23 11:19:35,259 [foster.py] => SNet: Task 2, Epoch 61/100 => Loss 0.199,  Train_accy 100.00, Test_accy 72.17
2024-10-23 11:19:44,590 [foster.py] => SNet: Task 2, Epoch 66/100 => Loss 0.199,  Train_accy 100.00, Test_accy 70.96
2024-10-23 11:19:54,084 [foster.py] => SNet: Task 2, Epoch 71/100 => Loss 0.200,  Train_accy 100.00, Test_accy 70.28
2024-10-23 11:20:03,433 [foster.py] => SNet: Task 2, Epoch 76/100 => Loss 0.202,  Train_accy 100.00, Test_accy 68.96
2024-10-23 11:20:12,300 [foster.py] => SNet: Task 2, Epoch 81/100 => Loss 0.198,  Train_accy 100.00, Test_accy 71.44
2024-10-23 11:20:20,927 [foster.py] => SNet: Task 2, Epoch 86/100 => Loss 0.201,  Train_accy 100.00, Test_accy 68.74
2024-10-23 11:20:29,713 [foster.py] => SNet: Task 2, Epoch 91/100 => Loss 0.198,  Train_accy 100.00, Test_accy 70.52
2024-10-23 11:20:38,311 [foster.py] => SNet: Task 2, Epoch 96/100 => Loss 0.199,  Train_accy 100.00, Test_accy 70.26
2024-10-23 11:20:44,849 [foster.py] => SNet: Task 2, Epoch 100/100 => Loss 0.200,  Train_accy 100.00
2024-10-23 11:20:44,852 [inc_net.py] => align weights, gamma = 1.015164852142334 
2024-10-23 11:20:45,528 [foster.py] => darknet eval: 
2024-10-23 11:20:45,529 [foster.py] => CNN top1 curve: 71.94
2024-10-23 11:20:45,529 [foster.py] => CNN top5 curve: 96.87
2024-10-23 11:20:45,530 [base.py] => Reducing exemplars...(55 per classes)
2024-10-23 11:20:46,896 [base.py] => Constructing exemplars...(55 per classes)
2024-10-23 11:20:49,645 [trainer.py] => All params: 7705241
2024-10-23 11:20:51,646 [foster.py] => Exemplar size: 495
2024-10-23 11:20:51,646 [trainer.py] => CNN: {'total': 64.96, '00-04': 45.93, '05-06': 79.92, '07-08': 97.58, 'old': 55.64, 'new': 97.58}
2024-10-23 11:20:51,646 [trainer.py] => NME: {'total': 71.63, '00-04': 58.4, '05-06': 81.17, '07-08': 95.17, 'old': 64.9, 'new': 95.17}
2024-10-23 11:20:51,646 [trainer.py] => CNN top1 curve: [89.93, 72.9, 64.96]
2024-10-23 11:20:51,646 [trainer.py] => CNN top5 curve: [100.0, 99.07, 96.69]
2024-10-23 11:20:51,646 [trainer.py] => NME top1 curve: [90.0, 75.57, 71.63]
2024-10-23 11:20:51,646 [trainer.py] => NME top5 curve: [100.0, 99.02, 96.7]

2024-10-23 11:20:51,647 [trainer.py] => Average Accuracy (CNN): 75.93
2024-10-23 11:20:51,647 [trainer.py] => Average Accuracy (NME): 79.06666666666666
2024-10-23 11:20:51,647 [trainer.py] => Forgetting (CNN): 30.580000000000002
