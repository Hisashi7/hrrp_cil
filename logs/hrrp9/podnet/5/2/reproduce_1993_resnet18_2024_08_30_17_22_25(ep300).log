2024-08-30 17:22:25,655 [trainer.py] => config: ./exps/podnet.json
2024-08-30 17:22:25,655 [trainer.py] => prefix: reproduce
2024-08-30 17:22:25,655 [trainer.py] => dataset: hrrp9
2024-08-30 17:22:25,655 [trainer.py] => memory_size: 500
2024-08-30 17:22:25,656 [trainer.py] => memory_per_class: 20
2024-08-30 17:22:25,656 [trainer.py] => fixed_memory: False
2024-08-30 17:22:25,656 [trainer.py] => shuffle: True
2024-08-30 17:22:25,656 [trainer.py] => init_cls: 5
2024-08-30 17:22:25,656 [trainer.py] => increment: 2
2024-08-30 17:22:25,656 [trainer.py] => model_name: podnet
2024-08-30 17:22:25,656 [trainer.py] => convnet_type: resnet18
2024-08-30 17:22:25,656 [trainer.py] => init_train: True
2024-08-30 17:22:25,656 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 17:22:25,656 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 17:22:25,656 [trainer.py] => device: [device(type='cuda', index=3)]
2024-08-30 17:22:25,656 [trainer.py] => seed: 1993
2024-08-30 17:22:26,189 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 17:22:26,299 [trainer.py] => All params: 3843904
2024-08-30 17:22:26,300 [trainer.py] => Trainable params: 3843904
2024-08-30 17:22:26,300 [podnet.py] => Learning on 0-5
2024-08-30 17:22:26,336 [podnet.py] => Adaptive factor: 0
2024-08-30 17:22:28,912 [podnet.py] => Task 0, Epoch 1/300 (LR 0.10000) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 17:22:30,458 [podnet.py] => Task 0, Epoch 2/300 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.86, Test_acc 32.63
2024-08-30 17:22:31,978 [podnet.py] => Task 0, Epoch 3/300 (LR 0.09998) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.47, Test_acc 36.30
2024-08-30 17:22:33,491 [podnet.py] => Task 0, Epoch 4/300 (LR 0.09996) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.33, Test_acc 59.63
2024-08-30 17:22:34,964 [podnet.py] => Task 0, Epoch 5/300 (LR 0.09993) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.73, Test_acc 55.37
2024-08-30 17:22:36,522 [podnet.py] => Task 0, Epoch 6/300 (LR 0.09990) => LSC_loss 0.25, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.02, Test_acc 72.20
2024-08-30 17:22:38,329 [podnet.py] => Task 0, Epoch 7/300 (LR 0.09987) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.42, Test_acc 67.40
2024-08-30 17:22:39,925 [podnet.py] => Task 0, Epoch 8/300 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.66, Test_acc 74.30
2024-08-30 17:22:41,535 [podnet.py] => Task 0, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.07, Test_acc 74.60
2024-08-30 17:22:43,100 [podnet.py] => Task 0, Epoch 10/300 (LR 0.09973) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.47, Test_acc 78.70
2024-08-30 17:22:44,622 [podnet.py] => Task 0, Epoch 11/300 (LR 0.09967) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 70.47
2024-08-30 17:22:46,259 [podnet.py] => Task 0, Epoch 12/300 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 82.27
2024-08-30 17:22:47,819 [podnet.py] => Task 0, Epoch 13/300 (LR 0.09954) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.29, Test_acc 82.53
2024-08-30 17:22:49,700 [podnet.py] => Task 0, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 67.27
2024-08-30 17:22:51,772 [podnet.py] => Task 0, Epoch 15/300 (LR 0.09938) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.94, Test_acc 79.93
2024-08-30 17:22:53,823 [podnet.py] => Task 0, Epoch 16/300 (LR 0.09930) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.08, Test_acc 83.07
2024-08-30 17:22:55,337 [podnet.py] => Task 0, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 72.60
2024-08-30 17:22:57,257 [podnet.py] => Task 0, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.56, Test_acc 80.30
2024-08-30 17:22:58,997 [podnet.py] => Task 0, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 81.57
2024-08-30 17:23:00,630 [podnet.py] => Task 0, Epoch 20/300 (LR 0.09891) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 79.03
2024-08-30 17:23:02,170 [podnet.py] => Task 0, Epoch 21/300 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 83.07
2024-08-30 17:23:03,837 [podnet.py] => Task 0, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 74.17
2024-08-30 17:23:05,570 [podnet.py] => Task 0, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 82.63
2024-08-30 17:23:07,192 [podnet.py] => Task 0, Epoch 24/300 (LR 0.09843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 82.83
2024-08-30 17:23:08,937 [podnet.py] => Task 0, Epoch 25/300 (LR 0.09830) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.27, Test_acc 84.33
2024-08-30 17:23:10,616 [podnet.py] => Task 0, Epoch 26/300 (LR 0.09816) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.83, Test_acc 85.20
2024-08-30 17:23:12,671 [podnet.py] => Task 0, Epoch 27/300 (LR 0.09801) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 85.90
2024-08-30 17:23:14,231 [podnet.py] => Task 0, Epoch 28/300 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 74.20
2024-08-30 17:23:15,721 [podnet.py] => Task 0, Epoch 29/300 (LR 0.09771) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 82.03
2024-08-30 17:23:17,141 [podnet.py] => Task 0, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 76.93
2024-08-30 17:23:18,743 [podnet.py] => Task 0, Epoch 31/300 (LR 0.09739) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 79.40
2024-08-30 17:23:20,319 [podnet.py] => Task 0, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.66, Test_acc 77.10
2024-08-30 17:23:21,916 [podnet.py] => Task 0, Epoch 33/300 (LR 0.09704) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.02, Test_acc 79.23
2024-08-30 17:23:23,802 [podnet.py] => Task 0, Epoch 34/300 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.07, Test_acc 86.00
2024-08-30 17:23:25,532 [podnet.py] => Task 0, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 74.73
2024-08-30 17:23:27,413 [podnet.py] => Task 0, Epoch 36/300 (LR 0.09649) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.14, Test_acc 79.27
2024-08-30 17:23:28,953 [podnet.py] => Task 0, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 84.97
2024-08-30 17:23:30,812 [podnet.py] => Task 0, Epoch 38/300 (LR 0.09609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 84.70
2024-08-30 17:23:32,324 [podnet.py] => Task 0, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 82.83
2024-08-30 17:23:33,777 [podnet.py] => Task 0, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 75.70
2024-08-30 17:23:35,437 [podnet.py] => Task 0, Epoch 41/300 (LR 0.09546) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.97, Test_acc 83.57
2024-08-30 17:23:37,553 [podnet.py] => Task 0, Epoch 42/300 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 82.43
2024-08-30 17:23:39,633 [podnet.py] => Task 0, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 86.70
2024-08-30 17:23:41,274 [podnet.py] => Task 0, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 71.70
2024-08-30 17:23:43,003 [podnet.py] => Task 0, Epoch 45/300 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.30
2024-08-30 17:23:44,559 [podnet.py] => Task 0, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 80.70
2024-08-30 17:23:46,131 [podnet.py] => Task 0, Epoch 47/300 (LR 0.09407) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 84.67
2024-08-30 17:23:47,796 [podnet.py] => Task 0, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 83.63
2024-08-30 17:23:49,694 [podnet.py] => Task 0, Epoch 49/300 (LR 0.09356) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 85.03
2024-08-30 17:23:51,578 [podnet.py] => Task 0, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 82.57
2024-08-30 17:23:53,144 [podnet.py] => Task 0, Epoch 51/300 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 88.17
2024-08-30 17:23:54,859 [podnet.py] => Task 0, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 80.73
2024-08-30 17:23:56,400 [podnet.py] => Task 0, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 78.07
2024-08-30 17:23:57,898 [podnet.py] => Task 0, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.67
2024-08-30 17:23:59,653 [podnet.py] => Task 0, Epoch 55/300 (LR 0.09193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 82.83
2024-08-30 17:24:01,575 [podnet.py] => Task 0, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 82.33
2024-08-30 17:24:03,358 [podnet.py] => Task 0, Epoch 57/300 (LR 0.09135) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 82.83
2024-08-30 17:24:05,103 [podnet.py] => Task 0, Epoch 58/300 (LR 0.09106) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.50, Test_acc 84.40
2024-08-30 17:24:06,753 [podnet.py] => Task 0, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 70.30
2024-08-30 17:24:08,538 [podnet.py] => Task 0, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.83
2024-08-30 17:24:10,313 [podnet.py] => Task 0, Epoch 61/300 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.69, Test_acc 83.20
2024-08-30 17:24:11,907 [podnet.py] => Task 0, Epoch 62/300 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 86.53
2024-08-30 17:24:13,440 [podnet.py] => Task 0, Epoch 63/300 (LR 0.08951) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 84.83
2024-08-30 17:24:15,118 [podnet.py] => Task 0, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 84.63
2024-08-30 17:24:16,637 [podnet.py] => Task 0, Epoch 65/300 (LR 0.08886) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 81.80
2024-08-30 17:24:18,226 [podnet.py] => Task 0, Epoch 66/300 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 81.17
2024-08-30 17:24:20,148 [podnet.py] => Task 0, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 82.20
2024-08-30 17:24:22,141 [podnet.py] => Task 0, Epoch 68/300 (LR 0.08785) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.91, Test_acc 83.43
2024-08-30 17:24:24,321 [podnet.py] => Task 0, Epoch 69/300 (LR 0.08751) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 84.27
2024-08-30 17:24:26,090 [podnet.py] => Task 0, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.47, Test_acc 84.80
2024-08-30 17:24:27,741 [podnet.py] => Task 0, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 88.13
2024-08-30 17:24:29,439 [podnet.py] => Task 0, Epoch 72/300 (LR 0.08645) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 87.83
2024-08-30 17:24:31,132 [podnet.py] => Task 0, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 87.00
2024-08-30 17:24:32,919 [podnet.py] => Task 0, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 87.03
2024-08-30 17:24:35,248 [podnet.py] => Task 0, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.63
2024-08-30 17:24:37,195 [podnet.py] => Task 0, Epoch 76/300 (LR 0.08498) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.71, Test_acc 78.20
2024-08-30 17:24:38,947 [podnet.py] => Task 0, Epoch 77/300 (LR 0.08461) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 84.30
2024-08-30 17:24:40,739 [podnet.py] => Task 0, Epoch 78/300 (LR 0.08423) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 86.53
2024-08-30 17:24:42,602 [podnet.py] => Task 0, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 80.93
2024-08-30 17:24:44,579 [podnet.py] => Task 0, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 83.10
2024-08-30 17:24:46,360 [podnet.py] => Task 0, Epoch 81/300 (LR 0.08307) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.57
2024-08-30 17:24:47,924 [podnet.py] => Task 0, Epoch 82/300 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 86.83
2024-08-30 17:24:49,501 [podnet.py] => Task 0, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 84.97
2024-08-30 17:24:51,212 [podnet.py] => Task 0, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 80.90
2024-08-30 17:24:52,878 [podnet.py] => Task 0, Epoch 85/300 (LR 0.08147) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.01, Test_acc 86.27
2024-08-30 17:24:54,493 [podnet.py] => Task 0, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.85, Test_acc 80.87
2024-08-30 17:24:56,163 [podnet.py] => Task 0, Epoch 87/300 (LR 0.08065) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.98, Test_acc 78.57
2024-08-30 17:24:58,014 [podnet.py] => Task 0, Epoch 88/300 (LR 0.08023) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.55, Test_acc 85.57
2024-08-30 17:24:59,773 [podnet.py] => Task 0, Epoch 89/300 (LR 0.07981) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.87
2024-08-30 17:25:01,476 [podnet.py] => Task 0, Epoch 90/300 (LR 0.07939) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.83
2024-08-30 17:25:03,219 [podnet.py] => Task 0, Epoch 91/300 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 82.47
2024-08-30 17:25:04,683 [podnet.py] => Task 0, Epoch 92/300 (LR 0.07854) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 86.23
2024-08-30 17:25:06,155 [podnet.py] => Task 0, Epoch 93/300 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.07
2024-08-30 17:25:07,753 [podnet.py] => Task 0, Epoch 94/300 (LR 0.07767) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 84.03
2024-08-30 17:25:09,247 [podnet.py] => Task 0, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 80.70
2024-08-30 17:25:10,806 [podnet.py] => Task 0, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 85.07
2024-08-30 17:25:12,331 [podnet.py] => Task 0, Epoch 97/300 (LR 0.07635) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 79.37
2024-08-30 17:25:13,852 [podnet.py] => Task 0, Epoch 98/300 (LR 0.07590) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 87.33
2024-08-30 17:25:15,668 [podnet.py] => Task 0, Epoch 99/300 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.21, Test_acc 84.17
2024-08-30 17:25:17,694 [podnet.py] => Task 0, Epoch 100/300 (LR 0.07500) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 85.00
2024-08-30 17:25:19,848 [podnet.py] => Task 0, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 82.50
2024-08-30 17:25:21,667 [podnet.py] => Task 0, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.40
2024-08-30 17:25:23,535 [podnet.py] => Task 0, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 83.40
2024-08-30 17:25:25,209 [podnet.py] => Task 0, Epoch 104/300 (LR 0.07316) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 80.57
2024-08-30 17:25:26,946 [podnet.py] => Task 0, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 88.03
2024-08-30 17:25:28,756 [podnet.py] => Task 0, Epoch 106/300 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 85.80
2024-08-30 17:25:30,817 [podnet.py] => Task 0, Epoch 107/300 (LR 0.07176) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 80.70
2024-08-30 17:25:32,456 [podnet.py] => Task 0, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 85.73
2024-08-30 17:25:33,958 [podnet.py] => Task 0, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.50
2024-08-30 17:25:35,628 [podnet.py] => Task 0, Epoch 110/300 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.13
2024-08-30 17:25:37,670 [podnet.py] => Task 0, Epoch 111/300 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 87.13
2024-08-30 17:25:39,469 [podnet.py] => Task 0, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.84, Test_acc 84.77
2024-08-30 17:25:41,314 [podnet.py] => Task 0, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 85.40
2024-08-30 17:25:42,904 [podnet.py] => Task 0, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 83.33
2024-08-30 17:25:44,520 [podnet.py] => Task 0, Epoch 115/300 (LR 0.06792) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 84.27
2024-08-30 17:25:46,352 [podnet.py] => Task 0, Epoch 116/300 (LR 0.06743) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.73
2024-08-30 17:25:48,201 [podnet.py] => Task 0, Epoch 117/300 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 79.07
2024-08-30 17:25:49,853 [podnet.py] => Task 0, Epoch 118/300 (LR 0.06644) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 84.63
2024-08-30 17:25:51,671 [podnet.py] => Task 0, Epoch 119/300 (LR 0.06595) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.53
2024-08-30 17:25:53,459 [podnet.py] => Task 0, Epoch 120/300 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 83.80
2024-08-30 17:25:55,041 [podnet.py] => Task 0, Epoch 121/300 (LR 0.06495) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 87.63
2024-08-30 17:25:56,775 [podnet.py] => Task 0, Epoch 122/300 (LR 0.06445) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 83.27
2024-08-30 17:25:59,058 [podnet.py] => Task 0, Epoch 123/300 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.34, Test_acc 82.97
2024-08-30 17:26:00,639 [podnet.py] => Task 0, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 83.40
2024-08-30 17:26:02,326 [podnet.py] => Task 0, Epoch 125/300 (LR 0.06294) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.77
2024-08-30 17:26:04,093 [podnet.py] => Task 0, Epoch 126/300 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 86.57
2024-08-30 17:26:05,762 [podnet.py] => Task 0, Epoch 127/300 (LR 0.06193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.36, Test_acc 86.00
2024-08-30 17:26:07,402 [podnet.py] => Task 0, Epoch 128/300 (LR 0.06142) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 84.60
2024-08-30 17:26:09,130 [podnet.py] => Task 0, Epoch 129/300 (LR 0.06091) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 87.97
2024-08-30 17:26:11,057 [podnet.py] => Task 0, Epoch 130/300 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 82.23
2024-08-30 17:26:12,586 [podnet.py] => Task 0, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.50, Test_acc 86.90
2024-08-30 17:26:14,552 [podnet.py] => Task 0, Epoch 132/300 (LR 0.05937) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.73
2024-08-30 17:26:16,351 [podnet.py] => Task 0, Epoch 133/300 (LR 0.05885) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.57
2024-08-30 17:26:17,858 [podnet.py] => Task 0, Epoch 134/300 (LR 0.05834) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.87
2024-08-30 17:26:19,547 [podnet.py] => Task 0, Epoch 135/300 (LR 0.05782) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 85.53
2024-08-30 17:26:21,318 [podnet.py] => Task 0, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 79.13
2024-08-30 17:26:23,332 [podnet.py] => Task 0, Epoch 137/300 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 88.00
2024-08-30 17:26:25,326 [podnet.py] => Task 0, Epoch 138/300 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.33
2024-08-30 17:26:27,003 [podnet.py] => Task 0, Epoch 139/300 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 88.17
2024-08-30 17:26:29,155 [podnet.py] => Task 0, Epoch 140/300 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 82.47
2024-08-30 17:26:30,993 [podnet.py] => Task 0, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 84.93
2024-08-30 17:26:32,808 [podnet.py] => Task 0, Epoch 142/300 (LR 0.05418) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.70, Test_acc 84.13
2024-08-30 17:26:34,349 [podnet.py] => Task 0, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 84.60
2024-08-30 17:26:36,005 [podnet.py] => Task 0, Epoch 144/300 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 87.83
2024-08-30 17:26:37,659 [podnet.py] => Task 0, Epoch 145/300 (LR 0.05262) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.60
2024-08-30 17:26:39,331 [podnet.py] => Task 0, Epoch 146/300 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 81.50
2024-08-30 17:26:41,111 [podnet.py] => Task 0, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 86.77
2024-08-30 17:26:42,795 [podnet.py] => Task 0, Epoch 148/300 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 84.77
2024-08-30 17:26:44,634 [podnet.py] => Task 0, Epoch 149/300 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 85.70
2024-08-30 17:26:46,552 [podnet.py] => Task 0, Epoch 150/300 (LR 0.05000) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 86.57
2024-08-30 17:26:48,156 [podnet.py] => Task 0, Epoch 151/300 (LR 0.04948) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 84.43
2024-08-30 17:26:49,713 [podnet.py] => Task 0, Epoch 152/300 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 88.60
2024-08-30 17:26:51,288 [podnet.py] => Task 0, Epoch 153/300 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 87.07
2024-08-30 17:26:52,761 [podnet.py] => Task 0, Epoch 154/300 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 87.57
2024-08-30 17:26:54,361 [podnet.py] => Task 0, Epoch 155/300 (LR 0.04738) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 81.10
2024-08-30 17:26:56,081 [podnet.py] => Task 0, Epoch 156/300 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.67, Test_acc 88.10
2024-08-30 17:26:57,651 [podnet.py] => Task 0, Epoch 157/300 (LR 0.04634) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.87
2024-08-30 17:26:59,559 [podnet.py] => Task 0, Epoch 158/300 (LR 0.04582) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.53
2024-08-30 17:27:01,223 [podnet.py] => Task 0, Epoch 159/300 (LR 0.04529) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.43
2024-08-30 17:27:02,901 [podnet.py] => Task 0, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 86.20
2024-08-30 17:27:04,662 [podnet.py] => Task 0, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 84.87
2024-08-30 17:27:06,512 [podnet.py] => Task 0, Epoch 162/300 (LR 0.04373) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 88.27
2024-08-30 17:27:08,398 [podnet.py] => Task 0, Epoch 163/300 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 85.63
2024-08-30 17:27:09,943 [podnet.py] => Task 0, Epoch 164/300 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 83.27
2024-08-30 17:27:11,468 [podnet.py] => Task 0, Epoch 165/300 (LR 0.04218) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 85.37
2024-08-30 17:27:13,125 [podnet.py] => Task 0, Epoch 166/300 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 89.13
2024-08-30 17:27:15,265 [podnet.py] => Task 0, Epoch 167/300 (LR 0.04115) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.80
2024-08-30 17:27:17,067 [podnet.py] => Task 0, Epoch 168/300 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 90.17
2024-08-30 17:27:18,750 [podnet.py] => Task 0, Epoch 169/300 (LR 0.04012) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.03
2024-08-30 17:27:20,568 [podnet.py] => Task 0, Epoch 170/300 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 89.47
2024-08-30 17:27:22,152 [podnet.py] => Task 0, Epoch 171/300 (LR 0.03909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.13
2024-08-30 17:27:24,172 [podnet.py] => Task 0, Epoch 172/300 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 89.47
2024-08-30 17:27:26,344 [podnet.py] => Task 0, Epoch 173/300 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.13
2024-08-30 17:27:28,212 [podnet.py] => Task 0, Epoch 174/300 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 84.43
2024-08-30 17:27:29,840 [podnet.py] => Task 0, Epoch 175/300 (LR 0.03706) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.30
2024-08-30 17:27:31,393 [podnet.py] => Task 0, Epoch 176/300 (LR 0.03655) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 81.37
2024-08-30 17:27:33,059 [podnet.py] => Task 0, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.73, Test_acc 85.23
2024-08-30 17:27:34,670 [podnet.py] => Task 0, Epoch 178/300 (LR 0.03555) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 85.83
2024-08-30 17:27:36,233 [podnet.py] => Task 0, Epoch 179/300 (LR 0.03505) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 83.43
2024-08-30 17:27:38,023 [podnet.py] => Task 0, Epoch 180/300 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 88.90
2024-08-30 17:27:39,822 [podnet.py] => Task 0, Epoch 181/300 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 88.60
2024-08-30 17:27:41,404 [podnet.py] => Task 0, Epoch 182/300 (LR 0.03356) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-08-30 17:27:43,022 [podnet.py] => Task 0, Epoch 183/300 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.73
2024-08-30 17:27:44,694 [podnet.py] => Task 0, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.11, Test_acc 88.17
2024-08-30 17:27:46,292 [podnet.py] => Task 0, Epoch 185/300 (LR 0.03208) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 87.47
2024-08-30 17:27:47,993 [podnet.py] => Task 0, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 79.03
2024-08-30 17:27:49,726 [podnet.py] => Task 0, Epoch 187/300 (LR 0.03111) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.53, Test_acc 84.80
2024-08-30 17:27:51,390 [podnet.py] => Task 0, Epoch 188/300 (LR 0.03062) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 85.53
2024-08-30 17:27:53,162 [podnet.py] => Task 0, Epoch 189/300 (LR 0.03014) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 88.90
2024-08-30 17:27:54,848 [podnet.py] => Task 0, Epoch 190/300 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 86.80
2024-08-30 17:27:56,456 [podnet.py] => Task 0, Epoch 191/300 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.20
2024-08-30 17:27:58,217 [podnet.py] => Task 0, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 86.90
2024-08-30 17:27:59,993 [podnet.py] => Task 0, Epoch 193/300 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 89.30
2024-08-30 17:28:01,617 [podnet.py] => Task 0, Epoch 194/300 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.27
2024-08-30 17:28:03,276 [podnet.py] => Task 0, Epoch 195/300 (LR 0.02730) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.47
2024-08-30 17:28:04,882 [podnet.py] => Task 0, Epoch 196/300 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.73
2024-08-30 17:28:06,623 [podnet.py] => Task 0, Epoch 197/300 (LR 0.02637) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.63
2024-08-30 17:28:08,258 [podnet.py] => Task 0, Epoch 198/300 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.23
2024-08-30 17:28:10,141 [podnet.py] => Task 0, Epoch 199/300 (LR 0.02545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.60
2024-08-30 17:28:11,884 [podnet.py] => Task 0, Epoch 200/300 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.67
2024-08-30 17:28:13,638 [podnet.py] => Task 0, Epoch 201/300 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.67
2024-08-30 17:28:15,230 [podnet.py] => Task 0, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 88.40
2024-08-30 17:28:16,797 [podnet.py] => Task 0, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 88.30
2024-08-30 17:28:18,379 [podnet.py] => Task 0, Epoch 204/300 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 87.87
2024-08-30 17:28:19,976 [podnet.py] => Task 0, Epoch 205/300 (LR 0.02277) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 88.73
2024-08-30 17:28:21,657 [podnet.py] => Task 0, Epoch 206/300 (LR 0.02233) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.10
2024-08-30 17:28:23,525 [podnet.py] => Task 0, Epoch 207/300 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.13
2024-08-30 17:28:25,486 [podnet.py] => Task 0, Epoch 208/300 (LR 0.02146) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 88.40
2024-08-30 17:28:27,382 [podnet.py] => Task 0, Epoch 209/300 (LR 0.02104) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 89.43
2024-08-30 17:28:29,068 [podnet.py] => Task 0, Epoch 210/300 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.40
2024-08-30 17:28:30,619 [podnet.py] => Task 0, Epoch 211/300 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 86.90
2024-08-30 17:28:32,607 [podnet.py] => Task 0, Epoch 212/300 (LR 0.01977) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 86.67
2024-08-30 17:28:34,226 [podnet.py] => Task 0, Epoch 213/300 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-08-30 17:28:35,873 [podnet.py] => Task 0, Epoch 214/300 (LR 0.01894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 89.67
2024-08-30 17:28:37,539 [podnet.py] => Task 0, Epoch 215/300 (LR 0.01853) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 89.77
2024-08-30 17:28:39,267 [podnet.py] => Task 0, Epoch 216/300 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-30 17:28:40,902 [podnet.py] => Task 0, Epoch 217/300 (LR 0.01773) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.83
2024-08-30 17:28:42,573 [podnet.py] => Task 0, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 89.30
2024-08-30 17:28:44,207 [podnet.py] => Task 0, Epoch 219/300 (LR 0.01693) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.13
2024-08-30 17:28:45,906 [podnet.py] => Task 0, Epoch 220/300 (LR 0.01654) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.87
2024-08-30 17:28:47,694 [podnet.py] => Task 0, Epoch 221/300 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-30 17:28:49,333 [podnet.py] => Task 0, Epoch 222/300 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 17:28:51,096 [podnet.py] => Task 0, Epoch 223/300 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-30 17:28:52,752 [podnet.py] => Task 0, Epoch 224/300 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:28:54,660 [podnet.py] => Task 0, Epoch 225/300 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 17:28:56,402 [podnet.py] => Task 0, Epoch 226/300 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:28:58,277 [podnet.py] => Task 0, Epoch 227/300 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-30 17:29:00,095 [podnet.py] => Task 0, Epoch 228/300 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-08-30 17:29:02,011 [podnet.py] => Task 0, Epoch 229/300 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:29:03,874 [podnet.py] => Task 0, Epoch 230/300 (LR 0.01284) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-08-30 17:29:05,610 [podnet.py] => Task 0, Epoch 231/300 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.00
2024-08-30 17:29:07,371 [podnet.py] => Task 0, Epoch 232/300 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.90
2024-08-30 17:29:08,981 [podnet.py] => Task 0, Epoch 233/300 (LR 0.01181) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.07
2024-08-30 17:29:10,737 [podnet.py] => Task 0, Epoch 234/300 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-08-30 17:29:12,553 [podnet.py] => Task 0, Epoch 235/300 (LR 0.01114) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.03
2024-08-30 17:29:14,190 [podnet.py] => Task 0, Epoch 236/300 (LR 0.01082) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 89.43
2024-08-30 17:29:15,761 [podnet.py] => Task 0, Epoch 237/300 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:29:17,374 [podnet.py] => Task 0, Epoch 238/300 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:29:19,047 [podnet.py] => Task 0, Epoch 239/300 (LR 0.00986) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:29:20,704 [podnet.py] => Task 0, Epoch 240/300 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:29:22,327 [podnet.py] => Task 0, Epoch 241/300 (LR 0.00924) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:29:23,958 [podnet.py] => Task 0, Epoch 242/300 (LR 0.00894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:29:26,130 [podnet.py] => Task 0, Epoch 243/300 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:29:28,074 [podnet.py] => Task 0, Epoch 244/300 (LR 0.00835) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:29:29,942 [podnet.py] => Task 0, Epoch 245/300 (LR 0.00807) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-30 17:29:31,528 [podnet.py] => Task 0, Epoch 246/300 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:29:33,144 [podnet.py] => Task 0, Epoch 247/300 (LR 0.00751) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 17:29:34,879 [podnet.py] => Task 0, Epoch 248/300 (LR 0.00723) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:29:36,617 [podnet.py] => Task 0, Epoch 249/300 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:29:38,255 [podnet.py] => Task 0, Epoch 250/300 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:29:39,819 [podnet.py] => Task 0, Epoch 251/300 (LR 0.00644) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:29:41,432 [podnet.py] => Task 0, Epoch 252/300 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 17:29:43,091 [podnet.py] => Task 0, Epoch 253/300 (LR 0.00593) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:29:44,798 [podnet.py] => Task 0, Epoch 254/300 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:29:46,511 [podnet.py] => Task 0, Epoch 255/300 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:29:48,202 [podnet.py] => Task 0, Epoch 256/300 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-30 17:29:49,972 [podnet.py] => Task 0, Epoch 257/300 (LR 0.00498) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.57
2024-08-30 17:29:51,797 [podnet.py] => Task 0, Epoch 258/300 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.87
2024-08-30 17:29:53,626 [podnet.py] => Task 0, Epoch 259/300 (LR 0.00454) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-30 17:29:55,307 [podnet.py] => Task 0, Epoch 260/300 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:29:57,087 [podnet.py] => Task 0, Epoch 261/300 (LR 0.00411) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:29:58,728 [podnet.py] => Task 0, Epoch 262/300 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 17:30:00,300 [podnet.py] => Task 0, Epoch 263/300 (LR 0.00371) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-30 17:30:02,042 [podnet.py] => Task 0, Epoch 264/300 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:30:03,796 [podnet.py] => Task 0, Epoch 265/300 (LR 0.00332) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-30 17:30:05,628 [podnet.py] => Task 0, Epoch 266/300 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:30:07,360 [podnet.py] => Task 0, Epoch 267/300 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:30:09,415 [podnet.py] => Task 0, Epoch 268/300 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:30:11,345 [podnet.py] => Task 0, Epoch 269/300 (LR 0.00261) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:30:13,087 [podnet.py] => Task 0, Epoch 270/300 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:30:14,832 [podnet.py] => Task 0, Epoch 271/300 (LR 0.00229) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 17:30:16,413 [podnet.py] => Task 0, Epoch 272/300 (LR 0.00213) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 17:30:18,054 [podnet.py] => Task 0, Epoch 273/300 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:30:19,710 [podnet.py] => Task 0, Epoch 274/300 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 17:30:21,330 [podnet.py] => Task 0, Epoch 275/300 (LR 0.00170) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:30:22,962 [podnet.py] => Task 0, Epoch 276/300 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 17:30:24,625 [podnet.py] => Task 0, Epoch 277/300 (LR 0.00144) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:30:26,254 [podnet.py] => Task 0, Epoch 278/300 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.70
2024-08-30 17:30:27,904 [podnet.py] => Task 0, Epoch 279/300 (LR 0.00120) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.97
2024-08-30 17:30:29,572 [podnet.py] => Task 0, Epoch 280/300 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:30:31,423 [podnet.py] => Task 0, Epoch 281/300 (LR 0.00099) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:30:33,403 [podnet.py] => Task 0, Epoch 282/300 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-30 17:30:35,337 [podnet.py] => Task 0, Epoch 283/300 (LR 0.00079) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:30:37,146 [podnet.py] => Task 0, Epoch 284/300 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-30 17:30:38,835 [podnet.py] => Task 0, Epoch 285/300 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-30 17:30:40,385 [podnet.py] => Task 0, Epoch 286/300 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:30:41,906 [podnet.py] => Task 0, Epoch 287/300 (LR 0.00046) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 17:30:43,482 [podnet.py] => Task 0, Epoch 288/300 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:30:45,262 [podnet.py] => Task 0, Epoch 289/300 (LR 0.00033) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:30:47,182 [podnet.py] => Task 0, Epoch 290/300 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:30:48,927 [podnet.py] => Task 0, Epoch 291/300 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:30:50,617 [podnet.py] => Task 0, Epoch 292/300 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:30:52,331 [podnet.py] => Task 0, Epoch 293/300 (LR 0.00013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 17:30:54,053 [podnet.py] => Task 0, Epoch 294/300 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:30:55,891 [podnet.py] => Task 0, Epoch 295/300 (LR 0.00007) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:30:57,478 [podnet.py] => Task 0, Epoch 296/300 (LR 0.00004) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-08-30 17:30:59,083 [podnet.py] => Task 0, Epoch 297/300 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.87
2024-08-30 17:31:00,659 [podnet.py] => Task 0, Epoch 298/300 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:31:02,273 [podnet.py] => Task 0, Epoch 299/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:31:03,925 [podnet.py] => Task 0, Epoch 300/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.90
2024-08-30 17:31:04,246 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:31:04,246 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:31:10,171 [podnet.py] => Exemplar size: 500
2024-08-30 17:31:10,171 [trainer.py] => CNN: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-30 17:31:10,171 [trainer.py] => NME: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-30 17:31:10,171 [trainer.py] => CNN top1 curve: [88.9]
2024-08-30 17:31:10,171 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 17:31:10,171 [trainer.py] => NME top1 curve: [88.9]
2024-08-30 17:31:10,171 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 17:31:10,171 [trainer.py] => Average Accuracy (CNN): 88.9
2024-08-30 17:31:10,171 [trainer.py] => Average Accuracy (NME): 88.9
2024-08-30 17:31:10,172 [trainer.py] => All params: 3869505
2024-08-30 17:31:10,172 [trainer.py] => Trainable params: 3869505
2024-08-30 17:31:10,173 [podnet.py] => Learning on 5-7
2024-08-30 17:31:10,191 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 17:31:11,773 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 1.01, Spatial_loss 2.70, Flat_loss 0.69, Train_acc 74.00, Test_acc 33.86
2024-08-30 17:31:13,198 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.45, Spatial_loss 2.06, Flat_loss 0.47, Train_acc 88.60, Test_acc 34.19
2024-08-30 17:31:14,560 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.34, Spatial_loss 1.84, Flat_loss 0.39, Train_acc 91.44, Test_acc 46.88
2024-08-30 17:31:15,894 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.28, Spatial_loss 1.72, Flat_loss 0.35, Train_acc 93.78, Test_acc 48.19
2024-08-30 17:31:17,270 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.21, Spatial_loss 1.50, Flat_loss 0.31, Train_acc 95.22, Test_acc 48.52
2024-08-30 17:31:18,894 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.18, Spatial_loss 1.40, Flat_loss 0.28, Train_acc 95.91, Test_acc 54.62
2024-08-30 17:31:20,310 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.16, Spatial_loss 1.36, Flat_loss 0.27, Train_acc 96.71, Test_acc 52.19
2024-08-30 17:31:21,694 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.14, Spatial_loss 1.23, Flat_loss 0.25, Train_acc 97.09, Test_acc 55.95
2024-08-30 17:31:23,141 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.13, Spatial_loss 1.19, Flat_loss 0.24, Train_acc 97.58, Test_acc 61.67
2024-08-30 17:31:24,717 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.12, Spatial_loss 1.13, Flat_loss 0.23, Train_acc 98.18, Test_acc 57.21
2024-08-30 17:31:26,088 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.11, Spatial_loss 1.16, Flat_loss 0.23, Train_acc 98.42, Test_acc 57.98
2024-08-30 17:31:27,512 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.11, Spatial_loss 1.13, Flat_loss 0.23, Train_acc 98.16, Test_acc 59.05
2024-08-30 17:31:28,837 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.10, Spatial_loss 1.09, Flat_loss 0.22, Train_acc 98.87, Test_acc 57.52
2024-08-30 17:31:30,293 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.09, Spatial_loss 1.03, Flat_loss 0.21, Train_acc 98.87, Test_acc 60.64
2024-08-30 17:31:31,753 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.09, Spatial_loss 1.00, Flat_loss 0.21, Train_acc 99.33, Test_acc 62.12
2024-08-30 17:31:33,159 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.09, Spatial_loss 1.02, Flat_loss 0.21, Train_acc 99.13, Test_acc 62.52
2024-08-30 17:31:34,592 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 1.03, Flat_loss 0.21, Train_acc 99.09, Test_acc 65.69
2024-08-30 17:31:36,262 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.99, Flat_loss 0.20, Train_acc 99.36, Test_acc 66.79
2024-08-30 17:31:37,734 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.07, Spatial_loss 0.97, Flat_loss 0.20, Train_acc 99.58, Test_acc 66.45
2024-08-30 17:31:39,267 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.07, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 99.58, Test_acc 60.76
2024-08-30 17:31:40,594 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.08, Spatial_loss 1.01, Flat_loss 0.21, Train_acc 99.11, Test_acc 61.69
2024-08-30 17:31:41,985 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.07, Spatial_loss 0.96, Flat_loss 0.20, Train_acc 99.56, Test_acc 62.88
2024-08-30 17:31:43,600 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.07, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 99.64, Test_acc 59.02
2024-08-30 17:31:45,080 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.19, Train_acc 99.53, Test_acc 60.55
2024-08-30 17:31:46,385 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.91, Flat_loss 0.18, Train_acc 99.80, Test_acc 60.64
2024-08-30 17:31:47,769 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.18, Train_acc 99.64, Test_acc 61.98
2024-08-30 17:31:49,234 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.06, Spatial_loss 0.91, Flat_loss 0.18, Train_acc 99.71, Test_acc 61.74
2024-08-30 17:31:50,896 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.85, Flat_loss 0.18, Train_acc 99.78, Test_acc 57.93
2024-08-30 17:31:52,209 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.07, Spatial_loss 0.92, Flat_loss 0.18, Train_acc 99.13, Test_acc 63.45
2024-08-30 17:31:53,551 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.86, Flat_loss 0.17, Train_acc 99.78, Test_acc 66.50
2024-08-30 17:31:55,099 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.06, Spatial_loss 0.86, Flat_loss 0.17, Train_acc 99.73, Test_acc 66.55
2024-08-30 17:31:56,584 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.06, Spatial_loss 0.84, Flat_loss 0.17, Train_acc 99.82, Test_acc 65.57
2024-08-30 17:31:57,957 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.18, Train_acc 99.73, Test_acc 65.64
2024-08-30 17:31:59,427 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.06, Spatial_loss 0.85, Flat_loss 0.17, Train_acc 99.73, Test_acc 65.26
2024-08-30 17:32:00,893 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.86, Flat_loss 0.17, Train_acc 99.78, Test_acc 61.62
2024-08-30 17:32:02,562 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.05, Spatial_loss 0.81, Flat_loss 0.17, Train_acc 99.84, Test_acc 63.88
2024-08-30 17:32:03,945 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.17, Train_acc 99.56, Test_acc 63.26
2024-08-30 17:32:05,316 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.86, Flat_loss 0.17, Train_acc 99.78, Test_acc 64.57
2024-08-30 17:32:06,873 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.17, Train_acc 99.76, Test_acc 67.36
2024-08-30 17:32:08,546 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.82, Flat_loss 0.16, Train_acc 99.84, Test_acc 63.62
2024-08-30 17:32:10,046 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.81, Flat_loss 0.16, Train_acc 99.82, Test_acc 63.00
2024-08-30 17:32:11,459 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.16, Train_acc 99.80, Test_acc 68.45
2024-08-30 17:32:12,844 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.06, Spatial_loss 0.89, Flat_loss 0.18, Train_acc 99.42, Test_acc 64.79
2024-08-30 17:32:14,246 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.85, Flat_loss 0.16, Train_acc 99.89, Test_acc 67.55
2024-08-30 17:32:15,604 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.80, Flat_loss 0.16, Train_acc 99.89, Test_acc 64.57
2024-08-30 17:32:16,888 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.81, Flat_loss 0.16, Train_acc 99.76, Test_acc 61.98
2024-08-30 17:32:18,623 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.80, Flat_loss 0.16, Train_acc 99.82, Test_acc 63.07
2024-08-30 17:32:20,455 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.77, Flat_loss 0.16, Train_acc 99.96, Test_acc 65.83
2024-08-30 17:32:22,368 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.16, Train_acc 99.87, Test_acc 63.50
2024-08-30 17:32:23,756 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.16, Train_acc 99.89, Test_acc 65.17
2024-08-30 17:32:25,114 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 99.82, Test_acc 62.33
2024-08-30 17:32:26,454 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.82, Flat_loss 0.16, Train_acc 99.82, Test_acc 64.67
2024-08-30 17:32:27,898 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.05, Spatial_loss 0.82, Flat_loss 0.16, Train_acc 99.82, Test_acc 64.62
2024-08-30 17:32:29,250 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 99.84, Test_acc 65.88
2024-08-30 17:32:30,598 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.79, Flat_loss 0.16, Train_acc 99.73, Test_acc 64.24
2024-08-30 17:32:32,060 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.77, Flat_loss 0.16, Train_acc 99.87, Test_acc 62.52
2024-08-30 17:32:33,830 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.79, Flat_loss 0.15, Train_acc 99.91, Test_acc 62.19
2024-08-30 17:32:35,262 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.05, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 99.73, Test_acc 67.71
2024-08-30 17:32:36,603 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.05, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 99.78, Test_acc 70.45
2024-08-30 17:32:37,975 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 99.84, Test_acc 61.64
2024-08-30 17:32:39,399 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 99.89, Test_acc 64.12
2024-08-30 17:32:40,848 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.89, Test_acc 62.79
2024-08-30 17:32:42,223 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.89, Test_acc 63.93
2024-08-30 17:32:43,691 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 99.91, Test_acc 62.88
2024-08-30 17:32:45,419 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.06, Spatial_loss 0.87, Flat_loss 0.17, Train_acc 99.40, Test_acc 63.64
2024-08-30 17:32:46,826 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 99.87, Test_acc 63.62
2024-08-30 17:32:48,324 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 99.93, Test_acc 62.88
2024-08-30 17:32:49,703 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.91, Test_acc 61.64
2024-08-30 17:32:51,125 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.04, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 99.96, Test_acc 65.71
2024-08-30 17:32:52,523 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.04, Spatial_loss 0.74, Flat_loss 0.14, Train_acc 99.93, Test_acc 66.24
2024-08-30 17:32:53,877 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 99.89, Test_acc 65.62
2024-08-30 17:32:55,239 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.93, Test_acc 66.83
2024-08-30 17:32:56,801 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.75, Flat_loss 0.14, Train_acc 99.91, Test_acc 64.83
2024-08-30 17:32:58,394 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.04, Spatial_loss 0.74, Flat_loss 0.14, Train_acc 99.89, Test_acc 67.24
2024-08-30 17:33:00,283 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.74, Flat_loss 0.14, Train_acc 99.93, Test_acc 64.67
2024-08-30 17:33:01,671 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.04, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.93, Test_acc 62.24
2024-08-30 17:33:03,023 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.96, Test_acc 65.12
2024-08-30 17:33:04,526 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 99.89, Test_acc 69.38
2024-08-30 17:33:05,873 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.72, Flat_loss 0.14, Train_acc 99.91, Test_acc 63.31
2024-08-30 17:33:07,278 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.06, Spatial_loss 0.79, Flat_loss 0.16, Train_acc 99.40, Test_acc 61.05
2024-08-30 17:33:08,614 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.93, Test_acc 62.33
2024-08-30 17:33:10,171 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.04, Spatial_loss 0.72, Flat_loss 0.14, Train_acc 99.91, Test_acc 62.62
2024-08-30 17:33:11,762 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.91, Test_acc 64.62
2024-08-30 17:33:13,163 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.91, Test_acc 66.81
2024-08-30 17:33:14,575 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.72, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.21
2024-08-30 17:33:15,977 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.89, Test_acc 64.21
2024-08-30 17:33:17,507 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.87, Test_acc 65.19
2024-08-30 17:33:18,877 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 99.76, Test_acc 64.14
2024-08-30 17:33:20,255 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.04, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 99.91, Test_acc 65.55
2024-08-30 17:33:21,666 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.87, Test_acc 72.31
2024-08-30 17:33:23,073 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.44, Test_acc 60.86
2024-08-30 17:33:24,445 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.04, Spatial_loss 0.72, Flat_loss 0.14, Train_acc 99.87, Test_acc 65.17
2024-08-30 17:33:25,838 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.93, Test_acc 67.14
2024-08-30 17:33:27,209 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.89, Test_acc 64.33
2024-08-30 17:33:29,068 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.89, Test_acc 63.33
2024-08-30 17:33:30,840 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.14, Train_acc 99.89, Test_acc 62.02
2024-08-30 17:33:32,259 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.04, Spatial_loss 0.75, Flat_loss 0.14, Train_acc 99.96, Test_acc 59.79
2024-08-30 17:33:33,655 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.04, Spatial_loss 0.72, Flat_loss 0.14, Train_acc 99.91, Test_acc 64.69
2024-08-30 17:33:35,026 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.93, Test_acc 65.31
2024-08-30 17:33:36,394 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.14, Train_acc 99.93, Test_acc 63.86
2024-08-30 17:33:37,850 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.05, Spatial_loss 0.73, Flat_loss 0.15, Train_acc 99.78, Test_acc 65.71
2024-08-30 17:33:39,227 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.75, Flat_loss 0.14, Train_acc 99.91, Test_acc 62.88
2024-08-30 17:33:40,628 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.96, Test_acc 63.29
2024-08-30 17:33:41,984 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.12
2024-08-30 17:33:43,414 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.10
2024-08-30 17:33:44,887 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.67
2024-08-30 17:33:46,338 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.95
2024-08-30 17:33:47,750 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.71, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.10
2024-08-30 17:33:49,128 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.05
2024-08-30 17:33:50,459 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.52
2024-08-30 17:33:51,845 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.93, Test_acc 61.33
2024-08-30 17:33:53,684 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.71, Test_acc 66.14
2024-08-30 17:33:55,224 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.93, Test_acc 64.76
2024-08-30 17:33:56,531 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.89, Test_acc 64.74
2024-08-30 17:33:57,932 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.02
2024-08-30 17:33:59,256 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.96, Test_acc 65.86
2024-08-30 17:34:00,731 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.17
2024-08-30 17:34:02,118 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.96, Test_acc 63.79
2024-08-30 17:34:03,517 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.91, Test_acc 63.55
2024-08-30 17:34:04,889 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.74
2024-08-30 17:34:06,260 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.67
2024-08-30 17:34:07,753 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.96, Test_acc 62.90
2024-08-30 17:34:09,228 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.79
2024-08-30 17:34:10,567 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.64
2024-08-30 17:34:11,973 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.40
2024-08-30 17:34:13,467 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.95
2024-08-30 17:34:14,836 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.96, Test_acc 63.45
2024-08-30 17:34:16,170 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.76
2024-08-30 17:34:17,485 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 99.96, Test_acc 60.43
2024-08-30 17:34:18,933 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.64
2024-08-30 17:34:20,711 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.40
2024-08-30 17:34:22,156 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.87, Test_acc 59.50
2024-08-30 17:34:23,640 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.62, Test_acc 65.76
2024-08-30 17:34:25,029 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.89, Test_acc 65.95
2024-08-30 17:34:26,449 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 99.87, Test_acc 63.07
2024-08-30 17:34:27,807 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.48
2024-08-30 17:34:29,190 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.91, Test_acc 65.26
2024-08-30 17:34:30,540 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 100.00, Test_acc 66.10
2024-08-30 17:34:31,942 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.07
2024-08-30 17:34:33,749 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.83
2024-08-30 17:34:35,169 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.96, Test_acc 65.31
2024-08-30 17:34:36,517 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.12
2024-08-30 17:34:37,903 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 100.00, Test_acc 62.76
2024-08-30 17:34:39,296 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.98, Test_acc 61.19
2024-08-30 17:34:40,701 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.75, Flat_loss 0.14, Train_acc 99.73, Test_acc 66.81
2024-08-30 17:34:42,083 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 62.81
2024-08-30 17:34:43,565 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.93, Test_acc 62.86
2024-08-30 17:34:45,193 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.96, Test_acc 66.81
2024-08-30 17:34:46,671 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 99.89, Test_acc 65.45
2024-08-30 17:34:47,990 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.67
2024-08-30 17:34:49,371 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 99.98, Test_acc 63.52
2024-08-30 17:34:50,757 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.43
2024-08-30 17:34:52,225 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.96, Test_acc 64.62
2024-08-30 17:34:53,684 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 100.00, Test_acc 63.79
2024-08-30 17:34:55,031 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.96, Test_acc 66.83
2024-08-30 17:34:56,446 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.12
2024-08-30 17:34:58,160 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.74
2024-08-30 17:34:59,950 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.64
2024-08-30 17:35:01,513 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.93, Test_acc 64.24
2024-08-30 17:35:03,259 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.96, Test_acc 63.19
2024-08-30 17:35:04,643 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.96, Test_acc 68.17
2024-08-30 17:35:06,051 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.93
2024-08-30 17:35:07,565 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.81
2024-08-30 17:35:09,215 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.50
2024-08-30 17:35:10,696 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.98, Test_acc 63.45
2024-08-30 17:35:12,057 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 99.96, Test_acc 66.93
2024-08-30 17:35:13,414 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 99.98, Test_acc 64.45
2024-08-30 17:35:14,819 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.98, Test_acc 64.50
2024-08-30 17:35:16,163 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.21
2024-08-30 17:35:17,531 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.93, Test_acc 64.31
2024-08-30 17:35:18,971 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.10
2024-08-30 17:35:20,593 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.98
2024-08-30 17:35:22,294 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 99.98, Test_acc 67.17
2024-08-30 17:35:23,726 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.50
2024-08-30 17:35:25,126 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.14
2024-08-30 17:35:26,472 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.12
2024-08-30 17:35:27,934 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.36
2024-08-30 17:35:29,303 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.76
2024-08-30 17:35:30,733 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 99.96, Test_acc 65.57
2024-08-30 17:35:32,112 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.29
2024-08-30 17:35:33,592 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.21
2024-08-30 17:35:35,009 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.48
2024-08-30 17:35:36,435 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.07
2024-08-30 17:35:37,775 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.79
2024-08-30 17:35:39,176 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 99.93, Test_acc 64.90
2024-08-30 17:35:40,627 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 99.96, Test_acc 66.10
2024-08-30 17:35:42,093 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.96, Test_acc 64.79
2024-08-30 17:35:43,579 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.05
2024-08-30 17:35:45,054 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.98
2024-08-30 17:35:46,503 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.81
2024-08-30 17:35:47,928 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.71
2024-08-30 17:35:49,294 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.95
2024-08-30 17:35:50,700 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.38
2024-08-30 17:35:52,110 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 100.00, Test_acc 63.36
2024-08-30 17:35:53,565 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 63.95
2024-08-30 17:35:54,918 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.31
2024-08-30 17:35:56,247 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 99.96, Test_acc 66.95
2024-08-30 17:35:57,596 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.11, Train_acc 99.98, Test_acc 65.81
2024-08-30 17:35:59,146 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.10
2024-08-30 17:36:00,594 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.67
2024-08-30 17:36:01,986 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.50
2024-08-30 17:36:03,321 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.12
2024-08-30 17:36:04,817 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.10
2024-08-30 17:36:06,329 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.00
2024-08-30 17:36:07,665 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 99.98, Test_acc 65.57
2024-08-30 17:36:09,084 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.24
2024-08-30 17:36:10,565 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.69
2024-08-30 17:36:11,981 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.95
2024-08-30 17:36:13,312 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.07
2024-08-30 17:36:14,666 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.12
2024-08-30 17:36:16,193 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.24
2024-08-30 17:36:17,540 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.76
2024-08-30 17:36:18,953 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.98
2024-08-30 17:36:20,332 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.81
2024-08-30 17:36:21,898 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.62
2024-08-30 17:36:23,543 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.21
2024-08-30 17:36:25,011 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 99.98, Test_acc 66.31
2024-08-30 17:36:26,402 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.62
2024-08-30 17:36:27,847 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 99.98, Test_acc 66.67
2024-08-30 17:36:29,345 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.07
2024-08-30 17:36:30,707 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.71
2024-08-30 17:36:32,039 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.74
2024-08-30 17:36:33,445 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 99.98, Test_acc 66.48
2024-08-30 17:36:34,915 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.19
2024-08-30 17:36:36,273 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.10
2024-08-30 17:36:37,633 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.50
2024-08-30 17:36:39,112 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.55
2024-08-30 17:36:40,505 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.88
2024-08-30 17:36:41,909 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.90
2024-08-30 17:36:43,435 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.17
2024-08-30 17:36:44,838 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.00
2024-08-30 17:36:46,259 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.52
2024-08-30 17:36:47,696 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 65.93
2024-08-30 17:36:49,352 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.14
2024-08-30 17:36:50,816 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.67
2024-08-30 17:36:52,171 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 99.98, Test_acc 66.43
2024-08-30 17:36:53,599 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.62
2024-08-30 17:36:55,136 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.19
2024-08-30 17:36:56,785 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.60
2024-08-30 17:36:58,189 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.55
2024-08-30 17:36:59,650 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.21
2024-08-30 17:37:01,295 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 99.98, Test_acc 68.43
2024-08-30 17:37:02,939 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.88
2024-08-30 17:37:04,405 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.50
2024-08-30 17:37:05,797 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.69
2024-08-30 17:37:07,154 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.81
2024-08-30 17:37:08,749 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.29
2024-08-30 17:37:10,159 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.52
2024-08-30 17:37:11,478 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.98
2024-08-30 17:37:12,938 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.76
2024-08-30 17:37:14,451 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.83
2024-08-30 17:37:15,845 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.05
2024-08-30 17:37:17,217 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.45
2024-08-30 17:37:18,570 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.00
2024-08-30 17:37:20,030 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.50
2024-08-30 17:37:21,465 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-30 17:37:22,824 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.17
2024-08-30 17:37:24,204 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.62
2024-08-30 17:37:25,664 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.71
2024-08-30 17:37:27,280 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.50
2024-08-30 17:37:28,774 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.12
2024-08-30 17:37:30,157 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.95
2024-08-30 17:37:31,644 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.43
2024-08-30 17:37:33,317 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.45
2024-08-30 17:37:34,696 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.10
2024-08-30 17:37:36,062 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.07
2024-08-30 17:37:37,562 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.33
2024-08-30 17:37:39,104 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.95
2024-08-30 17:37:40,625 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.79
2024-08-30 17:37:42,014 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.05
2024-08-30 17:37:43,443 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.62
2024-08-30 17:37:44,994 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.24
2024-08-30 17:37:46,397 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.21
2024-08-30 17:37:47,755 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.69
2024-08-30 17:37:49,189 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.02
2024-08-30 17:37:50,575 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.67
2024-08-30 17:37:51,891 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.12
2024-08-30 17:37:53,341 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.21
2024-08-30 17:37:54,749 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.76
2024-08-30 17:37:56,220 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.21
2024-08-30 17:37:57,841 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.33
2024-08-30 17:37:59,213 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.00
2024-08-30 17:38:00,664 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.10
2024-08-30 17:38:02,074 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.00
2024-08-30 17:38:03,711 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.07
2024-08-30 17:38:05,230 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.62
2024-08-30 17:38:06,680 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.98
2024-08-30 17:38:08,180 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.24
2024-08-30 17:38:09,680 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.52
2024-08-30 17:38:11,027 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.26
2024-08-30 17:38:12,463 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.40
2024-08-30 17:38:13,932 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.31
2024-08-30 17:38:15,405 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.52
2024-08-30 17:38:16,824 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.21
2024-08-30 17:38:18,230 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.67
2024-08-30 17:38:19,589 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.05
2024-08-30 17:38:21,038 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.79
2024-08-30 17:38:22,458 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.36
2024-08-30 17:38:23,753 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.14
2024-08-30 17:38:25,108 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.76
2024-08-30 17:38:25,457 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:38:25,457 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:38:26,662 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:38:28,751 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:38:29,956 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:38:32,639 [podnet.py] => Exemplar size: 497
2024-08-30 17:38:32,640 [trainer.py] => CNN: {'total': 69.76, '00-04': 59.47, '05-06': 95.5, 'old': 59.47, 'new': 95.5}
2024-08-30 17:38:32,640 [trainer.py] => NME: {'total': 76.1, '00-04': 78.73, '05-06': 69.5, 'old': 78.73, 'new': 69.5}
2024-08-30 17:38:32,640 [trainer.py] => CNN top1 curve: [88.9, 69.76]
2024-08-30 17:38:32,640 [trainer.py] => CNN top5 curve: [100.0, 98.17]
2024-08-30 17:38:32,640 [trainer.py] => NME top1 curve: [88.9, 76.1]
2024-08-30 17:38:32,640 [trainer.py] => NME top5 curve: [100.0, 98.21]

2024-08-30 17:38:32,640 [trainer.py] => Average Accuracy (CNN): 79.33000000000001
2024-08-30 17:38:32,640 [trainer.py] => Average Accuracy (NME): 82.5
2024-08-30 17:38:32,641 [trainer.py] => All params: 3879745
2024-08-30 17:38:32,641 [trainer.py] => Trainable params: 3879745
2024-08-30 17:38:32,642 [podnet.py] => Learning on 7-9
2024-08-30 17:38:32,676 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 17:38:34,288 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 1.21, Spatial_loss 1.68, Flat_loss 0.57, Train_acc 79.72, Test_acc 40.26
2024-08-30 17:38:35,740 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.38, Spatial_loss 1.45, Flat_loss 0.35, Train_acc 91.62, Test_acc 43.19
2024-08-30 17:38:37,252 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.26, Spatial_loss 1.31, Flat_loss 0.30, Train_acc 94.53, Test_acc 51.59
2024-08-30 17:38:38,725 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.18, Spatial_loss 1.14, Flat_loss 0.25, Train_acc 97.31, Test_acc 48.00
2024-08-30 17:38:40,183 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.15, Spatial_loss 1.02, Flat_loss 0.23, Train_acc 98.24, Test_acc 55.04
2024-08-30 17:38:41,615 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.13, Spatial_loss 0.98, Flat_loss 0.23, Train_acc 98.71, Test_acc 54.26
2024-08-30 17:38:43,065 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.12, Spatial_loss 0.98, Flat_loss 0.22, Train_acc 98.98, Test_acc 51.37
2024-08-30 17:38:44,523 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.10, Spatial_loss 0.94, Flat_loss 0.21, Train_acc 99.11, Test_acc 53.07
2024-08-30 17:38:46,045 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.09, Spatial_loss 0.88, Flat_loss 0.20, Train_acc 99.60, Test_acc 59.26
2024-08-30 17:38:47,514 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.08, Spatial_loss 0.85, Flat_loss 0.19, Train_acc 99.76, Test_acc 56.83
2024-08-30 17:38:49,124 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.08, Spatial_loss 0.83, Flat_loss 0.19, Train_acc 99.84, Test_acc 50.39
2024-08-30 17:38:50,577 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.08, Spatial_loss 0.81, Flat_loss 0.18, Train_acc 99.84, Test_acc 53.81
2024-08-30 17:38:52,122 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.08, Spatial_loss 0.82, Flat_loss 0.18, Train_acc 99.84, Test_acc 58.30
2024-08-30 17:38:53,758 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.78, Flat_loss 0.18, Train_acc 99.93, Test_acc 57.28
2024-08-30 17:38:55,740 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.79, Flat_loss 0.18, Train_acc 99.82, Test_acc 50.31
2024-08-30 17:38:57,611 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.07, Spatial_loss 0.82, Flat_loss 0.18, Train_acc 99.78, Test_acc 54.33
2024-08-30 17:38:59,296 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.07, Spatial_loss 0.74, Flat_loss 0.17, Train_acc 99.82, Test_acc 57.56
2024-08-30 17:39:01,356 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.74, Flat_loss 0.17, Train_acc 99.91, Test_acc 58.15
2024-08-30 17:39:03,275 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.17, Train_acc 99.76, Test_acc 57.44
2024-08-30 17:39:04,766 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.16, Train_acc 99.91, Test_acc 55.57
2024-08-30 17:39:06,226 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.06, Spatial_loss 0.72, Flat_loss 0.16, Train_acc 99.93, Test_acc 59.06
2024-08-30 17:39:07,722 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.16, Train_acc 99.91, Test_acc 55.72
2024-08-30 17:39:09,187 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.16, Train_acc 99.96, Test_acc 54.33
2024-08-30 17:39:10,747 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.16, Train_acc 99.87, Test_acc 58.15
2024-08-30 17:39:12,258 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.16, Train_acc 99.91, Test_acc 53.31
2024-08-30 17:39:13,803 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.16, Train_acc 99.93, Test_acc 55.91
2024-08-30 17:39:15,326 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.16, Train_acc 99.98, Test_acc 57.19
2024-08-30 17:39:16,710 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.91, Test_acc 56.33
2024-08-30 17:39:18,448 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.15, Train_acc 99.91, Test_acc 59.46
2024-08-30 17:39:20,385 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.15, Train_acc 99.96, Test_acc 53.59
2024-08-30 17:39:22,383 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.15, Train_acc 99.98, Test_acc 58.11
2024-08-30 17:39:23,922 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.06, Spatial_loss 0.75, Flat_loss 0.16, Train_acc 99.69, Test_acc 58.61
2024-08-30 17:39:25,784 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.06, Spatial_loss 0.73, Flat_loss 0.15, Train_acc 99.96, Test_acc 61.00
2024-08-30 17:39:27,253 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.06, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.93, Test_acc 59.30
2024-08-30 17:39:28,857 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.96, Test_acc 61.02
2024-08-30 17:39:30,396 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.15, Train_acc 99.98, Test_acc 57.41
2024-08-30 17:39:31,963 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.96, Test_acc 55.72
2024-08-30 17:39:33,426 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.15, Train_acc 100.00, Test_acc 55.04
2024-08-30 17:39:34,939 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.15, Train_acc 99.96, Test_acc 54.00
2024-08-30 17:39:36,367 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.93, Test_acc 56.33
2024-08-30 17:39:37,903 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.15, Train_acc 99.96, Test_acc 59.89
2024-08-30 17:39:39,611 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.96, Test_acc 56.43
2024-08-30 17:39:41,219 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.05, Spatial_loss 0.70, Flat_loss 0.14, Train_acc 99.93, Test_acc 52.96
2024-08-30 17:39:43,116 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.14, Train_acc 99.96, Test_acc 55.20
2024-08-30 17:39:44,529 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.16, Train_acc 99.67, Test_acc 58.13
2024-08-30 17:39:46,477 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.15, Train_acc 99.98, Test_acc 60.56
2024-08-30 17:39:48,290 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.15, Train_acc 99.93, Test_acc 56.65
2024-08-30 17:39:49,968 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.96, Test_acc 61.06
2024-08-30 17:39:51,689 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.15, Train_acc 99.91, Test_acc 59.07
2024-08-30 17:39:53,391 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.98, Test_acc 57.56
2024-08-30 17:39:54,889 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.14, Train_acc 99.93, Test_acc 60.61
2024-08-30 17:39:56,328 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.14, Train_acc 99.98, Test_acc 57.80
2024-08-30 17:39:58,548 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.14, Train_acc 99.93, Test_acc 56.00
2024-08-30 17:40:00,492 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 100.00, Test_acc 58.11
2024-08-30 17:40:02,252 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.14, Train_acc 100.00, Test_acc 58.09
2024-08-30 17:40:03,765 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.14, Train_acc 99.98, Test_acc 59.39
2024-08-30 17:40:05,194 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.98, Test_acc 61.15
2024-08-30 17:40:06,871 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.14, Train_acc 99.93, Test_acc 57.81
2024-08-30 17:40:08,456 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.14, Train_acc 99.96, Test_acc 57.59
2024-08-30 17:40:10,209 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 100.00, Test_acc 59.02
2024-08-30 17:40:11,793 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 100.00, Test_acc 58.48
2024-08-30 17:40:13,410 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.14, Train_acc 100.00, Test_acc 56.50
2024-08-30 17:40:14,873 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 100.00, Test_acc 57.19
2024-08-30 17:40:16,363 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 100.00, Test_acc 53.83
2024-08-30 17:40:17,838 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.98
2024-08-30 17:40:19,484 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 55.22
2024-08-30 17:40:21,479 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.14, Train_acc 100.00, Test_acc 57.52
2024-08-30 17:40:23,342 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 99.98, Test_acc 54.78
2024-08-30 17:40:25,078 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.96, Test_acc 59.61
2024-08-30 17:40:27,105 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 100.00, Test_acc 59.09
2024-08-30 17:40:28,644 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.98, Test_acc 55.06
2024-08-30 17:40:30,229 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 100.00, Test_acc 54.80
2024-08-30 17:40:31,811 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.98, Test_acc 56.22
2024-08-30 17:40:33,388 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 100.00, Test_acc 61.19
2024-08-30 17:40:35,096 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.98, Test_acc 56.20
2024-08-30 17:40:36,813 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.98, Test_acc 56.31
2024-08-30 17:40:38,368 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 100.00, Test_acc 55.93
2024-08-30 17:40:40,095 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.13, Train_acc 99.98, Test_acc 57.93
2024-08-30 17:40:41,883 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.96, Test_acc 58.98
2024-08-30 17:40:43,350 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 99.98, Test_acc 58.59
2024-08-30 17:40:44,855 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 100.00, Test_acc 56.20
2024-08-30 17:40:46,461 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.98, Test_acc 54.67
2024-08-30 17:40:48,132 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.06, Spatial_loss 0.73, Flat_loss 0.16, Train_acc 99.69, Test_acc 60.37
2024-08-30 17:40:49,840 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.14, Train_acc 99.91, Test_acc 59.57
2024-08-30 17:40:51,608 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.10, Spatial_loss 0.78, Flat_loss 0.17, Train_acc 98.89, Test_acc 65.00
2024-08-30 17:40:53,363 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.06, Spatial_loss 0.75, Flat_loss 0.16, Train_acc 99.67, Test_acc 62.61
2024-08-30 17:40:55,341 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.15, Train_acc 99.87, Test_acc 59.52
2024-08-30 17:40:56,913 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.98, Test_acc 56.98
2024-08-30 17:40:58,424 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.87, Test_acc 54.94
2024-08-30 17:40:59,939 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 99.98, Test_acc 57.89
2024-08-30 17:41:01,397 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 100.00, Test_acc 55.98
2024-08-30 17:41:03,003 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.13, Train_acc 99.98, Test_acc 59.74
2024-08-30 17:41:04,710 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.15, Train_acc 99.78, Test_acc 52.41
2024-08-30 17:41:06,435 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.14, Train_acc 99.89, Test_acc 59.50
2024-08-30 17:41:07,803 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.14, Train_acc 99.93, Test_acc 54.65
2024-08-30 17:41:09,582 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.13, Train_acc 100.00, Test_acc 56.13
2024-08-30 17:41:11,208 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.94
2024-08-30 17:41:12,872 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.14, Train_acc 99.98, Test_acc 58.43
2024-08-30 17:41:14,671 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.13, Train_acc 99.98, Test_acc 55.33
2024-08-30 17:41:16,378 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.48
2024-08-30 17:41:18,033 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 100.00, Test_acc 60.35
2024-08-30 17:41:19,627 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.13, Train_acc 100.00, Test_acc 55.28
2024-08-30 17:41:21,162 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.13, Train_acc 100.00, Test_acc 54.89
2024-08-30 17:41:22,694 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.00
2024-08-30 17:41:24,382 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 100.00, Test_acc 54.13
2024-08-30 17:41:26,259 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 99.96, Test_acc 57.91
2024-08-30 17:41:28,244 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 100.00, Test_acc 55.63
2024-08-30 17:41:30,235 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.37
2024-08-30 17:41:32,056 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.61
2024-08-30 17:41:33,698 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.74
2024-08-30 17:41:35,428 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.98
2024-08-30 17:41:36,951 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 99.98, Test_acc 56.89
2024-08-30 17:41:38,691 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 99.96, Test_acc 56.89
2024-08-30 17:41:40,448 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 100.00, Test_acc 51.56
2024-08-30 17:41:42,160 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.13, Train_acc 99.98, Test_acc 60.22
2024-08-30 17:41:44,079 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.37
2024-08-30 17:41:45,746 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.57
2024-08-30 17:41:47,371 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 100.00, Test_acc 53.15
2024-08-30 17:41:49,108 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 100.00, Test_acc 60.30
2024-08-30 17:41:51,083 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 54.43
2024-08-30 17:41:53,019 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.70
2024-08-30 17:41:55,209 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 99.96, Test_acc 56.85
2024-08-30 17:41:57,471 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.96, Test_acc 55.56
2024-08-30 17:41:59,191 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 99.98, Test_acc 53.65
2024-08-30 17:42:00,666 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.13, Train_acc 100.00, Test_acc 59.00
2024-08-30 17:42:02,113 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 99.98, Test_acc 55.00
2024-08-30 17:42:03,636 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.00
2024-08-30 17:42:05,116 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.33
2024-08-30 17:42:06,635 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 58.96
2024-08-30 17:42:08,065 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.76, Test_acc 60.00
2024-08-30 17:42:09,690 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.13, Train_acc 99.98, Test_acc 57.87
2024-08-30 17:42:11,313 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 99.96, Test_acc 54.67
2024-08-30 17:42:12,844 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.06
2024-08-30 17:42:14,423 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.69
2024-08-30 17:42:15,995 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.13, Train_acc 99.98, Test_acc 54.59
2024-08-30 17:42:17,885 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.11
2024-08-30 17:42:19,543 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.89
2024-08-30 17:42:21,416 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 57.50
2024-08-30 17:42:23,002 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:42:24,550 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.65
2024-08-30 17:42:26,476 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.07
2024-08-30 17:42:28,197 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.96, Test_acc 55.48
2024-08-30 17:42:30,078 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 61.33
2024-08-30 17:42:31,801 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.94
2024-08-30 17:42:33,454 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.56
2024-08-30 17:42:35,340 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.50
2024-08-30 17:42:37,112 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.28
2024-08-30 17:42:38,754 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.93
2024-08-30 17:42:40,290 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.00
2024-08-30 17:42:41,873 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 99.98, Test_acc 60.26
2024-08-30 17:42:43,647 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.31
2024-08-30 17:42:45,689 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.41
2024-08-30 17:42:47,881 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.43
2024-08-30 17:42:50,033 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.85
2024-08-30 17:42:51,723 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.65
2024-08-30 17:42:53,189 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.11
2024-08-30 17:42:54,783 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.67
2024-08-30 17:42:56,508 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.72
2024-08-30 17:42:58,347 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.57
2024-08-30 17:43:00,200 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.85
2024-08-30 17:43:01,902 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.12, Train_acc 99.98, Test_acc 54.30
2024-08-30 17:43:03,464 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.58, Flat_loss 0.14, Train_acc 99.69, Test_acc 58.61
2024-08-30 17:43:04,862 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.13, Train_acc 99.87, Test_acc 58.28
2024-08-30 17:43:06,351 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 99.98, Test_acc 56.11
2024-08-30 17:43:07,905 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.46
2024-08-30 17:43:09,406 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.56
2024-08-30 17:43:10,912 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.19
2024-08-30 17:43:12,425 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.44
2024-08-30 17:43:14,036 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.15
2024-08-30 17:43:15,505 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.61
2024-08-30 17:43:17,105 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.56
2024-08-30 17:43:18,542 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 99.98, Test_acc 59.39
2024-08-30 17:43:20,345 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.12, Train_acc 99.98, Test_acc 58.15
2024-08-30 17:43:21,922 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.15
2024-08-30 17:43:23,383 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.39
2024-08-30 17:43:24,919 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.91
2024-08-30 17:43:26,486 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.35
2024-08-30 17:43:28,099 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.83
2024-08-30 17:43:29,697 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.76
2024-08-30 17:43:31,250 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.61
2024-08-30 17:43:32,764 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.80
2024-08-30 17:43:34,408 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.85
2024-08-30 17:43:36,149 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.39
2024-08-30 17:43:37,880 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.59
2024-08-30 17:43:39,368 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.83
2024-08-30 17:43:40,834 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.94
2024-08-30 17:43:42,309 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.59
2024-08-30 17:43:43,971 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.91
2024-08-30 17:43:45,569 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.06, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 99.98, Test_acc 56.54
2024-08-30 17:43:47,177 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.12, Train_acc 99.93, Test_acc 57.61
2024-08-30 17:43:48,890 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.12, Train_acc 100.00, Test_acc 61.26
2024-08-30 17:43:50,927 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.81
2024-08-30 17:43:52,970 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 99.98, Test_acc 58.72
2024-08-30 17:43:54,635 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.26
2024-08-30 17:43:56,146 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.96
2024-08-30 17:43:57,684 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.61
2024-08-30 17:43:59,094 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.24
2024-08-30 17:44:00,865 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.04
2024-08-30 17:44:02,641 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.19
2024-08-30 17:44:04,245 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.74
2024-08-30 17:44:05,798 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.65
2024-08-30 17:44:07,327 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.65
2024-08-30 17:44:08,752 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.56
2024-08-30 17:44:10,213 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.39
2024-08-30 17:44:11,761 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.20
2024-08-30 17:44:13,277 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.33
2024-08-30 17:44:14,972 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.31
2024-08-30 17:44:16,616 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.41
2024-08-30 17:44:18,240 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.41
2024-08-30 17:44:19,685 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.48
2024-08-30 17:44:21,141 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.76
2024-08-30 17:44:22,741 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.74
2024-08-30 17:44:24,352 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.81
2024-08-30 17:44:25,870 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.11, Train_acc 100.00, Test_acc 61.57
2024-08-30 17:44:27,484 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.94
2024-08-30 17:44:29,114 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.89
2024-08-30 17:44:30,778 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.70
2024-08-30 17:44:32,489 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.11, Train_acc 99.98, Test_acc 59.15
2024-08-30 17:44:34,407 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.11, Train_acc 100.00, Test_acc 61.15
2024-08-30 17:44:36,145 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.02
2024-08-30 17:44:37,974 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.81
2024-08-30 17:44:39,408 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.94
2024-08-30 17:44:40,929 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.56
2024-08-30 17:44:42,417 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.61
2024-08-30 17:44:43,871 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.22
2024-08-30 17:44:45,399 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.93
2024-08-30 17:44:47,096 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:44:48,767 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.91
2024-08-30 17:44:50,368 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.93
2024-08-30 17:44:52,029 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:44:53,572 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.98
2024-08-30 17:44:55,069 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.11
2024-08-30 17:44:56,765 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.30
2024-08-30 17:44:58,295 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:45:00,002 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.63
2024-08-30 17:45:01,469 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.48
2024-08-30 17:45:03,012 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.98
2024-08-30 17:45:04,568 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.24
2024-08-30 17:45:06,097 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.81
2024-08-30 17:45:07,727 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.44
2024-08-30 17:45:09,277 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.19
2024-08-30 17:45:10,735 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.69
2024-08-30 17:45:12,304 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.24
2024-08-30 17:45:13,942 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.61
2024-08-30 17:45:15,806 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.56
2024-08-30 17:45:17,318 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 99.98, Test_acc 61.00
2024-08-30 17:45:18,757 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.35
2024-08-30 17:45:20,290 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.39
2024-08-30 17:45:21,929 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.15
2024-08-30 17:45:23,452 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.50
2024-08-30 17:45:25,123 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.65
2024-08-30 17:45:26,785 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.85
2024-08-30 17:45:28,312 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.00
2024-08-30 17:45:30,161 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.31
2024-08-30 17:45:31,722 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.65
2024-08-30 17:45:33,333 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.78
2024-08-30 17:45:34,874 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.37
2024-08-30 17:45:36,789 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.35
2024-08-30 17:45:38,312 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.65
2024-08-30 17:45:39,968 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.50
2024-08-30 17:45:41,689 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.83
2024-08-30 17:45:43,547 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.63
2024-08-30 17:45:45,627 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.56
2024-08-30 17:45:47,136 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.96
2024-08-30 17:45:48,782 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.69
2024-08-30 17:45:50,935 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.70
2024-08-30 17:45:52,894 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.78
2024-08-30 17:45:54,790 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.87
2024-08-30 17:45:56,467 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.80
2024-08-30 17:45:58,136 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.56
2024-08-30 17:45:59,904 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.94
2024-08-30 17:46:01,536 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.81
2024-08-30 17:46:03,242 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.59
2024-08-30 17:46:05,445 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.67
2024-08-30 17:46:06,982 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.91
2024-08-30 17:46:08,766 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.89
2024-08-30 17:46:10,508 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.26
2024-08-30 17:46:12,707 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:46:14,428 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.59
2024-08-30 17:46:15,981 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.19
2024-08-30 17:46:17,507 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 99.98, Test_acc 61.41
2024-08-30 17:46:19,467 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.93
2024-08-30 17:46:21,502 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.81
2024-08-30 17:46:23,298 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.93
2024-08-30 17:46:24,909 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.37
2024-08-30 17:46:26,562 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.33
2024-08-30 17:46:28,256 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.74
2024-08-30 17:46:29,891 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.19
2024-08-30 17:46:31,692 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.31
2024-08-30 17:46:33,706 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.46
2024-08-30 17:46:35,575 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.24
2024-08-30 17:46:37,503 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.20
2024-08-30 17:46:39,321 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.78
2024-08-30 17:46:41,343 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.80
2024-08-30 17:46:43,237 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.09
2024-08-30 17:46:45,080 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.41
2024-08-30 17:46:46,908 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.20
2024-08-30 17:46:48,700 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.87
2024-08-30 17:46:50,468 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.30
2024-08-30 17:46:52,284 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:46:52,738 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:46:52,738 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:46:54,389 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:46:56,190 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 17:46:57,754 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 17:47:00,435 [podnet.py] => Exemplar size: 495
2024-08-30 17:47:00,436 [trainer.py] => CNN: {'total': 61.22, '00-04': 50.6, '05-06': 50.58, '07-08': 98.42, 'old': 50.6, 'new': 98.42}
2024-08-30 17:47:00,436 [trainer.py] => NME: {'total': 68.28, '00-04': 69.6, '05-06': 43.75, '07-08': 89.5, 'old': 62.21, 'new': 89.5}
2024-08-30 17:47:00,436 [trainer.py] => CNN top1 curve: [88.9, 69.76, 61.22]
2024-08-30 17:47:00,436 [trainer.py] => CNN top5 curve: [100.0, 98.17, 93.65]
2024-08-30 17:47:00,436 [trainer.py] => NME top1 curve: [88.9, 76.1, 68.28]
2024-08-30 17:47:00,436 [trainer.py] => NME top5 curve: [100.0, 98.21, 95.35]

2024-08-30 17:47:00,436 [trainer.py] => Average Accuracy (CNN): 73.29333333333334
2024-08-30 17:47:00,436 [trainer.py] => Average Accuracy (NME): 77.76
2024-08-30 17:47:00,437 [trainer.py] => Forgetting (CNN): 41.61
