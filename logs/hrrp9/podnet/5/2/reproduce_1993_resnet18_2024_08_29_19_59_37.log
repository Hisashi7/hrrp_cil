2024-08-29 19:59:37,855 [trainer.py] => config: ./exps/podnet.json
2024-08-29 19:59:37,855 [trainer.py] => prefix: reproduce
2024-08-29 19:59:37,855 [trainer.py] => dataset: hrrp9
2024-08-29 19:59:37,856 [trainer.py] => memory_size: 500
2024-08-29 19:59:37,856 [trainer.py] => memory_per_class: 20
2024-08-29 19:59:37,856 [trainer.py] => fixed_memory: False
2024-08-29 19:59:37,856 [trainer.py] => shuffle: True
2024-08-29 19:59:37,857 [trainer.py] => init_cls: 5
2024-08-29 19:59:37,857 [trainer.py] => increment: 2
2024-08-29 19:59:37,857 [trainer.py] => model_name: podnet
2024-08-29 19:59:37,857 [trainer.py] => convnet_type: resnet18
2024-08-29 19:59:37,858 [trainer.py] => init_train: True
2024-08-29 19:59:37,858 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-29 19:59:37,858 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-29 19:59:37,858 [trainer.py] => device: [device(type='cuda', index=3)]
2024-08-29 19:59:37,858 [trainer.py] => seed: 1993
2024-08-29 19:59:38,631 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-29 19:59:38,731 [trainer.py] => All params: 3843904
2024-08-29 19:59:38,732 [trainer.py] => Trainable params: 3843904
2024-08-29 19:59:38,732 [podnet.py] => Learning on 0-5
2024-08-29 19:59:38,795 [podnet.py] => Adaptive factor: 0
2024-08-29 19:59:42,464 [podnet.py] => Task 0, Epoch 1/150 (LR 0.09999) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 22.09
2024-08-29 19:59:45,386 [podnet.py] => Task 0, Epoch 2/150 (LR 0.09996) => LSC_loss 1.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.33, Test_acc 18.07
2024-08-29 19:59:48,401 [podnet.py] => Task 0, Epoch 3/150 (LR 0.09990) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.79, Test_acc 50.58
2024-08-29 19:59:51,048 [podnet.py] => Task 0, Epoch 4/150 (LR 0.09982) => LSC_loss 0.50, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.56, Test_acc 65.48
2024-08-29 19:59:53,495 [podnet.py] => Task 0, Epoch 5/150 (LR 0.09973) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.45, Test_acc 62.52
2024-08-29 19:59:56,098 [podnet.py] => Task 0, Epoch 6/150 (LR 0.09961) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.10, Test_acc 80.62
2024-08-29 19:59:58,560 [podnet.py] => Task 0, Epoch 7/150 (LR 0.09946) => LSC_loss 0.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.42, Test_acc 81.81
2024-08-29 20:00:01,529 [podnet.py] => Task 0, Epoch 8/150 (LR 0.09930) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.45, Test_acc 68.50
2024-08-29 20:00:04,527 [podnet.py] => Task 0, Epoch 9/150 (LR 0.09911) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.96, Test_acc 84.72
2024-08-29 20:00:07,384 [podnet.py] => Task 0, Epoch 10/150 (LR 0.09891) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.95, Test_acc 81.11
2024-08-29 20:00:09,838 [podnet.py] => Task 0, Epoch 11/150 (LR 0.09868) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 76.28
2024-08-29 20:00:12,433 [podnet.py] => Task 0, Epoch 12/150 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 90.48
2024-08-29 20:00:15,352 [podnet.py] => Task 0, Epoch 13/150 (LR 0.09816) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 87.63
2024-08-29 20:00:18,066 [podnet.py] => Task 0, Epoch 14/150 (LR 0.09787) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 80.29
2024-08-29 20:00:21,001 [podnet.py] => Task 0, Epoch 15/150 (LR 0.09755) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 47.36
2024-08-29 20:00:23,993 [podnet.py] => Task 0, Epoch 16/150 (LR 0.09722) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.84, Test_acc 70.30
2024-08-29 20:00:26,641 [podnet.py] => Task 0, Epoch 17/150 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 85.99
2024-08-29 20:00:29,194 [podnet.py] => Task 0, Epoch 18/150 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 84.64
2024-08-29 20:00:31,870 [podnet.py] => Task 0, Epoch 19/150 (LR 0.09609) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.24, Test_acc 85.80
2024-08-29 20:00:34,848 [podnet.py] => Task 0, Epoch 20/150 (LR 0.09568) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.26, Test_acc 83.43
2024-08-29 20:00:38,030 [podnet.py] => Task 0, Epoch 21/150 (LR 0.09524) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 88.33
2024-08-29 20:00:40,546 [podnet.py] => Task 0, Epoch 22/150 (LR 0.09479) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 90.30
2024-08-29 20:00:42,958 [podnet.py] => Task 0, Epoch 23/150 (LR 0.09431) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.93, Test_acc 82.26
2024-08-29 20:00:45,566 [podnet.py] => Task 0, Epoch 24/150 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 73.74
2024-08-29 20:00:48,473 [podnet.py] => Task 0, Epoch 25/150 (LR 0.09330) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.73, Test_acc 87.26
2024-08-29 20:00:51,451 [podnet.py] => Task 0, Epoch 26/150 (LR 0.09277) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 84.01
2024-08-29 20:00:54,314 [podnet.py] => Task 0, Epoch 27/150 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.77, Test_acc 87.89
2024-08-29 20:00:56,786 [podnet.py] => Task 0, Epoch 28/150 (LR 0.09165) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.34, Test_acc 83.65
2024-08-29 20:00:59,538 [podnet.py] => Task 0, Epoch 29/150 (LR 0.09106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 90.88
2024-08-29 20:01:02,535 [podnet.py] => Task 0, Epoch 30/150 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.04, Test_acc 87.74
2024-08-29 20:01:04,986 [podnet.py] => Task 0, Epoch 31/150 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 86.68
2024-08-29 20:01:07,997 [podnet.py] => Task 0, Epoch 32/150 (LR 0.08918) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 89.32
2024-08-29 20:01:10,979 [podnet.py] => Task 0, Epoch 33/150 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.86, Test_acc 81.35
2024-08-29 20:01:13,597 [podnet.py] => Task 0, Epoch 34/150 (LR 0.08785) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.42, Test_acc 86.87
2024-08-29 20:01:16,320 [podnet.py] => Task 0, Epoch 35/150 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 76.38
2024-08-29 20:01:19,243 [podnet.py] => Task 0, Epoch 36/150 (LR 0.08645) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.51, Test_acc 86.54
2024-08-29 20:01:22,004 [podnet.py] => Task 0, Epoch 37/150 (LR 0.08572) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 86.01
2024-08-29 20:01:24,146 [podnet.py] => Task 0, Epoch 38/150 (LR 0.08498) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 86.44
2024-08-29 20:01:27,245 [podnet.py] => Task 0, Epoch 39/150 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 70.57
2024-08-29 20:01:30,319 [podnet.py] => Task 0, Epoch 40/150 (LR 0.08346) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 86.26
2024-08-29 20:01:33,277 [podnet.py] => Task 0, Epoch 41/150 (LR 0.08267) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 90.55
2024-08-29 20:01:36,050 [podnet.py] => Task 0, Epoch 42/150 (LR 0.08187) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 88.70
2024-08-29 20:01:39,080 [podnet.py] => Task 0, Epoch 43/150 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.49
2024-08-29 20:01:41,884 [podnet.py] => Task 0, Epoch 44/150 (LR 0.08023) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 83.72
2024-08-29 20:01:44,650 [podnet.py] => Task 0, Epoch 45/150 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 81.79
2024-08-29 20:01:47,386 [podnet.py] => Task 0, Epoch 46/150 (LR 0.07854) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 86.32
2024-08-29 20:01:50,081 [podnet.py] => Task 0, Epoch 47/150 (LR 0.07767) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 87.00
2024-08-29 20:01:52,592 [podnet.py] => Task 0, Epoch 48/150 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 82.77
2024-08-29 20:01:55,552 [podnet.py] => Task 0, Epoch 49/150 (LR 0.07590) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 89.07
2024-08-29 20:01:58,689 [podnet.py] => Task 0, Epoch 50/150 (LR 0.07500) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.48, Test_acc 88.53
2024-08-29 20:02:01,572 [podnet.py] => Task 0, Epoch 51/150 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.42, Test_acc 83.80
2024-08-29 20:02:04,450 [podnet.py] => Task 0, Epoch 52/150 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 87.39
2024-08-29 20:02:07,266 [podnet.py] => Task 0, Epoch 53/150 (LR 0.07223) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 84.50
2024-08-29 20:02:10,162 [podnet.py] => Task 0, Epoch 54/150 (LR 0.07129) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 88.91
2024-08-29 20:02:12,610 [podnet.py] => Task 0, Epoch 55/150 (LR 0.07034) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 76.63
2024-08-29 20:02:15,016 [podnet.py] => Task 0, Epoch 56/150 (LR 0.06938) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 84.99
2024-08-29 20:02:17,387 [podnet.py] => Task 0, Epoch 57/150 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.67, Test_acc 91.06
2024-08-29 20:02:20,401 [podnet.py] => Task 0, Epoch 58/150 (LR 0.06743) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 89.04
2024-08-29 20:02:23,485 [podnet.py] => Task 0, Epoch 59/150 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.05, Test_acc 81.02
2024-08-29 20:02:26,453 [podnet.py] => Task 0, Epoch 60/150 (LR 0.06545) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 82.21
2024-08-29 20:02:28,917 [podnet.py] => Task 0, Epoch 61/150 (LR 0.06445) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.80, Test_acc 88.24
2024-08-29 20:02:31,513 [podnet.py] => Task 0, Epoch 62/150 (LR 0.06345) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 88.03
2024-08-29 20:02:34,313 [podnet.py] => Task 0, Epoch 63/150 (LR 0.06243) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.10, Test_acc 84.79
2024-08-29 20:02:37,150 [podnet.py] => Task 0, Epoch 64/150 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 77.03
2024-08-29 20:02:39,995 [podnet.py] => Task 0, Epoch 65/150 (LR 0.06040) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 89.38
2024-08-29 20:02:42,685 [podnet.py] => Task 0, Epoch 66/150 (LR 0.05937) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.98, Test_acc 91.14
2024-08-29 20:02:45,182 [podnet.py] => Task 0, Epoch 67/150 (LR 0.05834) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 83.77
2024-08-29 20:02:47,761 [podnet.py] => Task 0, Epoch 68/150 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 92.38
2024-08-29 20:02:49,914 [podnet.py] => Task 0, Epoch 69/150 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 89.02
2024-08-29 20:02:52,839 [podnet.py] => Task 0, Epoch 70/150 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 91.84
2024-08-29 20:02:55,895 [podnet.py] => Task 0, Epoch 71/150 (LR 0.05418) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 89.42
2024-08-29 20:02:58,801 [podnet.py] => Task 0, Epoch 72/150 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 92.94
2024-08-29 20:03:01,583 [podnet.py] => Task 0, Epoch 73/150 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.54, Test_acc 87.23
2024-08-29 20:03:03,864 [podnet.py] => Task 0, Epoch 74/150 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 90.76
2024-08-29 20:03:06,441 [podnet.py] => Task 0, Epoch 75/150 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 90.57
2024-08-29 20:03:09,148 [podnet.py] => Task 0, Epoch 76/150 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 84.21
2024-08-29 20:03:12,046 [podnet.py] => Task 0, Epoch 77/150 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 88.14
2024-08-29 20:03:14,187 [podnet.py] => Task 0, Epoch 78/150 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.26, Test_acc 89.42
2024-08-29 20:03:16,248 [podnet.py] => Task 0, Epoch 79/150 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 89.80
2024-08-29 20:03:18,671 [podnet.py] => Task 0, Epoch 80/150 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 83.44
2024-08-29 20:03:21,183 [podnet.py] => Task 0, Epoch 81/150 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 89.16
2024-08-29 20:03:23,401 [podnet.py] => Task 0, Epoch 82/150 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 87.40
2024-08-29 20:03:25,823 [podnet.py] => Task 0, Epoch 83/150 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 93.15
2024-08-29 20:03:28,073 [podnet.py] => Task 0, Epoch 84/150 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 92.60
2024-08-29 20:03:30,409 [podnet.py] => Task 0, Epoch 85/150 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.72
2024-08-29 20:03:32,718 [podnet.py] => Task 0, Epoch 86/150 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 90.30
2024-08-29 20:03:35,073 [podnet.py] => Task 0, Epoch 87/150 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 91.25
2024-08-29 20:03:37,444 [podnet.py] => Task 0, Epoch 88/150 (LR 0.03655) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 88.91
2024-08-29 20:03:39,995 [podnet.py] => Task 0, Epoch 89/150 (LR 0.03555) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 90.66
2024-08-29 20:03:41,958 [podnet.py] => Task 0, Epoch 90/150 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 83.45
2024-08-29 20:03:44,819 [podnet.py] => Task 0, Epoch 91/150 (LR 0.03356) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 92.60
2024-08-29 20:03:47,630 [podnet.py] => Task 0, Epoch 92/150 (LR 0.03257) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 90.70
2024-08-29 20:03:50,292 [podnet.py] => Task 0, Epoch 93/150 (LR 0.03159) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 87.68
2024-08-29 20:03:53,086 [podnet.py] => Task 0, Epoch 94/150 (LR 0.03062) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 88.95
2024-08-29 20:03:55,798 [podnet.py] => Task 0, Epoch 95/150 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 91.80
2024-08-29 20:03:58,621 [podnet.py] => Task 0, Epoch 96/150 (LR 0.02871) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 90.04
2024-08-29 20:04:01,493 [podnet.py] => Task 0, Epoch 97/150 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 91.57
2024-08-29 20:04:04,382 [podnet.py] => Task 0, Epoch 98/150 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 92.44
2024-08-29 20:04:07,289 [podnet.py] => Task 0, Epoch 99/150 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.78
2024-08-29 20:04:10,328 [podnet.py] => Task 0, Epoch 100/150 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.93
2024-08-29 20:04:13,298 [podnet.py] => Task 0, Epoch 101/150 (LR 0.02410) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.82
2024-08-29 20:04:16,250 [podnet.py] => Task 0, Epoch 102/150 (LR 0.02321) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 92.47
2024-08-29 20:04:19,210 [podnet.py] => Task 0, Epoch 103/150 (LR 0.02233) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.34
2024-08-29 20:04:22,306 [podnet.py] => Task 0, Epoch 104/150 (LR 0.02146) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.27
2024-08-29 20:04:25,230 [podnet.py] => Task 0, Epoch 105/150 (LR 0.02061) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.52
2024-08-29 20:04:27,557 [podnet.py] => Task 0, Epoch 106/150 (LR 0.01977) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.63
2024-08-29 20:04:30,269 [podnet.py] => Task 0, Epoch 107/150 (LR 0.01894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.52
2024-08-29 20:04:33,371 [podnet.py] => Task 0, Epoch 108/150 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.78
2024-08-29 20:04:36,565 [podnet.py] => Task 0, Epoch 109/150 (LR 0.01733) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.97
2024-08-29 20:04:39,578 [podnet.py] => Task 0, Epoch 110/150 (LR 0.01654) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.64
2024-08-29 20:04:42,550 [podnet.py] => Task 0, Epoch 111/150 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.82
2024-08-29 20:04:45,641 [podnet.py] => Task 0, Epoch 112/150 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 92.83
2024-08-29 20:04:48,224 [podnet.py] => Task 0, Epoch 113/150 (LR 0.01428) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 90.79
2024-08-29 20:04:50,802 [podnet.py] => Task 0, Epoch 114/150 (LR 0.01355) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 91.15
2024-08-29 20:04:53,349 [podnet.py] => Task 0, Epoch 115/150 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 91.18
2024-08-29 20:04:55,981 [podnet.py] => Task 0, Epoch 116/150 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 92.77
2024-08-29 20:04:59,269 [podnet.py] => Task 0, Epoch 117/150 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.42
2024-08-29 20:05:02,090 [podnet.py] => Task 0, Epoch 118/150 (LR 0.01082) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.27
2024-08-29 20:05:04,968 [podnet.py] => Task 0, Epoch 119/150 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.45
2024-08-29 20:05:08,012 [podnet.py] => Task 0, Epoch 120/150 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.61
2024-08-29 20:05:11,023 [podnet.py] => Task 0, Epoch 121/150 (LR 0.00894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 92.63
2024-08-29 20:05:13,818 [podnet.py] => Task 0, Epoch 122/150 (LR 0.00835) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 92.38
2024-08-29 20:05:16,687 [podnet.py] => Task 0, Epoch 123/150 (LR 0.00778) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 92.64
2024-08-29 20:05:19,576 [podnet.py] => Task 0, Epoch 124/150 (LR 0.00723) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 93.00
2024-08-29 20:05:22,858 [podnet.py] => Task 0, Epoch 125/150 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 93.17
2024-08-29 20:05:25,506 [podnet.py] => Task 0, Epoch 126/150 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 93.09
2024-08-29 20:05:28,417 [podnet.py] => Task 0, Epoch 127/150 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 93.17
2024-08-29 20:05:31,188 [podnet.py] => Task 0, Epoch 128/150 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.96
2024-08-29 20:05:33,584 [podnet.py] => Task 0, Epoch 129/150 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 92.46
2024-08-29 20:05:36,002 [podnet.py] => Task 0, Epoch 130/150 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.77
2024-08-29 20:05:38,585 [podnet.py] => Task 0, Epoch 131/150 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.89
2024-08-29 20:05:41,460 [podnet.py] => Task 0, Epoch 132/150 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.57
2024-08-29 20:05:44,508 [podnet.py] => Task 0, Epoch 133/150 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 93.27
2024-08-29 20:05:47,157 [podnet.py] => Task 0, Epoch 134/150 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.72
2024-08-29 20:05:49,858 [podnet.py] => Task 0, Epoch 135/150 (LR 0.00245) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 92.27
2024-08-29 20:05:53,040 [podnet.py] => Task 0, Epoch 136/150 (LR 0.00213) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 91.68
2024-08-29 20:05:55,610 [podnet.py] => Task 0, Epoch 137/150 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.38
2024-08-29 20:05:58,280 [podnet.py] => Task 0, Epoch 138/150 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.66
2024-08-29 20:06:01,012 [podnet.py] => Task 0, Epoch 139/150 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.85
2024-08-29 20:06:03,653 [podnet.py] => Task 0, Epoch 140/150 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.05
2024-08-29 20:06:06,309 [podnet.py] => Task 0, Epoch 141/150 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.91
2024-08-29 20:06:08,689 [podnet.py] => Task 0, Epoch 142/150 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 91.80
2024-08-29 20:06:11,369 [podnet.py] => Task 0, Epoch 143/150 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.80
2024-08-29 20:06:14,224 [podnet.py] => Task 0, Epoch 144/150 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.99
2024-08-29 20:06:17,094 [podnet.py] => Task 0, Epoch 145/150 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 91.83
2024-08-29 20:06:19,930 [podnet.py] => Task 0, Epoch 146/150 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.02
2024-08-29 20:06:22,547 [podnet.py] => Task 0, Epoch 147/150 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.86
2024-08-29 20:06:24,922 [podnet.py] => Task 0, Epoch 148/150 (LR 0.00004) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.86
2024-08-29 20:06:27,347 [podnet.py] => Task 0, Epoch 149/150 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 91.97
2024-08-29 20:06:29,909 [podnet.py] => Task 0, Epoch 150/150 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 92.02
2024-08-29 20:06:30,575 [base.py] => Reducing exemplars...(100 per classes)
2024-08-29 20:06:30,575 [base.py] => Constructing exemplars...(100 per classes)
2024-08-29 20:06:36,774 [podnet.py] => Exemplar size: 500
2024-08-29 20:06:36,774 [trainer.py] => CNN: {'total': 92.02, '00-04': 92.02, 'old': 0, 'new': 92.02}
2024-08-29 20:06:36,774 [trainer.py] => NME: {'total': 92.04, '00-04': 92.04, 'old': 0, 'new': 92.04}
2024-08-29 20:06:36,774 [trainer.py] => CNN top1 curve: [92.02]
2024-08-29 20:06:36,774 [trainer.py] => CNN top5 curve: [100.0]
2024-08-29 20:06:36,774 [trainer.py] => NME top1 curve: [92.04]
2024-08-29 20:06:36,774 [trainer.py] => NME top5 curve: [100.0]

2024-08-29 20:06:36,774 [trainer.py] => Average Accuracy (CNN): 92.02
2024-08-29 20:06:36,774 [trainer.py] => Average Accuracy (NME): 92.04
2024-08-29 20:06:36,774 [trainer.py] => All params: 3869505
2024-08-29 20:06:36,775 [trainer.py] => Trainable params: 3869505
2024-08-29 20:06:36,775 [podnet.py] => Learning on 5-7
2024-08-29 20:06:36,840 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-29 20:06:39,081 [podnet.py] => Task 1, Epoch 1/150 (LR 0.09999) => LSC_loss 1.02, Spatial_loss 2.42, Flat_loss 0.50, Train_acc 73.07, Test_acc 34.11
2024-08-29 20:06:41,428 [podnet.py] => Task 1, Epoch 2/150 (LR 0.09996) => LSC_loss 0.41, Spatial_loss 1.85, Flat_loss 0.32, Train_acc 89.62, Test_acc 58.19
2024-08-29 20:06:43,836 [podnet.py] => Task 1, Epoch 3/150 (LR 0.09990) => LSC_loss 0.27, Spatial_loss 1.61, Flat_loss 0.26, Train_acc 94.18, Test_acc 68.89
2024-08-29 20:06:46,017 [podnet.py] => Task 1, Epoch 4/150 (LR 0.09982) => LSC_loss 0.22, Spatial_loss 1.51, Flat_loss 0.24, Train_acc 95.07, Test_acc 69.24
2024-08-29 20:06:48,268 [podnet.py] => Task 1, Epoch 5/150 (LR 0.09973) => LSC_loss 0.19, Spatial_loss 1.37, Flat_loss 0.21, Train_acc 95.98, Test_acc 65.73
2024-08-29 20:06:50,456 [podnet.py] => Task 1, Epoch 6/150 (LR 0.09961) => LSC_loss 0.18, Spatial_loss 1.41, Flat_loss 0.21, Train_acc 95.93, Test_acc 57.59
2024-08-29 20:06:53,137 [podnet.py] => Task 1, Epoch 7/150 (LR 0.09946) => LSC_loss 0.15, Spatial_loss 1.33, Flat_loss 0.20, Train_acc 96.78, Test_acc 67.28
2024-08-29 20:06:55,417 [podnet.py] => Task 1, Epoch 8/150 (LR 0.09930) => LSC_loss 0.13, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 97.53, Test_acc 71.20
2024-08-29 20:06:57,623 [podnet.py] => Task 1, Epoch 9/150 (LR 0.09911) => LSC_loss 0.12, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 98.22, Test_acc 73.88
2024-08-29 20:06:59,973 [podnet.py] => Task 1, Epoch 10/150 (LR 0.09891) => LSC_loss 0.11, Spatial_loss 1.16, Flat_loss 0.17, Train_acc 98.49, Test_acc 71.98
2024-08-29 20:07:02,202 [podnet.py] => Task 1, Epoch 11/150 (LR 0.09868) => LSC_loss 0.11, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 98.56, Test_acc 74.29
2024-08-29 20:07:04,601 [podnet.py] => Task 1, Epoch 12/150 (LR 0.09843) => LSC_loss 0.11, Spatial_loss 1.20, Flat_loss 0.17, Train_acc 98.67, Test_acc 76.79
2024-08-29 20:07:06,699 [podnet.py] => Task 1, Epoch 13/150 (LR 0.09816) => LSC_loss 0.09, Spatial_loss 1.10, Flat_loss 0.16, Train_acc 99.16, Test_acc 66.13
2024-08-29 20:07:09,254 [podnet.py] => Task 1, Epoch 14/150 (LR 0.09787) => LSC_loss 0.09, Spatial_loss 1.10, Flat_loss 0.16, Train_acc 99.29, Test_acc 63.46
2024-08-29 20:07:11,634 [podnet.py] => Task 1, Epoch 15/150 (LR 0.09755) => LSC_loss 0.10, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 98.73, Test_acc 65.47
2024-08-29 20:07:14,219 [podnet.py] => Task 1, Epoch 16/150 (LR 0.09722) => LSC_loss 0.08, Spatial_loss 1.10, Flat_loss 0.16, Train_acc 99.22, Test_acc 73.65
2024-08-29 20:07:16,775 [podnet.py] => Task 1, Epoch 17/150 (LR 0.09686) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.15, Train_acc 99.58, Test_acc 68.17
2024-08-29 20:07:19,418 [podnet.py] => Task 1, Epoch 18/150 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 1.04, Flat_loss 0.15, Train_acc 99.47, Test_acc 71.06
2024-08-29 20:07:21,943 [podnet.py] => Task 1, Epoch 19/150 (LR 0.09609) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.15, Train_acc 99.67, Test_acc 69.91
2024-08-29 20:07:24,432 [podnet.py] => Task 1, Epoch 20/150 (LR 0.09568) => LSC_loss 0.06, Spatial_loss 1.03, Flat_loss 0.15, Train_acc 99.76, Test_acc 64.99
2024-08-29 20:07:27,096 [podnet.py] => Task 1, Epoch 21/150 (LR 0.09524) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 99.69, Test_acc 63.44
2024-08-29 20:07:29,657 [podnet.py] => Task 1, Epoch 22/150 (LR 0.09479) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 99.76, Test_acc 69.66
2024-08-29 20:07:32,156 [podnet.py] => Task 1, Epoch 23/150 (LR 0.09431) => LSC_loss 0.07, Spatial_loss 1.04, Flat_loss 0.14, Train_acc 99.29, Test_acc 69.40
2024-08-29 20:07:34,580 [podnet.py] => Task 1, Epoch 24/150 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.14, Train_acc 99.67, Test_acc 78.39
2024-08-29 20:07:37,109 [podnet.py] => Task 1, Epoch 25/150 (LR 0.09330) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.14, Train_acc 99.76, Test_acc 68.60
2024-08-29 20:07:39,966 [podnet.py] => Task 1, Epoch 26/150 (LR 0.09277) => LSC_loss 0.06, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 99.76, Test_acc 69.36
2024-08-29 20:07:42,601 [podnet.py] => Task 1, Epoch 27/150 (LR 0.09222) => LSC_loss 0.08, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 99.51, Test_acc 56.01
2024-08-29 20:07:45,149 [podnet.py] => Task 1, Epoch 28/150 (LR 0.09165) => LSC_loss 0.09, Spatial_loss 0.92, Flat_loss 0.15, Train_acc 98.82, Test_acc 73.92
2024-08-29 20:07:47,673 [podnet.py] => Task 1, Epoch 29/150 (LR 0.09106) => LSC_loss 0.09, Spatial_loss 0.92, Flat_loss 0.16, Train_acc 98.53, Test_acc 67.83
2024-08-29 20:07:50,012 [podnet.py] => Task 1, Epoch 30/150 (LR 0.09045) => LSC_loss 0.06, Spatial_loss 0.79, Flat_loss 0.14, Train_acc 99.71, Test_acc 73.29
2024-08-29 20:07:52,460 [podnet.py] => Task 1, Epoch 31/150 (LR 0.08983) => LSC_loss 0.06, Spatial_loss 0.79, Flat_loss 0.14, Train_acc 99.73, Test_acc 68.04
2024-08-29 20:07:54,925 [podnet.py] => Task 1, Epoch 32/150 (LR 0.08918) => LSC_loss 0.06, Spatial_loss 0.78, Flat_loss 0.14, Train_acc 99.64, Test_acc 69.75
2024-08-29 20:07:57,386 [podnet.py] => Task 1, Epoch 33/150 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.80, Flat_loss 0.14, Train_acc 99.80, Test_acc 75.62
2024-08-29 20:07:59,966 [podnet.py] => Task 1, Epoch 34/150 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.73, Flat_loss 0.13, Train_acc 99.82, Test_acc 71.93
2024-08-29 20:08:01,940 [podnet.py] => Task 1, Epoch 35/150 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.13, Train_acc 99.91, Test_acc 68.98
2024-08-29 20:08:04,232 [podnet.py] => Task 1, Epoch 36/150 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.72, Flat_loss 0.13, Train_acc 99.78, Test_acc 70.01
2024-08-29 20:08:06,650 [podnet.py] => Task 1, Epoch 37/150 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.12, Train_acc 99.89, Test_acc 70.53
2024-08-29 20:08:08,993 [podnet.py] => Task 1, Epoch 38/150 (LR 0.08498) => LSC_loss 0.05, Spatial_loss 0.70, Flat_loss 0.12, Train_acc 99.91, Test_acc 77.07
2024-08-29 20:08:11,312 [podnet.py] => Task 1, Epoch 39/150 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.12, Train_acc 99.91, Test_acc 69.97
2024-08-29 20:08:13,697 [podnet.py] => Task 1, Epoch 40/150 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.96, Test_acc 72.19
2024-08-29 20:08:15,812 [podnet.py] => Task 1, Epoch 41/150 (LR 0.08267) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.12, Train_acc 99.87, Test_acc 71.86
2024-08-29 20:08:18,177 [podnet.py] => Task 1, Epoch 42/150 (LR 0.08187) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.91, Test_acc 73.00
2024-08-29 20:08:20,453 [podnet.py] => Task 1, Epoch 43/150 (LR 0.08106) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.84, Test_acc 77.05
2024-08-29 20:08:22,370 [podnet.py] => Task 1, Epoch 44/150 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 99.87, Test_acc 68.29
2024-08-29 20:08:25,116 [podnet.py] => Task 1, Epoch 45/150 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 99.56, Test_acc 71.51
2024-08-29 20:08:27,763 [podnet.py] => Task 1, Epoch 46/150 (LR 0.07854) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.67, Test_acc 72.73
2024-08-29 20:08:30,089 [podnet.py] => Task 1, Epoch 47/150 (LR 0.07767) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 99.89, Test_acc 74.99
2024-08-29 20:08:32,389 [podnet.py] => Task 1, Epoch 48/150 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.12, Train_acc 99.96, Test_acc 75.89
2024-08-29 20:08:34,965 [podnet.py] => Task 1, Epoch 49/150 (LR 0.07590) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 99.89, Test_acc 70.61
2024-08-29 20:08:37,359 [podnet.py] => Task 1, Epoch 50/150 (LR 0.07500) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.08
2024-08-29 20:08:39,971 [podnet.py] => Task 1, Epoch 51/150 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 99.84, Test_acc 77.45
2024-08-29 20:08:42,225 [podnet.py] => Task 1, Epoch 52/150 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.27
2024-08-29 20:08:44,547 [podnet.py] => Task 1, Epoch 53/150 (LR 0.07223) => LSC_loss 0.04, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.87, Test_acc 78.40
2024-08-29 20:08:46,877 [podnet.py] => Task 1, Epoch 54/150 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.96, Test_acc 77.06
2024-08-29 20:08:49,319 [podnet.py] => Task 1, Epoch 55/150 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.80, Test_acc 71.56
2024-08-29 20:08:52,019 [podnet.py] => Task 1, Epoch 56/150 (LR 0.06938) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.12, Train_acc 99.93, Test_acc 72.76
2024-08-29 20:08:53,890 [podnet.py] => Task 1, Epoch 57/150 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.11, Train_acc 99.98, Test_acc 73.47
2024-08-29 20:08:56,503 [podnet.py] => Task 1, Epoch 58/150 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.82, Test_acc 75.38
2024-08-29 20:08:58,722 [podnet.py] => Task 1, Epoch 59/150 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.91, Test_acc 73.70
2024-08-29 20:09:01,095 [podnet.py] => Task 1, Epoch 60/150 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.61, Flat_loss 0.11, Train_acc 99.98, Test_acc 75.64
2024-08-29 20:09:03,374 [podnet.py] => Task 1, Epoch 61/150 (LR 0.06445) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.81
2024-08-29 20:09:05,776 [podnet.py] => Task 1, Epoch 62/150 (LR 0.06345) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.11, Train_acc 99.98, Test_acc 73.94
2024-08-29 20:09:08,011 [podnet.py] => Task 1, Epoch 63/150 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.11, Train_acc 99.91, Test_acc 75.88
2024-08-29 20:09:10,255 [podnet.py] => Task 1, Epoch 64/150 (LR 0.06142) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.11, Train_acc 99.93, Test_acc 77.34
2024-08-29 20:09:12,550 [podnet.py] => Task 1, Epoch 65/150 (LR 0.06040) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 99.93, Test_acc 74.47
2024-08-29 20:09:14,978 [podnet.py] => Task 1, Epoch 66/150 (LR 0.05937) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.11, Train_acc 99.93, Test_acc 74.62
2024-08-29 20:09:17,587 [podnet.py] => Task 1, Epoch 67/150 (LR 0.05834) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.36
2024-08-29 20:09:19,982 [podnet.py] => Task 1, Epoch 68/150 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.11, Train_acc 99.96, Test_acc 75.53
2024-08-29 20:09:22,623 [podnet.py] => Task 1, Epoch 69/150 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.96, Test_acc 73.20
2024-08-29 20:09:25,407 [podnet.py] => Task 1, Epoch 70/150 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 100.00, Test_acc 76.39
2024-08-29 20:09:27,828 [podnet.py] => Task 1, Epoch 71/150 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 99.96, Test_acc 75.63
2024-08-29 20:09:30,353 [podnet.py] => Task 1, Epoch 72/150 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.98, Test_acc 78.43
2024-08-29 20:09:33,052 [podnet.py] => Task 1, Epoch 73/150 (LR 0.05209) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.89, Test_acc 67.97
2024-08-29 20:09:35,817 [podnet.py] => Task 1, Epoch 74/150 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.80, Test_acc 72.67
2024-08-29 20:09:38,569 [podnet.py] => Task 1, Epoch 75/150 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 99.98, Test_acc 75.26
2024-08-29 20:09:41,235 [podnet.py] => Task 1, Epoch 76/150 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 99.96, Test_acc 75.00
2024-08-29 20:09:43,568 [podnet.py] => Task 1, Epoch 77/150 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 99.93, Test_acc 74.16
2024-08-29 20:09:46,159 [podnet.py] => Task 1, Epoch 78/150 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.62, Test_acc 77.80
2024-08-29 20:09:48,788 [podnet.py] => Task 1, Epoch 79/150 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.27
2024-08-29 20:09:51,098 [podnet.py] => Task 1, Epoch 80/150 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.10, Train_acc 99.98, Test_acc 75.88
2024-08-29 20:09:53,851 [podnet.py] => Task 1, Epoch 81/150 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.10, Train_acc 99.98, Test_acc 76.09
2024-08-29 20:09:56,551 [podnet.py] => Task 1, Epoch 82/150 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.10, Train_acc 99.96, Test_acc 72.72
2024-08-29 20:09:58,913 [podnet.py] => Task 1, Epoch 83/150 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.10, Train_acc 99.98, Test_acc 74.26
2024-08-29 20:10:01,287 [podnet.py] => Task 1, Epoch 84/150 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 99.96, Test_acc 73.84
2024-08-29 20:10:03,193 [podnet.py] => Task 1, Epoch 85/150 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.10, Train_acc 99.98, Test_acc 75.20
2024-08-29 20:10:05,192 [podnet.py] => Task 1, Epoch 86/150 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.10, Train_acc 100.00, Test_acc 76.93
2024-08-29 20:10:07,177 [podnet.py] => Task 1, Epoch 87/150 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.85
2024-08-29 20:10:09,431 [podnet.py] => Task 1, Epoch 88/150 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.47
2024-08-29 20:10:11,959 [podnet.py] => Task 1, Epoch 89/150 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 99.98, Test_acc 75.67
2024-08-29 20:10:13,921 [podnet.py] => Task 1, Epoch 90/150 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 99.98, Test_acc 76.42
2024-08-29 20:10:16,063 [podnet.py] => Task 1, Epoch 91/150 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 99.96, Test_acc 77.13
2024-08-29 20:10:18,380 [podnet.py] => Task 1, Epoch 92/150 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 76.06
2024-08-29 20:10:20,613 [podnet.py] => Task 1, Epoch 93/150 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.32
2024-08-29 20:10:22,562 [podnet.py] => Task 1, Epoch 94/150 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 77.12
2024-08-29 20:10:24,964 [podnet.py] => Task 1, Epoch 95/150 (LR 0.02966) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 78.38
2024-08-29 20:10:27,370 [podnet.py] => Task 1, Epoch 96/150 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 77.33
2024-08-29 20:10:29,600 [podnet.py] => Task 1, Epoch 97/150 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-08-29 20:10:31,672 [podnet.py] => Task 1, Epoch 98/150 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 76.52
2024-08-29 20:10:33,508 [podnet.py] => Task 1, Epoch 99/150 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 99.96, Test_acc 73.06
2024-08-29 20:10:35,891 [podnet.py] => Task 1, Epoch 100/150 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.77
2024-08-29 20:10:38,425 [podnet.py] => Task 1, Epoch 101/150 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.09, Train_acc 99.98, Test_acc 77.87
2024-08-29 20:10:41,041 [podnet.py] => Task 1, Epoch 102/150 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.09
2024-08-29 20:10:43,237 [podnet.py] => Task 1, Epoch 103/150 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.93
2024-08-29 20:10:45,890 [podnet.py] => Task 1, Epoch 104/150 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 99.98, Test_acc 77.90
2024-08-29 20:10:48,561 [podnet.py] => Task 1, Epoch 105/150 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.18
2024-08-29 20:10:51,445 [podnet.py] => Task 1, Epoch 106/150 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 77.90
2024-08-29 20:10:53,915 [podnet.py] => Task 1, Epoch 107/150 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.19
2024-08-29 20:10:56,618 [podnet.py] => Task 1, Epoch 108/150 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.21
2024-08-29 20:10:59,510 [podnet.py] => Task 1, Epoch 109/150 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.09, Train_acc 100.00, Test_acc 76.57
2024-08-29 20:11:02,332 [podnet.py] => Task 1, Epoch 110/150 (LR 0.01654) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.07
2024-08-29 20:11:04,866 [podnet.py] => Task 1, Epoch 111/150 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.09, Train_acc 99.98, Test_acc 77.97
2024-08-29 20:11:07,484 [podnet.py] => Task 1, Epoch 112/150 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.09, Train_acc 100.00, Test_acc 75.24
2024-08-29 20:11:10,044 [podnet.py] => Task 1, Epoch 113/150 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.55
2024-08-29 20:11:12,675 [podnet.py] => Task 1, Epoch 114/150 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.55
2024-08-29 20:11:15,177 [podnet.py] => Task 1, Epoch 115/150 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.33
2024-08-29 20:11:17,892 [podnet.py] => Task 1, Epoch 116/150 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.09, Train_acc 100.00, Test_acc 79.01
2024-08-29 20:11:20,519 [podnet.py] => Task 1, Epoch 117/150 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.78
2024-08-29 20:11:22,956 [podnet.py] => Task 1, Epoch 118/150 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.09, Train_acc 100.00, Test_acc 74.32
2024-08-29 20:11:25,364 [podnet.py] => Task 1, Epoch 119/150 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.73
2024-08-29 20:11:27,821 [podnet.py] => Task 1, Epoch 120/150 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.37
2024-08-29 20:11:29,442 [podnet.py] => Task 1, Epoch 121/150 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 99.98, Test_acc 79.20
2024-08-29 20:11:32,239 [podnet.py] => Task 1, Epoch 122/150 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.09, Train_acc 100.00, Test_acc 79.67
2024-08-29 20:11:34,930 [podnet.py] => Task 1, Epoch 123/150 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.45
2024-08-29 20:11:37,674 [podnet.py] => Task 1, Epoch 124/150 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.97
2024-08-29 20:11:40,262 [podnet.py] => Task 1, Epoch 125/150 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.09, Train_acc 100.00, Test_acc 79.57
2024-08-29 20:11:43,188 [podnet.py] => Task 1, Epoch 126/150 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.10
2024-08-29 20:11:44,939 [podnet.py] => Task 1, Epoch 127/150 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.57
2024-08-29 20:11:47,531 [podnet.py] => Task 1, Epoch 128/150 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 76.92
2024-08-29 20:11:50,212 [podnet.py] => Task 1, Epoch 129/150 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.36
2024-08-29 20:11:52,647 [podnet.py] => Task 1, Epoch 130/150 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 77.05
2024-08-29 20:11:55,292 [podnet.py] => Task 1, Epoch 131/150 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.63
2024-08-29 20:11:57,898 [podnet.py] => Task 1, Epoch 132/150 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.39
2024-08-29 20:12:00,556 [podnet.py] => Task 1, Epoch 133/150 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.38
2024-08-29 20:12:03,149 [podnet.py] => Task 1, Epoch 134/150 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.72
2024-08-29 20:12:05,813 [podnet.py] => Task 1, Epoch 135/150 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.73
2024-08-29 20:12:08,411 [podnet.py] => Task 1, Epoch 136/150 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.71
2024-08-29 20:12:11,020 [podnet.py] => Task 1, Epoch 137/150 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.20
2024-08-29 20:12:13,485 [podnet.py] => Task 1, Epoch 138/150 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.52
2024-08-29 20:12:16,145 [podnet.py] => Task 1, Epoch 139/150 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.49
2024-08-29 20:12:18,782 [podnet.py] => Task 1, Epoch 140/150 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.97
2024-08-29 20:12:21,355 [podnet.py] => Task 1, Epoch 141/150 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.86
2024-08-29 20:12:23,992 [podnet.py] => Task 1, Epoch 142/150 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 79.23
2024-08-29 20:12:26,693 [podnet.py] => Task 1, Epoch 143/150 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.43
2024-08-29 20:12:29,400 [podnet.py] => Task 1, Epoch 144/150 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.63
2024-08-29 20:12:32,125 [podnet.py] => Task 1, Epoch 145/150 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.87
2024-08-29 20:12:34,799 [podnet.py] => Task 1, Epoch 146/150 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.22
2024-08-29 20:12:37,397 [podnet.py] => Task 1, Epoch 147/150 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.34
2024-08-29 20:12:39,841 [podnet.py] => Task 1, Epoch 148/150 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.65
2024-08-29 20:12:42,460 [podnet.py] => Task 1, Epoch 149/150 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 78.90
2024-08-29 20:12:44,707 [podnet.py] => Task 1, Epoch 150/150 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 79.02
2024-08-29 20:12:45,455 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-29 20:12:45,456 [base.py] => Reducing exemplars...(100 per classes)
2024-08-29 20:12:46,446 [base.py] => Constructing exemplars...(100 per classes)
2024-08-29 20:12:48,177 [podnet.py] => The size of finetune dataset: 700
2024-08-29 20:12:49,359 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.05, Train_acc 100.00, Test_acc 77.60
2024-08-29 20:12:50,477 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.04, Train_acc 100.00, Test_acc 77.18
2024-08-29 20:12:51,545 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.03, Spatial_loss 0.34, Flat_loss 0.04, Train_acc 100.00, Test_acc 76.57
2024-08-29 20:12:52,670 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.03, Spatial_loss 0.31, Flat_loss 0.03, Train_acc 100.00, Test_acc 76.72
2024-08-29 20:12:53,775 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.03, Spatial_loss 0.32, Flat_loss 0.03, Train_acc 100.00, Test_acc 76.50
2024-08-29 20:12:54,976 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.03, Spatial_loss 0.28, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.75
2024-08-29 20:12:56,061 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.02, Spatial_loss 0.30, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.73
2024-08-29 20:12:57,158 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.02, Spatial_loss 0.28, Flat_loss 0.03, Train_acc 100.00, Test_acc 78.17
2024-08-29 20:12:58,269 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.02, Spatial_loss 0.30, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.19
2024-08-29 20:12:59,500 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.02, Spatial_loss 0.29, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.61
2024-08-29 20:13:00,558 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.02, Spatial_loss 0.26, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.64
2024-08-29 20:13:01,651 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.02, Spatial_loss 0.29, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.78
2024-08-29 20:13:02,679 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.02, Spatial_loss 0.27, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.72
2024-08-29 20:13:03,742 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.02, Spatial_loss 0.28, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.69
2024-08-29 20:13:04,852 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.02, Spatial_loss 0.25, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.53
2024-08-29 20:13:05,961 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.02, Spatial_loss 0.30, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.73
2024-08-29 20:13:06,964 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.02, Spatial_loss 0.32, Flat_loss 0.04, Train_acc 100.00, Test_acc 77.69
2024-08-29 20:13:08,085 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.02, Spatial_loss 0.27, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.44
2024-08-29 20:13:09,141 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.02, Spatial_loss 0.28, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.60
2024-08-29 20:13:10,221 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.02, Spatial_loss 0.28, Flat_loss 0.03, Train_acc 100.00, Test_acc 77.51
2024-08-29 20:13:11,076 [base.py] => Reducing exemplars...(71 per classes)
2024-08-29 20:13:12,232 [base.py] => Constructing exemplars...(71 per classes)
2024-08-29 20:13:15,566 [podnet.py] => Exemplar size: 497
2024-08-29 20:13:15,567 [trainer.py] => CNN: {'total': 77.51, '00-04': 72.5, '05-06': 97.19, 'old': 72.5, 'new': 97.19}
2024-08-29 20:13:15,567 [trainer.py] => NME: {'total': 82.92, '00-04': 85.73, '05-06': 71.88, 'old': 85.73, 'new': 71.88}
2024-08-29 20:13:15,567 [trainer.py] => CNN top1 curve: [92.02, 77.51]
2024-08-29 20:13:15,567 [trainer.py] => CNN top5 curve: [100.0, 99.02]
2024-08-29 20:13:15,567 [trainer.py] => NME top1 curve: [92.04, 82.92]
2024-08-29 20:13:15,567 [trainer.py] => NME top5 curve: [100.0, 99.07]

2024-08-29 20:13:15,567 [trainer.py] => Average Accuracy (CNN): 84.765
2024-08-29 20:13:15,567 [trainer.py] => Average Accuracy (NME): 87.48
2024-08-29 20:13:15,567 [trainer.py] => All params: 3879745
2024-08-29 20:13:15,567 [trainer.py] => Trainable params: 3879745
2024-08-29 20:13:15,568 [podnet.py] => Learning on 7-9
2024-08-29 20:13:15,635 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-29 20:13:18,666 [podnet.py] => Task 2, Epoch 1/150 (LR 0.09999) => LSC_loss 1.15, Spatial_loss 1.43, Flat_loss 0.38, Train_acc 80.21, Test_acc 25.97
2024-08-29 20:13:21,521 [podnet.py] => Task 2, Epoch 2/150 (LR 0.09996) => LSC_loss 0.39, Spatial_loss 1.39, Flat_loss 0.25, Train_acc 91.46, Test_acc 26.35
2024-08-29 20:13:24,131 [podnet.py] => Task 2, Epoch 3/150 (LR 0.09990) => LSC_loss 0.42, Spatial_loss 1.57, Flat_loss 0.26, Train_acc 90.88, Test_acc 30.41
2024-08-29 20:13:27,017 [podnet.py] => Task 2, Epoch 4/150 (LR 0.09982) => LSC_loss 0.22, Spatial_loss 1.14, Flat_loss 0.19, Train_acc 95.82, Test_acc 40.47
2024-08-29 20:13:29,909 [podnet.py] => Task 2, Epoch 5/150 (LR 0.09973) => LSC_loss 0.17, Spatial_loss 0.98, Flat_loss 0.16, Train_acc 97.64, Test_acc 36.52
2024-08-29 20:13:32,843 [podnet.py] => Task 2, Epoch 6/150 (LR 0.09961) => LSC_loss 0.14, Spatial_loss 0.91, Flat_loss 0.14, Train_acc 98.55, Test_acc 46.11
2024-08-29 20:13:35,625 [podnet.py] => Task 2, Epoch 7/150 (LR 0.09946) => LSC_loss 0.13, Spatial_loss 0.87, Flat_loss 0.14, Train_acc 98.78, Test_acc 37.85
2024-08-29 20:13:38,656 [podnet.py] => Task 2, Epoch 8/150 (LR 0.09930) => LSC_loss 0.12, Spatial_loss 0.83, Flat_loss 0.14, Train_acc 98.80, Test_acc 46.55
2024-08-29 20:13:41,225 [podnet.py] => Task 2, Epoch 9/150 (LR 0.09911) => LSC_loss 0.11, Spatial_loss 0.84, Flat_loss 0.13, Train_acc 99.47, Test_acc 46.99
2024-08-29 20:13:43,971 [podnet.py] => Task 2, Epoch 10/150 (LR 0.09891) => LSC_loss 0.10, Spatial_loss 0.82, Flat_loss 0.13, Train_acc 99.24, Test_acc 53.22
2024-08-29 20:13:46,895 [podnet.py] => Task 2, Epoch 11/150 (LR 0.09868) => LSC_loss 0.09, Spatial_loss 0.77, Flat_loss 0.12, Train_acc 99.73, Test_acc 55.11
2024-08-29 20:13:49,435 [podnet.py] => Task 2, Epoch 12/150 (LR 0.09843) => LSC_loss 0.08, Spatial_loss 0.73, Flat_loss 0.12, Train_acc 99.73, Test_acc 47.61
2024-08-29 20:13:52,334 [podnet.py] => Task 2, Epoch 13/150 (LR 0.09816) => LSC_loss 0.08, Spatial_loss 0.73, Flat_loss 0.12, Train_acc 99.87, Test_acc 45.07
2024-08-29 20:13:55,310 [podnet.py] => Task 2, Epoch 14/150 (LR 0.09787) => LSC_loss 0.08, Spatial_loss 0.78, Flat_loss 0.11, Train_acc 99.84, Test_acc 50.95
2024-08-29 20:13:58,239 [podnet.py] => Task 2, Epoch 15/150 (LR 0.09755) => LSC_loss 0.08, Spatial_loss 0.73, Flat_loss 0.11, Train_acc 99.89, Test_acc 47.53
2024-08-29 20:14:01,091 [podnet.py] => Task 2, Epoch 16/150 (LR 0.09722) => LSC_loss 0.07, Spatial_loss 0.77, Flat_loss 0.11, Train_acc 99.84, Test_acc 50.91
2024-08-29 20:14:04,160 [podnet.py] => Task 2, Epoch 17/150 (LR 0.09686) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.11, Train_acc 99.89, Test_acc 49.11
2024-08-29 20:14:07,018 [podnet.py] => Task 2, Epoch 18/150 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 0.68, Flat_loss 0.11, Train_acc 99.93, Test_acc 58.52
2024-08-29 20:14:09,787 [podnet.py] => Task 2, Epoch 19/150 (LR 0.09609) => LSC_loss 0.07, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 99.91, Test_acc 47.74
2024-08-29 20:14:12,758 [podnet.py] => Task 2, Epoch 20/150 (LR 0.09568) => LSC_loss 0.07, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 99.87, Test_acc 51.29
2024-08-29 20:14:15,694 [podnet.py] => Task 2, Epoch 21/150 (LR 0.09524) => LSC_loss 0.07, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 99.93, Test_acc 49.44
2024-08-29 20:14:18,657 [podnet.py] => Task 2, Epoch 22/150 (LR 0.09479) => LSC_loss 0.07, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 99.93, Test_acc 46.78
2024-08-29 20:14:21,445 [podnet.py] => Task 2, Epoch 23/150 (LR 0.09431) => LSC_loss 0.07, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 99.80, Test_acc 51.02
2024-08-29 20:14:24,187 [podnet.py] => Task 2, Epoch 24/150 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 99.91, Test_acc 52.01
2024-08-29 20:14:27,106 [podnet.py] => Task 2, Epoch 25/150 (LR 0.09330) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 99.91, Test_acc 51.78
2024-08-29 20:14:28,957 [podnet.py] => Task 2, Epoch 26/150 (LR 0.09277) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 99.96, Test_acc 53.80
2024-08-29 20:14:31,892 [podnet.py] => Task 2, Epoch 27/150 (LR 0.09222) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 99.96, Test_acc 51.09
2024-08-29 20:14:34,655 [podnet.py] => Task 2, Epoch 28/150 (LR 0.09165) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.09, Train_acc 99.93, Test_acc 51.69
2024-08-29 20:14:37,539 [podnet.py] => Task 2, Epoch 29/150 (LR 0.09106) => LSC_loss 0.08, Spatial_loss 0.75, Flat_loss 0.11, Train_acc 99.44, Test_acc 46.03
2024-08-29 20:14:40,432 [podnet.py] => Task 2, Epoch 30/150 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.11, Train_acc 99.53, Test_acc 53.51
2024-08-29 20:14:43,228 [podnet.py] => Task 2, Epoch 31/150 (LR 0.08983) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 99.93, Test_acc 56.46
2024-08-29 20:14:45,984 [podnet.py] => Task 2, Epoch 32/150 (LR 0.08918) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 99.93, Test_acc 50.42
2024-08-29 20:14:48,625 [podnet.py] => Task 2, Epoch 33/150 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 0.70, Flat_loss 0.11, Train_acc 99.67, Test_acc 46.71
2024-08-29 20:14:51,304 [podnet.py] => Task 2, Epoch 34/150 (LR 0.08785) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 99.91, Test_acc 52.38
2024-08-29 20:14:54,458 [podnet.py] => Task 2, Epoch 35/150 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.09, Train_acc 99.98, Test_acc 49.79
2024-08-29 20:14:57,373 [podnet.py] => Task 2, Epoch 36/150 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.09, Train_acc 99.98, Test_acc 61.64
2024-08-29 20:15:00,265 [podnet.py] => Task 2, Epoch 37/150 (LR 0.08572) => LSC_loss 0.07, Spatial_loss 0.65, Flat_loss 0.10, Train_acc 99.91, Test_acc 50.16
2024-08-29 20:15:03,067 [podnet.py] => Task 2, Epoch 38/150 (LR 0.08498) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.12, Train_acc 99.38, Test_acc 53.90
2024-08-29 20:15:05,936 [podnet.py] => Task 2, Epoch 39/150 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 100.00, Test_acc 51.01
2024-08-29 20:15:08,661 [podnet.py] => Task 2, Epoch 40/150 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.09, Train_acc 99.98, Test_acc 50.58
2024-08-29 20:15:11,621 [podnet.py] => Task 2, Epoch 41/150 (LR 0.08267) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.09, Train_acc 99.98, Test_acc 47.08
2024-08-29 20:15:14,511 [podnet.py] => Task 2, Epoch 42/150 (LR 0.08187) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.09, Train_acc 100.00, Test_acc 51.60
2024-08-29 20:15:17,352 [podnet.py] => Task 2, Epoch 43/150 (LR 0.08106) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.09, Train_acc 100.00, Test_acc 46.32
2024-08-29 20:15:20,249 [podnet.py] => Task 2, Epoch 44/150 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.09, Train_acc 99.98, Test_acc 53.76
2024-08-29 20:15:23,249 [podnet.py] => Task 2, Epoch 45/150 (LR 0.07939) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.09, Train_acc 99.93, Test_acc 50.30
2024-08-29 20:15:26,068 [podnet.py] => Task 2, Epoch 46/150 (LR 0.07854) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.09, Train_acc 100.00, Test_acc 53.29
2024-08-29 20:15:28,842 [podnet.py] => Task 2, Epoch 47/150 (LR 0.07767) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.09, Train_acc 99.96, Test_acc 57.81
2024-08-29 20:15:31,641 [podnet.py] => Task 2, Epoch 48/150 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.09, Train_acc 99.98, Test_acc 52.12
2024-08-29 20:15:34,397 [podnet.py] => Task 2, Epoch 49/150 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.09, Train_acc 99.98, Test_acc 48.21
2024-08-29 20:15:37,308 [podnet.py] => Task 2, Epoch 50/150 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.08, Train_acc 100.00, Test_acc 52.95
2024-08-29 20:15:39,975 [podnet.py] => Task 2, Epoch 51/150 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.09, Train_acc 99.96, Test_acc 56.39
2024-08-29 20:15:42,799 [podnet.py] => Task 2, Epoch 52/150 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.09, Train_acc 99.98, Test_acc 50.88
2024-08-29 20:15:45,500 [podnet.py] => Task 2, Epoch 53/150 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.09, Train_acc 100.00, Test_acc 52.23
2024-08-29 20:15:48,305 [podnet.py] => Task 2, Epoch 54/150 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.09, Train_acc 100.00, Test_acc 55.73
2024-08-29 20:15:51,302 [podnet.py] => Task 2, Epoch 55/150 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.09, Train_acc 100.00, Test_acc 53.55
2024-08-29 20:15:54,240 [podnet.py] => Task 2, Epoch 56/150 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.09, Train_acc 99.98, Test_acc 51.08
2024-08-29 20:15:57,113 [podnet.py] => Task 2, Epoch 57/150 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 99.96, Test_acc 48.88
2024-08-29 20:15:59,892 [podnet.py] => Task 2, Epoch 58/150 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.08, Train_acc 99.98, Test_acc 55.76
2024-08-29 20:16:02,770 [podnet.py] => Task 2, Epoch 59/150 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 99.96, Test_acc 53.34
2024-08-29 20:16:05,897 [podnet.py] => Task 2, Epoch 60/150 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 100.00, Test_acc 51.92
2024-08-29 20:16:08,753 [podnet.py] => Task 2, Epoch 61/150 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 100.00, Test_acc 50.87
2024-08-29 20:16:11,454 [podnet.py] => Task 2, Epoch 62/150 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.11
2024-08-29 20:16:14,175 [podnet.py] => Task 2, Epoch 63/150 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 53.41
2024-08-29 20:16:16,581 [podnet.py] => Task 2, Epoch 64/150 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.08, Train_acc 100.00, Test_acc 56.67
2024-08-29 20:16:19,170 [podnet.py] => Task 2, Epoch 65/150 (LR 0.06040) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 54.71
2024-08-29 20:16:21,921 [podnet.py] => Task 2, Epoch 66/150 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.08, Train_acc 100.00, Test_acc 50.32
2024-08-29 20:16:24,691 [podnet.py] => Task 2, Epoch 67/150 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 99.98, Test_acc 52.43
2024-08-29 20:16:27,550 [podnet.py] => Task 2, Epoch 68/150 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 53.54
2024-08-29 20:16:30,245 [podnet.py] => Task 2, Epoch 69/150 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 50.66
2024-08-29 20:16:33,149 [podnet.py] => Task 2, Epoch 70/150 (LR 0.05523) => LSC_loss 0.08, Spatial_loss 0.57, Flat_loss 0.09, Train_acc 99.87, Test_acc 55.66
2024-08-29 20:16:35,931 [podnet.py] => Task 2, Epoch 71/150 (LR 0.05418) => LSC_loss 0.18, Spatial_loss 1.05, Flat_loss 0.17, Train_acc 95.98, Test_acc 50.39
2024-08-29 20:16:38,629 [podnet.py] => Task 2, Epoch 72/150 (LR 0.05314) => LSC_loss 0.07, Spatial_loss 0.74, Flat_loss 0.12, Train_acc 99.51, Test_acc 55.40
2024-08-29 20:16:41,696 [podnet.py] => Task 2, Epoch 73/150 (LR 0.05209) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.10, Train_acc 99.91, Test_acc 50.69
2024-08-29 20:16:44,444 [podnet.py] => Task 2, Epoch 74/150 (LR 0.05105) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.09, Train_acc 99.96, Test_acc 54.37
2024-08-29 20:16:47,171 [podnet.py] => Task 2, Epoch 75/150 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.09, Train_acc 100.00, Test_acc 54.76
2024-08-29 20:16:49,971 [podnet.py] => Task 2, Epoch 76/150 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.09, Train_acc 100.00, Test_acc 51.67
2024-08-29 20:16:52,784 [podnet.py] => Task 2, Epoch 77/150 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.08, Train_acc 99.96, Test_acc 54.04
2024-08-29 20:16:55,619 [podnet.py] => Task 2, Epoch 78/150 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.08, Train_acc 100.00, Test_acc 55.64
2024-08-29 20:16:58,729 [podnet.py] => Task 2, Epoch 79/150 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.43
2024-08-29 20:17:01,684 [podnet.py] => Task 2, Epoch 80/150 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.08, Train_acc 99.98, Test_acc 53.63
2024-08-29 20:17:04,313 [podnet.py] => Task 2, Epoch 81/150 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.09, Train_acc 100.00, Test_acc 51.28
2024-08-29 20:17:07,122 [podnet.py] => Task 2, Epoch 82/150 (LR 0.04270) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 51.33
2024-08-29 20:17:10,004 [podnet.py] => Task 2, Epoch 83/150 (LR 0.04166) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.08, Train_acc 99.98, Test_acc 51.98
2024-08-29 20:17:12,611 [podnet.py] => Task 2, Epoch 84/150 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.08, Train_acc 100.00, Test_acc 54.60
2024-08-29 20:17:15,396 [podnet.py] => Task 2, Epoch 85/150 (LR 0.03960) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.08, Train_acc 99.96, Test_acc 54.71
2024-08-29 20:17:18,322 [podnet.py] => Task 2, Epoch 86/150 (LR 0.03858) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.09, Train_acc 99.71, Test_acc 53.44
2024-08-29 20:17:21,062 [podnet.py] => Task 2, Epoch 87/150 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.08, Train_acc 100.00, Test_acc 55.73
2024-08-29 20:17:23,759 [podnet.py] => Task 2, Epoch 88/150 (LR 0.03655) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.08, Train_acc 99.98, Test_acc 53.63
2024-08-29 20:17:26,597 [podnet.py] => Task 2, Epoch 89/150 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.08, Train_acc 100.00, Test_acc 56.27
2024-08-29 20:17:29,483 [podnet.py] => Task 2, Epoch 90/150 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.08, Train_acc 100.00, Test_acc 55.50
2024-08-29 20:17:32,216 [podnet.py] => Task 2, Epoch 91/150 (LR 0.03356) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.08, Train_acc 100.00, Test_acc 56.40
2024-08-29 20:17:35,045 [podnet.py] => Task 2, Epoch 92/150 (LR 0.03257) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.08, Train_acc 99.98, Test_acc 53.75
2024-08-29 20:17:37,984 [podnet.py] => Task 2, Epoch 93/150 (LR 0.03159) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.08, Train_acc 100.00, Test_acc 55.23
2024-08-29 20:17:40,681 [podnet.py] => Task 2, Epoch 94/150 (LR 0.03062) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.08, Train_acc 100.00, Test_acc 55.07
2024-08-29 20:17:42,510 [podnet.py] => Task 2, Epoch 95/150 (LR 0.02966) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.89
2024-08-29 20:17:45,222 [podnet.py] => Task 2, Epoch 96/150 (LR 0.02871) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.77
2024-08-29 20:17:48,143 [podnet.py] => Task 2, Epoch 97/150 (LR 0.02777) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.08, Train_acc 100.00, Test_acc 54.32
2024-08-29 20:17:51,071 [podnet.py] => Task 2, Epoch 98/150 (LR 0.02684) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.08, Train_acc 100.00, Test_acc 53.18
2024-08-29 20:17:53,780 [podnet.py] => Task 2, Epoch 99/150 (LR 0.02591) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.08, Train_acc 100.00, Test_acc 53.63
2024-08-29 20:17:56,429 [podnet.py] => Task 2, Epoch 100/150 (LR 0.02500) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.08, Train_acc 100.00, Test_acc 56.64
2024-08-29 20:17:59,264 [podnet.py] => Task 2, Epoch 101/150 (LR 0.02410) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.08, Train_acc 100.00, Test_acc 54.89
2024-08-29 20:18:01,860 [podnet.py] => Task 2, Epoch 102/150 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.63
2024-08-29 20:18:04,735 [podnet.py] => Task 2, Epoch 103/150 (LR 0.02233) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.07, Train_acc 99.98, Test_acc 58.72
2024-08-29 20:18:07,286 [podnet.py] => Task 2, Epoch 104/150 (LR 0.02146) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.78
2024-08-29 20:18:10,262 [podnet.py] => Task 2, Epoch 105/150 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.00
2024-08-29 20:18:12,752 [podnet.py] => Task 2, Epoch 106/150 (LR 0.01977) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.08, Train_acc 100.00, Test_acc 52.55
2024-08-29 20:18:15,287 [podnet.py] => Task 2, Epoch 107/150 (LR 0.01894) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.07, Train_acc 100.00, Test_acc 52.21
2024-08-29 20:18:18,284 [podnet.py] => Task 2, Epoch 108/150 (LR 0.01813) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.31
2024-08-29 20:18:21,106 [podnet.py] => Task 2, Epoch 109/150 (LR 0.01733) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.81
2024-08-29 20:18:24,073 [podnet.py] => Task 2, Epoch 110/150 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.34
2024-08-29 20:18:26,899 [podnet.py] => Task 2, Epoch 111/150 (LR 0.01577) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.44
2024-08-29 20:18:29,633 [podnet.py] => Task 2, Epoch 112/150 (LR 0.01502) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.10
2024-08-29 20:18:32,427 [podnet.py] => Task 2, Epoch 113/150 (LR 0.01428) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.22
2024-08-29 20:18:35,304 [podnet.py] => Task 2, Epoch 114/150 (LR 0.01355) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.11
2024-08-29 20:18:38,172 [podnet.py] => Task 2, Epoch 115/150 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.97
2024-08-29 20:18:40,966 [podnet.py] => Task 2, Epoch 116/150 (LR 0.01215) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.67
2024-08-29 20:18:43,931 [podnet.py] => Task 2, Epoch 117/150 (LR 0.01147) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.96
2024-08-29 20:18:46,764 [podnet.py] => Task 2, Epoch 118/150 (LR 0.01082) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.32
2024-08-29 20:18:49,471 [podnet.py] => Task 2, Epoch 119/150 (LR 0.01017) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.50
2024-08-29 20:18:52,238 [podnet.py] => Task 2, Epoch 120/150 (LR 0.00955) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.13
2024-08-29 20:18:55,316 [podnet.py] => Task 2, Epoch 121/150 (LR 0.00894) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.10
2024-08-29 20:18:58,457 [podnet.py] => Task 2, Epoch 122/150 (LR 0.00835) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.20
2024-08-29 20:19:01,626 [podnet.py] => Task 2, Epoch 123/150 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.26
2024-08-29 20:19:04,441 [podnet.py] => Task 2, Epoch 124/150 (LR 0.00723) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.56
2024-08-29 20:19:07,034 [podnet.py] => Task 2, Epoch 125/150 (LR 0.00670) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.35
2024-08-29 20:19:10,136 [podnet.py] => Task 2, Epoch 126/150 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.10
2024-08-29 20:19:12,906 [podnet.py] => Task 2, Epoch 127/150 (LR 0.00569) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.62
2024-08-29 20:19:15,883 [podnet.py] => Task 2, Epoch 128/150 (LR 0.00521) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.60
2024-08-29 20:19:18,582 [podnet.py] => Task 2, Epoch 129/150 (LR 0.00476) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.39
2024-08-29 20:19:21,390 [podnet.py] => Task 2, Epoch 130/150 (LR 0.00432) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.26
2024-08-29 20:19:24,362 [podnet.py] => Task 2, Epoch 131/150 (LR 0.00391) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.45
2024-08-29 20:19:27,176 [podnet.py] => Task 2, Epoch 132/150 (LR 0.00351) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.64
2024-08-29 20:19:29,793 [podnet.py] => Task 2, Epoch 133/150 (LR 0.00314) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.68
2024-08-29 20:19:31,564 [podnet.py] => Task 2, Epoch 134/150 (LR 0.00278) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.69
2024-08-29 20:19:34,315 [podnet.py] => Task 2, Epoch 135/150 (LR 0.00245) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.33
2024-08-29 20:19:37,157 [podnet.py] => Task 2, Epoch 136/150 (LR 0.00213) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.08
2024-08-29 20:19:40,063 [podnet.py] => Task 2, Epoch 137/150 (LR 0.00184) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.98
2024-08-29 20:19:43,045 [podnet.py] => Task 2, Epoch 138/150 (LR 0.00157) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.29
2024-08-29 20:19:46,045 [podnet.py] => Task 2, Epoch 139/150 (LR 0.00132) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.36
2024-08-29 20:19:48,693 [podnet.py] => Task 2, Epoch 140/150 (LR 0.00109) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.44
2024-08-29 20:19:51,184 [podnet.py] => Task 2, Epoch 141/150 (LR 0.00089) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.16
2024-08-29 20:19:54,016 [podnet.py] => Task 2, Epoch 142/150 (LR 0.00070) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.15
2024-08-29 20:19:57,028 [podnet.py] => Task 2, Epoch 143/150 (LR 0.00054) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.95
2024-08-29 20:19:59,441 [podnet.py] => Task 2, Epoch 144/150 (LR 0.00039) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.23
2024-08-29 20:20:01,826 [podnet.py] => Task 2, Epoch 145/150 (LR 0.00027) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.12
2024-08-29 20:20:04,735 [podnet.py] => Task 2, Epoch 146/150 (LR 0.00018) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.17
2024-08-29 20:20:07,660 [podnet.py] => Task 2, Epoch 147/150 (LR 0.00010) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.54
2024-08-29 20:20:10,717 [podnet.py] => Task 2, Epoch 148/150 (LR 0.00004) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.10
2024-08-29 20:20:13,575 [podnet.py] => Task 2, Epoch 149/150 (LR 0.00001) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.98
2024-08-29 20:20:16,492 [podnet.py] => Task 2, Epoch 150/150 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.41
2024-08-29 20:20:17,459 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-29 20:20:17,460 [base.py] => Reducing exemplars...(71 per classes)
2024-08-29 20:20:18,790 [base.py] => Constructing exemplars...(71 per classes)
2024-08-29 20:20:20,384 [podnet.py] => The size of finetune dataset: 639
2024-08-29 20:20:21,676 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.12, Spatial_loss 0.52, Flat_loss 0.07, Train_acc 99.37, Test_acc 56.29
2024-08-29 20:20:22,818 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.08, Spatial_loss 0.40, Flat_loss 0.06, Train_acc 100.00, Test_acc 51.70
2024-08-29 20:20:23,991 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.06, Spatial_loss 0.37, Flat_loss 0.06, Train_acc 100.00, Test_acc 49.03
2024-08-29 20:20:25,181 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.05, Train_acc 100.00, Test_acc 48.14
2024-08-29 20:20:26,408 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.05, Train_acc 100.00, Test_acc 49.55
2024-08-29 20:20:27,612 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.05, Train_acc 100.00, Test_acc 51.52
2024-08-29 20:20:28,861 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.05, Train_acc 100.00, Test_acc 52.89
2024-08-29 20:20:30,034 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.05, Train_acc 100.00, Test_acc 53.75
2024-08-29 20:20:31,317 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.05, Train_acc 100.00, Test_acc 53.93
2024-08-29 20:20:32,716 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.10
2024-08-29 20:20:33,953 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.05, Train_acc 100.00, Test_acc 54.35
2024-08-29 20:20:35,170 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.21
2024-08-29 20:20:36,580 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.19
2024-08-29 20:20:37,737 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.05, Train_acc 100.00, Test_acc 54.27
2024-08-29 20:20:38,919 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.41
2024-08-29 20:20:40,109 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.05, Train_acc 100.00, Test_acc 54.35
2024-08-29 20:20:41,252 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.43
2024-08-29 20:20:42,462 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.41
2024-08-29 20:20:43,643 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.38
2024-08-29 20:20:44,844 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.04, Train_acc 100.00, Test_acc 54.36
2024-08-29 20:20:45,681 [base.py] => Reducing exemplars...(55 per classes)
2024-08-29 20:20:47,097 [base.py] => Constructing exemplars...(55 per classes)
2024-08-29 20:20:50,713 [podnet.py] => Exemplar size: 495
2024-08-29 20:20:50,713 [trainer.py] => CNN: {'total': 54.36, '00-04': 43.06, '05-06': 54.05, '07-08': 98.86, 'old': 45.29, 'new': 98.86}
2024-08-29 20:20:50,713 [trainer.py] => NME: {'total': 64.77, '00-04': 63.74, '05-06': 44.31, '07-08': 89.16, 'old': 59.8, 'new': 89.16}
2024-08-29 20:20:50,713 [trainer.py] => CNN top1 curve: [92.02, 77.51, 54.36]
2024-08-29 20:20:50,713 [trainer.py] => CNN top5 curve: [100.0, 99.02, 94.98]
2024-08-29 20:20:50,713 [trainer.py] => NME top1 curve: [92.04, 82.92, 64.77]
2024-08-29 20:20:50,713 [trainer.py] => NME top5 curve: [100.0, 99.07, 96.69]

2024-08-29 20:20:50,713 [trainer.py] => Average Accuracy (CNN): 74.63
2024-08-29 20:20:50,713 [trainer.py] => Average Accuracy (NME): 79.91000000000001
2024-08-29 20:20:50,714 [trainer.py] => Forgetting (CNN): 46.05
