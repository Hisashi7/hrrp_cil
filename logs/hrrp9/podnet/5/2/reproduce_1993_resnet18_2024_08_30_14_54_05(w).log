2024-08-30 14:54:05,820 [trainer.py] => config: ./exps/podnet.json
2024-08-30 14:54:05,820 [trainer.py] => prefix: reproduce
2024-08-30 14:54:05,820 [trainer.py] => dataset: hrrp9
2024-08-30 14:54:05,820 [trainer.py] => memory_size: 500
2024-08-30 14:54:05,820 [trainer.py] => memory_per_class: 20
2024-08-30 14:54:05,820 [trainer.py] => fixed_memory: False
2024-08-30 14:54:05,820 [trainer.py] => shuffle: True
2024-08-30 14:54:05,820 [trainer.py] => init_cls: 5
2024-08-30 14:54:05,820 [trainer.py] => increment: 2
2024-08-30 14:54:05,820 [trainer.py] => model_name: podnet
2024-08-30 14:54:05,820 [trainer.py] => convnet_type: resnet18
2024-08-30 14:54:05,820 [trainer.py] => init_train: True
2024-08-30 14:54:05,820 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 14:54:05,820 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 14:54:05,820 [trainer.py] => device: [device(type='cuda', index=6)]
2024-08-30 14:54:05,820 [trainer.py] => seed: 1993
2024-08-30 14:54:06,299 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 14:54:06,399 [trainer.py] => All params: 3843904
2024-08-30 14:54:06,400 [trainer.py] => Trainable params: 3843904
2024-08-30 14:54:06,400 [podnet.py] => Learning on 0-5
2024-08-30 14:54:06,436 [podnet.py] => Adaptive factor: 0
2024-08-30 14:54:09,796 [podnet.py] => Task 0, Epoch 1/150 (LR 0.09999) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 14:54:12,278 [podnet.py] => Task 0, Epoch 2/150 (LR 0.09996) => LSC_loss 1.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.33, Test_acc 39.37
2024-08-30 14:54:14,150 [podnet.py] => Task 0, Epoch 3/150 (LR 0.09990) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.79, Test_acc 51.00
2024-08-30 14:54:16,942 [podnet.py] => Task 0, Epoch 4/150 (LR 0.09982) => LSC_loss 0.50, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.56, Test_acc 58.00
2024-08-30 14:54:18,605 [podnet.py] => Task 0, Epoch 5/150 (LR 0.09973) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.45, Test_acc 64.33
2024-08-30 14:54:20,329 [podnet.py] => Task 0, Epoch 6/150 (LR 0.09961) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.10, Test_acc 68.70
2024-08-30 14:54:22,674 [podnet.py] => Task 0, Epoch 7/150 (LR 0.09946) => LSC_loss 0.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.42, Test_acc 76.07
2024-08-30 14:54:25,186 [podnet.py] => Task 0, Epoch 8/150 (LR 0.09930) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.45, Test_acc 70.30
2024-08-30 14:54:27,244 [podnet.py] => Task 0, Epoch 9/150 (LR 0.09911) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.96, Test_acc 74.53
2024-08-30 14:54:29,511 [podnet.py] => Task 0, Epoch 10/150 (LR 0.09891) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.95, Test_acc 76.40
2024-08-30 14:54:32,296 [podnet.py] => Task 0, Epoch 11/150 (LR 0.09868) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 80.87
2024-08-30 14:54:34,253 [podnet.py] => Task 0, Epoch 12/150 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 84.30
2024-08-30 14:54:36,580 [podnet.py] => Task 0, Epoch 13/150 (LR 0.09816) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 81.73
2024-08-30 14:54:38,860 [podnet.py] => Task 0, Epoch 14/150 (LR 0.09787) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 76.60
2024-08-30 14:54:40,725 [podnet.py] => Task 0, Epoch 15/150 (LR 0.09755) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 51.23
2024-08-30 14:54:42,580 [podnet.py] => Task 0, Epoch 16/150 (LR 0.09722) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.84, Test_acc 74.47
2024-08-30 14:54:45,469 [podnet.py] => Task 0, Epoch 17/150 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 80.63
2024-08-30 14:54:47,229 [podnet.py] => Task 0, Epoch 18/150 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 81.60
2024-08-30 14:54:49,656 [podnet.py] => Task 0, Epoch 19/150 (LR 0.09609) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.24, Test_acc 84.33
2024-08-30 14:54:51,760 [podnet.py] => Task 0, Epoch 20/150 (LR 0.09568) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.26, Test_acc 81.07
2024-08-30 14:54:53,592 [podnet.py] => Task 0, Epoch 21/150 (LR 0.09524) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 83.27
2024-08-30 14:54:56,131 [podnet.py] => Task 0, Epoch 22/150 (LR 0.09479) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 84.17
2024-08-30 14:54:58,811 [podnet.py] => Task 0, Epoch 23/150 (LR 0.09431) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.93, Test_acc 86.40
2024-08-30 14:55:01,006 [podnet.py] => Task 0, Epoch 24/150 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 71.60
2024-08-30 14:55:02,968 [podnet.py] => Task 0, Epoch 25/150 (LR 0.09330) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.73, Test_acc 82.07
2024-08-30 14:55:05,116 [podnet.py] => Task 0, Epoch 26/150 (LR 0.09277) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 80.47
2024-08-30 14:55:07,056 [podnet.py] => Task 0, Epoch 27/150 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.77, Test_acc 85.33
2024-08-30 14:55:09,111 [podnet.py] => Task 0, Epoch 28/150 (LR 0.09165) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.34, Test_acc 77.97
2024-08-30 14:55:11,613 [podnet.py] => Task 0, Epoch 29/150 (LR 0.09106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 86.90
2024-08-30 14:55:14,245 [podnet.py] => Task 0, Epoch 30/150 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.04, Test_acc 85.23
2024-08-30 14:55:16,289 [podnet.py] => Task 0, Epoch 31/150 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 81.53
2024-08-30 14:55:18,049 [podnet.py] => Task 0, Epoch 32/150 (LR 0.08918) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 87.70
2024-08-30 14:55:19,980 [podnet.py] => Task 0, Epoch 33/150 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.86, Test_acc 84.40
2024-08-30 14:55:22,000 [podnet.py] => Task 0, Epoch 34/150 (LR 0.08785) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.42, Test_acc 85.53
2024-08-30 14:55:24,142 [podnet.py] => Task 0, Epoch 35/150 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 78.30
2024-08-30 14:55:25,907 [podnet.py] => Task 0, Epoch 36/150 (LR 0.08645) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.51, Test_acc 83.23
2024-08-30 14:55:28,626 [podnet.py] => Task 0, Epoch 37/150 (LR 0.08572) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 87.10
2024-08-30 14:55:31,169 [podnet.py] => Task 0, Epoch 38/150 (LR 0.08498) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 87.13
2024-08-30 14:55:32,989 [podnet.py] => Task 0, Epoch 39/150 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 79.27
2024-08-30 14:55:34,722 [podnet.py] => Task 0, Epoch 40/150 (LR 0.08346) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 80.30
2024-08-30 14:55:36,724 [podnet.py] => Task 0, Epoch 41/150 (LR 0.08267) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 86.10
2024-08-30 14:55:38,774 [podnet.py] => Task 0, Epoch 42/150 (LR 0.08187) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.33
2024-08-30 14:55:40,604 [podnet.py] => Task 0, Epoch 43/150 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 80.40
2024-08-30 14:55:42,794 [podnet.py] => Task 0, Epoch 44/150 (LR 0.08023) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 82.53
2024-08-30 14:55:44,587 [podnet.py] => Task 0, Epoch 45/150 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 81.37
2024-08-30 14:55:46,904 [podnet.py] => Task 0, Epoch 46/150 (LR 0.07854) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 76.63
2024-08-30 14:55:48,778 [podnet.py] => Task 0, Epoch 47/150 (LR 0.07767) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 76.87
2024-08-30 14:55:51,238 [podnet.py] => Task 0, Epoch 48/150 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.13
2024-08-30 14:55:53,479 [podnet.py] => Task 0, Epoch 49/150 (LR 0.07590) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 82.60
2024-08-30 14:55:56,273 [podnet.py] => Task 0, Epoch 50/150 (LR 0.07500) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.48, Test_acc 83.70
2024-08-30 14:55:58,910 [podnet.py] => Task 0, Epoch 51/150 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.42, Test_acc 83.47
2024-08-30 14:56:01,387 [podnet.py] => Task 0, Epoch 52/150 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 88.80
2024-08-30 14:56:03,337 [podnet.py] => Task 0, Epoch 53/150 (LR 0.07223) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 85.73
2024-08-30 14:56:05,124 [podnet.py] => Task 0, Epoch 54/150 (LR 0.07129) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 87.63
2024-08-30 14:56:07,554 [podnet.py] => Task 0, Epoch 55/150 (LR 0.07034) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 83.60
2024-08-30 14:56:09,725 [podnet.py] => Task 0, Epoch 56/150 (LR 0.06938) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 84.83
2024-08-30 14:56:12,307 [podnet.py] => Task 0, Epoch 57/150 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.67, Test_acc 87.57
2024-08-30 14:56:14,738 [podnet.py] => Task 0, Epoch 58/150 (LR 0.06743) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 85.37
2024-08-30 14:56:17,164 [podnet.py] => Task 0, Epoch 59/150 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.05, Test_acc 83.40
2024-08-30 14:56:19,076 [podnet.py] => Task 0, Epoch 60/150 (LR 0.06545) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 82.47
2024-08-30 14:56:21,731 [podnet.py] => Task 0, Epoch 61/150 (LR 0.06445) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.80, Test_acc 86.27
2024-08-30 14:56:23,788 [podnet.py] => Task 0, Epoch 62/150 (LR 0.06345) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 82.77
2024-08-30 14:56:26,204 [podnet.py] => Task 0, Epoch 63/150 (LR 0.06243) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.10, Test_acc 83.17
2024-08-30 14:56:28,238 [podnet.py] => Task 0, Epoch 64/150 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 73.30
2024-08-30 14:56:30,286 [podnet.py] => Task 0, Epoch 65/150 (LR 0.06040) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 88.10
2024-08-30 14:56:32,419 [podnet.py] => Task 0, Epoch 66/150 (LR 0.05937) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.98, Test_acc 86.10
2024-08-30 14:56:35,074 [podnet.py] => Task 0, Epoch 67/150 (LR 0.05834) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 86.17
2024-08-30 14:56:37,581 [podnet.py] => Task 0, Epoch 68/150 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 88.47
2024-08-30 14:56:38,931 [podnet.py] => Task 0, Epoch 69/150 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 84.67
2024-08-30 14:56:40,861 [podnet.py] => Task 0, Epoch 70/150 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 86.90
2024-08-30 14:56:42,528 [podnet.py] => Task 0, Epoch 71/150 (LR 0.05418) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 90.27
2024-08-30 14:56:44,867 [podnet.py] => Task 0, Epoch 72/150 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 90.67
2024-08-30 14:56:46,665 [podnet.py] => Task 0, Epoch 73/150 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.54, Test_acc 86.53
2024-08-30 14:56:49,095 [podnet.py] => Task 0, Epoch 74/150 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 87.43
2024-08-30 14:56:51,537 [podnet.py] => Task 0, Epoch 75/150 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 89.50
2024-08-30 14:56:53,757 [podnet.py] => Task 0, Epoch 76/150 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 83.67
2024-08-30 14:56:55,398 [podnet.py] => Task 0, Epoch 77/150 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 85.23
2024-08-30 14:56:57,239 [podnet.py] => Task 0, Epoch 78/150 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.26, Test_acc 85.97
2024-08-30 14:56:59,806 [podnet.py] => Task 0, Epoch 79/150 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 88.43
2024-08-30 14:57:02,410 [podnet.py] => Task 0, Epoch 80/150 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 86.60
2024-08-30 14:57:04,859 [podnet.py] => Task 0, Epoch 81/150 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 86.57
2024-08-30 14:57:07,419 [podnet.py] => Task 0, Epoch 82/150 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 86.47
2024-08-30 14:57:09,374 [podnet.py] => Task 0, Epoch 83/150 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 89.90
2024-08-30 14:57:11,248 [podnet.py] => Task 0, Epoch 84/150 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 88.27
2024-08-30 14:57:13,087 [podnet.py] => Task 0, Epoch 85/150 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 89.70
2024-08-30 14:57:14,909 [podnet.py] => Task 0, Epoch 86/150 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 87.57
2024-08-30 14:57:17,178 [podnet.py] => Task 0, Epoch 87/150 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 88.60
2024-08-30 14:57:19,014 [podnet.py] => Task 0, Epoch 88/150 (LR 0.03655) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 87.10
2024-08-30 14:57:21,150 [podnet.py] => Task 0, Epoch 89/150 (LR 0.03555) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 88.93
2024-08-30 14:57:22,790 [podnet.py] => Task 0, Epoch 90/150 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 86.87
2024-08-30 14:57:25,224 [podnet.py] => Task 0, Epoch 91/150 (LR 0.03356) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.40
2024-08-30 14:57:26,831 [podnet.py] => Task 0, Epoch 92/150 (LR 0.03257) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.40
2024-08-30 14:57:28,536 [podnet.py] => Task 0, Epoch 93/150 (LR 0.03159) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 82.60
2024-08-30 14:57:30,262 [podnet.py] => Task 0, Epoch 94/150 (LR 0.03062) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 87.33
2024-08-30 14:57:31,991 [podnet.py] => Task 0, Epoch 95/150 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 88.00
2024-08-30 14:57:33,549 [podnet.py] => Task 0, Epoch 96/150 (LR 0.02871) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 87.47
2024-08-30 14:57:35,112 [podnet.py] => Task 0, Epoch 97/150 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 87.77
2024-08-30 14:57:36,914 [podnet.py] => Task 0, Epoch 98/150 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.17
2024-08-30 14:57:38,489 [podnet.py] => Task 0, Epoch 99/150 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.00
2024-08-30 14:57:40,011 [podnet.py] => Task 0, Epoch 100/150 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 14:57:41,518 [podnet.py] => Task 0, Epoch 101/150 (LR 0.02410) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 14:57:43,121 [podnet.py] => Task 0, Epoch 102/150 (LR 0.02321) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 88.80
2024-08-30 14:57:44,809 [podnet.py] => Task 0, Epoch 103/150 (LR 0.02233) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.10
2024-08-30 14:57:46,320 [podnet.py] => Task 0, Epoch 104/150 (LR 0.02146) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 14:57:48,188 [podnet.py] => Task 0, Epoch 105/150 (LR 0.02061) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.27
2024-08-30 14:57:50,319 [podnet.py] => Task 0, Epoch 106/150 (LR 0.01977) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 14:57:51,972 [podnet.py] => Task 0, Epoch 107/150 (LR 0.01894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 14:57:53,473 [podnet.py] => Task 0, Epoch 108/150 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 14:57:54,866 [podnet.py] => Task 0, Epoch 109/150 (LR 0.01733) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 14:57:56,590 [podnet.py] => Task 0, Epoch 110/150 (LR 0.01654) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 14:57:58,288 [podnet.py] => Task 0, Epoch 111/150 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 14:58:00,203 [podnet.py] => Task 0, Epoch 112/150 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.57
2024-08-30 14:58:01,811 [podnet.py] => Task 0, Epoch 113/150 (LR 0.01428) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 87.27
2024-08-30 14:58:03,279 [podnet.py] => Task 0, Epoch 114/150 (LR 0.01355) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 86.53
2024-08-30 14:58:04,770 [podnet.py] => Task 0, Epoch 115/150 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 87.50
2024-08-30 14:58:06,200 [podnet.py] => Task 0, Epoch 116/150 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 89.33
2024-08-30 14:58:07,825 [podnet.py] => Task 0, Epoch 117/150 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 14:58:09,496 [podnet.py] => Task 0, Epoch 118/150 (LR 0.01082) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 14:58:11,145 [podnet.py] => Task 0, Epoch 119/150 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-08-30 14:58:12,979 [podnet.py] => Task 0, Epoch 120/150 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.17
2024-08-30 14:58:14,693 [podnet.py] => Task 0, Epoch 121/150 (LR 0.00894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.27
2024-08-30 14:58:16,526 [podnet.py] => Task 0, Epoch 122/150 (LR 0.00835) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 89.37
2024-08-30 14:58:18,355 [podnet.py] => Task 0, Epoch 123/150 (LR 0.00778) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 89.43
2024-08-30 14:58:20,013 [podnet.py] => Task 0, Epoch 124/150 (LR 0.00723) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.90
2024-08-30 14:58:21,670 [podnet.py] => Task 0, Epoch 125/150 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 14:58:23,413 [podnet.py] => Task 0, Epoch 126/150 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 14:58:25,329 [podnet.py] => Task 0, Epoch 127/150 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 14:58:27,234 [podnet.py] => Task 0, Epoch 128/150 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 14:58:28,904 [podnet.py] => Task 0, Epoch 129/150 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 88.97
2024-08-30 14:58:30,487 [podnet.py] => Task 0, Epoch 130/150 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.03
2024-08-30 14:58:32,019 [podnet.py] => Task 0, Epoch 131/150 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.37
2024-08-30 14:58:33,693 [podnet.py] => Task 0, Epoch 132/150 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 14:58:35,116 [podnet.py] => Task 0, Epoch 133/150 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.27
2024-08-30 14:58:36,694 [podnet.py] => Task 0, Epoch 134/150 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 14:58:38,087 [podnet.py] => Task 0, Epoch 135/150 (LR 0.00245) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.23
2024-08-30 14:58:39,805 [podnet.py] => Task 0, Epoch 136/150 (LR 0.00213) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 88.20
2024-08-30 14:58:41,352 [podnet.py] => Task 0, Epoch 137/150 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.33
2024-08-30 14:58:43,128 [podnet.py] => Task 0, Epoch 138/150 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.53
2024-08-30 14:58:44,600 [podnet.py] => Task 0, Epoch 139/150 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.07
2024-08-30 14:58:46,106 [podnet.py] => Task 0, Epoch 140/150 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.30
2024-08-30 14:58:47,780 [podnet.py] => Task 0, Epoch 141/150 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.33
2024-08-30 14:58:49,791 [podnet.py] => Task 0, Epoch 142/150 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 88.63
2024-08-30 14:58:51,253 [podnet.py] => Task 0, Epoch 143/150 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.57
2024-08-30 14:58:52,885 [podnet.py] => Task 0, Epoch 144/150 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.57
2024-08-30 14:58:54,278 [podnet.py] => Task 0, Epoch 145/150 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 88.60
2024-08-30 14:58:55,869 [podnet.py] => Task 0, Epoch 146/150 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.50
2024-08-30 14:58:57,346 [podnet.py] => Task 0, Epoch 147/150 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.60
2024-08-30 14:58:59,028 [podnet.py] => Task 0, Epoch 148/150 (LR 0.00004) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.63
2024-08-30 14:59:00,985 [podnet.py] => Task 0, Epoch 149/150 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.60
2024-08-30 14:59:02,484 [podnet.py] => Task 0, Epoch 150/150 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.50
2024-08-30 14:59:02,720 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 14:59:02,720 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 14:59:07,744 [podnet.py] => Exemplar size: 500
2024-08-30 14:59:07,744 [trainer.py] => CNN: {'total': 88.5, '00-04': 88.5, 'old': 0, 'new': 88.5}
2024-08-30 14:59:07,744 [trainer.py] => NME: {'total': 88.5, '00-04': 88.5, 'old': 0, 'new': 88.5}
2024-08-30 14:59:07,744 [trainer.py] => CNN top1 curve: [88.5]
2024-08-30 14:59:07,744 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 14:59:07,744 [trainer.py] => NME top1 curve: [88.5]
2024-08-30 14:59:07,744 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 14:59:07,744 [trainer.py] => Average Accuracy (CNN): 88.5
2024-08-30 14:59:07,744 [trainer.py] => Average Accuracy (NME): 88.5
2024-08-30 14:59:07,744 [trainer.py] => All params: 3869505
2024-08-30 14:59:07,745 [trainer.py] => Trainable params: 3869505
2024-08-30 14:59:07,745 [podnet.py] => Learning on 5-7
2024-08-30 14:59:07,770 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 14:59:09,300 [podnet.py] => Task 1, Epoch 1/150 (LR 0.09999) => LSC_loss 1.04, Spatial_loss 3.89, Flat_loss 0.43, Train_acc 70.76, Test_acc 40.24
2024-08-30 14:59:10,649 [podnet.py] => Task 1, Epoch 2/150 (LR 0.09996) => LSC_loss 0.47, Spatial_loss 2.81, Flat_loss 0.27, Train_acc 87.58, Test_acc 47.24
2024-08-30 14:59:12,263 [podnet.py] => Task 1, Epoch 3/150 (LR 0.09990) => LSC_loss 0.32, Spatial_loss 2.21, Flat_loss 0.22, Train_acc 92.00, Test_acc 57.69
2024-08-30 14:59:13,883 [podnet.py] => Task 1, Epoch 4/150 (LR 0.09982) => LSC_loss 0.26, Spatial_loss 1.95, Flat_loss 0.20, Train_acc 94.20, Test_acc 51.57
2024-08-30 14:59:15,378 [podnet.py] => Task 1, Epoch 5/150 (LR 0.09973) => LSC_loss 0.24, Spatial_loss 1.76, Flat_loss 0.18, Train_acc 94.36, Test_acc 57.26
2024-08-30 14:59:16,884 [podnet.py] => Task 1, Epoch 6/150 (LR 0.09961) => LSC_loss 0.22, Spatial_loss 1.66, Flat_loss 0.17, Train_acc 95.29, Test_acc 59.12
2024-08-30 14:59:18,356 [podnet.py] => Task 1, Epoch 7/150 (LR 0.09946) => LSC_loss 0.18, Spatial_loss 1.66, Flat_loss 0.17, Train_acc 95.80, Test_acc 59.12
2024-08-30 14:59:19,680 [podnet.py] => Task 1, Epoch 8/150 (LR 0.09930) => LSC_loss 0.16, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 96.91, Test_acc 58.64
2024-08-30 14:59:20,973 [podnet.py] => Task 1, Epoch 9/150 (LR 0.09911) => LSC_loss 0.15, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 97.27, Test_acc 56.81
2024-08-30 14:59:22,210 [podnet.py] => Task 1, Epoch 10/150 (LR 0.09891) => LSC_loss 0.16, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 96.93, Test_acc 58.14
2024-08-30 14:59:23,566 [podnet.py] => Task 1, Epoch 11/150 (LR 0.09868) => LSC_loss 0.14, Spatial_loss 1.41, Flat_loss 0.15, Train_acc 97.78, Test_acc 61.02
2024-08-30 14:59:24,923 [podnet.py] => Task 1, Epoch 12/150 (LR 0.09843) => LSC_loss 0.13, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 97.76, Test_acc 63.40
2024-08-30 14:59:26,254 [podnet.py] => Task 1, Epoch 13/150 (LR 0.09816) => LSC_loss 0.11, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 98.69, Test_acc 61.26
2024-08-30 14:59:27,493 [podnet.py] => Task 1, Epoch 14/150 (LR 0.09787) => LSC_loss 0.11, Spatial_loss 1.28, Flat_loss 0.14, Train_acc 99.00, Test_acc 57.57
2024-08-30 14:59:28,740 [podnet.py] => Task 1, Epoch 15/150 (LR 0.09755) => LSC_loss 0.11, Spatial_loss 1.28, Flat_loss 0.14, Train_acc 98.89, Test_acc 57.79
2024-08-30 14:59:30,009 [podnet.py] => Task 1, Epoch 16/150 (LR 0.09722) => LSC_loss 0.11, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 98.51, Test_acc 61.79
2024-08-30 14:59:31,302 [podnet.py] => Task 1, Epoch 17/150 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 99.13, Test_acc 60.33
2024-08-30 14:59:32,776 [podnet.py] => Task 1, Epoch 18/150 (LR 0.09649) => LSC_loss 0.10, Spatial_loss 1.26, Flat_loss 0.13, Train_acc 98.78, Test_acc 58.90
2024-08-30 14:59:34,278 [podnet.py] => Task 1, Epoch 19/150 (LR 0.09609) => LSC_loss 0.09, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 99.27, Test_acc 59.26
2024-08-30 14:59:35,526 [podnet.py] => Task 1, Epoch 20/150 (LR 0.09568) => LSC_loss 0.09, Spatial_loss 1.19, Flat_loss 0.13, Train_acc 99.36, Test_acc 57.12
2024-08-30 14:59:36,758 [podnet.py] => Task 1, Epoch 21/150 (LR 0.09524) => LSC_loss 0.08, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 99.49, Test_acc 58.83
2024-08-30 14:59:37,993 [podnet.py] => Task 1, Epoch 22/150 (LR 0.09479) => LSC_loss 0.08, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 99.69, Test_acc 63.81
2024-08-30 14:59:39,331 [podnet.py] => Task 1, Epoch 23/150 (LR 0.09431) => LSC_loss 0.08, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 99.49, Test_acc 61.50
2024-08-30 14:59:40,572 [podnet.py] => Task 1, Epoch 24/150 (LR 0.09382) => LSC_loss 0.08, Spatial_loss 1.09, Flat_loss 0.12, Train_acc 99.56, Test_acc 63.64
2024-08-30 14:59:42,118 [podnet.py] => Task 1, Epoch 25/150 (LR 0.09330) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 99.60, Test_acc 66.64
2024-08-30 14:59:43,464 [podnet.py] => Task 1, Epoch 26/150 (LR 0.09277) => LSC_loss 0.08, Spatial_loss 1.09, Flat_loss 0.12, Train_acc 99.60, Test_acc 62.60
2024-08-30 14:59:44,724 [podnet.py] => Task 1, Epoch 27/150 (LR 0.09222) => LSC_loss 0.08, Spatial_loss 1.10, Flat_loss 0.12, Train_acc 99.60, Test_acc 60.81
2024-08-30 14:59:46,052 [podnet.py] => Task 1, Epoch 28/150 (LR 0.09165) => LSC_loss 0.09, Spatial_loss 1.15, Flat_loss 0.13, Train_acc 99.13, Test_acc 60.05
2024-08-30 14:59:47,373 [podnet.py] => Task 1, Epoch 29/150 (LR 0.09106) => LSC_loss 0.10, Spatial_loss 1.20, Flat_loss 0.13, Train_acc 98.56, Test_acc 61.00
2024-08-30 14:59:48,583 [podnet.py] => Task 1, Epoch 30/150 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 1.06, Flat_loss 0.12, Train_acc 99.62, Test_acc 61.29
2024-08-30 14:59:49,909 [podnet.py] => Task 1, Epoch 31/150 (LR 0.08983) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 99.60, Test_acc 62.05
2024-08-30 14:59:51,206 [podnet.py] => Task 1, Epoch 32/150 (LR 0.08918) => LSC_loss 0.07, Spatial_loss 1.09, Flat_loss 0.12, Train_acc 99.71, Test_acc 64.48
2024-08-30 14:59:52,693 [podnet.py] => Task 1, Epoch 33/150 (LR 0.08853) => LSC_loss 0.06, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 99.84, Test_acc 64.05
2024-08-30 14:59:54,110 [podnet.py] => Task 1, Epoch 34/150 (LR 0.08785) => LSC_loss 0.06, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 99.80, Test_acc 59.00
2024-08-30 14:59:55,484 [podnet.py] => Task 1, Epoch 35/150 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 1.04, Flat_loss 0.11, Train_acc 99.84, Test_acc 62.33
2024-08-30 14:59:57,021 [podnet.py] => Task 1, Epoch 36/150 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 99.71, Test_acc 62.24
2024-08-30 14:59:58,671 [podnet.py] => Task 1, Epoch 37/150 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.97, Flat_loss 0.11, Train_acc 99.78, Test_acc 63.17
2024-08-30 14:59:59,897 [podnet.py] => Task 1, Epoch 38/150 (LR 0.08498) => LSC_loss 0.06, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 99.84, Test_acc 64.95
2024-08-30 15:00:01,209 [podnet.py] => Task 1, Epoch 39/150 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 0.98, Flat_loss 0.11, Train_acc 99.87, Test_acc 61.02
2024-08-30 15:00:02,512 [podnet.py] => Task 1, Epoch 40/150 (LR 0.08346) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 99.78, Test_acc 62.26
2024-08-30 15:00:03,980 [podnet.py] => Task 1, Epoch 41/150 (LR 0.08267) => LSC_loss 0.05, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 99.87, Test_acc 62.45
2024-08-30 15:00:05,630 [podnet.py] => Task 1, Epoch 42/150 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 99.82, Test_acc 62.19
2024-08-30 15:00:07,114 [podnet.py] => Task 1, Epoch 43/150 (LR 0.08106) => LSC_loss 0.05, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 99.80, Test_acc 64.12
2024-08-30 15:00:08,304 [podnet.py] => Task 1, Epoch 44/150 (LR 0.08023) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 99.96, Test_acc 63.55
2024-08-30 15:00:09,582 [podnet.py] => Task 1, Epoch 45/150 (LR 0.07939) => LSC_loss 0.07, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 99.58, Test_acc 63.45
2024-08-30 15:00:10,875 [podnet.py] => Task 1, Epoch 46/150 (LR 0.07854) => LSC_loss 0.06, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 99.73, Test_acc 64.36
2024-08-30 15:00:12,176 [podnet.py] => Task 1, Epoch 47/150 (LR 0.07767) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.11, Train_acc 99.76, Test_acc 65.02
2024-08-30 15:00:13,544 [podnet.py] => Task 1, Epoch 48/150 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.94, Flat_loss 0.10, Train_acc 99.87, Test_acc 66.29
2024-08-30 15:00:14,939 [podnet.py] => Task 1, Epoch 49/150 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.93, Flat_loss 0.11, Train_acc 99.87, Test_acc 64.02
2024-08-30 15:00:16,198 [podnet.py] => Task 1, Epoch 50/150 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.96, Flat_loss 0.10, Train_acc 99.82, Test_acc 63.36
2024-08-30 15:00:17,514 [podnet.py] => Task 1, Epoch 51/150 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.96, Flat_loss 0.10, Train_acc 99.89, Test_acc 65.07
2024-08-30 15:00:18,955 [podnet.py] => Task 1, Epoch 52/150 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.89, Flat_loss 0.10, Train_acc 99.96, Test_acc 66.19
2024-08-30 15:00:20,204 [podnet.py] => Task 1, Epoch 53/150 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.91, Flat_loss 0.11, Train_acc 99.93, Test_acc 63.38
2024-08-30 15:00:21,749 [podnet.py] => Task 1, Epoch 54/150 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.88, Flat_loss 0.10, Train_acc 99.96, Test_acc 62.36
2024-08-30 15:00:23,149 [podnet.py] => Task 1, Epoch 55/150 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.89, Flat_loss 0.10, Train_acc 99.87, Test_acc 64.14
2024-08-30 15:00:24,610 [podnet.py] => Task 1, Epoch 56/150 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.85, Flat_loss 0.10, Train_acc 99.98, Test_acc 65.00
2024-08-30 15:00:26,020 [podnet.py] => Task 1, Epoch 57/150 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.86, Flat_loss 0.10, Train_acc 100.00, Test_acc 63.19
2024-08-30 15:00:27,255 [podnet.py] => Task 1, Epoch 58/150 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.91, Flat_loss 0.11, Train_acc 99.89, Test_acc 63.05
2024-08-30 15:00:28,430 [podnet.py] => Task 1, Epoch 59/150 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.89, Flat_loss 0.10, Train_acc 99.87, Test_acc 63.43
2024-08-30 15:00:29,843 [podnet.py] => Task 1, Epoch 60/150 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.90, Flat_loss 0.10, Train_acc 99.93, Test_acc 62.67
2024-08-30 15:00:31,144 [podnet.py] => Task 1, Epoch 61/150 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.88, Flat_loss 0.10, Train_acc 99.87, Test_acc 63.07
2024-08-30 15:00:32,394 [podnet.py] => Task 1, Epoch 62/150 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.83, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.50
2024-08-30 15:00:33,755 [podnet.py] => Task 1, Epoch 63/150 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.89, Flat_loss 0.10, Train_acc 99.89, Test_acc 64.60
2024-08-30 15:00:35,021 [podnet.py] => Task 1, Epoch 64/150 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.84, Flat_loss 0.10, Train_acc 99.96, Test_acc 59.86
2024-08-30 15:00:36,396 [podnet.py] => Task 1, Epoch 65/150 (LR 0.06040) => LSC_loss 0.05, Spatial_loss 0.80, Flat_loss 0.10, Train_acc 99.91, Test_acc 60.48
2024-08-30 15:00:37,820 [podnet.py] => Task 1, Epoch 66/150 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.88, Flat_loss 0.10, Train_acc 99.78, Test_acc 63.93
2024-08-30 15:00:39,078 [podnet.py] => Task 1, Epoch 67/150 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.88, Flat_loss 0.10, Train_acc 99.93, Test_acc 65.21
2024-08-30 15:00:40,228 [podnet.py] => Task 1, Epoch 68/150 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 99.91, Test_acc 62.69
2024-08-30 15:00:41,513 [podnet.py] => Task 1, Epoch 69/150 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 99.98, Test_acc 65.55
2024-08-30 15:00:42,894 [podnet.py] => Task 1, Epoch 70/150 (LR 0.05523) => LSC_loss 0.05, Spatial_loss 0.87, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.36
2024-08-30 15:00:44,347 [podnet.py] => Task 1, Epoch 71/150 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.86, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.40
2024-08-30 15:00:45,675 [podnet.py] => Task 1, Epoch 72/150 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.81, Flat_loss 0.10, Train_acc 99.96, Test_acc 66.60
2024-08-30 15:00:46,996 [podnet.py] => Task 1, Epoch 73/150 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.81, Flat_loss 0.09, Train_acc 99.93, Test_acc 64.81
2024-08-30 15:00:48,715 [podnet.py] => Task 1, Epoch 74/150 (LR 0.05105) => LSC_loss 0.05, Spatial_loss 0.80, Flat_loss 0.10, Train_acc 99.96, Test_acc 64.36
2024-08-30 15:00:50,037 [podnet.py] => Task 1, Epoch 75/150 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.79, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.90
2024-08-30 15:00:51,366 [podnet.py] => Task 1, Epoch 76/150 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.80, Flat_loss 0.10, Train_acc 99.93, Test_acc 65.05
2024-08-30 15:00:52,674 [podnet.py] => Task 1, Epoch 77/150 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.09, Train_acc 99.93, Test_acc 63.33
2024-08-30 15:00:54,043 [podnet.py] => Task 1, Epoch 78/150 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.86, Flat_loss 0.11, Train_acc 99.76, Test_acc 67.21
2024-08-30 15:00:55,411 [podnet.py] => Task 1, Epoch 79/150 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.81, Flat_loss 0.10, Train_acc 99.80, Test_acc 64.79
2024-08-30 15:00:56,653 [podnet.py] => Task 1, Epoch 80/150 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.09, Train_acc 99.98, Test_acc 66.10
2024-08-30 15:00:57,846 [podnet.py] => Task 1, Epoch 81/150 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.05
2024-08-30 15:00:59,052 [podnet.py] => Task 1, Epoch 82/150 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.09, Train_acc 99.96, Test_acc 65.05
2024-08-30 15:01:00,414 [podnet.py] => Task 1, Epoch 83/150 (LR 0.04166) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.09, Train_acc 99.98, Test_acc 66.90
2024-08-30 15:01:01,902 [podnet.py] => Task 1, Epoch 84/150 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.76, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.38
2024-08-30 15:01:03,164 [podnet.py] => Task 1, Epoch 85/150 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.75, Flat_loss 0.09, Train_acc 99.96, Test_acc 62.98
2024-08-30 15:01:04,548 [podnet.py] => Task 1, Epoch 86/150 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.09, Train_acc 99.96, Test_acc 66.29
2024-08-30 15:01:05,922 [podnet.py] => Task 1, Epoch 87/150 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.74, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.83
2024-08-30 15:01:07,181 [podnet.py] => Task 1, Epoch 88/150 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.72, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.48
2024-08-30 15:01:08,726 [podnet.py] => Task 1, Epoch 89/150 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.73, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.12
2024-08-30 15:01:10,082 [podnet.py] => Task 1, Epoch 90/150 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.93
2024-08-30 15:01:11,267 [podnet.py] => Task 1, Epoch 91/150 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.09, Train_acc 99.98, Test_acc 63.76
2024-08-30 15:01:12,503 [podnet.py] => Task 1, Epoch 92/150 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.69, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.26
2024-08-30 15:01:13,826 [podnet.py] => Task 1, Epoch 93/150 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.45
2024-08-30 15:01:15,184 [podnet.py] => Task 1, Epoch 94/150 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.67
2024-08-30 15:01:16,553 [podnet.py] => Task 1, Epoch 95/150 (LR 0.02966) => LSC_loss 0.04, Spatial_loss 0.70, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.74
2024-08-30 15:01:18,079 [podnet.py] => Task 1, Epoch 96/150 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.68, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.31
2024-08-30 15:01:19,361 [podnet.py] => Task 1, Epoch 97/150 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.62
2024-08-30 15:01:20,618 [podnet.py] => Task 1, Epoch 98/150 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.68, Flat_loss 0.09, Train_acc 99.96, Test_acc 67.36
2024-08-30 15:01:21,824 [podnet.py] => Task 1, Epoch 99/150 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.09, Train_acc 99.98, Test_acc 64.14
2024-08-30 15:01:23,169 [podnet.py] => Task 1, Epoch 100/150 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.00
2024-08-30 15:01:24,737 [podnet.py] => Task 1, Epoch 101/150 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.09, Train_acc 99.96, Test_acc 64.69
2024-08-30 15:01:26,065 [podnet.py] => Task 1, Epoch 102/150 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.29
2024-08-30 15:01:27,239 [podnet.py] => Task 1, Epoch 103/150 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.43
2024-08-30 15:01:28,579 [podnet.py] => Task 1, Epoch 104/150 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.09, Train_acc 99.96, Test_acc 65.38
2024-08-30 15:01:29,880 [podnet.py] => Task 1, Epoch 105/150 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.09, Train_acc 99.89, Test_acc 65.12
2024-08-30 15:01:31,282 [podnet.py] => Task 1, Epoch 106/150 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.67, Flat_loss 0.09, Train_acc 99.98, Test_acc 66.60
2024-08-30 15:01:32,623 [podnet.py] => Task 1, Epoch 107/150 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.74
2024-08-30 15:01:34,288 [podnet.py] => Task 1, Epoch 108/150 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.09, Train_acc 99.98, Test_acc 66.71
2024-08-30 15:01:35,530 [podnet.py] => Task 1, Epoch 109/150 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.09, Train_acc 99.98, Test_acc 64.67
2024-08-30 15:01:36,780 [podnet.py] => Task 1, Epoch 110/150 (LR 0.01654) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.12
2024-08-30 15:01:38,083 [podnet.py] => Task 1, Epoch 111/150 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.61, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.60
2024-08-30 15:01:39,508 [podnet.py] => Task 1, Epoch 112/150 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.08, Train_acc 99.98, Test_acc 64.64
2024-08-30 15:01:40,898 [podnet.py] => Task 1, Epoch 113/150 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.64
2024-08-30 15:01:42,091 [podnet.py] => Task 1, Epoch 114/150 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.60
2024-08-30 15:01:43,258 [podnet.py] => Task 1, Epoch 115/150 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.59, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.86
2024-08-30 15:01:44,437 [podnet.py] => Task 1, Epoch 116/150 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.50
2024-08-30 15:01:45,763 [podnet.py] => Task 1, Epoch 117/150 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.55
2024-08-30 15:01:47,079 [podnet.py] => Task 1, Epoch 118/150 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.17
2024-08-30 15:01:48,674 [podnet.py] => Task 1, Epoch 119/150 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.71
2024-08-30 15:01:50,067 [podnet.py] => Task 1, Epoch 120/150 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.21
2024-08-30 15:01:51,512 [podnet.py] => Task 1, Epoch 121/150 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.08, Train_acc 99.98, Test_acc 67.26
2024-08-30 15:01:52,710 [podnet.py] => Task 1, Epoch 122/150 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.10
2024-08-30 15:01:54,012 [podnet.py] => Task 1, Epoch 123/150 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.19
2024-08-30 15:01:55,223 [podnet.py] => Task 1, Epoch 124/150 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.45
2024-08-30 15:01:56,654 [podnet.py] => Task 1, Epoch 125/150 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.31
2024-08-30 15:01:58,072 [podnet.py] => Task 1, Epoch 126/150 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.29
2024-08-30 15:01:59,225 [podnet.py] => Task 1, Epoch 127/150 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.57
2024-08-30 15:02:00,456 [podnet.py] => Task 1, Epoch 128/150 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.93
2024-08-30 15:02:01,825 [podnet.py] => Task 1, Epoch 129/150 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.98
2024-08-30 15:02:03,197 [podnet.py] => Task 1, Epoch 130/150 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.93
2024-08-30 15:02:04,632 [podnet.py] => Task 1, Epoch 131/150 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.38
2024-08-30 15:02:06,116 [podnet.py] => Task 1, Epoch 132/150 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.40
2024-08-30 15:02:07,537 [podnet.py] => Task 1, Epoch 133/150 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.98
2024-08-30 15:02:08,810 [podnet.py] => Task 1, Epoch 134/150 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.21
2024-08-30 15:02:10,176 [podnet.py] => Task 1, Epoch 135/150 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.74
2024-08-30 15:02:11,432 [podnet.py] => Task 1, Epoch 136/150 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.48
2024-08-30 15:02:12,751 [podnet.py] => Task 1, Epoch 137/150 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.52
2024-08-30 15:02:14,125 [podnet.py] => Task 1, Epoch 138/150 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.62
2024-08-30 15:02:15,371 [podnet.py] => Task 1, Epoch 139/150 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.86
2024-08-30 15:02:16,646 [podnet.py] => Task 1, Epoch 140/150 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.67
2024-08-30 15:02:17,999 [podnet.py] => Task 1, Epoch 141/150 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.50
2024-08-30 15:02:19,403 [podnet.py] => Task 1, Epoch 142/150 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.21
2024-08-30 15:02:20,766 [podnet.py] => Task 1, Epoch 143/150 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.52
2024-08-30 15:02:22,149 [podnet.py] => Task 1, Epoch 144/150 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.00
2024-08-30 15:02:23,601 [podnet.py] => Task 1, Epoch 145/150 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.90
2024-08-30 15:02:25,215 [podnet.py] => Task 1, Epoch 146/150 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.50
2024-08-30 15:02:26,474 [podnet.py] => Task 1, Epoch 147/150 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.67
2024-08-30 15:02:27,794 [podnet.py] => Task 1, Epoch 148/150 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.55
2024-08-30 15:02:29,262 [podnet.py] => Task 1, Epoch 149/150 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.57
2024-08-30 15:02:30,671 [podnet.py] => Task 1, Epoch 150/150 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.64
2024-08-30 15:02:30,973 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 15:02:30,973 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 15:02:31,839 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 15:02:33,573 [podnet.py] => The size of finetune dataset: 700
2024-08-30 15:02:34,275 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.05, Train_acc 100.00, Test_acc 64.21
2024-08-30 15:02:34,865 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.04, Train_acc 99.86, Test_acc 64.57
2024-08-30 15:02:35,519 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.04, Train_acc 100.00, Test_acc 65.43
2024-08-30 15:02:36,113 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.03, Spatial_loss 0.51, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.60
2024-08-30 15:02:36,715 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.03, Spatial_loss 0.50, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.17
2024-08-30 15:02:37,315 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.03, Spatial_loss 0.46, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.50
2024-08-30 15:02:37,949 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.03, Spatial_loss 0.47, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.83
2024-08-30 15:02:38,569 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.03, Spatial_loss 0.45, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.50
2024-08-30 15:02:39,187 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.02, Spatial_loss 0.46, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.48
2024-08-30 15:02:39,792 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.02, Spatial_loss 0.47, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.86
2024-08-30 15:02:40,383 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.02, Spatial_loss 0.43, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.12
2024-08-30 15:02:40,984 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.02, Spatial_loss 0.43, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.90
2024-08-30 15:02:41,612 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.02, Spatial_loss 0.43, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.10
2024-08-30 15:02:42,183 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.02, Spatial_loss 0.45, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.38
2024-08-30 15:02:42,776 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.02, Spatial_loss 0.39, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.93
2024-08-30 15:02:43,365 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.02, Spatial_loss 0.43, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.00
2024-08-30 15:02:43,948 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.02, Spatial_loss 0.49, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.98
2024-08-30 15:02:44,559 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.02, Spatial_loss 0.46, Flat_loss 0.03, Train_acc 100.00, Test_acc 66.93
2024-08-30 15:02:45,215 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.02, Spatial_loss 0.45, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.10
2024-08-30 15:02:45,813 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.02, Spatial_loss 0.47, Flat_loss 0.03, Train_acc 100.00, Test_acc 67.02
2024-08-30 15:02:46,105 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 15:02:47,033 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 15:02:49,319 [podnet.py] => Exemplar size: 497
2024-08-30 15:02:49,319 [trainer.py] => CNN: {'total': 67.02, '00-04': 55.7, '05-06': 95.33, 'old': 55.7, 'new': 95.33}
2024-08-30 15:02:49,319 [trainer.py] => NME: {'total': 71.95, '00-04': 77.1, '05-06': 59.08, 'old': 77.1, 'new': 59.08}
2024-08-30 15:02:49,319 [trainer.py] => CNN top1 curve: [88.5, 67.02]
2024-08-30 15:02:49,319 [trainer.py] => CNN top5 curve: [100.0, 98.26]
2024-08-30 15:02:49,319 [trainer.py] => NME top1 curve: [88.5, 71.95]
2024-08-30 15:02:49,319 [trainer.py] => NME top5 curve: [100.0, 98.21]

2024-08-30 15:02:49,319 [trainer.py] => Average Accuracy (CNN): 77.75999999999999
2024-08-30 15:02:49,319 [trainer.py] => Average Accuracy (NME): 80.225
2024-08-30 15:02:49,320 [trainer.py] => All params: 3879745
2024-08-30 15:02:49,320 [trainer.py] => Trainable params: 3879745
2024-08-30 15:02:49,321 [podnet.py] => Learning on 7-9
2024-08-30 15:02:49,348 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 15:02:50,714 [podnet.py] => Task 2, Epoch 1/150 (LR 0.09999) => LSC_loss 1.38, Spatial_loss 2.72, Flat_loss 0.34, Train_acc 78.50, Test_acc 25.35
2024-08-30 15:02:52,146 [podnet.py] => Task 2, Epoch 2/150 (LR 0.09996) => LSC_loss 0.44, Spatial_loss 2.22, Flat_loss 0.20, Train_acc 90.62, Test_acc 41.07
2024-08-30 15:02:53,737 [podnet.py] => Task 2, Epoch 3/150 (LR 0.09990) => LSC_loss 0.31, Spatial_loss 1.86, Flat_loss 0.15, Train_acc 93.46, Test_acc 40.41
2024-08-30 15:02:54,981 [podnet.py] => Task 2, Epoch 4/150 (LR 0.09982) => LSC_loss 0.24, Spatial_loss 1.61, Flat_loss 0.13, Train_acc 95.91, Test_acc 47.20
2024-08-30 15:02:56,281 [podnet.py] => Task 2, Epoch 5/150 (LR 0.09973) => LSC_loss 0.21, Spatial_loss 1.56, Flat_loss 0.12, Train_acc 96.64, Test_acc 47.37
2024-08-30 15:02:57,777 [podnet.py] => Task 2, Epoch 6/150 (LR 0.09961) => LSC_loss 0.17, Spatial_loss 1.50, Flat_loss 0.11, Train_acc 97.87, Test_acc 53.20
2024-08-30 15:02:59,224 [podnet.py] => Task 2, Epoch 7/150 (LR 0.09946) => LSC_loss 0.16, Spatial_loss 1.43, Flat_loss 0.11, Train_acc 98.38, Test_acc 47.33
2024-08-30 15:03:00,472 [podnet.py] => Task 2, Epoch 8/150 (LR 0.09930) => LSC_loss 0.14, Spatial_loss 1.37, Flat_loss 0.11, Train_acc 98.40, Test_acc 53.98
2024-08-30 15:03:01,962 [podnet.py] => Task 2, Epoch 9/150 (LR 0.09911) => LSC_loss 0.13, Spatial_loss 1.36, Flat_loss 0.10, Train_acc 98.98, Test_acc 49.04
2024-08-30 15:03:03,331 [podnet.py] => Task 2, Epoch 10/150 (LR 0.09891) => LSC_loss 0.14, Spatial_loss 1.37, Flat_loss 0.11, Train_acc 98.49, Test_acc 53.93
2024-08-30 15:03:04,613 [podnet.py] => Task 2, Epoch 11/150 (LR 0.09868) => LSC_loss 0.12, Spatial_loss 1.27, Flat_loss 0.10, Train_acc 99.11, Test_acc 56.91
2024-08-30 15:03:06,021 [podnet.py] => Task 2, Epoch 12/150 (LR 0.09843) => LSC_loss 0.11, Spatial_loss 1.22, Flat_loss 0.09, Train_acc 99.49, Test_acc 48.81
2024-08-30 15:03:07,389 [podnet.py] => Task 2, Epoch 13/150 (LR 0.09816) => LSC_loss 0.10, Spatial_loss 1.21, Flat_loss 0.09, Train_acc 99.76, Test_acc 54.02
2024-08-30 15:03:08,700 [podnet.py] => Task 2, Epoch 14/150 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 1.27, Flat_loss 0.09, Train_acc 99.58, Test_acc 53.31
2024-08-30 15:03:09,958 [podnet.py] => Task 2, Epoch 15/150 (LR 0.09755) => LSC_loss 0.09, Spatial_loss 1.20, Flat_loss 0.09, Train_acc 99.73, Test_acc 52.98
2024-08-30 15:03:11,213 [podnet.py] => Task 2, Epoch 16/150 (LR 0.09722) => LSC_loss 0.09, Spatial_loss 1.26, Flat_loss 0.09, Train_acc 99.62, Test_acc 58.04
2024-08-30 15:03:12,608 [podnet.py] => Task 2, Epoch 17/150 (LR 0.09686) => LSC_loss 0.09, Spatial_loss 1.21, Flat_loss 0.09, Train_acc 99.76, Test_acc 51.70
2024-08-30 15:03:14,138 [podnet.py] => Task 2, Epoch 18/150 (LR 0.09649) => LSC_loss 0.08, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 99.80, Test_acc 57.63
2024-08-30 15:03:15,611 [podnet.py] => Task 2, Epoch 19/150 (LR 0.09609) => LSC_loss 0.08, Spatial_loss 1.11, Flat_loss 0.08, Train_acc 99.89, Test_acc 48.50
2024-08-30 15:03:17,565 [podnet.py] => Task 2, Epoch 20/150 (LR 0.09568) => LSC_loss 0.08, Spatial_loss 1.14, Flat_loss 0.08, Train_acc 99.89, Test_acc 52.74
2024-08-30 15:03:19,481 [podnet.py] => Task 2, Epoch 21/150 (LR 0.09524) => LSC_loss 0.09, Spatial_loss 1.13, Flat_loss 0.08, Train_acc 99.67, Test_acc 54.11
2024-08-30 15:03:21,541 [podnet.py] => Task 2, Epoch 22/150 (LR 0.09479) => LSC_loss 0.08, Spatial_loss 1.13, Flat_loss 0.08, Train_acc 99.78, Test_acc 54.56
2024-08-30 15:03:23,708 [podnet.py] => Task 2, Epoch 23/150 (LR 0.09431) => LSC_loss 0.08, Spatial_loss 1.14, Flat_loss 0.08, Train_acc 99.62, Test_acc 58.57
2024-08-30 15:03:25,596 [podnet.py] => Task 2, Epoch 24/150 (LR 0.09382) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 99.89, Test_acc 53.31
2024-08-30 15:03:27,358 [podnet.py] => Task 2, Epoch 25/150 (LR 0.09330) => LSC_loss 0.07, Spatial_loss 1.17, Flat_loss 0.08, Train_acc 99.73, Test_acc 54.02
2024-08-30 15:03:29,257 [podnet.py] => Task 2, Epoch 26/150 (LR 0.09277) => LSC_loss 0.07, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 99.84, Test_acc 56.22
2024-08-30 15:03:31,050 [podnet.py] => Task 2, Epoch 27/150 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 99.89, Test_acc 54.78
2024-08-30 15:03:32,828 [podnet.py] => Task 2, Epoch 28/150 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 1.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.93
2024-08-30 15:03:34,928 [podnet.py] => Task 2, Epoch 29/150 (LR 0.09106) => LSC_loss 0.07, Spatial_loss 1.11, Flat_loss 0.08, Train_acc 99.87, Test_acc 51.61
2024-08-30 15:03:36,730 [podnet.py] => Task 2, Epoch 30/150 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 1.15, Flat_loss 0.08, Train_acc 99.87, Test_acc 55.83
2024-08-30 15:03:38,736 [podnet.py] => Task 2, Epoch 31/150 (LR 0.08983) => LSC_loss 0.07, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 99.84, Test_acc 50.39
2024-08-30 15:03:40,231 [podnet.py] => Task 2, Epoch 32/150 (LR 0.08918) => LSC_loss 0.09, Spatial_loss 1.20, Flat_loss 0.09, Train_acc 99.42, Test_acc 56.96
2024-08-30 15:03:42,031 [podnet.py] => Task 2, Epoch 33/150 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 99.73, Test_acc 55.41
2024-08-30 15:03:43,484 [podnet.py] => Task 2, Epoch 34/150 (LR 0.08785) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 99.84, Test_acc 54.41
2024-08-30 15:03:45,372 [podnet.py] => Task 2, Epoch 35/150 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 1.03, Flat_loss 0.08, Train_acc 99.96, Test_acc 56.56
2024-08-30 15:03:46,912 [podnet.py] => Task 2, Epoch 36/150 (LR 0.08645) => LSC_loss 0.07, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 99.93, Test_acc 57.17
2024-08-30 15:03:49,058 [podnet.py] => Task 2, Epoch 37/150 (LR 0.08572) => LSC_loss 0.08, Spatial_loss 1.08, Flat_loss 0.08, Train_acc 99.71, Test_acc 54.33
2024-08-30 15:03:51,028 [podnet.py] => Task 2, Epoch 38/150 (LR 0.08498) => LSC_loss 0.09, Spatial_loss 1.22, Flat_loss 0.10, Train_acc 99.27, Test_acc 55.80
2024-08-30 15:03:52,609 [podnet.py] => Task 2, Epoch 39/150 (LR 0.08423) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 99.82, Test_acc 57.22
2024-08-30 15:03:54,562 [podnet.py] => Task 2, Epoch 40/150 (LR 0.08346) => LSC_loss 0.06, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.13
2024-08-30 15:03:56,461 [podnet.py] => Task 2, Epoch 41/150 (LR 0.08267) => LSC_loss 0.06, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 99.98, Test_acc 59.06
2024-08-30 15:03:58,740 [podnet.py] => Task 2, Epoch 42/150 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 99.98, Test_acc 59.13
2024-08-30 15:04:00,665 [podnet.py] => Task 2, Epoch 43/150 (LR 0.08106) => LSC_loss 0.06, Spatial_loss 1.07, Flat_loss 0.08, Train_acc 99.89, Test_acc 49.76
2024-08-30 15:04:02,654 [podnet.py] => Task 2, Epoch 44/150 (LR 0.08023) => LSC_loss 0.06, Spatial_loss 1.10, Flat_loss 0.08, Train_acc 99.96, Test_acc 52.46
2024-08-30 15:04:04,392 [podnet.py] => Task 2, Epoch 45/150 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 1.07, Flat_loss 0.08, Train_acc 99.93, Test_acc 56.48
2024-08-30 15:04:06,395 [podnet.py] => Task 2, Epoch 46/150 (LR 0.07854) => LSC_loss 0.06, Spatial_loss 1.03, Flat_loss 0.07, Train_acc 99.98, Test_acc 55.17
2024-08-30 15:04:08,254 [podnet.py] => Task 2, Epoch 47/150 (LR 0.07767) => LSC_loss 0.06, Spatial_loss 0.97, Flat_loss 0.07, Train_acc 99.98, Test_acc 58.13
2024-08-30 15:04:09,580 [podnet.py] => Task 2, Epoch 48/150 (LR 0.07679) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 99.91, Test_acc 53.26
2024-08-30 15:04:11,532 [podnet.py] => Task 2, Epoch 49/150 (LR 0.07590) => LSC_loss 0.06, Spatial_loss 0.97, Flat_loss 0.07, Train_acc 99.98, Test_acc 52.61
2024-08-30 15:04:13,800 [podnet.py] => Task 2, Epoch 50/150 (LR 0.07500) => LSC_loss 0.06, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 99.96, Test_acc 51.98
2024-08-30 15:04:15,915 [podnet.py] => Task 2, Epoch 51/150 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 1.03, Flat_loss 0.07, Train_acc 99.96, Test_acc 58.02
2024-08-30 15:04:17,930 [podnet.py] => Task 2, Epoch 52/150 (LR 0.07316) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.07, Train_acc 100.00, Test_acc 50.74
2024-08-30 15:04:19,859 [podnet.py] => Task 2, Epoch 53/150 (LR 0.07223) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.67
2024-08-30 15:04:21,698 [podnet.py] => Task 2, Epoch 54/150 (LR 0.07129) => LSC_loss 0.06, Spatial_loss 0.94, Flat_loss 0.07, Train_acc 99.96, Test_acc 54.26
2024-08-30 15:04:23,182 [podnet.py] => Task 2, Epoch 55/150 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 99.98, Test_acc 57.30
2024-08-30 15:04:25,339 [podnet.py] => Task 2, Epoch 56/150 (LR 0.06938) => LSC_loss 0.06, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 99.93, Test_acc 53.85
2024-08-30 15:04:27,593 [podnet.py] => Task 2, Epoch 57/150 (LR 0.06841) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.06
2024-08-30 15:04:29,622 [podnet.py] => Task 2, Epoch 58/150 (LR 0.06743) => LSC_loss 0.06, Spatial_loss 1.01, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.69
2024-08-30 15:04:31,722 [podnet.py] => Task 2, Epoch 59/150 (LR 0.06644) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.59
2024-08-30 15:04:33,741 [podnet.py] => Task 2, Epoch 60/150 (LR 0.06545) => LSC_loss 0.06, Spatial_loss 0.95, Flat_loss 0.07, Train_acc 99.98, Test_acc 56.54
2024-08-30 15:04:35,884 [podnet.py] => Task 2, Epoch 61/150 (LR 0.06445) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.07, Train_acc 100.00, Test_acc 54.94
2024-08-30 15:04:37,882 [podnet.py] => Task 2, Epoch 62/150 (LR 0.06345) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.35
2024-08-30 15:04:40,042 [podnet.py] => Task 2, Epoch 63/150 (LR 0.06243) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 99.98, Test_acc 54.63
2024-08-30 15:04:42,137 [podnet.py] => Task 2, Epoch 64/150 (LR 0.06142) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 99.98, Test_acc 58.67
2024-08-30 15:04:44,020 [podnet.py] => Task 2, Epoch 65/150 (LR 0.06040) => LSC_loss 0.06, Spatial_loss 0.90, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.57
2024-08-30 15:04:46,022 [podnet.py] => Task 2, Epoch 66/150 (LR 0.05937) => LSC_loss 0.06, Spatial_loss 0.95, Flat_loss 0.07, Train_acc 100.00, Test_acc 55.61
2024-08-30 15:04:48,140 [podnet.py] => Task 2, Epoch 67/150 (LR 0.05834) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.07, Train_acc 99.96, Test_acc 56.81
2024-08-30 15:04:50,146 [podnet.py] => Task 2, Epoch 68/150 (LR 0.05730) => LSC_loss 0.06, Spatial_loss 0.89, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.20
2024-08-30 15:04:52,302 [podnet.py] => Task 2, Epoch 69/150 (LR 0.05627) => LSC_loss 0.06, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.48
2024-08-30 15:04:53,578 [podnet.py] => Task 2, Epoch 70/150 (LR 0.05523) => LSC_loss 0.10, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 99.87, Test_acc 41.39
2024-08-30 15:04:55,582 [podnet.py] => Task 2, Epoch 71/150 (LR 0.05418) => LSC_loss 0.25, Spatial_loss 1.70, Flat_loss 0.15, Train_acc 94.08, Test_acc 56.06
2024-08-30 15:04:57,139 [podnet.py] => Task 2, Epoch 72/150 (LR 0.05314) => LSC_loss 0.10, Spatial_loss 1.26, Flat_loss 0.10, Train_acc 98.98, Test_acc 55.04
2024-08-30 15:04:59,067 [podnet.py] => Task 2, Epoch 73/150 (LR 0.05209) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 99.62, Test_acc 53.07
2024-08-30 15:05:00,812 [podnet.py] => Task 2, Epoch 74/150 (LR 0.05105) => LSC_loss 0.07, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 99.96, Test_acc 57.67
2024-08-30 15:05:02,940 [podnet.py] => Task 2, Epoch 75/150 (LR 0.05000) => LSC_loss 0.06, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 99.96, Test_acc 56.70
2024-08-30 15:05:04,259 [podnet.py] => Task 2, Epoch 76/150 (LR 0.04895) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 99.98, Test_acc 56.19
2024-08-30 15:05:05,730 [podnet.py] => Task 2, Epoch 77/150 (LR 0.04791) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 99.98, Test_acc 57.70
2024-08-30 15:05:07,259 [podnet.py] => Task 2, Epoch 78/150 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.48
2024-08-30 15:05:09,311 [podnet.py] => Task 2, Epoch 79/150 (LR 0.04582) => LSC_loss 0.06, Spatial_loss 0.91, Flat_loss 0.07, Train_acc 99.98, Test_acc 57.85
2024-08-30 15:05:11,317 [podnet.py] => Task 2, Epoch 80/150 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 99.98, Test_acc 55.39
2024-08-30 15:05:13,468 [podnet.py] => Task 2, Epoch 81/150 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.90, Flat_loss 0.07, Train_acc 100.00, Test_acc 58.78
2024-08-30 15:05:15,507 [podnet.py] => Task 2, Epoch 82/150 (LR 0.04270) => LSC_loss 0.06, Spatial_loss 0.91, Flat_loss 0.07, Train_acc 99.98, Test_acc 58.48
2024-08-30 15:05:17,550 [podnet.py] => Task 2, Epoch 83/150 (LR 0.04166) => LSC_loss 0.06, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 99.98, Test_acc 54.35
2024-08-30 15:05:19,494 [podnet.py] => Task 2, Epoch 84/150 (LR 0.04063) => LSC_loss 0.06, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.09
2024-08-30 15:05:21,414 [podnet.py] => Task 2, Epoch 85/150 (LR 0.03960) => LSC_loss 0.07, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 99.98, Test_acc 57.33
2024-08-30 15:05:23,307 [podnet.py] => Task 2, Epoch 86/150 (LR 0.03858) => LSC_loss 0.06, Spatial_loss 0.90, Flat_loss 0.07, Train_acc 99.96, Test_acc 57.20
2024-08-30 15:05:25,132 [podnet.py] => Task 2, Epoch 87/150 (LR 0.03757) => LSC_loss 0.06, Spatial_loss 0.84, Flat_loss 0.07, Train_acc 99.98, Test_acc 55.00
2024-08-30 15:05:27,042 [podnet.py] => Task 2, Epoch 88/150 (LR 0.03655) => LSC_loss 0.06, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 100.00, Test_acc 57.93
2024-08-30 15:05:29,063 [podnet.py] => Task 2, Epoch 89/150 (LR 0.03555) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 99.98, Test_acc 57.00
2024-08-30 15:05:31,062 [podnet.py] => Task 2, Epoch 90/150 (LR 0.03455) => LSC_loss 0.06, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.63
2024-08-30 15:05:33,005 [podnet.py] => Task 2, Epoch 91/150 (LR 0.03356) => LSC_loss 0.06, Spatial_loss 0.77, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.91
2024-08-30 15:05:34,955 [podnet.py] => Task 2, Epoch 92/150 (LR 0.03257) => LSC_loss 0.06, Spatial_loss 0.78, Flat_loss 0.06, Train_acc 99.98, Test_acc 56.37
2024-08-30 15:05:36,613 [podnet.py] => Task 2, Epoch 93/150 (LR 0.03159) => LSC_loss 0.05, Spatial_loss 0.77, Flat_loss 0.07, Train_acc 100.00, Test_acc 56.72
2024-08-30 15:05:38,742 [podnet.py] => Task 2, Epoch 94/150 (LR 0.03062) => LSC_loss 0.06, Spatial_loss 0.78, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.11
2024-08-30 15:05:40,702 [podnet.py] => Task 2, Epoch 95/150 (LR 0.02966) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.06, Train_acc 100.00, Test_acc 56.39
2024-08-30 15:05:42,854 [podnet.py] => Task 2, Epoch 96/150 (LR 0.02871) => LSC_loss 0.06, Spatial_loss 0.75, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.70
2024-08-30 15:05:44,923 [podnet.py] => Task 2, Epoch 97/150 (LR 0.02777) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.06, Train_acc 100.00, Test_acc 54.30
2024-08-30 15:05:46,872 [podnet.py] => Task 2, Epoch 98/150 (LR 0.02684) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.61
2024-08-30 15:05:48,794 [podnet.py] => Task 2, Epoch 99/150 (LR 0.02591) => LSC_loss 0.06, Spatial_loss 0.77, Flat_loss 0.06, Train_acc 100.00, Test_acc 55.96
2024-08-30 15:05:50,961 [podnet.py] => Task 2, Epoch 100/150 (LR 0.02500) => LSC_loss 0.06, Spatial_loss 0.75, Flat_loss 0.06, Train_acc 100.00, Test_acc 55.17
2024-08-30 15:05:53,043 [podnet.py] => Task 2, Epoch 101/150 (LR 0.02410) => LSC_loss 0.05, Spatial_loss 0.78, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.39
2024-08-30 15:05:55,181 [podnet.py] => Task 2, Epoch 102/150 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.06, Train_acc 100.00, Test_acc 59.13
2024-08-30 15:05:57,169 [podnet.py] => Task 2, Epoch 103/150 (LR 0.02233) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.37
2024-08-30 15:05:59,317 [podnet.py] => Task 2, Epoch 104/150 (LR 0.02146) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.06, Train_acc 100.00, Test_acc 55.52
2024-08-30 15:06:01,535 [podnet.py] => Task 2, Epoch 105/150 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.73, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.17
2024-08-30 15:06:03,441 [podnet.py] => Task 2, Epoch 106/150 (LR 0.01977) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.06, Train_acc 100.00, Test_acc 56.67
2024-08-30 15:06:05,302 [podnet.py] => Task 2, Epoch 107/150 (LR 0.01894) => LSC_loss 0.05, Spatial_loss 0.70, Flat_loss 0.06, Train_acc 100.00, Test_acc 56.96
2024-08-30 15:06:06,495 [podnet.py] => Task 2, Epoch 108/150 (LR 0.01813) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.30
2024-08-30 15:06:08,420 [podnet.py] => Task 2, Epoch 109/150 (LR 0.01733) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.41
2024-08-30 15:06:09,861 [podnet.py] => Task 2, Epoch 110/150 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.72, Flat_loss 0.06, Train_acc 100.00, Test_acc 56.80
2024-08-30 15:06:11,747 [podnet.py] => Task 2, Epoch 111/150 (LR 0.01577) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.06, Train_acc 100.00, Test_acc 56.91
2024-08-30 15:06:13,761 [podnet.py] => Task 2, Epoch 112/150 (LR 0.01502) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.43
2024-08-30 15:06:15,771 [podnet.py] => Task 2, Epoch 113/150 (LR 0.01428) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.06
2024-08-30 15:06:17,516 [podnet.py] => Task 2, Epoch 114/150 (LR 0.01355) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.06, Train_acc 100.00, Test_acc 55.96
2024-08-30 15:06:19,285 [podnet.py] => Task 2, Epoch 115/150 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.76
2024-08-30 15:06:20,615 [podnet.py] => Task 2, Epoch 116/150 (LR 0.01215) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.31
2024-08-30 15:06:22,175 [podnet.py] => Task 2, Epoch 117/150 (LR 0.01147) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.06, Train_acc 100.00, Test_acc 56.74
2024-08-30 15:06:24,240 [podnet.py] => Task 2, Epoch 118/150 (LR 0.01082) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.81
2024-08-30 15:06:26,325 [podnet.py] => Task 2, Epoch 119/150 (LR 0.01017) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.89
2024-08-30 15:06:28,503 [podnet.py] => Task 2, Epoch 120/150 (LR 0.00955) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.61
2024-08-30 15:06:30,609 [podnet.py] => Task 2, Epoch 121/150 (LR 0.00894) => LSC_loss 0.06, Spatial_loss 0.59, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.44
2024-08-30 15:06:32,750 [podnet.py] => Task 2, Epoch 122/150 (LR 0.00835) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.76
2024-08-30 15:06:34,781 [podnet.py] => Task 2, Epoch 123/150 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.83
2024-08-30 15:06:36,875 [podnet.py] => Task 2, Epoch 124/150 (LR 0.00723) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.30
2024-08-30 15:06:38,867 [podnet.py] => Task 2, Epoch 125/150 (LR 0.00670) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.37
2024-08-30 15:06:41,039 [podnet.py] => Task 2, Epoch 126/150 (LR 0.00618) => LSC_loss 0.06, Spatial_loss 0.60, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.91
2024-08-30 15:06:43,084 [podnet.py] => Task 2, Epoch 127/150 (LR 0.00569) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.65
2024-08-30 15:06:45,128 [podnet.py] => Task 2, Epoch 128/150 (LR 0.00521) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.44
2024-08-30 15:06:47,425 [podnet.py] => Task 2, Epoch 129/150 (LR 0.00476) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.39
2024-08-30 15:06:49,613 [podnet.py] => Task 2, Epoch 130/150 (LR 0.00432) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.17
2024-08-30 15:06:51,509 [podnet.py] => Task 2, Epoch 131/150 (LR 0.00391) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.98
2024-08-30 15:06:53,631 [podnet.py] => Task 2, Epoch 132/150 (LR 0.00351) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.07
2024-08-30 15:06:55,726 [podnet.py] => Task 2, Epoch 133/150 (LR 0.00314) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.15
2024-08-30 15:06:56,897 [podnet.py] => Task 2, Epoch 134/150 (LR 0.00278) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.11
2024-08-30 15:06:58,778 [podnet.py] => Task 2, Epoch 135/150 (LR 0.00245) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.91
2024-08-30 15:07:00,938 [podnet.py] => Task 2, Epoch 136/150 (LR 0.00213) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.57
2024-08-30 15:07:02,917 [podnet.py] => Task 2, Epoch 137/150 (LR 0.00184) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.07
2024-08-30 15:07:05,012 [podnet.py] => Task 2, Epoch 138/150 (LR 0.00157) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.72
2024-08-30 15:07:07,129 [podnet.py] => Task 2, Epoch 139/150 (LR 0.00132) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.91
2024-08-30 15:07:09,132 [podnet.py] => Task 2, Epoch 140/150 (LR 0.00109) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.11
2024-08-30 15:07:11,151 [podnet.py] => Task 2, Epoch 141/150 (LR 0.00089) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.61
2024-08-30 15:07:13,281 [podnet.py] => Task 2, Epoch 142/150 (LR 0.00070) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.41
2024-08-30 15:07:15,472 [podnet.py] => Task 2, Epoch 143/150 (LR 0.00054) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.46
2024-08-30 15:07:17,676 [podnet.py] => Task 2, Epoch 144/150 (LR 0.00039) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.06, Train_acc 100.00, Test_acc 57.98
2024-08-30 15:07:19,825 [podnet.py] => Task 2, Epoch 145/150 (LR 0.00027) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.48
2024-08-30 15:07:21,862 [podnet.py] => Task 2, Epoch 146/150 (LR 0.00018) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.69
2024-08-30 15:07:23,929 [podnet.py] => Task 2, Epoch 147/150 (LR 0.00010) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.26
2024-08-30 15:07:25,768 [podnet.py] => Task 2, Epoch 148/150 (LR 0.00004) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.50
2024-08-30 15:07:27,279 [podnet.py] => Task 2, Epoch 149/150 (LR 0.00001) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.31
2024-08-30 15:07:29,379 [podnet.py] => Task 2, Epoch 150/150 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.06, Train_acc 100.00, Test_acc 58.78
2024-08-30 15:07:29,723 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 15:07:29,723 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 15:07:30,875 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 15:07:32,314 [podnet.py] => The size of finetune dataset: 639
2024-08-30 15:07:33,090 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.14, Spatial_loss 0.88, Flat_loss 0.05, Train_acc 99.37, Test_acc 56.69
2024-08-30 15:07:33,689 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.09, Spatial_loss 0.67, Flat_loss 0.04, Train_acc 100.00, Test_acc 57.65
2024-08-30 15:07:34,293 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.07, Spatial_loss 0.56, Flat_loss 0.04, Train_acc 100.00, Test_acc 57.80
2024-08-30 15:07:34,920 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.57
2024-08-30 15:07:35,558 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.03, Train_acc 100.00, Test_acc 55.78
2024-08-30 15:07:36,147 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.03, Train_acc 100.00, Test_acc 55.93
2024-08-30 15:07:36,730 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.30
2024-08-30 15:07:37,376 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.69
2024-08-30 15:07:37,963 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.78
2024-08-30 15:07:38,550 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.59
2024-08-30 15:07:39,186 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.59
2024-08-30 15:07:39,899 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.57
2024-08-30 15:07:40,527 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.65
2024-08-30 15:07:41,129 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.72
2024-08-30 15:07:41,734 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.76
2024-08-30 15:07:42,338 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.70
2024-08-30 15:07:42,919 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.76
2024-08-30 15:07:43,529 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.83
2024-08-30 15:07:44,133 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.78
2024-08-30 15:07:44,733 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.03, Train_acc 100.00, Test_acc 56.67
2024-08-30 15:07:45,073 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 15:07:46,199 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 15:07:48,362 [podnet.py] => Exemplar size: 495
2024-08-30 15:07:48,362 [trainer.py] => CNN: {'total': 56.67, '00-04': 43.17, '05-06': 48.17, '07-08': 98.92, 'old': 44.6, 'new': 98.92}
2024-08-30 15:07:48,362 [trainer.py] => NME: {'total': 65.15, '00-04': 68.17, '05-06': 40.75, '07-08': 82.0, 'old': 60.33, 'new': 82.0}
2024-08-30 15:07:48,362 [trainer.py] => CNN top1 curve: [88.5, 67.02, 56.67]
2024-08-30 15:07:48,362 [trainer.py] => CNN top5 curve: [100.0, 98.26, 93.17]
2024-08-30 15:07:48,363 [trainer.py] => NME top1 curve: [88.5, 71.95, 65.15]
2024-08-30 15:07:48,363 [trainer.py] => NME top5 curve: [100.0, 98.21, 95.07]

2024-08-30 15:07:48,363 [trainer.py] => Average Accuracy (CNN): 70.73
2024-08-30 15:07:48,363 [trainer.py] => Average Accuracy (NME): 75.2
2024-08-30 15:07:48,363 [trainer.py] => Forgetting (CNN): 46.245
