2024-08-30 17:06:39,028 [trainer.py] => config: ./exps/podnet.json
2024-08-30 17:06:39,028 [trainer.py] => prefix: reproduce
2024-08-30 17:06:39,028 [trainer.py] => dataset: hrrp9
2024-08-30 17:06:39,028 [trainer.py] => memory_size: 500
2024-08-30 17:06:39,028 [trainer.py] => memory_per_class: 20
2024-08-30 17:06:39,028 [trainer.py] => fixed_memory: False
2024-08-30 17:06:39,028 [trainer.py] => shuffle: True
2024-08-30 17:06:39,028 [trainer.py] => init_cls: 5
2024-08-30 17:06:39,028 [trainer.py] => increment: 2
2024-08-30 17:06:39,028 [trainer.py] => model_name: podnet
2024-08-30 17:06:39,028 [trainer.py] => convnet_type: resnet18
2024-08-30 17:06:39,029 [trainer.py] => init_train: True
2024-08-30 17:06:39,029 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 17:06:39,029 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 17:06:39,029 [trainer.py] => device: [device(type='cuda', index=2)]
2024-08-30 17:06:39,029 [trainer.py] => seed: 1993
2024-08-30 17:06:39,546 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 17:06:39,643 [trainer.py] => All params: 3843904
2024-08-30 17:06:39,644 [trainer.py] => Trainable params: 3843904
2024-08-30 17:06:39,644 [podnet.py] => Learning on 0-5
2024-08-30 17:06:39,680 [podnet.py] => Adaptive factor: 0
2024-08-30 17:06:42,319 [podnet.py] => Task 0, Epoch 1/200 (LR 0.09999) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 17:06:44,084 [podnet.py] => Task 0, Epoch 2/200 (LR 0.09998) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.70, Test_acc 28.10
2024-08-30 17:06:45,834 [podnet.py] => Task 0, Epoch 3/200 (LR 0.09994) => LSC_loss 0.74, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.65, Test_acc 46.50
2024-08-30 17:06:47,491 [podnet.py] => Task 0, Epoch 4/200 (LR 0.09990) => LSC_loss 0.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.48, Test_acc 56.47
2024-08-30 17:06:49,185 [podnet.py] => Task 0, Epoch 5/200 (LR 0.09985) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.35, Test_acc 57.07
2024-08-30 17:06:50,890 [podnet.py] => Task 0, Epoch 6/200 (LR 0.09978) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.01, Test_acc 70.80
2024-08-30 17:06:52,723 [podnet.py] => Task 0, Epoch 7/200 (LR 0.09970) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.09, Test_acc 78.63
2024-08-30 17:06:54,698 [podnet.py] => Task 0, Epoch 8/200 (LR 0.09961) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.03, Test_acc 73.23
2024-08-30 17:06:56,672 [podnet.py] => Task 0, Epoch 9/200 (LR 0.09950) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.07, Test_acc 72.00
2024-08-30 17:06:58,551 [podnet.py] => Task 0, Epoch 10/200 (LR 0.09938) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.89, Test_acc 78.53
2024-08-30 17:07:00,144 [podnet.py] => Task 0, Epoch 11/200 (LR 0.09926) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.73, Test_acc 70.67
2024-08-30 17:07:01,831 [podnet.py] => Task 0, Epoch 12/200 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.93, Test_acc 72.97
2024-08-30 17:07:03,534 [podnet.py] => Task 0, Epoch 13/200 (LR 0.09896) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.33, Test_acc 82.83
2024-08-30 17:07:05,092 [podnet.py] => Task 0, Epoch 14/200 (LR 0.09880) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.98, Test_acc 73.07
2024-08-30 17:07:06,824 [podnet.py] => Task 0, Epoch 15/200 (LR 0.09862) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.00, Test_acc 50.33
2024-08-30 17:07:08,460 [podnet.py] => Task 0, Epoch 16/200 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.34, Test_acc 86.00
2024-08-30 17:07:10,180 [podnet.py] => Task 0, Epoch 17/200 (LR 0.09823) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.15, Test_acc 76.53
2024-08-30 17:07:12,058 [podnet.py] => Task 0, Epoch 18/200 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 84.10
2024-08-30 17:07:13,934 [podnet.py] => Task 0, Epoch 19/200 (LR 0.09779) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 76.60
2024-08-30 17:07:15,561 [podnet.py] => Task 0, Epoch 20/200 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 75.47
2024-08-30 17:07:17,251 [podnet.py] => Task 0, Epoch 21/200 (LR 0.09730) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 79.90
2024-08-30 17:07:18,964 [podnet.py] => Task 0, Epoch 22/200 (LR 0.09704) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.93, Test_acc 81.03
2024-08-30 17:07:20,611 [podnet.py] => Task 0, Epoch 23/200 (LR 0.09677) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 81.97
2024-08-30 17:07:22,451 [podnet.py] => Task 0, Epoch 24/200 (LR 0.09649) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.72, Test_acc 79.50
2024-08-30 17:07:24,092 [podnet.py] => Task 0, Epoch 25/200 (LR 0.09619) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 83.03
2024-08-30 17:07:25,819 [podnet.py] => Task 0, Epoch 26/200 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 84.80
2024-08-30 17:07:27,447 [podnet.py] => Task 0, Epoch 27/200 (LR 0.09557) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 80.00
2024-08-30 17:07:29,043 [podnet.py] => Task 0, Epoch 28/200 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 83.80
2024-08-30 17:07:30,698 [podnet.py] => Task 0, Epoch 29/200 (LR 0.09490) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 84.60
2024-08-30 17:07:32,478 [podnet.py] => Task 0, Epoch 30/200 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.21, Test_acc 77.83
2024-08-30 17:07:34,148 [podnet.py] => Task 0, Epoch 31/200 (LR 0.09419) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 80.53
2024-08-30 17:07:35,912 [podnet.py] => Task 0, Epoch 32/200 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.13
2024-08-30 17:07:37,674 [podnet.py] => Task 0, Epoch 33/200 (LR 0.09343) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.99, Test_acc 82.00
2024-08-30 17:07:39,544 [podnet.py] => Task 0, Epoch 34/200 (LR 0.09304) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 78.77
2024-08-30 17:07:41,441 [podnet.py] => Task 0, Epoch 35/200 (LR 0.09263) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.99, Test_acc 83.30
2024-08-30 17:07:43,217 [podnet.py] => Task 0, Epoch 36/200 (LR 0.09222) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.97, Test_acc 80.60
2024-08-30 17:07:44,905 [podnet.py] => Task 0, Epoch 37/200 (LR 0.09179) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.96, Test_acc 85.33
2024-08-30 17:07:46,583 [podnet.py] => Task 0, Epoch 38/200 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 78.57
2024-08-30 17:07:48,213 [podnet.py] => Task 0, Epoch 39/200 (LR 0.09091) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 87.57
2024-08-30 17:07:49,865 [podnet.py] => Task 0, Epoch 40/200 (LR 0.09045) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.39, Test_acc 86.33
2024-08-30 17:07:51,555 [podnet.py] => Task 0, Epoch 41/200 (LR 0.08998) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 78.50
2024-08-30 17:07:53,340 [podnet.py] => Task 0, Epoch 42/200 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 84.73
2024-08-30 17:07:55,248 [podnet.py] => Task 0, Epoch 43/200 (LR 0.08902) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 75.87
2024-08-30 17:07:57,045 [podnet.py] => Task 0, Epoch 44/200 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 81.53
2024-08-30 17:07:58,898 [podnet.py] => Task 0, Epoch 45/200 (LR 0.08802) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.49, Test_acc 75.03
2024-08-30 17:08:00,612 [podnet.py] => Task 0, Epoch 46/200 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 83.07
2024-08-30 17:08:02,338 [podnet.py] => Task 0, Epoch 47/200 (LR 0.08698) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 84.73
2024-08-30 17:08:04,153 [podnet.py] => Task 0, Epoch 48/200 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.23
2024-08-30 17:08:05,794 [podnet.py] => Task 0, Epoch 49/200 (LR 0.08591) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 80.70
2024-08-30 17:08:07,438 [podnet.py] => Task 0, Epoch 50/200 (LR 0.08536) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.64, Test_acc 80.33
2024-08-30 17:08:09,239 [podnet.py] => Task 0, Epoch 51/200 (LR 0.08480) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 86.60
2024-08-30 17:08:11,064 [podnet.py] => Task 0, Epoch 52/200 (LR 0.08423) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.62, Test_acc 81.07
2024-08-30 17:08:13,181 [podnet.py] => Task 0, Epoch 53/200 (LR 0.08365) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.46, Test_acc 81.60
2024-08-30 17:08:14,959 [podnet.py] => Task 0, Epoch 54/200 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 79.40
2024-08-30 17:08:16,693 [podnet.py] => Task 0, Epoch 55/200 (LR 0.08247) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.85, Test_acc 78.10
2024-08-30 17:08:18,544 [podnet.py] => Task 0, Epoch 56/200 (LR 0.08187) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 87.00
2024-08-30 17:08:20,553 [podnet.py] => Task 0, Epoch 57/200 (LR 0.08126) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 82.57
2024-08-30 17:08:22,601 [podnet.py] => Task 0, Epoch 58/200 (LR 0.08065) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 86.30
2024-08-30 17:08:24,242 [podnet.py] => Task 0, Epoch 59/200 (LR 0.08002) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 87.63
2024-08-30 17:08:25,926 [podnet.py] => Task 0, Epoch 60/200 (LR 0.07939) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 85.37
2024-08-30 17:08:27,800 [podnet.py] => Task 0, Epoch 61/200 (LR 0.07875) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.14, Test_acc 78.47
2024-08-30 17:08:29,732 [podnet.py] => Task 0, Epoch 62/200 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.34, Test_acc 82.63
2024-08-30 17:08:31,808 [podnet.py] => Task 0, Epoch 63/200 (LR 0.07745) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 85.27
2024-08-30 17:08:33,544 [podnet.py] => Task 0, Epoch 64/200 (LR 0.07679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 86.43
2024-08-30 17:08:35,358 [podnet.py] => Task 0, Epoch 65/200 (LR 0.07612) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.57
2024-08-30 17:08:36,999 [podnet.py] => Task 0, Epoch 66/200 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.17, Test_acc 84.67
2024-08-30 17:08:38,664 [podnet.py] => Task 0, Epoch 67/200 (LR 0.07477) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 82.73
2024-08-30 17:08:40,365 [podnet.py] => Task 0, Epoch 68/200 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.20
2024-08-30 17:08:42,035 [podnet.py] => Task 0, Epoch 69/200 (LR 0.07340) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.27, Test_acc 83.47
2024-08-30 17:08:43,879 [podnet.py] => Task 0, Epoch 70/200 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.58, Test_acc 76.37
2024-08-30 17:08:45,712 [podnet.py] => Task 0, Epoch 71/200 (LR 0.07200) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.67, Test_acc 83.77
2024-08-30 17:08:47,346 [podnet.py] => Task 0, Epoch 72/200 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 85.97
2024-08-30 17:08:49,076 [podnet.py] => Task 0, Epoch 73/200 (LR 0.07058) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 85.37
2024-08-30 17:08:50,655 [podnet.py] => Task 0, Epoch 74/200 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 87.87
2024-08-30 17:08:52,375 [podnet.py] => Task 0, Epoch 75/200 (LR 0.06913) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 84.93
2024-08-30 17:08:54,145 [podnet.py] => Task 0, Epoch 76/200 (LR 0.06841) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.94, Test_acc 83.30
2024-08-30 17:08:55,878 [podnet.py] => Task 0, Epoch 77/200 (LR 0.06767) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 86.93
2024-08-30 17:08:57,596 [podnet.py] => Task 0, Epoch 78/200 (LR 0.06694) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 80.80
2024-08-30 17:08:59,154 [podnet.py] => Task 0, Epoch 79/200 (LR 0.06620) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.43
2024-08-30 17:09:00,930 [podnet.py] => Task 0, Epoch 80/200 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 87.10
2024-08-30 17:09:02,625 [podnet.py] => Task 0, Epoch 81/200 (LR 0.06470) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.77
2024-08-30 17:09:04,382 [podnet.py] => Task 0, Epoch 82/200 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 77.37
2024-08-30 17:09:06,139 [podnet.py] => Task 0, Epoch 83/200 (LR 0.06319) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.58, Test_acc 88.90
2024-08-30 17:09:07,827 [podnet.py] => Task 0, Epoch 84/200 (LR 0.06243) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.37
2024-08-30 17:09:09,454 [podnet.py] => Task 0, Epoch 85/200 (LR 0.06167) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-30 17:09:11,119 [podnet.py] => Task 0, Epoch 86/200 (LR 0.06091) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 91.13
2024-08-30 17:09:12,702 [podnet.py] => Task 0, Epoch 87/200 (LR 0.06014) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.77
2024-08-30 17:09:14,461 [podnet.py] => Task 0, Epoch 88/200 (LR 0.05937) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.80, Test_acc 77.43
2024-08-30 17:09:16,058 [podnet.py] => Task 0, Epoch 89/200 (LR 0.05860) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.27, Test_acc 82.77
2024-08-30 17:09:17,953 [podnet.py] => Task 0, Epoch 90/200 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 84.93
2024-08-30 17:09:19,889 [podnet.py] => Task 0, Epoch 91/200 (LR 0.05705) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.26, Test_acc 87.63
2024-08-30 17:09:21,477 [podnet.py] => Task 0, Epoch 92/200 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 86.30
2024-08-30 17:09:23,391 [podnet.py] => Task 0, Epoch 93/200 (LR 0.05549) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 82.70
2024-08-30 17:09:25,069 [podnet.py] => Task 0, Epoch 94/200 (LR 0.05471) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 86.67
2024-08-30 17:09:26,786 [podnet.py] => Task 0, Epoch 95/200 (LR 0.05392) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 88.53
2024-08-30 17:09:28,557 [podnet.py] => Task 0, Epoch 96/200 (LR 0.05314) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.65, Test_acc 75.93
2024-08-30 17:09:30,183 [podnet.py] => Task 0, Epoch 97/200 (LR 0.05236) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.81, Test_acc 86.07
2024-08-30 17:09:31,907 [podnet.py] => Task 0, Epoch 98/200 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.53
2024-08-30 17:09:33,568 [podnet.py] => Task 0, Epoch 99/200 (LR 0.05079) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 87.07
2024-08-30 17:09:35,411 [podnet.py] => Task 0, Epoch 100/200 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.53, Test_acc 86.03
2024-08-30 17:09:37,120 [podnet.py] => Task 0, Epoch 101/200 (LR 0.04921) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 87.93
2024-08-30 17:09:38,788 [podnet.py] => Task 0, Epoch 102/200 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 88.53
2024-08-30 17:09:40,448 [podnet.py] => Task 0, Epoch 103/200 (LR 0.04764) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 89.13
2024-08-30 17:09:42,135 [podnet.py] => Task 0, Epoch 104/200 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 84.87
2024-08-30 17:09:44,064 [podnet.py] => Task 0, Epoch 105/200 (LR 0.04608) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 88.63
2024-08-30 17:09:46,298 [podnet.py] => Task 0, Epoch 106/200 (LR 0.04529) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 86.70
2024-08-30 17:09:48,374 [podnet.py] => Task 0, Epoch 107/200 (LR 0.04451) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 85.30
2024-08-30 17:09:50,323 [podnet.py] => Task 0, Epoch 108/200 (LR 0.04373) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 86.13
2024-08-30 17:09:52,078 [podnet.py] => Task 0, Epoch 109/200 (LR 0.04295) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 86.57
2024-08-30 17:09:53,785 [podnet.py] => Task 0, Epoch 110/200 (LR 0.04218) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 88.03
2024-08-30 17:09:55,463 [podnet.py] => Task 0, Epoch 111/200 (LR 0.04140) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 88.87
2024-08-30 17:09:57,029 [podnet.py] => Task 0, Epoch 112/200 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 86.90
2024-08-30 17:09:58,865 [podnet.py] => Task 0, Epoch 113/200 (LR 0.03986) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 85.40
2024-08-30 17:10:00,462 [podnet.py] => Task 0, Epoch 114/200 (LR 0.03909) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 86.57
2024-08-30 17:10:02,052 [podnet.py] => Task 0, Epoch 115/200 (LR 0.03833) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 89.27
2024-08-30 17:10:03,710 [podnet.py] => Task 0, Epoch 116/200 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 90.97
2024-08-30 17:10:05,306 [podnet.py] => Task 0, Epoch 117/200 (LR 0.03681) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.73
2024-08-30 17:10:06,858 [podnet.py] => Task 0, Epoch 118/200 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 84.87
2024-08-30 17:10:08,553 [podnet.py] => Task 0, Epoch 119/200 (LR 0.03530) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 87.47
2024-08-30 17:10:10,249 [podnet.py] => Task 0, Epoch 120/200 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 88.33
2024-08-30 17:10:12,068 [podnet.py] => Task 0, Epoch 121/200 (LR 0.03380) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 71.53
2024-08-30 17:10:13,834 [podnet.py] => Task 0, Epoch 122/200 (LR 0.03306) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.85, Test_acc 84.37
2024-08-30 17:10:15,623 [podnet.py] => Task 0, Epoch 123/200 (LR 0.03233) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 87.00
2024-08-30 17:10:17,476 [podnet.py] => Task 0, Epoch 124/200 (LR 0.03159) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.87
2024-08-30 17:10:19,206 [podnet.py] => Task 0, Epoch 125/200 (LR 0.03087) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 87.83
2024-08-30 17:10:20,952 [podnet.py] => Task 0, Epoch 126/200 (LR 0.03014) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.53
2024-08-30 17:10:22,746 [podnet.py] => Task 0, Epoch 127/200 (LR 0.02942) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.17
2024-08-30 17:10:24,585 [podnet.py] => Task 0, Epoch 128/200 (LR 0.02871) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.03
2024-08-30 17:10:26,791 [podnet.py] => Task 0, Epoch 129/200 (LR 0.02800) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 87.80
2024-08-30 17:10:28,755 [podnet.py] => Task 0, Epoch 130/200 (LR 0.02730) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.13
2024-08-30 17:10:30,885 [podnet.py] => Task 0, Epoch 131/200 (LR 0.02660) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.50
2024-08-30 17:10:32,705 [podnet.py] => Task 0, Epoch 132/200 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:10:34,812 [podnet.py] => Task 0, Epoch 133/200 (LR 0.02523) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.03
2024-08-30 17:10:37,036 [podnet.py] => Task 0, Epoch 134/200 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:10:38,689 [podnet.py] => Task 0, Epoch 135/200 (LR 0.02388) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:10:40,318 [podnet.py] => Task 0, Epoch 136/200 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 88.10
2024-08-30 17:10:42,004 [podnet.py] => Task 0, Epoch 137/200 (LR 0.02255) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 89.17
2024-08-30 17:10:43,771 [podnet.py] => Task 0, Epoch 138/200 (LR 0.02190) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 89.93
2024-08-30 17:10:45,580 [podnet.py] => Task 0, Epoch 139/200 (LR 0.02125) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:10:47,279 [podnet.py] => Task 0, Epoch 140/200 (LR 0.02061) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-08-30 17:10:48,939 [podnet.py] => Task 0, Epoch 141/200 (LR 0.01998) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 90.00
2024-08-30 17:10:50,637 [podnet.py] => Task 0, Epoch 142/200 (LR 0.01935) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 90.20
2024-08-30 17:10:52,461 [podnet.py] => Task 0, Epoch 143/200 (LR 0.01874) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 86.43
2024-08-30 17:10:54,185 [podnet.py] => Task 0, Epoch 144/200 (LR 0.01813) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 87.17
2024-08-30 17:10:55,925 [podnet.py] => Task 0, Epoch 145/200 (LR 0.01753) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.57
2024-08-30 17:10:57,572 [podnet.py] => Task 0, Epoch 146/200 (LR 0.01693) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.30
2024-08-30 17:10:59,225 [podnet.py] => Task 0, Epoch 147/200 (LR 0.01635) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:11:00,966 [podnet.py] => Task 0, Epoch 148/200 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 17:11:02,838 [podnet.py] => Task 0, Epoch 149/200 (LR 0.01520) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:11:04,625 [podnet.py] => Task 0, Epoch 150/200 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:11:06,375 [podnet.py] => Task 0, Epoch 151/200 (LR 0.01409) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:11:08,411 [podnet.py] => Task 0, Epoch 152/200 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:11:10,312 [podnet.py] => Task 0, Epoch 153/200 (LR 0.01302) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:11:12,205 [podnet.py] => Task 0, Epoch 154/200 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-30 17:11:14,257 [podnet.py] => Task 0, Epoch 155/200 (LR 0.01198) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.60
2024-08-30 17:11:16,059 [podnet.py] => Task 0, Epoch 156/200 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 88.73
2024-08-30 17:11:17,855 [podnet.py] => Task 0, Epoch 157/200 (LR 0.01098) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:11:19,664 [podnet.py] => Task 0, Epoch 158/200 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-30 17:11:21,421 [podnet.py] => Task 0, Epoch 159/200 (LR 0.01002) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.83
2024-08-30 17:11:23,122 [podnet.py] => Task 0, Epoch 160/200 (LR 0.00955) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.47
2024-08-30 17:11:24,797 [podnet.py] => Task 0, Epoch 161/200 (LR 0.00909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.73
2024-08-30 17:11:26,543 [podnet.py] => Task 0, Epoch 162/200 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 17:11:28,391 [podnet.py] => Task 0, Epoch 163/200 (LR 0.00821) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:11:30,301 [podnet.py] => Task 0, Epoch 164/200 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:11:32,238 [podnet.py] => Task 0, Epoch 165/200 (LR 0.00737) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:11:34,034 [podnet.py] => Task 0, Epoch 166/200 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:11:35,780 [podnet.py] => Task 0, Epoch 167/200 (LR 0.00657) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:11:37,524 [podnet.py] => Task 0, Epoch 168/200 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:11:39,270 [podnet.py] => Task 0, Epoch 169/200 (LR 0.00581) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:11:41,224 [podnet.py] => Task 0, Epoch 170/200 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:11:43,041 [podnet.py] => Task 0, Epoch 171/200 (LR 0.00510) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:11:44,873 [podnet.py] => Task 0, Epoch 172/200 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:11:46,549 [podnet.py] => Task 0, Epoch 173/200 (LR 0.00443) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:11:48,390 [podnet.py] => Task 0, Epoch 174/200 (LR 0.00411) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.03
2024-08-30 17:11:50,150 [podnet.py] => Task 0, Epoch 175/200 (LR 0.00381) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.07
2024-08-30 17:11:51,914 [podnet.py] => Task 0, Epoch 176/200 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:11:53,632 [podnet.py] => Task 0, Epoch 177/200 (LR 0.00323) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:11:55,222 [podnet.py] => Task 0, Epoch 178/200 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-30 17:11:56,903 [podnet.py] => Task 0, Epoch 179/200 (LR 0.00270) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.93
2024-08-30 17:11:58,714 [podnet.py] => Task 0, Epoch 180/200 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:12:00,747 [podnet.py] => Task 0, Epoch 181/200 (LR 0.00221) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.47
2024-08-30 17:12:02,708 [podnet.py] => Task 0, Epoch 182/200 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.37
2024-08-30 17:12:04,488 [podnet.py] => Task 0, Epoch 183/200 (LR 0.00177) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:12:06,218 [podnet.py] => Task 0, Epoch 184/200 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:12:07,910 [podnet.py] => Task 0, Epoch 185/200 (LR 0.00138) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:12:09,645 [podnet.py] => Task 0, Epoch 186/200 (LR 0.00120) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.43
2024-08-30 17:12:11,285 [podnet.py] => Task 0, Epoch 187/200 (LR 0.00104) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:12:13,256 [podnet.py] => Task 0, Epoch 188/200 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:12:14,862 [podnet.py] => Task 0, Epoch 189/200 (LR 0.00074) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:12:16,744 [podnet.py] => Task 0, Epoch 190/200 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:12:18,466 [podnet.py] => Task 0, Epoch 191/200 (LR 0.00050) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:12:20,153 [podnet.py] => Task 0, Epoch 192/200 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:12:21,884 [podnet.py] => Task 0, Epoch 193/200 (LR 0.00030) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:12:23,544 [podnet.py] => Task 0, Epoch 194/200 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:12:25,231 [podnet.py] => Task 0, Epoch 195/200 (LR 0.00015) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:12:26,952 [podnet.py] => Task 0, Epoch 196/200 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:12:28,755 [podnet.py] => Task 0, Epoch 197/200 (LR 0.00006) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:12:30,796 [podnet.py] => Task 0, Epoch 198/200 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:12:32,689 [podnet.py] => Task 0, Epoch 199/200 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:12:34,295 [podnet.py] => Task 0, Epoch 200/200 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:12:34,608 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:12:34,609 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:12:40,529 [podnet.py] => Exemplar size: 500
2024-08-30 17:12:40,530 [trainer.py] => CNN: {'total': 89.63, '00-04': 89.63, 'old': 0, 'new': 89.63}
2024-08-30 17:12:40,530 [trainer.py] => NME: {'total': 89.6, '00-04': 89.6, 'old': 0, 'new': 89.6}
2024-08-30 17:12:40,530 [trainer.py] => CNN top1 curve: [89.63]
2024-08-30 17:12:40,530 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 17:12:40,530 [trainer.py] => NME top1 curve: [89.6]
2024-08-30 17:12:40,530 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 17:12:40,530 [trainer.py] => Average Accuracy (CNN): 89.63
2024-08-30 17:12:40,530 [trainer.py] => Average Accuracy (NME): 89.6
2024-08-30 17:12:40,530 [trainer.py] => All params: 3869505
2024-08-30 17:12:40,530 [trainer.py] => Trainable params: 3869505
2024-08-30 17:12:40,531 [podnet.py] => Learning on 5-7
2024-08-30 17:12:40,549 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 17:12:42,218 [podnet.py] => Task 1, Epoch 1/200 (LR 0.09999) => LSC_loss 1.01, Spatial_loss 3.98, Flat_loss 0.50, Train_acc 73.29, Test_acc 28.29
2024-08-30 17:12:43,636 [podnet.py] => Task 1, Epoch 2/200 (LR 0.09998) => LSC_loss 0.48, Spatial_loss 2.96, Flat_loss 0.32, Train_acc 87.44, Test_acc 47.69
2024-08-30 17:12:45,033 [podnet.py] => Task 1, Epoch 3/200 (LR 0.09994) => LSC_loss 0.35, Spatial_loss 2.57, Flat_loss 0.27, Train_acc 91.31, Test_acc 58.76
2024-08-30 17:12:46,434 [podnet.py] => Task 1, Epoch 4/200 (LR 0.09990) => LSC_loss 0.28, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 93.56, Test_acc 47.17
2024-08-30 17:12:47,857 [podnet.py] => Task 1, Epoch 5/200 (LR 0.09985) => LSC_loss 0.24, Spatial_loss 2.10, Flat_loss 0.21, Train_acc 94.47, Test_acc 57.14
2024-08-30 17:12:49,285 [podnet.py] => Task 1, Epoch 6/200 (LR 0.09978) => LSC_loss 0.22, Spatial_loss 2.03, Flat_loss 0.20, Train_acc 94.84, Test_acc 59.33
2024-08-30 17:12:50,731 [podnet.py] => Task 1, Epoch 7/200 (LR 0.09970) => LSC_loss 0.19, Spatial_loss 1.90, Flat_loss 0.18, Train_acc 96.11, Test_acc 55.79
2024-08-30 17:12:52,116 [podnet.py] => Task 1, Epoch 8/200 (LR 0.09961) => LSC_loss 0.18, Spatial_loss 1.90, Flat_loss 0.18, Train_acc 96.00, Test_acc 56.14
2024-08-30 17:12:53,526 [podnet.py] => Task 1, Epoch 9/200 (LR 0.09950) => LSC_loss 0.19, Spatial_loss 1.92, Flat_loss 0.19, Train_acc 95.93, Test_acc 55.17
2024-08-30 17:12:54,947 [podnet.py] => Task 1, Epoch 10/200 (LR 0.09938) => LSC_loss 0.18, Spatial_loss 2.01, Flat_loss 0.19, Train_acc 95.84, Test_acc 57.64
2024-08-30 17:12:56,348 [podnet.py] => Task 1, Epoch 11/200 (LR 0.09926) => LSC_loss 0.15, Spatial_loss 1.81, Flat_loss 0.17, Train_acc 96.69, Test_acc 60.21
2024-08-30 17:12:57,763 [podnet.py] => Task 1, Epoch 12/200 (LR 0.09911) => LSC_loss 0.14, Spatial_loss 1.69, Flat_loss 0.16, Train_acc 97.60, Test_acc 59.10
2024-08-30 17:12:59,151 [podnet.py] => Task 1, Epoch 13/200 (LR 0.09896) => LSC_loss 0.12, Spatial_loss 1.69, Flat_loss 0.16, Train_acc 97.98, Test_acc 58.95
2024-08-30 17:13:00,518 [podnet.py] => Task 1, Epoch 14/200 (LR 0.09880) => LSC_loss 0.12, Spatial_loss 1.65, Flat_loss 0.15, Train_acc 98.38, Test_acc 60.98
2024-08-30 17:13:01,923 [podnet.py] => Task 1, Epoch 15/200 (LR 0.09862) => LSC_loss 0.11, Spatial_loss 1.60, Flat_loss 0.15, Train_acc 98.76, Test_acc 61.98
2024-08-30 17:13:03,273 [podnet.py] => Task 1, Epoch 16/200 (LR 0.09843) => LSC_loss 0.12, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 98.02, Test_acc 58.62
2024-08-30 17:13:04,664 [podnet.py] => Task 1, Epoch 17/200 (LR 0.09823) => LSC_loss 0.12, Spatial_loss 1.70, Flat_loss 0.16, Train_acc 97.98, Test_acc 59.07
2024-08-30 17:13:06,095 [podnet.py] => Task 1, Epoch 18/200 (LR 0.09801) => LSC_loss 0.10, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 98.89, Test_acc 61.21
2024-08-30 17:13:07,514 [podnet.py] => Task 1, Epoch 19/200 (LR 0.09779) => LSC_loss 0.11, Spatial_loss 1.61, Flat_loss 0.15, Train_acc 98.56, Test_acc 59.26
2024-08-30 17:13:08,891 [podnet.py] => Task 1, Epoch 20/200 (LR 0.09755) => LSC_loss 0.10, Spatial_loss 1.61, Flat_loss 0.15, Train_acc 99.00, Test_acc 61.52
2024-08-30 17:13:10,279 [podnet.py] => Task 1, Epoch 21/200 (LR 0.09730) => LSC_loss 0.09, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 98.96, Test_acc 60.79
2024-08-30 17:13:11,674 [podnet.py] => Task 1, Epoch 22/200 (LR 0.09704) => LSC_loss 0.09, Spatial_loss 1.54, Flat_loss 0.14, Train_acc 99.47, Test_acc 64.38
2024-08-30 17:13:13,320 [podnet.py] => Task 1, Epoch 23/200 (LR 0.09677) => LSC_loss 0.08, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 99.51, Test_acc 61.79
2024-08-30 17:13:14,754 [podnet.py] => Task 1, Epoch 24/200 (LR 0.09649) => LSC_loss 0.08, Spatial_loss 1.49, Flat_loss 0.13, Train_acc 99.51, Test_acc 61.36
2024-08-30 17:13:16,288 [podnet.py] => Task 1, Epoch 25/200 (LR 0.09619) => LSC_loss 0.08, Spatial_loss 1.56, Flat_loss 0.14, Train_acc 99.42, Test_acc 70.52
2024-08-30 17:13:17,690 [podnet.py] => Task 1, Epoch 26/200 (LR 0.09589) => LSC_loss 0.08, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 99.18, Test_acc 59.33
2024-08-30 17:13:19,194 [podnet.py] => Task 1, Epoch 27/200 (LR 0.09557) => LSC_loss 0.09, Spatial_loss 1.52, Flat_loss 0.14, Train_acc 99.20, Test_acc 61.88
2024-08-30 17:13:20,708 [podnet.py] => Task 1, Epoch 28/200 (LR 0.09524) => LSC_loss 0.09, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 98.93, Test_acc 66.57
2024-08-30 17:13:22,198 [podnet.py] => Task 1, Epoch 29/200 (LR 0.09490) => LSC_loss 0.07, Spatial_loss 1.45, Flat_loss 0.13, Train_acc 99.69, Test_acc 62.07
2024-08-30 17:13:23,710 [podnet.py] => Task 1, Epoch 30/200 (LR 0.09455) => LSC_loss 0.07, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 99.62, Test_acc 63.48
2024-08-30 17:13:25,232 [podnet.py] => Task 1, Epoch 31/200 (LR 0.09419) => LSC_loss 0.07, Spatial_loss 1.46, Flat_loss 0.13, Train_acc 99.64, Test_acc 63.24
2024-08-30 17:13:26,793 [podnet.py] => Task 1, Epoch 32/200 (LR 0.09382) => LSC_loss 0.09, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 98.98, Test_acc 65.88
2024-08-30 17:13:28,226 [podnet.py] => Task 1, Epoch 33/200 (LR 0.09343) => LSC_loss 0.08, Spatial_loss 1.48, Flat_loss 0.14, Train_acc 99.22, Test_acc 64.83
2024-08-30 17:13:29,590 [podnet.py] => Task 1, Epoch 34/200 (LR 0.09304) => LSC_loss 0.07, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 99.64, Test_acc 62.12
2024-08-30 17:13:30,996 [podnet.py] => Task 1, Epoch 35/200 (LR 0.09263) => LSC_loss 0.07, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 99.64, Test_acc 60.24
2024-08-30 17:13:32,468 [podnet.py] => Task 1, Epoch 36/200 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 1.43, Flat_loss 0.13, Train_acc 99.67, Test_acc 66.36
2024-08-30 17:13:33,942 [podnet.py] => Task 1, Epoch 37/200 (LR 0.09179) => LSC_loss 0.06, Spatial_loss 1.43, Flat_loss 0.13, Train_acc 99.71, Test_acc 63.14
2024-08-30 17:13:35,445 [podnet.py] => Task 1, Epoch 38/200 (LR 0.09135) => LSC_loss 0.06, Spatial_loss 1.40, Flat_loss 0.12, Train_acc 99.71, Test_acc 61.21
2024-08-30 17:13:36,917 [podnet.py] => Task 1, Epoch 39/200 (LR 0.09091) => LSC_loss 0.06, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 99.73, Test_acc 60.76
2024-08-30 17:13:38,409 [podnet.py] => Task 1, Epoch 40/200 (LR 0.09045) => LSC_loss 0.06, Spatial_loss 1.41, Flat_loss 0.13, Train_acc 99.60, Test_acc 62.64
2024-08-30 17:13:39,869 [podnet.py] => Task 1, Epoch 41/200 (LR 0.08998) => LSC_loss 0.06, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 99.58, Test_acc 62.43
2024-08-30 17:13:41,322 [podnet.py] => Task 1, Epoch 42/200 (LR 0.08951) => LSC_loss 0.06, Spatial_loss 1.35, Flat_loss 0.12, Train_acc 99.82, Test_acc 66.07
2024-08-30 17:13:42,838 [podnet.py] => Task 1, Epoch 43/200 (LR 0.08902) => LSC_loss 0.06, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 99.71, Test_acc 64.86
2024-08-30 17:13:44,318 [podnet.py] => Task 1, Epoch 44/200 (LR 0.08853) => LSC_loss 0.06, Spatial_loss 1.36, Flat_loss 0.12, Train_acc 99.76, Test_acc 63.02
2024-08-30 17:13:45,858 [podnet.py] => Task 1, Epoch 45/200 (LR 0.08802) => LSC_loss 0.06, Spatial_loss 1.41, Flat_loss 0.12, Train_acc 99.76, Test_acc 64.36
2024-08-30 17:13:47,341 [podnet.py] => Task 1, Epoch 46/200 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 99.84, Test_acc 64.40
2024-08-30 17:13:48,748 [podnet.py] => Task 1, Epoch 47/200 (LR 0.08698) => LSC_loss 0.06, Spatial_loss 1.38, Flat_loss 0.12, Train_acc 99.73, Test_acc 62.86
2024-08-30 17:13:50,179 [podnet.py] => Task 1, Epoch 48/200 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 1.39, Flat_loss 0.12, Train_acc 99.89, Test_acc 63.14
2024-08-30 17:13:51,639 [podnet.py] => Task 1, Epoch 49/200 (LR 0.08591) => LSC_loss 0.06, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 99.71, Test_acc 65.60
2024-08-30 17:13:53,052 [podnet.py] => Task 1, Epoch 50/200 (LR 0.08536) => LSC_loss 0.06, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 99.80, Test_acc 67.19
2024-08-30 17:13:54,628 [podnet.py] => Task 1, Epoch 51/200 (LR 0.08480) => LSC_loss 0.05, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 99.93, Test_acc 64.10
2024-08-30 17:13:56,120 [podnet.py] => Task 1, Epoch 52/200 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 1.36, Flat_loss 0.12, Train_acc 99.91, Test_acc 66.81
2024-08-30 17:13:57,649 [podnet.py] => Task 1, Epoch 53/200 (LR 0.08365) => LSC_loss 0.06, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 99.69, Test_acc 67.19
2024-08-30 17:13:59,079 [podnet.py] => Task 1, Epoch 54/200 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 1.35, Flat_loss 0.12, Train_acc 99.96, Test_acc 68.29
2024-08-30 17:14:00,489 [podnet.py] => Task 1, Epoch 55/200 (LR 0.08247) => LSC_loss 0.06, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 99.67, Test_acc 64.07
2024-08-30 17:14:01,906 [podnet.py] => Task 1, Epoch 56/200 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 99.89, Test_acc 64.45
2024-08-30 17:14:03,283 [podnet.py] => Task 1, Epoch 57/200 (LR 0.08126) => LSC_loss 0.05, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 99.96, Test_acc 66.69
2024-08-30 17:14:04,781 [podnet.py] => Task 1, Epoch 58/200 (LR 0.08065) => LSC_loss 0.05, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 99.82, Test_acc 65.50
2024-08-30 17:14:06,285 [podnet.py] => Task 1, Epoch 59/200 (LR 0.08002) => LSC_loss 0.06, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 99.82, Test_acc 63.64
2024-08-30 17:14:07,833 [podnet.py] => Task 1, Epoch 60/200 (LR 0.07939) => LSC_loss 0.07, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 99.51, Test_acc 67.14
2024-08-30 17:14:09,446 [podnet.py] => Task 1, Epoch 61/200 (LR 0.07875) => LSC_loss 0.06, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 99.64, Test_acc 63.67
2024-08-30 17:14:10,903 [podnet.py] => Task 1, Epoch 62/200 (LR 0.07810) => LSC_loss 0.05, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 99.84, Test_acc 61.55
2024-08-30 17:14:12,384 [podnet.py] => Task 1, Epoch 63/200 (LR 0.07745) => LSC_loss 0.05, Spatial_loss 1.32, Flat_loss 0.11, Train_acc 99.91, Test_acc 65.98
2024-08-30 17:14:14,005 [podnet.py] => Task 1, Epoch 64/200 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 99.96, Test_acc 68.05
2024-08-30 17:14:15,387 [podnet.py] => Task 1, Epoch 65/200 (LR 0.07612) => LSC_loss 0.05, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 99.91, Test_acc 66.83
2024-08-30 17:14:16,856 [podnet.py] => Task 1, Epoch 66/200 (LR 0.07545) => LSC_loss 0.06, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 99.67, Test_acc 65.76
2024-08-30 17:14:18,185 [podnet.py] => Task 1, Epoch 67/200 (LR 0.07477) => LSC_loss 0.05, Spatial_loss 1.31, Flat_loss 0.11, Train_acc 99.96, Test_acc 68.02
2024-08-30 17:14:19,605 [podnet.py] => Task 1, Epoch 68/200 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 1.30, Flat_loss 0.11, Train_acc 99.93, Test_acc 60.79
2024-08-30 17:14:21,069 [podnet.py] => Task 1, Epoch 69/200 (LR 0.07340) => LSC_loss 0.05, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 99.87, Test_acc 61.36
2024-08-30 17:14:22,540 [podnet.py] => Task 1, Epoch 70/200 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 99.78, Test_acc 62.45
2024-08-30 17:14:23,944 [podnet.py] => Task 1, Epoch 71/200 (LR 0.07200) => LSC_loss 0.05, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 99.91, Test_acc 64.38
2024-08-30 17:14:25,577 [podnet.py] => Task 1, Epoch 72/200 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 1.26, Flat_loss 0.11, Train_acc 99.91, Test_acc 66.60
2024-08-30 17:14:27,175 [podnet.py] => Task 1, Epoch 73/200 (LR 0.07058) => LSC_loss 0.05, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 99.93, Test_acc 69.05
2024-08-30 17:14:28,648 [podnet.py] => Task 1, Epoch 74/200 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 1.31, Flat_loss 0.11, Train_acc 99.87, Test_acc 63.40
2024-08-30 17:14:30,072 [podnet.py] => Task 1, Epoch 75/200 (LR 0.06913) => LSC_loss 0.05, Spatial_loss 1.32, Flat_loss 0.11, Train_acc 99.89, Test_acc 67.48
2024-08-30 17:14:31,674 [podnet.py] => Task 1, Epoch 76/200 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 1.30, Flat_loss 0.11, Train_acc 99.93, Test_acc 65.76
2024-08-30 17:14:33,195 [podnet.py] => Task 1, Epoch 77/200 (LR 0.06767) => LSC_loss 0.05, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 99.98, Test_acc 64.36
2024-08-30 17:14:34,658 [podnet.py] => Task 1, Epoch 78/200 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.45
2024-08-30 17:14:36,294 [podnet.py] => Task 1, Epoch 79/200 (LR 0.06620) => LSC_loss 0.05, Spatial_loss 1.26, Flat_loss 0.11, Train_acc 99.91, Test_acc 64.69
2024-08-30 17:14:37,702 [podnet.py] => Task 1, Epoch 80/200 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 1.25, Flat_loss 0.11, Train_acc 99.98, Test_acc 64.95
2024-08-30 17:14:39,124 [podnet.py] => Task 1, Epoch 81/200 (LR 0.06470) => LSC_loss 0.05, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 99.87, Test_acc 65.45
2024-08-30 17:14:40,529 [podnet.py] => Task 1, Epoch 82/200 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 99.93, Test_acc 67.19
2024-08-30 17:14:41,979 [podnet.py] => Task 1, Epoch 83/200 (LR 0.06319) => LSC_loss 0.05, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 99.91, Test_acc 64.67
2024-08-30 17:14:43,482 [podnet.py] => Task 1, Epoch 84/200 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 99.98, Test_acc 64.19
2024-08-30 17:14:44,927 [podnet.py] => Task 1, Epoch 85/200 (LR 0.06167) => LSC_loss 0.05, Spatial_loss 1.25, Flat_loss 0.11, Train_acc 99.89, Test_acc 62.60
2024-08-30 17:14:46,361 [podnet.py] => Task 1, Epoch 86/200 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 99.82, Test_acc 64.29
2024-08-30 17:14:47,756 [podnet.py] => Task 1, Epoch 87/200 (LR 0.06014) => LSC_loss 0.05, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 99.93, Test_acc 65.05
2024-08-30 17:14:49,217 [podnet.py] => Task 1, Epoch 88/200 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 99.89, Test_acc 64.79
2024-08-30 17:14:50,645 [podnet.py] => Task 1, Epoch 89/200 (LR 0.05860) => LSC_loss 0.05, Spatial_loss 1.25, Flat_loss 0.11, Train_acc 99.98, Test_acc 62.60
2024-08-30 17:14:52,065 [podnet.py] => Task 1, Epoch 90/200 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 100.00, Test_acc 64.64
2024-08-30 17:14:53,798 [podnet.py] => Task 1, Epoch 91/200 (LR 0.05705) => LSC_loss 0.05, Spatial_loss 1.23, Flat_loss 0.11, Train_acc 99.93, Test_acc 63.60
2024-08-30 17:14:55,617 [podnet.py] => Task 1, Epoch 92/200 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 99.87, Test_acc 63.71
2024-08-30 17:14:57,286 [podnet.py] => Task 1, Epoch 93/200 (LR 0.05549) => LSC_loss 0.05, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 99.89, Test_acc 63.55
2024-08-30 17:14:58,798 [podnet.py] => Task 1, Epoch 94/200 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 99.98, Test_acc 63.52
2024-08-30 17:15:00,289 [podnet.py] => Task 1, Epoch 95/200 (LR 0.05392) => LSC_loss 0.05, Spatial_loss 1.23, Flat_loss 0.11, Train_acc 99.93, Test_acc 67.45
2024-08-30 17:15:01,630 [podnet.py] => Task 1, Epoch 96/200 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 1.17, Flat_loss 0.10, Train_acc 99.96, Test_acc 66.02
2024-08-30 17:15:03,059 [podnet.py] => Task 1, Epoch 97/200 (LR 0.05236) => LSC_loss 0.05, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 99.91, Test_acc 63.86
2024-08-30 17:15:04,521 [podnet.py] => Task 1, Epoch 98/200 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 1.18, Flat_loss 0.10, Train_acc 99.96, Test_acc 67.29
2024-08-30 17:15:05,939 [podnet.py] => Task 1, Epoch 99/200 (LR 0.05079) => LSC_loss 0.05, Spatial_loss 1.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.19
2024-08-30 17:15:07,370 [podnet.py] => Task 1, Epoch 100/200 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 1.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.26
2024-08-30 17:15:08,829 [podnet.py] => Task 1, Epoch 101/200 (LR 0.04921) => LSC_loss 0.05, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 99.93, Test_acc 67.02
2024-08-30 17:15:10,396 [podnet.py] => Task 1, Epoch 102/200 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 99.96, Test_acc 65.24
2024-08-30 17:15:11,863 [podnet.py] => Task 1, Epoch 103/200 (LR 0.04764) => LSC_loss 0.05, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 99.89, Test_acc 67.79
2024-08-30 17:15:13,365 [podnet.py] => Task 1, Epoch 104/200 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 99.93, Test_acc 66.14
2024-08-30 17:15:14,809 [podnet.py] => Task 1, Epoch 105/200 (LR 0.04608) => LSC_loss 0.05, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 99.93, Test_acc 65.26
2024-08-30 17:15:16,383 [podnet.py] => Task 1, Epoch 106/200 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 99.96, Test_acc 64.64
2024-08-30 17:15:17,997 [podnet.py] => Task 1, Epoch 107/200 (LR 0.04451) => LSC_loss 0.05, Spatial_loss 1.18, Flat_loss 0.10, Train_acc 99.89, Test_acc 65.67
2024-08-30 17:15:19,515 [podnet.py] => Task 1, Epoch 108/200 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 1.16, Flat_loss 0.10, Train_acc 99.96, Test_acc 64.93
2024-08-30 17:15:21,078 [podnet.py] => Task 1, Epoch 109/200 (LR 0.04295) => LSC_loss 0.05, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 99.91, Test_acc 65.79
2024-08-30 17:15:22,699 [podnet.py] => Task 1, Epoch 110/200 (LR 0.04218) => LSC_loss 0.05, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.86
2024-08-30 17:15:24,228 [podnet.py] => Task 1, Epoch 111/200 (LR 0.04140) => LSC_loss 0.04, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.67
2024-08-30 17:15:25,636 [podnet.py] => Task 1, Epoch 112/200 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 99.98, Test_acc 65.50
2024-08-30 17:15:26,979 [podnet.py] => Task 1, Epoch 113/200 (LR 0.03986) => LSC_loss 0.04, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 99.98, Test_acc 66.40
2024-08-30 17:15:28,289 [podnet.py] => Task 1, Epoch 114/200 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 1.14, Flat_loss 0.10, Train_acc 99.98, Test_acc 67.29
2024-08-30 17:15:29,823 [podnet.py] => Task 1, Epoch 115/200 (LR 0.03833) => LSC_loss 0.05, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 99.93, Test_acc 66.52
2024-08-30 17:15:31,737 [podnet.py] => Task 1, Epoch 116/200 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 63.26
2024-08-30 17:15:33,969 [podnet.py] => Task 1, Epoch 117/200 (LR 0.03681) => LSC_loss 0.04, Spatial_loss 1.17, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.43
2024-08-30 17:15:35,997 [podnet.py] => Task 1, Epoch 118/200 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 1.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.71
2024-08-30 17:15:37,500 [podnet.py] => Task 1, Epoch 119/200 (LR 0.03530) => LSC_loss 0.04, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.62
2024-08-30 17:15:39,003 [podnet.py] => Task 1, Epoch 120/200 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.62
2024-08-30 17:15:40,504 [podnet.py] => Task 1, Epoch 121/200 (LR 0.03380) => LSC_loss 0.05, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 99.96, Test_acc 65.45
2024-08-30 17:15:41,869 [podnet.py] => Task 1, Epoch 122/200 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.57
2024-08-30 17:15:43,250 [podnet.py] => Task 1, Epoch 123/200 (LR 0.03233) => LSC_loss 0.05, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 65.21
2024-08-30 17:15:44,612 [podnet.py] => Task 1, Epoch 124/200 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 99.93, Test_acc 66.67
2024-08-30 17:15:45,994 [podnet.py] => Task 1, Epoch 125/200 (LR 0.03087) => LSC_loss 0.05, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 99.96, Test_acc 67.00
2024-08-30 17:15:47,386 [podnet.py] => Task 1, Epoch 126/200 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 99.96, Test_acc 67.10
2024-08-30 17:15:48,947 [podnet.py] => Task 1, Epoch 127/200 (LR 0.02942) => LSC_loss 0.04, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 99.98, Test_acc 65.98
2024-08-30 17:15:50,343 [podnet.py] => Task 1, Epoch 128/200 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 99.98, Test_acc 63.62
2024-08-30 17:15:51,735 [podnet.py] => Task 1, Epoch 129/200 (LR 0.02800) => LSC_loss 0.04, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.31
2024-08-30 17:15:53,053 [podnet.py] => Task 1, Epoch 130/200 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.21
2024-08-30 17:15:54,422 [podnet.py] => Task 1, Epoch 131/200 (LR 0.02660) => LSC_loss 0.04, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.76
2024-08-30 17:15:55,798 [podnet.py] => Task 1, Epoch 132/200 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.50
2024-08-30 17:15:57,196 [podnet.py] => Task 1, Epoch 133/200 (LR 0.02523) => LSC_loss 0.04, Spatial_loss 1.08, Flat_loss 0.09, Train_acc 99.98, Test_acc 67.19
2024-08-30 17:15:58,544 [podnet.py] => Task 1, Epoch 134/200 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.17
2024-08-30 17:15:59,912 [podnet.py] => Task 1, Epoch 135/200 (LR 0.02388) => LSC_loss 0.04, Spatial_loss 1.06, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.48
2024-08-30 17:16:01,376 [podnet.py] => Task 1, Epoch 136/200 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.12
2024-08-30 17:16:02,944 [podnet.py] => Task 1, Epoch 137/200 (LR 0.02255) => LSC_loss 0.04, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.02
2024-08-30 17:16:04,389 [podnet.py] => Task 1, Epoch 138/200 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.69
2024-08-30 17:16:05,795 [podnet.py] => Task 1, Epoch 139/200 (LR 0.02125) => LSC_loss 0.04, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 99.98, Test_acc 67.05
2024-08-30 17:16:07,370 [podnet.py] => Task 1, Epoch 140/200 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.21
2024-08-30 17:16:08,742 [podnet.py] => Task 1, Epoch 141/200 (LR 0.01998) => LSC_loss 0.04, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 99.98, Test_acc 66.17
2024-08-30 17:16:10,180 [podnet.py] => Task 1, Epoch 142/200 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.00
2024-08-30 17:16:11,610 [podnet.py] => Task 1, Epoch 143/200 (LR 0.01874) => LSC_loss 0.04, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.83
2024-08-30 17:16:13,056 [podnet.py] => Task 1, Epoch 144/200 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.69
2024-08-30 17:16:14,436 [podnet.py] => Task 1, Epoch 145/200 (LR 0.01753) => LSC_loss 0.04, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.81
2024-08-30 17:16:15,797 [podnet.py] => Task 1, Epoch 146/200 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.36
2024-08-30 17:16:17,179 [podnet.py] => Task 1, Epoch 147/200 (LR 0.01635) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.50
2024-08-30 17:16:18,652 [podnet.py] => Task 1, Epoch 148/200 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.67
2024-08-30 17:16:20,067 [podnet.py] => Task 1, Epoch 149/200 (LR 0.01520) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 99.98, Test_acc 67.50
2024-08-30 17:16:21,439 [podnet.py] => Task 1, Epoch 150/200 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.45
2024-08-30 17:16:22,826 [podnet.py] => Task 1, Epoch 151/200 (LR 0.01409) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.57
2024-08-30 17:16:24,350 [podnet.py] => Task 1, Epoch 152/200 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.67
2024-08-30 17:16:25,807 [podnet.py] => Task 1, Epoch 153/200 (LR 0.01302) => LSC_loss 0.04, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.17
2024-08-30 17:16:27,237 [podnet.py] => Task 1, Epoch 154/200 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.19
2024-08-30 17:16:28,675 [podnet.py] => Task 1, Epoch 155/200 (LR 0.01198) => LSC_loss 0.04, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.71
2024-08-30 17:16:30,196 [podnet.py] => Task 1, Epoch 156/200 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.64
2024-08-30 17:16:31,606 [podnet.py] => Task 1, Epoch 157/200 (LR 0.01098) => LSC_loss 0.04, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.26
2024-08-30 17:16:32,984 [podnet.py] => Task 1, Epoch 158/200 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 99.98, Test_acc 66.93
2024-08-30 17:16:34,371 [podnet.py] => Task 1, Epoch 159/200 (LR 0.01002) => LSC_loss 0.04, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.69
2024-08-30 17:16:35,819 [podnet.py] => Task 1, Epoch 160/200 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.93
2024-08-30 17:16:37,217 [podnet.py] => Task 1, Epoch 161/200 (LR 0.00909) => LSC_loss 0.04, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.19
2024-08-30 17:16:38,714 [podnet.py] => Task 1, Epoch 162/200 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.24
2024-08-30 17:16:40,085 [podnet.py] => Task 1, Epoch 163/200 (LR 0.00821) => LSC_loss 0.04, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.60
2024-08-30 17:16:41,467 [podnet.py] => Task 1, Epoch 164/200 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.64
2024-08-30 17:16:42,839 [podnet.py] => Task 1, Epoch 165/200 (LR 0.00737) => LSC_loss 0.04, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.21
2024-08-30 17:16:44,153 [podnet.py] => Task 1, Epoch 166/200 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.83
2024-08-30 17:16:45,498 [podnet.py] => Task 1, Epoch 167/200 (LR 0.00657) => LSC_loss 0.04, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.67
2024-08-30 17:16:47,045 [podnet.py] => Task 1, Epoch 168/200 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.60
2024-08-30 17:16:48,664 [podnet.py] => Task 1, Epoch 169/200 (LR 0.00581) => LSC_loss 0.04, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.81
2024-08-30 17:16:49,992 [podnet.py] => Task 1, Epoch 170/200 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.86
2024-08-30 17:16:51,426 [podnet.py] => Task 1, Epoch 171/200 (LR 0.00510) => LSC_loss 0.04, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.79
2024-08-30 17:16:53,104 [podnet.py] => Task 1, Epoch 172/200 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.17
2024-08-30 17:16:54,553 [podnet.py] => Task 1, Epoch 173/200 (LR 0.00443) => LSC_loss 0.04, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.26
2024-08-30 17:16:55,968 [podnet.py] => Task 1, Epoch 174/200 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.02
2024-08-30 17:16:57,364 [podnet.py] => Task 1, Epoch 175/200 (LR 0.00381) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.62
2024-08-30 17:16:58,729 [podnet.py] => Task 1, Epoch 176/200 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.76
2024-08-30 17:17:00,150 [podnet.py] => Task 1, Epoch 177/200 (LR 0.00323) => LSC_loss 0.04, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.31
2024-08-30 17:17:01,572 [podnet.py] => Task 1, Epoch 178/200 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.81
2024-08-30 17:17:02,942 [podnet.py] => Task 1, Epoch 179/200 (LR 0.00270) => LSC_loss 0.04, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.38
2024-08-30 17:17:04,327 [podnet.py] => Task 1, Epoch 180/200 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.79
2024-08-30 17:17:05,801 [podnet.py] => Task 1, Epoch 181/200 (LR 0.00221) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.76
2024-08-30 17:17:07,165 [podnet.py] => Task 1, Epoch 182/200 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.76
2024-08-30 17:17:08,499 [podnet.py] => Task 1, Epoch 183/200 (LR 0.00177) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.45
2024-08-30 17:17:09,894 [podnet.py] => Task 1, Epoch 184/200 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.71
2024-08-30 17:17:11,369 [podnet.py] => Task 1, Epoch 185/200 (LR 0.00138) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.67
2024-08-30 17:17:12,736 [podnet.py] => Task 1, Epoch 186/200 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.36
2024-08-30 17:17:14,129 [podnet.py] => Task 1, Epoch 187/200 (LR 0.00104) => LSC_loss 0.04, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.00
2024-08-30 17:17:15,659 [podnet.py] => Task 1, Epoch 188/200 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.86
2024-08-30 17:17:17,061 [podnet.py] => Task 1, Epoch 189/200 (LR 0.00074) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.26
2024-08-30 17:17:18,428 [podnet.py] => Task 1, Epoch 190/200 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.00
2024-08-30 17:17:19,780 [podnet.py] => Task 1, Epoch 191/200 (LR 0.00050) => LSC_loss 0.04, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.71
2024-08-30 17:17:21,321 [podnet.py] => Task 1, Epoch 192/200 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.52
2024-08-30 17:17:22,638 [podnet.py] => Task 1, Epoch 193/200 (LR 0.00030) => LSC_loss 0.04, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.57
2024-08-30 17:17:24,001 [podnet.py] => Task 1, Epoch 194/200 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.86
2024-08-30 17:17:25,386 [podnet.py] => Task 1, Epoch 195/200 (LR 0.00015) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.79
2024-08-30 17:17:26,866 [podnet.py] => Task 1, Epoch 196/200 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.43
2024-08-30 17:17:28,410 [podnet.py] => Task 1, Epoch 197/200 (LR 0.00006) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.55
2024-08-30 17:17:29,738 [podnet.py] => Task 1, Epoch 198/200 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.98
2024-08-30 17:17:31,113 [podnet.py] => Task 1, Epoch 199/200 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.67
2024-08-30 17:17:32,491 [podnet.py] => Task 1, Epoch 200/200 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.95
2024-08-30 17:17:32,865 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:17:32,865 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:17:34,113 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:17:36,239 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:17:37,456 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:17:40,301 [podnet.py] => Exemplar size: 497
2024-08-30 17:17:40,301 [trainer.py] => CNN: {'total': 68.95, '00-04': 57.9, '05-06': 96.58, 'old': 57.9, 'new': 96.58}
2024-08-30 17:17:40,301 [trainer.py] => NME: {'total': 75.17, '00-04': 78.13, '05-06': 67.75, 'old': 78.13, 'new': 67.75}
2024-08-30 17:17:40,301 [trainer.py] => CNN top1 curve: [89.63, 68.95]
2024-08-30 17:17:40,301 [trainer.py] => CNN top5 curve: [100.0, 98.36]
2024-08-30 17:17:40,302 [trainer.py] => NME top1 curve: [89.6, 75.17]
2024-08-30 17:17:40,302 [trainer.py] => NME top5 curve: [100.0, 98.31]

2024-08-30 17:17:40,302 [trainer.py] => Average Accuracy (CNN): 79.28999999999999
2024-08-30 17:17:40,302 [trainer.py] => Average Accuracy (NME): 82.38499999999999
2024-08-30 17:17:40,302 [trainer.py] => All params: 3879745
2024-08-30 17:17:40,302 [trainer.py] => Trainable params: 3879745
2024-08-30 17:17:40,303 [podnet.py] => Learning on 7-9
2024-08-30 17:17:40,326 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 17:17:41,903 [podnet.py] => Task 2, Epoch 1/200 (LR 0.09999) => LSC_loss 1.25, Spatial_loss 2.45, Flat_loss 0.32, Train_acc 77.54, Test_acc 33.85
2024-08-30 17:17:43,371 [podnet.py] => Task 2, Epoch 2/200 (LR 0.09998) => LSC_loss 0.38, Spatial_loss 2.15, Flat_loss 0.22, Train_acc 91.59, Test_acc 45.67
2024-08-30 17:17:44,896 [podnet.py] => Task 2, Epoch 3/200 (LR 0.09994) => LSC_loss 0.28, Spatial_loss 1.85, Flat_loss 0.18, Train_acc 94.60, Test_acc 49.30
2024-08-30 17:17:46,305 [podnet.py] => Task 2, Epoch 4/200 (LR 0.09990) => LSC_loss 0.23, Spatial_loss 1.77, Flat_loss 0.17, Train_acc 95.66, Test_acc 55.04
2024-08-30 17:17:47,745 [podnet.py] => Task 2, Epoch 5/200 (LR 0.09985) => LSC_loss 0.19, Spatial_loss 1.62, Flat_loss 0.16, Train_acc 97.29, Test_acc 56.65
2024-08-30 17:17:49,211 [podnet.py] => Task 2, Epoch 6/200 (LR 0.09978) => LSC_loss 0.16, Spatial_loss 1.52, Flat_loss 0.14, Train_acc 98.44, Test_acc 52.50
2024-08-30 17:17:50,645 [podnet.py] => Task 2, Epoch 7/200 (LR 0.09970) => LSC_loss 0.14, Spatial_loss 1.44, Flat_loss 0.14, Train_acc 98.75, Test_acc 53.15
2024-08-30 17:17:52,105 [podnet.py] => Task 2, Epoch 8/200 (LR 0.09961) => LSC_loss 0.14, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 99.00, Test_acc 56.85
2024-08-30 17:17:53,524 [podnet.py] => Task 2, Epoch 9/200 (LR 0.09950) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 99.18, Test_acc 54.30
2024-08-30 17:17:54,950 [podnet.py] => Task 2, Epoch 10/200 (LR 0.09938) => LSC_loss 0.12, Spatial_loss 1.32, Flat_loss 0.13, Train_acc 99.24, Test_acc 55.50
2024-08-30 17:17:56,364 [podnet.py] => Task 2, Epoch 11/200 (LR 0.09926) => LSC_loss 0.12, Spatial_loss 1.39, Flat_loss 0.13, Train_acc 99.13, Test_acc 53.80
2024-08-30 17:17:57,748 [podnet.py] => Task 2, Epoch 12/200 (LR 0.09911) => LSC_loss 0.11, Spatial_loss 1.29, Flat_loss 0.13, Train_acc 99.27, Test_acc 57.17
2024-08-30 17:17:59,200 [podnet.py] => Task 2, Epoch 13/200 (LR 0.09896) => LSC_loss 0.10, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 99.71, Test_acc 56.89
2024-08-30 17:18:00,617 [podnet.py] => Task 2, Epoch 14/200 (LR 0.09880) => LSC_loss 0.09, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 99.78, Test_acc 58.39
2024-08-30 17:18:02,082 [podnet.py] => Task 2, Epoch 15/200 (LR 0.09862) => LSC_loss 0.09, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 99.78, Test_acc 55.17
2024-08-30 17:18:03,547 [podnet.py] => Task 2, Epoch 16/200 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 99.84, Test_acc 54.35
2024-08-30 17:18:05,012 [podnet.py] => Task 2, Epoch 17/200 (LR 0.09823) => LSC_loss 0.09, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 99.69, Test_acc 57.67
2024-08-30 17:18:06,452 [podnet.py] => Task 2, Epoch 18/200 (LR 0.09801) => LSC_loss 0.09, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 99.67, Test_acc 55.26
2024-08-30 17:18:08,004 [podnet.py] => Task 2, Epoch 19/200 (LR 0.09779) => LSC_loss 0.08, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 99.87, Test_acc 53.65
2024-08-30 17:18:09,503 [podnet.py] => Task 2, Epoch 20/200 (LR 0.09755) => LSC_loss 0.08, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 99.82, Test_acc 54.19
2024-08-30 17:18:10,985 [podnet.py] => Task 2, Epoch 21/200 (LR 0.09730) => LSC_loss 0.08, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 99.80, Test_acc 55.63
2024-08-30 17:18:12,389 [podnet.py] => Task 2, Epoch 22/200 (LR 0.09704) => LSC_loss 0.08, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 99.80, Test_acc 52.31
2024-08-30 17:18:13,803 [podnet.py] => Task 2, Epoch 23/200 (LR 0.09677) => LSC_loss 0.08, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 99.80, Test_acc 56.94
2024-08-30 17:18:15,293 [podnet.py] => Task 2, Epoch 24/200 (LR 0.09649) => LSC_loss 0.08, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 99.84, Test_acc 54.72
2024-08-30 17:18:16,816 [podnet.py] => Task 2, Epoch 25/200 (LR 0.09619) => LSC_loss 0.08, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 99.76, Test_acc 50.26
2024-08-30 17:18:18,278 [podnet.py] => Task 2, Epoch 26/200 (LR 0.09589) => LSC_loss 0.10, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 99.29, Test_acc 58.20
2024-08-30 17:18:19,737 [podnet.py] => Task 2, Epoch 27/200 (LR 0.09557) => LSC_loss 0.08, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 99.76, Test_acc 56.33
2024-08-30 17:18:21,165 [podnet.py] => Task 2, Epoch 28/200 (LR 0.09524) => LSC_loss 0.07, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 99.91, Test_acc 58.26
2024-08-30 17:18:22,622 [podnet.py] => Task 2, Epoch 29/200 (LR 0.09490) => LSC_loss 0.07, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 99.91, Test_acc 57.19
2024-08-30 17:18:24,163 [podnet.py] => Task 2, Epoch 30/200 (LR 0.09455) => LSC_loss 0.08, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 99.73, Test_acc 54.30
2024-08-30 17:18:25,709 [podnet.py] => Task 2, Epoch 31/200 (LR 0.09419) => LSC_loss 0.07, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 99.87, Test_acc 58.67
2024-08-30 17:18:27,168 [podnet.py] => Task 2, Epoch 32/200 (LR 0.09382) => LSC_loss 0.07, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 99.96, Test_acc 56.76
2024-08-30 17:18:28,715 [podnet.py] => Task 2, Epoch 33/200 (LR 0.09343) => LSC_loss 0.07, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 99.89, Test_acc 57.22
2024-08-30 17:18:30,255 [podnet.py] => Task 2, Epoch 34/200 (LR 0.09304) => LSC_loss 0.07, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 99.91, Test_acc 53.20
2024-08-30 17:18:31,803 [podnet.py] => Task 2, Epoch 35/200 (LR 0.09263) => LSC_loss 0.07, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 99.98, Test_acc 50.39
2024-08-30 17:18:33,332 [podnet.py] => Task 2, Epoch 36/200 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 99.93, Test_acc 56.31
2024-08-30 17:18:34,709 [podnet.py] => Task 2, Epoch 37/200 (LR 0.09179) => LSC_loss 0.07, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 99.93, Test_acc 57.41
2024-08-30 17:18:36,063 [podnet.py] => Task 2, Epoch 38/200 (LR 0.09135) => LSC_loss 0.06, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.22
2024-08-30 17:18:37,498 [podnet.py] => Task 2, Epoch 39/200 (LR 0.09091) => LSC_loss 0.07, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 99.91, Test_acc 58.93
2024-08-30 17:18:39,107 [podnet.py] => Task 2, Epoch 40/200 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 99.93, Test_acc 59.94
2024-08-30 17:18:40,695 [podnet.py] => Task 2, Epoch 41/200 (LR 0.08998) => LSC_loss 0.07, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 99.89, Test_acc 57.26
2024-08-30 17:18:42,123 [podnet.py] => Task 2, Epoch 42/200 (LR 0.08951) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 99.98, Test_acc 53.83
2024-08-30 17:18:43,590 [podnet.py] => Task 2, Epoch 43/200 (LR 0.08902) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 99.91, Test_acc 56.11
2024-08-30 17:18:45,147 [podnet.py] => Task 2, Epoch 44/200 (LR 0.08853) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 100.00, Test_acc 55.89
2024-08-30 17:18:46,972 [podnet.py] => Task 2, Epoch 45/200 (LR 0.08802) => LSC_loss 0.07, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 99.87, Test_acc 51.04
2024-08-30 17:18:48,467 [podnet.py] => Task 2, Epoch 46/200 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 99.98, Test_acc 56.37
2024-08-30 17:18:49,987 [podnet.py] => Task 2, Epoch 47/200 (LR 0.08698) => LSC_loss 0.07, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 99.93, Test_acc 57.35
2024-08-30 17:18:51,393 [podnet.py] => Task 2, Epoch 48/200 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 99.98, Test_acc 56.63
2024-08-30 17:18:52,844 [podnet.py] => Task 2, Epoch 49/200 (LR 0.08591) => LSC_loss 0.06, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 99.91, Test_acc 56.57
2024-08-30 17:18:54,687 [podnet.py] => Task 2, Epoch 50/200 (LR 0.08536) => LSC_loss 0.06, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 100.00, Test_acc 54.43
2024-08-30 17:18:56,351 [podnet.py] => Task 2, Epoch 51/200 (LR 0.08480) => LSC_loss 0.06, Spatial_loss 1.06, Flat_loss 0.09, Train_acc 99.98, Test_acc 55.04
2024-08-30 17:18:57,869 [podnet.py] => Task 2, Epoch 52/200 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 99.93, Test_acc 54.50
2024-08-30 17:18:59,249 [podnet.py] => Task 2, Epoch 53/200 (LR 0.08365) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 99.98, Test_acc 55.56
2024-08-30 17:19:00,645 [podnet.py] => Task 2, Epoch 54/200 (LR 0.08307) => LSC_loss 0.07, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 99.84, Test_acc 58.07
2024-08-30 17:19:02,215 [podnet.py] => Task 2, Epoch 55/200 (LR 0.08247) => LSC_loss 0.07, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 99.87, Test_acc 53.69
2024-08-30 17:19:04,051 [podnet.py] => Task 2, Epoch 56/200 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 99.93, Test_acc 55.96
2024-08-30 17:19:05,588 [podnet.py] => Task 2, Epoch 57/200 (LR 0.08126) => LSC_loss 0.06, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 99.96, Test_acc 57.78
2024-08-30 17:19:06,979 [podnet.py] => Task 2, Epoch 58/200 (LR 0.08065) => LSC_loss 0.06, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 99.98, Test_acc 58.85
2024-08-30 17:19:08,446 [podnet.py] => Task 2, Epoch 59/200 (LR 0.08002) => LSC_loss 0.06, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 99.96, Test_acc 59.67
2024-08-30 17:19:09,933 [podnet.py] => Task 2, Epoch 60/200 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 99.98, Test_acc 57.39
2024-08-30 17:19:11,367 [podnet.py] => Task 2, Epoch 61/200 (LR 0.07875) => LSC_loss 0.06, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 99.91, Test_acc 55.89
2024-08-30 17:19:12,886 [podnet.py] => Task 2, Epoch 62/200 (LR 0.07810) => LSC_loss 0.11, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 99.71, Test_acc 28.17
2024-08-30 17:19:14,257 [podnet.py] => Task 2, Epoch 63/200 (LR 0.07745) => LSC_loss 0.33, Spatial_loss 1.93, Flat_loss 0.20, Train_acc 91.62, Test_acc 50.59
2024-08-30 17:19:15,664 [podnet.py] => Task 2, Epoch 64/200 (LR 0.07679) => LSC_loss 0.14, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 97.33, Test_acc 59.28
2024-08-30 17:19:17,163 [podnet.py] => Task 2, Epoch 65/200 (LR 0.07612) => LSC_loss 0.09, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 99.33, Test_acc 58.15
2024-08-30 17:19:18,815 [podnet.py] => Task 2, Epoch 66/200 (LR 0.07545) => LSC_loss 0.08, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 99.73, Test_acc 58.33
2024-08-30 17:19:20,218 [podnet.py] => Task 2, Epoch 67/200 (LR 0.07477) => LSC_loss 0.07, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 99.84, Test_acc 55.50
2024-08-30 17:19:21,623 [podnet.py] => Task 2, Epoch 68/200 (LR 0.07409) => LSC_loss 0.08, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 99.73, Test_acc 55.50
2024-08-30 17:19:23,042 [podnet.py] => Task 2, Epoch 69/200 (LR 0.07340) => LSC_loss 0.10, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 99.22, Test_acc 57.52
2024-08-30 17:19:24,674 [podnet.py] => Task 2, Epoch 70/200 (LR 0.07270) => LSC_loss 0.07, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 99.91, Test_acc 58.37
2024-08-30 17:19:26,330 [podnet.py] => Task 2, Epoch 71/200 (LR 0.07200) => LSC_loss 0.07, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 99.93, Test_acc 59.52
2024-08-30 17:19:27,682 [podnet.py] => Task 2, Epoch 72/200 (LR 0.07129) => LSC_loss 0.07, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 99.93, Test_acc 58.94
2024-08-30 17:19:29,016 [podnet.py] => Task 2, Epoch 73/200 (LR 0.07058) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 99.98, Test_acc 59.00
2024-08-30 17:19:30,502 [podnet.py] => Task 2, Epoch 74/200 (LR 0.06986) => LSC_loss 0.06, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 100.00, Test_acc 56.63
2024-08-30 17:19:32,124 [podnet.py] => Task 2, Epoch 75/200 (LR 0.06913) => LSC_loss 0.06, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 99.96, Test_acc 57.59
2024-08-30 17:19:33,496 [podnet.py] => Task 2, Epoch 76/200 (LR 0.06841) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 99.93, Test_acc 58.96
2024-08-30 17:19:34,868 [podnet.py] => Task 2, Epoch 77/200 (LR 0.06767) => LSC_loss 0.06, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 100.00, Test_acc 59.46
2024-08-30 17:19:36,197 [podnet.py] => Task 2, Epoch 78/200 (LR 0.06694) => LSC_loss 0.06, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 99.93, Test_acc 58.31
2024-08-30 17:19:37,650 [podnet.py] => Task 2, Epoch 79/200 (LR 0.06620) => LSC_loss 0.06, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 99.98, Test_acc 58.13
2024-08-30 17:19:39,153 [podnet.py] => Task 2, Epoch 80/200 (LR 0.06545) => LSC_loss 0.06, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 99.96, Test_acc 56.52
2024-08-30 17:19:40,481 [podnet.py] => Task 2, Epoch 81/200 (LR 0.06470) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 99.96, Test_acc 60.26
2024-08-30 17:19:41,857 [podnet.py] => Task 2, Epoch 82/200 (LR 0.06395) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 99.98, Test_acc 55.22
2024-08-30 17:19:43,328 [podnet.py] => Task 2, Epoch 83/200 (LR 0.06319) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 99.98, Test_acc 57.52
2024-08-30 17:19:44,769 [podnet.py] => Task 2, Epoch 84/200 (LR 0.06243) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 99.96, Test_acc 60.57
2024-08-30 17:19:46,371 [podnet.py] => Task 2, Epoch 85/200 (LR 0.06167) => LSC_loss 0.07, Spatial_loss 0.97, Flat_loss 0.10, Train_acc 99.71, Test_acc 57.52
2024-08-30 17:19:47,748 [podnet.py] => Task 2, Epoch 86/200 (LR 0.06091) => LSC_loss 0.06, Spatial_loss 0.94, Flat_loss 0.10, Train_acc 99.98, Test_acc 59.31
2024-08-30 17:19:49,074 [podnet.py] => Task 2, Epoch 87/200 (LR 0.06014) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 99.98, Test_acc 55.30
2024-08-30 17:19:50,573 [podnet.py] => Task 2, Epoch 88/200 (LR 0.05937) => LSC_loss 0.06, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 99.89, Test_acc 57.26
2024-08-30 17:19:52,017 [podnet.py] => Task 2, Epoch 89/200 (LR 0.05860) => LSC_loss 0.06, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 99.89, Test_acc 56.80
2024-08-30 17:19:53,374 [podnet.py] => Task 2, Epoch 90/200 (LR 0.05782) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 99.96, Test_acc 55.87
2024-08-30 17:19:54,686 [podnet.py] => Task 2, Epoch 91/200 (LR 0.05705) => LSC_loss 0.06, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 99.98, Test_acc 58.30
2024-08-30 17:19:56,064 [podnet.py] => Task 2, Epoch 92/200 (LR 0.05627) => LSC_loss 0.06, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 99.98, Test_acc 60.46
2024-08-30 17:19:57,544 [podnet.py] => Task 2, Epoch 93/200 (LR 0.05549) => LSC_loss 0.06, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 99.98, Test_acc 56.94
2024-08-30 17:19:58,975 [podnet.py] => Task 2, Epoch 94/200 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 99.98, Test_acc 61.02
2024-08-30 17:20:00,368 [podnet.py] => Task 2, Epoch 95/200 (LR 0.05392) => LSC_loss 0.06, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 99.98, Test_acc 58.33
2024-08-30 17:20:01,732 [podnet.py] => Task 2, Epoch 96/200 (LR 0.05314) => LSC_loss 0.06, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 99.96, Test_acc 58.33
2024-08-30 17:20:03,117 [podnet.py] => Task 2, Epoch 97/200 (LR 0.05236) => LSC_loss 0.06, Spatial_loss 0.83, Flat_loss 0.09, Train_acc 100.00, Test_acc 56.39
2024-08-30 17:20:04,533 [podnet.py] => Task 2, Epoch 98/200 (LR 0.05157) => LSC_loss 0.06, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 99.96, Test_acc 59.07
2024-08-30 17:20:05,863 [podnet.py] => Task 2, Epoch 99/200 (LR 0.05079) => LSC_loss 0.06, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 100.00, Test_acc 56.83
2024-08-30 17:20:07,229 [podnet.py] => Task 2, Epoch 100/200 (LR 0.05000) => LSC_loss 0.06, Spatial_loss 0.85, Flat_loss 0.09, Train_acc 100.00, Test_acc 56.02
2024-08-30 17:20:08,605 [podnet.py] => Task 2, Epoch 101/200 (LR 0.04921) => LSC_loss 0.06, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 99.96, Test_acc 58.33
2024-08-30 17:20:10,111 [podnet.py] => Task 2, Epoch 102/200 (LR 0.04843) => LSC_loss 0.06, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 99.98, Test_acc 59.78
2024-08-30 17:20:11,587 [podnet.py] => Task 2, Epoch 103/200 (LR 0.04764) => LSC_loss 0.06, Spatial_loss 0.84, Flat_loss 0.09, Train_acc 100.00, Test_acc 58.78
2024-08-30 17:20:12,934 [podnet.py] => Task 2, Epoch 104/200 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.86, Flat_loss 0.09, Train_acc 99.98, Test_acc 54.93
2024-08-30 17:20:14,378 [podnet.py] => Task 2, Epoch 105/200 (LR 0.04608) => LSC_loss 0.06, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 99.93, Test_acc 57.93
2024-08-30 17:20:15,804 [podnet.py] => Task 2, Epoch 106/200 (LR 0.04529) => LSC_loss 0.06, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.52
2024-08-30 17:20:17,215 [podnet.py] => Task 2, Epoch 107/200 (LR 0.04451) => LSC_loss 0.06, Spatial_loss 0.83, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.65
2024-08-30 17:20:18,673 [podnet.py] => Task 2, Epoch 108/200 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.30
2024-08-30 17:20:19,987 [podnet.py] => Task 2, Epoch 109/200 (LR 0.04295) => LSC_loss 0.06, Spatial_loss 0.85, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.28
2024-08-30 17:20:21,391 [podnet.py] => Task 2, Epoch 110/200 (LR 0.04218) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.65
2024-08-30 17:20:22,909 [podnet.py] => Task 2, Epoch 111/200 (LR 0.04140) => LSC_loss 0.06, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.61
2024-08-30 17:20:24,325 [podnet.py] => Task 2, Epoch 112/200 (LR 0.04063) => LSC_loss 0.06, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 99.98, Test_acc 59.22
2024-08-30 17:20:25,680 [podnet.py] => Task 2, Epoch 113/200 (LR 0.03986) => LSC_loss 0.06, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 99.98, Test_acc 57.96
2024-08-30 17:20:27,229 [podnet.py] => Task 2, Epoch 114/200 (LR 0.03909) => LSC_loss 0.06, Spatial_loss 0.83, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.33
2024-08-30 17:20:28,655 [podnet.py] => Task 2, Epoch 115/200 (LR 0.03833) => LSC_loss 0.06, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 99.98, Test_acc 59.26
2024-08-30 17:20:30,389 [podnet.py] => Task 2, Epoch 116/200 (LR 0.03757) => LSC_loss 0.06, Spatial_loss 0.84, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.02
2024-08-30 17:20:31,843 [podnet.py] => Task 2, Epoch 117/200 (LR 0.03681) => LSC_loss 0.06, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.13
2024-08-30 17:20:33,223 [podnet.py] => Task 2, Epoch 118/200 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.85
2024-08-30 17:20:34,606 [podnet.py] => Task 2, Epoch 119/200 (LR 0.03530) => LSC_loss 0.06, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.26
2024-08-30 17:20:36,225 [podnet.py] => Task 2, Epoch 120/200 (LR 0.03455) => LSC_loss 0.06, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.80
2024-08-30 17:20:37,676 [podnet.py] => Task 2, Epoch 121/200 (LR 0.03380) => LSC_loss 0.06, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.30
2024-08-30 17:20:39,051 [podnet.py] => Task 2, Epoch 122/200 (LR 0.03306) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 100.00, Test_acc 57.67
2024-08-30 17:20:40,405 [podnet.py] => Task 2, Epoch 123/200 (LR 0.03233) => LSC_loss 0.06, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 99.98, Test_acc 58.37
2024-08-30 17:20:41,784 [podnet.py] => Task 2, Epoch 124/200 (LR 0.03159) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.63
2024-08-30 17:20:43,124 [podnet.py] => Task 2, Epoch 125/200 (LR 0.03087) => LSC_loss 0.06, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 99.98, Test_acc 58.41
2024-08-30 17:20:44,502 [podnet.py] => Task 2, Epoch 126/200 (LR 0.03014) => LSC_loss 0.06, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 99.96, Test_acc 58.48
2024-08-30 17:20:45,849 [podnet.py] => Task 2, Epoch 127/200 (LR 0.02942) => LSC_loss 0.06, Spatial_loss 0.73, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.17
2024-08-30 17:20:47,315 [podnet.py] => Task 2, Epoch 128/200 (LR 0.02871) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 99.96, Test_acc 59.59
2024-08-30 17:20:49,049 [podnet.py] => Task 2, Epoch 129/200 (LR 0.02800) => LSC_loss 0.06, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.67
2024-08-30 17:20:50,499 [podnet.py] => Task 2, Epoch 130/200 (LR 0.02730) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.08, Train_acc 99.98, Test_acc 58.56
2024-08-30 17:20:52,035 [podnet.py] => Task 2, Epoch 131/200 (LR 0.02660) => LSC_loss 0.06, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.83
2024-08-30 17:20:53,387 [podnet.py] => Task 2, Epoch 132/200 (LR 0.02591) => LSC_loss 0.08, Spatial_loss 0.74, Flat_loss 0.08, Train_acc 99.91, Test_acc 57.96
2024-08-30 17:20:54,745 [podnet.py] => Task 2, Epoch 133/200 (LR 0.02523) => LSC_loss 0.08, Spatial_loss 0.95, Flat_loss 0.11, Train_acc 99.60, Test_acc 58.30
2024-08-30 17:20:56,373 [podnet.py] => Task 2, Epoch 134/200 (LR 0.02455) => LSC_loss 0.07, Spatial_loss 0.87, Flat_loss 0.10, Train_acc 99.76, Test_acc 59.39
2024-08-30 17:20:57,949 [podnet.py] => Task 2, Epoch 135/200 (LR 0.02388) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.09, Train_acc 100.00, Test_acc 58.54
2024-08-30 17:20:59,346 [podnet.py] => Task 2, Epoch 136/200 (LR 0.02321) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.09, Train_acc 100.00, Test_acc 58.74
2024-08-30 17:21:00,743 [podnet.py] => Task 2, Epoch 137/200 (LR 0.02255) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.07
2024-08-30 17:21:02,171 [podnet.py] => Task 2, Epoch 138/200 (LR 0.02190) => LSC_loss 0.07, Spatial_loss 0.78, Flat_loss 0.09, Train_acc 99.98, Test_acc 55.48
2024-08-30 17:21:03,617 [podnet.py] => Task 2, Epoch 139/200 (LR 0.02125) => LSC_loss 0.07, Spatial_loss 0.95, Flat_loss 0.10, Train_acc 99.62, Test_acc 59.93
2024-08-30 17:21:05,143 [podnet.py] => Task 2, Epoch 140/200 (LR 0.02061) => LSC_loss 0.06, Spatial_loss 0.79, Flat_loss 0.09, Train_acc 99.96, Test_acc 58.85
2024-08-30 17:21:06,485 [podnet.py] => Task 2, Epoch 141/200 (LR 0.01998) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.19
2024-08-30 17:21:07,950 [podnet.py] => Task 2, Epoch 142/200 (LR 0.01935) => LSC_loss 0.06, Spatial_loss 0.74, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.19
2024-08-30 17:21:09,364 [podnet.py] => Task 2, Epoch 143/200 (LR 0.01874) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.69
2024-08-30 17:21:11,106 [podnet.py] => Task 2, Epoch 144/200 (LR 0.01813) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.11
2024-08-30 17:21:12,500 [podnet.py] => Task 2, Epoch 145/200 (LR 0.01753) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:21:13,933 [podnet.py] => Task 2, Epoch 146/200 (LR 0.01693) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.41
2024-08-30 17:21:15,342 [podnet.py] => Task 2, Epoch 147/200 (LR 0.01635) => LSC_loss 0.06, Spatial_loss 0.72, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.44
2024-08-30 17:21:16,860 [podnet.py] => Task 2, Epoch 148/200 (LR 0.01577) => LSC_loss 0.06, Spatial_loss 0.72, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.31
2024-08-30 17:21:18,348 [podnet.py] => Task 2, Epoch 149/200 (LR 0.01520) => LSC_loss 0.07, Spatial_loss 0.68, Flat_loss 0.08, Train_acc 99.98, Test_acc 60.78
2024-08-30 17:21:19,743 [podnet.py] => Task 2, Epoch 150/200 (LR 0.01464) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.08, Train_acc 99.98, Test_acc 60.35
2024-08-30 17:21:21,106 [podnet.py] => Task 2, Epoch 151/200 (LR 0.01409) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.81
2024-08-30 17:21:22,594 [podnet.py] => Task 2, Epoch 152/200 (LR 0.01355) => LSC_loss 0.06, Spatial_loss 0.65, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.74
2024-08-30 17:21:24,058 [podnet.py] => Task 2, Epoch 153/200 (LR 0.01302) => LSC_loss 0.06, Spatial_loss 0.65, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.46
2024-08-30 17:21:25,550 [podnet.py] => Task 2, Epoch 154/200 (LR 0.01249) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.35
2024-08-30 17:21:26,913 [podnet.py] => Task 2, Epoch 155/200 (LR 0.01198) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.61
2024-08-30 17:21:28,275 [podnet.py] => Task 2, Epoch 156/200 (LR 0.01147) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.08, Train_acc 100.00, Test_acc 58.81
2024-08-30 17:21:29,707 [podnet.py] => Task 2, Epoch 157/200 (LR 0.01098) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.48
2024-08-30 17:21:31,256 [podnet.py] => Task 2, Epoch 158/200 (LR 0.01049) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.08, Train_acc 99.98, Test_acc 59.28
2024-08-30 17:21:32,679 [podnet.py] => Task 2, Epoch 159/200 (LR 0.01002) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.59
2024-08-30 17:21:34,051 [podnet.py] => Task 2, Epoch 160/200 (LR 0.00955) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.08, Train_acc 99.98, Test_acc 59.91
2024-08-30 17:21:35,514 [podnet.py] => Task 2, Epoch 161/200 (LR 0.00909) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.44
2024-08-30 17:21:37,053 [podnet.py] => Task 2, Epoch 162/200 (LR 0.00865) => LSC_loss 0.06, Spatial_loss 0.61, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:21:38,454 [podnet.py] => Task 2, Epoch 163/200 (LR 0.00821) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.08, Train_acc 100.00, Test_acc 59.87
2024-08-30 17:21:39,853 [podnet.py] => Task 2, Epoch 164/200 (LR 0.00778) => LSC_loss 0.06, Spatial_loss 0.61, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:21:41,409 [podnet.py] => Task 2, Epoch 165/200 (LR 0.00737) => LSC_loss 0.06, Spatial_loss 0.59, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.56
2024-08-30 17:21:43,196 [podnet.py] => Task 2, Epoch 166/200 (LR 0.00696) => LSC_loss 0.06, Spatial_loss 0.59, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.24
2024-08-30 17:21:45,096 [podnet.py] => Task 2, Epoch 167/200 (LR 0.00657) => LSC_loss 0.06, Spatial_loss 0.58, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.70
2024-08-30 17:21:46,588 [podnet.py] => Task 2, Epoch 168/200 (LR 0.00618) => LSC_loss 0.06, Spatial_loss 0.61, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:21:48,117 [podnet.py] => Task 2, Epoch 169/200 (LR 0.00581) => LSC_loss 0.06, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.70
2024-08-30 17:21:49,513 [podnet.py] => Task 2, Epoch 170/200 (LR 0.00545) => LSC_loss 0.06, Spatial_loss 0.60, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.94
2024-08-30 17:21:51,296 [podnet.py] => Task 2, Epoch 171/200 (LR 0.00510) => LSC_loss 0.06, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.02
2024-08-30 17:21:53,121 [podnet.py] => Task 2, Epoch 172/200 (LR 0.00476) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.35
2024-08-30 17:21:55,147 [podnet.py] => Task 2, Epoch 173/200 (LR 0.00443) => LSC_loss 0.06, Spatial_loss 0.58, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.20
2024-08-30 17:21:56,991 [podnet.py] => Task 2, Epoch 174/200 (LR 0.00411) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.07, Train_acc 100.00, Test_acc 59.85
2024-08-30 17:21:58,454 [podnet.py] => Task 2, Epoch 175/200 (LR 0.00381) => LSC_loss 0.06, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.94
2024-08-30 17:22:00,043 [podnet.py] => Task 2, Epoch 176/200 (LR 0.00351) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.31
2024-08-30 17:22:01,959 [podnet.py] => Task 2, Epoch 177/200 (LR 0.00323) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.80
2024-08-30 17:22:03,871 [podnet.py] => Task 2, Epoch 178/200 (LR 0.00296) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.59
2024-08-30 17:22:05,460 [podnet.py] => Task 2, Epoch 179/200 (LR 0.00270) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.04
2024-08-30 17:22:06,836 [podnet.py] => Task 2, Epoch 180/200 (LR 0.00245) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.08, Train_acc 99.98, Test_acc 61.02
2024-08-30 17:22:08,253 [podnet.py] => Task 2, Epoch 181/200 (LR 0.00221) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.54
2024-08-30 17:22:09,613 [podnet.py] => Task 2, Epoch 182/200 (LR 0.00199) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.41
2024-08-30 17:22:11,361 [podnet.py] => Task 2, Epoch 183/200 (LR 0.00177) => LSC_loss 0.06, Spatial_loss 0.57, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.07
2024-08-30 17:22:13,157 [podnet.py] => Task 2, Epoch 184/200 (LR 0.00157) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.89
2024-08-30 17:22:14,838 [podnet.py] => Task 2, Epoch 185/200 (LR 0.00138) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:22:16,300 [podnet.py] => Task 2, Epoch 186/200 (LR 0.00120) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.46
2024-08-30 17:22:18,022 [podnet.py] => Task 2, Epoch 187/200 (LR 0.00104) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:22:20,188 [podnet.py] => Task 2, Epoch 188/200 (LR 0.00089) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.43
2024-08-30 17:22:21,783 [podnet.py] => Task 2, Epoch 189/200 (LR 0.00074) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.96
2024-08-30 17:22:23,589 [podnet.py] => Task 2, Epoch 190/200 (LR 0.00062) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.61
2024-08-30 17:22:25,248 [podnet.py] => Task 2, Epoch 191/200 (LR 0.00050) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:22:26,832 [podnet.py] => Task 2, Epoch 192/200 (LR 0.00039) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.39
2024-08-30 17:22:28,403 [podnet.py] => Task 2, Epoch 193/200 (LR 0.00030) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.33
2024-08-30 17:22:29,784 [podnet.py] => Task 2, Epoch 194/200 (LR 0.00022) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.61
2024-08-30 17:22:31,205 [podnet.py] => Task 2, Epoch 195/200 (LR 0.00015) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:22:32,562 [podnet.py] => Task 2, Epoch 196/200 (LR 0.00010) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.83
2024-08-30 17:22:33,973 [podnet.py] => Task 2, Epoch 197/200 (LR 0.00006) => LSC_loss 0.06, Spatial_loss 0.51, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.46
2024-08-30 17:22:35,448 [podnet.py] => Task 2, Epoch 198/200 (LR 0.00002) => LSC_loss 0.06, Spatial_loss 0.51, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.54
2024-08-30 17:22:36,823 [podnet.py] => Task 2, Epoch 199/200 (LR 0.00001) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.07, Train_acc 100.00, Test_acc 60.39
2024-08-30 17:22:38,399 [podnet.py] => Task 2, Epoch 200/200 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.07, Train_acc 100.00, Test_acc 61.50
2024-08-30 17:22:38,822 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:22:38,823 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:22:40,360 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:22:42,109 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 17:22:43,675 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 17:22:46,240 [podnet.py] => Exemplar size: 495
2024-08-30 17:22:46,240 [trainer.py] => CNN: {'total': 61.5, '00-04': 50.5, '05-06': 53.58, '07-08': 96.92, 'old': 51.38, 'new': 96.92}
2024-08-30 17:22:46,240 [trainer.py] => NME: {'total': 67.06, '00-04': 70.07, '05-06': 43.75, '07-08': 82.83, 'old': 62.55, 'new': 82.83}
2024-08-30 17:22:46,240 [trainer.py] => CNN top1 curve: [89.63, 68.95, 61.5]
2024-08-30 17:22:46,240 [trainer.py] => CNN top5 curve: [100.0, 98.36, 93.57]
2024-08-30 17:22:46,240 [trainer.py] => NME top1 curve: [89.6, 75.17, 67.06]
2024-08-30 17:22:46,240 [trainer.py] => NME top5 curve: [100.0, 98.31, 95.02]

2024-08-30 17:22:46,240 [trainer.py] => Average Accuracy (CNN): 73.36
2024-08-30 17:22:46,240 [trainer.py] => Average Accuracy (NME): 77.27666666666666
2024-08-30 17:22:46,241 [trainer.py] => Forgetting (CNN): 41.065
