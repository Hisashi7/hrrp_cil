2024-08-30 17:05:54,614 [trainer.py] => config: ./exps/podnet.json
2024-08-30 17:05:54,615 [trainer.py] => prefix: reproduce
2024-08-30 17:05:54,615 [trainer.py] => dataset: hrrp9
2024-08-30 17:05:54,615 [trainer.py] => memory_size: 500
2024-08-30 17:05:54,615 [trainer.py] => memory_per_class: 20
2024-08-30 17:05:54,615 [trainer.py] => fixed_memory: False
2024-08-30 17:05:54,615 [trainer.py] => shuffle: True
2024-08-30 17:05:54,615 [trainer.py] => init_cls: 5
2024-08-30 17:05:54,615 [trainer.py] => increment: 2
2024-08-30 17:05:54,615 [trainer.py] => model_name: podnet
2024-08-30 17:05:54,615 [trainer.py] => convnet_type: resnet18
2024-08-30 17:05:54,615 [trainer.py] => init_train: True
2024-08-30 17:05:54,615 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 17:05:54,615 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 17:05:54,615 [trainer.py] => device: [device(type='cuda', index=1)]
2024-08-30 17:05:54,615 [trainer.py] => seed: 1993
2024-08-30 17:05:55,070 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 17:05:55,166 [trainer.py] => All params: 3843904
2024-08-30 17:05:55,166 [trainer.py] => Trainable params: 3843904
2024-08-30 17:05:55,167 [podnet.py] => Learning on 0-5
2024-08-30 17:05:55,201 [podnet.py] => Adaptive factor: 0
2024-08-30 17:05:58,369 [podnet.py] => Task 0, Epoch 1/200 (LR 0.09999) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 17:06:00,492 [podnet.py] => Task 0, Epoch 2/200 (LR 0.09998) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.70, Test_acc 28.10
2024-08-30 17:06:02,218 [podnet.py] => Task 0, Epoch 3/200 (LR 0.09994) => LSC_loss 0.74, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.65, Test_acc 46.50
2024-08-30 17:06:03,973 [podnet.py] => Task 0, Epoch 4/200 (LR 0.09990) => LSC_loss 0.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.48, Test_acc 56.47
2024-08-30 17:06:05,722 [podnet.py] => Task 0, Epoch 5/200 (LR 0.09985) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.35, Test_acc 57.07
2024-08-30 17:06:07,507 [podnet.py] => Task 0, Epoch 6/200 (LR 0.09978) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.01, Test_acc 70.80
2024-08-30 17:06:09,184 [podnet.py] => Task 0, Epoch 7/200 (LR 0.09970) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.09, Test_acc 78.63
2024-08-30 17:06:11,792 [podnet.py] => Task 0, Epoch 8/200 (LR 0.09961) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.03, Test_acc 73.23
2024-08-30 17:06:13,813 [podnet.py] => Task 0, Epoch 9/200 (LR 0.09950) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.07, Test_acc 72.00
2024-08-30 17:06:16,173 [podnet.py] => Task 0, Epoch 10/200 (LR 0.09938) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.89, Test_acc 78.53
2024-08-30 17:06:18,035 [podnet.py] => Task 0, Epoch 11/200 (LR 0.09926) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.73, Test_acc 70.67
2024-08-30 17:06:20,076 [podnet.py] => Task 0, Epoch 12/200 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.93, Test_acc 72.97
2024-08-30 17:06:22,295 [podnet.py] => Task 0, Epoch 13/200 (LR 0.09896) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.33, Test_acc 82.83
2024-08-30 17:06:24,407 [podnet.py] => Task 0, Epoch 14/200 (LR 0.09880) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.98, Test_acc 73.07
2024-08-30 17:06:26,358 [podnet.py] => Task 0, Epoch 15/200 (LR 0.09862) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.00, Test_acc 50.33
2024-08-30 17:06:27,987 [podnet.py] => Task 0, Epoch 16/200 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.34, Test_acc 86.00
2024-08-30 17:06:29,745 [podnet.py] => Task 0, Epoch 17/200 (LR 0.09823) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.15, Test_acc 76.53
2024-08-30 17:06:31,583 [podnet.py] => Task 0, Epoch 18/200 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 84.10
2024-08-30 17:06:33,286 [podnet.py] => Task 0, Epoch 19/200 (LR 0.09779) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 76.60
2024-08-30 17:06:35,205 [podnet.py] => Task 0, Epoch 20/200 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 75.47
2024-08-30 17:06:36,900 [podnet.py] => Task 0, Epoch 21/200 (LR 0.09730) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 79.90
2024-08-30 17:06:38,679 [podnet.py] => Task 0, Epoch 22/200 (LR 0.09704) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.93, Test_acc 81.03
2024-08-30 17:06:40,310 [podnet.py] => Task 0, Epoch 23/200 (LR 0.09677) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 81.97
2024-08-30 17:06:41,964 [podnet.py] => Task 0, Epoch 24/200 (LR 0.09649) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.72, Test_acc 79.50
2024-08-30 17:06:43,694 [podnet.py] => Task 0, Epoch 25/200 (LR 0.09619) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 83.03
2024-08-30 17:06:45,345 [podnet.py] => Task 0, Epoch 26/200 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 84.80
2024-08-30 17:06:46,920 [podnet.py] => Task 0, Epoch 27/200 (LR 0.09557) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 80.00
2024-08-30 17:06:48,412 [podnet.py] => Task 0, Epoch 28/200 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 83.80
2024-08-30 17:06:50,019 [podnet.py] => Task 0, Epoch 29/200 (LR 0.09490) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 84.60
2024-08-30 17:06:51,673 [podnet.py] => Task 0, Epoch 30/200 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.21, Test_acc 77.83
2024-08-30 17:06:53,375 [podnet.py] => Task 0, Epoch 31/200 (LR 0.09419) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 80.53
2024-08-30 17:06:55,052 [podnet.py] => Task 0, Epoch 32/200 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.13
2024-08-30 17:06:56,801 [podnet.py] => Task 0, Epoch 33/200 (LR 0.09343) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.99, Test_acc 82.00
2024-08-30 17:06:58,704 [podnet.py] => Task 0, Epoch 34/200 (LR 0.09304) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 78.77
2024-08-30 17:07:00,483 [podnet.py] => Task 0, Epoch 35/200 (LR 0.09263) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.99, Test_acc 83.30
2024-08-30 17:07:02,352 [podnet.py] => Task 0, Epoch 36/200 (LR 0.09222) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.97, Test_acc 80.60
2024-08-30 17:07:03,968 [podnet.py] => Task 0, Epoch 37/200 (LR 0.09179) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.96, Test_acc 85.33
2024-08-30 17:07:05,612 [podnet.py] => Task 0, Epoch 38/200 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 78.57
2024-08-30 17:07:07,345 [podnet.py] => Task 0, Epoch 39/200 (LR 0.09091) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 87.57
2024-08-30 17:07:09,107 [podnet.py] => Task 0, Epoch 40/200 (LR 0.09045) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.39, Test_acc 86.33
2024-08-30 17:07:10,950 [podnet.py] => Task 0, Epoch 41/200 (LR 0.08998) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 78.50
2024-08-30 17:07:12,846 [podnet.py] => Task 0, Epoch 42/200 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 84.73
2024-08-30 17:07:14,385 [podnet.py] => Task 0, Epoch 43/200 (LR 0.08902) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 75.87
2024-08-30 17:07:15,980 [podnet.py] => Task 0, Epoch 44/200 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 81.53
2024-08-30 17:07:17,625 [podnet.py] => Task 0, Epoch 45/200 (LR 0.08802) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.49, Test_acc 75.03
2024-08-30 17:07:19,377 [podnet.py] => Task 0, Epoch 46/200 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 83.07
2024-08-30 17:07:21,283 [podnet.py] => Task 0, Epoch 47/200 (LR 0.08698) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 84.73
2024-08-30 17:07:23,016 [podnet.py] => Task 0, Epoch 48/200 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.23
2024-08-30 17:07:24,634 [podnet.py] => Task 0, Epoch 49/200 (LR 0.08591) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 80.70
2024-08-30 17:07:26,361 [podnet.py] => Task 0, Epoch 50/200 (LR 0.08536) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.64, Test_acc 80.33
2024-08-30 17:07:28,020 [podnet.py] => Task 0, Epoch 51/200 (LR 0.08480) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 86.60
2024-08-30 17:07:29,718 [podnet.py] => Task 0, Epoch 52/200 (LR 0.08423) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.62, Test_acc 81.07
2024-08-30 17:07:31,549 [podnet.py] => Task 0, Epoch 53/200 (LR 0.08365) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.46, Test_acc 81.60
2024-08-30 17:07:33,295 [podnet.py] => Task 0, Epoch 54/200 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 79.40
2024-08-30 17:07:35,259 [podnet.py] => Task 0, Epoch 55/200 (LR 0.08247) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.85, Test_acc 78.10
2024-08-30 17:07:36,973 [podnet.py] => Task 0, Epoch 56/200 (LR 0.08187) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 87.00
2024-08-30 17:07:39,084 [podnet.py] => Task 0, Epoch 57/200 (LR 0.08126) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 82.57
2024-08-30 17:07:40,831 [podnet.py] => Task 0, Epoch 58/200 (LR 0.08065) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 86.30
2024-08-30 17:07:42,462 [podnet.py] => Task 0, Epoch 59/200 (LR 0.08002) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 87.63
2024-08-30 17:07:44,237 [podnet.py] => Task 0, Epoch 60/200 (LR 0.07939) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 85.37
2024-08-30 17:07:46,077 [podnet.py] => Task 0, Epoch 61/200 (LR 0.07875) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.14, Test_acc 78.47
2024-08-30 17:07:47,639 [podnet.py] => Task 0, Epoch 62/200 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.34, Test_acc 82.63
2024-08-30 17:07:49,339 [podnet.py] => Task 0, Epoch 63/200 (LR 0.07745) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 85.27
2024-08-30 17:07:51,071 [podnet.py] => Task 0, Epoch 64/200 (LR 0.07679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 86.43
2024-08-30 17:07:52,932 [podnet.py] => Task 0, Epoch 65/200 (LR 0.07612) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.57
2024-08-30 17:07:54,696 [podnet.py] => Task 0, Epoch 66/200 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.17, Test_acc 84.67
2024-08-30 17:07:56,257 [podnet.py] => Task 0, Epoch 67/200 (LR 0.07477) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 82.73
2024-08-30 17:07:58,034 [podnet.py] => Task 0, Epoch 68/200 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.20
2024-08-30 17:07:59,700 [podnet.py] => Task 0, Epoch 69/200 (LR 0.07340) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.27, Test_acc 83.47
2024-08-30 17:08:01,213 [podnet.py] => Task 0, Epoch 70/200 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.58, Test_acc 76.37
2024-08-30 17:08:02,895 [podnet.py] => Task 0, Epoch 71/200 (LR 0.07200) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.67, Test_acc 83.77
2024-08-30 17:08:04,565 [podnet.py] => Task 0, Epoch 72/200 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 85.97
2024-08-30 17:08:06,191 [podnet.py] => Task 0, Epoch 73/200 (LR 0.07058) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 85.37
2024-08-30 17:08:07,779 [podnet.py] => Task 0, Epoch 74/200 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 87.87
2024-08-30 17:08:09,353 [podnet.py] => Task 0, Epoch 75/200 (LR 0.06913) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 84.93
2024-08-30 17:08:11,128 [podnet.py] => Task 0, Epoch 76/200 (LR 0.06841) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.94, Test_acc 83.30
2024-08-30 17:08:13,249 [podnet.py] => Task 0, Epoch 77/200 (LR 0.06767) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 86.93
2024-08-30 17:08:15,133 [podnet.py] => Task 0, Epoch 78/200 (LR 0.06694) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 80.80
2024-08-30 17:08:16,902 [podnet.py] => Task 0, Epoch 79/200 (LR 0.06620) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.43
2024-08-30 17:08:18,973 [podnet.py] => Task 0, Epoch 80/200 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 87.10
2024-08-30 17:08:20,839 [podnet.py] => Task 0, Epoch 81/200 (LR 0.06470) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.77
2024-08-30 17:08:22,727 [podnet.py] => Task 0, Epoch 82/200 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 77.37
2024-08-30 17:08:24,520 [podnet.py] => Task 0, Epoch 83/200 (LR 0.06319) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.58, Test_acc 88.90
2024-08-30 17:08:26,150 [podnet.py] => Task 0, Epoch 84/200 (LR 0.06243) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.37
2024-08-30 17:08:27,875 [podnet.py] => Task 0, Epoch 85/200 (LR 0.06167) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-30 17:08:29,544 [podnet.py] => Task 0, Epoch 86/200 (LR 0.06091) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 91.13
2024-08-30 17:08:31,290 [podnet.py] => Task 0, Epoch 87/200 (LR 0.06014) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.77
2024-08-30 17:08:32,777 [podnet.py] => Task 0, Epoch 88/200 (LR 0.05937) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.80, Test_acc 77.43
2024-08-30 17:08:34,441 [podnet.py] => Task 0, Epoch 89/200 (LR 0.05860) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.27, Test_acc 82.77
2024-08-30 17:08:36,086 [podnet.py] => Task 0, Epoch 90/200 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 84.93
2024-08-30 17:08:37,617 [podnet.py] => Task 0, Epoch 91/200 (LR 0.05705) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.26, Test_acc 87.63
2024-08-30 17:08:39,423 [podnet.py] => Task 0, Epoch 92/200 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 86.30
2024-08-30 17:08:41,256 [podnet.py] => Task 0, Epoch 93/200 (LR 0.05549) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 82.70
2024-08-30 17:08:43,114 [podnet.py] => Task 0, Epoch 94/200 (LR 0.05471) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 86.67
2024-08-30 17:08:44,838 [podnet.py] => Task 0, Epoch 95/200 (LR 0.05392) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 88.53
2024-08-30 17:08:46,423 [podnet.py] => Task 0, Epoch 96/200 (LR 0.05314) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.65, Test_acc 75.93
2024-08-30 17:08:48,022 [podnet.py] => Task 0, Epoch 97/200 (LR 0.05236) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.81, Test_acc 86.07
2024-08-30 17:08:49,666 [podnet.py] => Task 0, Epoch 98/200 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.53
2024-08-30 17:08:51,336 [podnet.py] => Task 0, Epoch 99/200 (LR 0.05079) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 87.07
2024-08-30 17:08:53,138 [podnet.py] => Task 0, Epoch 100/200 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.53, Test_acc 86.03
2024-08-30 17:08:54,858 [podnet.py] => Task 0, Epoch 101/200 (LR 0.04921) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 87.93
2024-08-30 17:08:56,563 [podnet.py] => Task 0, Epoch 102/200 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 88.53
2024-08-30 17:08:58,183 [podnet.py] => Task 0, Epoch 103/200 (LR 0.04764) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 89.13
2024-08-30 17:08:59,732 [podnet.py] => Task 0, Epoch 104/200 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 84.87
2024-08-30 17:09:01,400 [podnet.py] => Task 0, Epoch 105/200 (LR 0.04608) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 88.63
2024-08-30 17:09:03,028 [podnet.py] => Task 0, Epoch 106/200 (LR 0.04529) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 86.70
2024-08-30 17:09:05,069 [podnet.py] => Task 0, Epoch 107/200 (LR 0.04451) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 85.30
2024-08-30 17:09:07,090 [podnet.py] => Task 0, Epoch 108/200 (LR 0.04373) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 86.13
2024-08-30 17:09:08,613 [podnet.py] => Task 0, Epoch 109/200 (LR 0.04295) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 86.57
2024-08-30 17:09:10,160 [podnet.py] => Task 0, Epoch 110/200 (LR 0.04218) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 88.03
2024-08-30 17:09:11,693 [podnet.py] => Task 0, Epoch 111/200 (LR 0.04140) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 88.87
2024-08-30 17:09:13,303 [podnet.py] => Task 0, Epoch 112/200 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 86.90
2024-08-30 17:09:14,934 [podnet.py] => Task 0, Epoch 113/200 (LR 0.03986) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 85.40
2024-08-30 17:09:16,604 [podnet.py] => Task 0, Epoch 114/200 (LR 0.03909) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 86.57
2024-08-30 17:09:18,294 [podnet.py] => Task 0, Epoch 115/200 (LR 0.03833) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 89.27
2024-08-30 17:09:20,130 [podnet.py] => Task 0, Epoch 116/200 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 90.97
2024-08-30 17:09:22,042 [podnet.py] => Task 0, Epoch 117/200 (LR 0.03681) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.73
2024-08-30 17:09:23,899 [podnet.py] => Task 0, Epoch 118/200 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 84.87
2024-08-30 17:09:25,694 [podnet.py] => Task 0, Epoch 119/200 (LR 0.03530) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 87.47
2024-08-30 17:09:27,282 [podnet.py] => Task 0, Epoch 120/200 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 88.33
2024-08-30 17:09:28,976 [podnet.py] => Task 0, Epoch 121/200 (LR 0.03380) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 71.53
2024-08-30 17:09:30,725 [podnet.py] => Task 0, Epoch 122/200 (LR 0.03306) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.85, Test_acc 84.37
2024-08-30 17:09:32,606 [podnet.py] => Task 0, Epoch 123/200 (LR 0.03233) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 87.00
2024-08-30 17:09:34,292 [podnet.py] => Task 0, Epoch 124/200 (LR 0.03159) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.87
2024-08-30 17:09:35,986 [podnet.py] => Task 0, Epoch 125/200 (LR 0.03087) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 87.83
2024-08-30 17:09:37,537 [podnet.py] => Task 0, Epoch 126/200 (LR 0.03014) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.53
2024-08-30 17:09:39,226 [podnet.py] => Task 0, Epoch 127/200 (LR 0.02942) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.17
2024-08-30 17:09:40,862 [podnet.py] => Task 0, Epoch 128/200 (LR 0.02871) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.03
2024-08-30 17:09:42,560 [podnet.py] => Task 0, Epoch 129/200 (LR 0.02800) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 87.80
2024-08-30 17:09:44,285 [podnet.py] => Task 0, Epoch 130/200 (LR 0.02730) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.13
2024-08-30 17:09:46,262 [podnet.py] => Task 0, Epoch 131/200 (LR 0.02660) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.50
2024-08-30 17:09:47,820 [podnet.py] => Task 0, Epoch 132/200 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:09:49,511 [podnet.py] => Task 0, Epoch 133/200 (LR 0.02523) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.03
2024-08-30 17:09:51,136 [podnet.py] => Task 0, Epoch 134/200 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:09:52,752 [podnet.py] => Task 0, Epoch 135/200 (LR 0.02388) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:09:54,470 [podnet.py] => Task 0, Epoch 136/200 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 88.10
2024-08-30 17:09:56,103 [podnet.py] => Task 0, Epoch 137/200 (LR 0.02255) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 89.17
2024-08-30 17:09:57,774 [podnet.py] => Task 0, Epoch 138/200 (LR 0.02190) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 89.93
2024-08-30 17:09:59,502 [podnet.py] => Task 0, Epoch 139/200 (LR 0.02125) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:10:01,207 [podnet.py] => Task 0, Epoch 140/200 (LR 0.02061) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-08-30 17:10:02,810 [podnet.py] => Task 0, Epoch 141/200 (LR 0.01998) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 90.00
2024-08-30 17:10:04,441 [podnet.py] => Task 0, Epoch 142/200 (LR 0.01935) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 90.20
2024-08-30 17:10:05,911 [podnet.py] => Task 0, Epoch 143/200 (LR 0.01874) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 86.43
2024-08-30 17:10:07,572 [podnet.py] => Task 0, Epoch 144/200 (LR 0.01813) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 87.17
2024-08-30 17:10:09,283 [podnet.py] => Task 0, Epoch 145/200 (LR 0.01753) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.57
2024-08-30 17:10:10,946 [podnet.py] => Task 0, Epoch 146/200 (LR 0.01693) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.30
2024-08-30 17:10:12,677 [podnet.py] => Task 0, Epoch 147/200 (LR 0.01635) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:10:14,430 [podnet.py] => Task 0, Epoch 148/200 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 17:10:16,143 [podnet.py] => Task 0, Epoch 149/200 (LR 0.01520) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:10:18,163 [podnet.py] => Task 0, Epoch 150/200 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:10:19,657 [podnet.py] => Task 0, Epoch 151/200 (LR 0.01409) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:10:21,251 [podnet.py] => Task 0, Epoch 152/200 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:10:23,040 [podnet.py] => Task 0, Epoch 153/200 (LR 0.01302) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:10:24,823 [podnet.py] => Task 0, Epoch 154/200 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-30 17:10:26,845 [podnet.py] => Task 0, Epoch 155/200 (LR 0.01198) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.60
2024-08-30 17:10:28,727 [podnet.py] => Task 0, Epoch 156/200 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 88.73
2024-08-30 17:10:30,677 [podnet.py] => Task 0, Epoch 157/200 (LR 0.01098) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:10:32,453 [podnet.py] => Task 0, Epoch 158/200 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-30 17:10:34,731 [podnet.py] => Task 0, Epoch 159/200 (LR 0.01002) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.83
2024-08-30 17:10:36,329 [podnet.py] => Task 0, Epoch 160/200 (LR 0.00955) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.47
2024-08-30 17:10:38,052 [podnet.py] => Task 0, Epoch 161/200 (LR 0.00909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.73
2024-08-30 17:10:39,743 [podnet.py] => Task 0, Epoch 162/200 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 17:10:41,289 [podnet.py] => Task 0, Epoch 163/200 (LR 0.00821) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:10:42,849 [podnet.py] => Task 0, Epoch 164/200 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:10:44,526 [podnet.py] => Task 0, Epoch 165/200 (LR 0.00737) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:10:46,228 [podnet.py] => Task 0, Epoch 166/200 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:10:48,051 [podnet.py] => Task 0, Epoch 167/200 (LR 0.00657) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:10:49,846 [podnet.py] => Task 0, Epoch 168/200 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:10:51,371 [podnet.py] => Task 0, Epoch 169/200 (LR 0.00581) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:10:53,177 [podnet.py] => Task 0, Epoch 170/200 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:10:54,877 [podnet.py] => Task 0, Epoch 171/200 (LR 0.00510) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:10:56,691 [podnet.py] => Task 0, Epoch 172/200 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:10:58,290 [podnet.py] => Task 0, Epoch 173/200 (LR 0.00443) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:10:59,920 [podnet.py] => Task 0, Epoch 174/200 (LR 0.00411) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.03
2024-08-30 17:11:01,544 [podnet.py] => Task 0, Epoch 175/200 (LR 0.00381) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.07
2024-08-30 17:11:03,286 [podnet.py] => Task 0, Epoch 176/200 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:11:05,065 [podnet.py] => Task 0, Epoch 177/200 (LR 0.00323) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:11:06,793 [podnet.py] => Task 0, Epoch 178/200 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-30 17:11:08,600 [podnet.py] => Task 0, Epoch 179/200 (LR 0.00270) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.93
2024-08-30 17:11:10,117 [podnet.py] => Task 0, Epoch 180/200 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:11:12,222 [podnet.py] => Task 0, Epoch 181/200 (LR 0.00221) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.47
2024-08-30 17:11:14,100 [podnet.py] => Task 0, Epoch 182/200 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.37
2024-08-30 17:11:15,914 [podnet.py] => Task 0, Epoch 183/200 (LR 0.00177) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:11:17,374 [podnet.py] => Task 0, Epoch 184/200 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:11:18,882 [podnet.py] => Task 0, Epoch 185/200 (LR 0.00138) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:11:20,497 [podnet.py] => Task 0, Epoch 186/200 (LR 0.00120) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.43
2024-08-30 17:11:22,035 [podnet.py] => Task 0, Epoch 187/200 (LR 0.00104) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:11:23,693 [podnet.py] => Task 0, Epoch 188/200 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:11:25,339 [podnet.py] => Task 0, Epoch 189/200 (LR 0.00074) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:11:27,182 [podnet.py] => Task 0, Epoch 190/200 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:11:29,021 [podnet.py] => Task 0, Epoch 191/200 (LR 0.00050) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:11:30,731 [podnet.py] => Task 0, Epoch 192/200 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:11:32,406 [podnet.py] => Task 0, Epoch 193/200 (LR 0.00030) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:11:34,116 [podnet.py] => Task 0, Epoch 194/200 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:11:35,775 [podnet.py] => Task 0, Epoch 195/200 (LR 0.00015) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:11:37,288 [podnet.py] => Task 0, Epoch 196/200 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:11:38,964 [podnet.py] => Task 0, Epoch 197/200 (LR 0.00006) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:11:40,612 [podnet.py] => Task 0, Epoch 198/200 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:11:42,270 [podnet.py] => Task 0, Epoch 199/200 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:11:44,083 [podnet.py] => Task 0, Epoch 200/200 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:11:44,375 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:11:44,376 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:11:49,770 [podnet.py] => Exemplar size: 500
2024-08-30 17:11:49,770 [trainer.py] => CNN: {'total': 89.63, '00-04': 89.63, 'old': 0, 'new': 89.63}
2024-08-30 17:11:49,770 [trainer.py] => NME: {'total': 89.6, '00-04': 89.6, 'old': 0, 'new': 89.6}
2024-08-30 17:11:49,770 [trainer.py] => CNN top1 curve: [89.63]
2024-08-30 17:11:49,770 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 17:11:49,770 [trainer.py] => NME top1 curve: [89.6]
2024-08-30 17:11:49,770 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 17:11:49,770 [trainer.py] => Average Accuracy (CNN): 89.63
2024-08-30 17:11:49,770 [trainer.py] => Average Accuracy (NME): 89.6
2024-08-30 17:11:49,771 [trainer.py] => All params: 3869505
2024-08-30 17:11:49,771 [trainer.py] => Trainable params: 3869505
2024-08-30 17:11:49,771 [podnet.py] => Learning on 5-7
2024-08-30 17:11:49,802 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 17:11:51,334 [podnet.py] => Task 1, Epoch 1/200 (LR 0.09999) => LSC_loss 1.16, Spatial_loss 2.11, Flat_loss 0.72, Train_acc 69.69, Test_acc 25.88
2024-08-30 17:11:52,694 [podnet.py] => Task 1, Epoch 2/200 (LR 0.09998) => LSC_loss 0.52, Spatial_loss 1.62, Flat_loss 0.45, Train_acc 87.69, Test_acc 46.24
2024-08-30 17:11:54,089 [podnet.py] => Task 1, Epoch 3/200 (LR 0.09994) => LSC_loss 0.36, Spatial_loss 1.50, Flat_loss 0.38, Train_acc 90.89, Test_acc 46.88
2024-08-30 17:11:55,649 [podnet.py] => Task 1, Epoch 4/200 (LR 0.09990) => LSC_loss 0.27, Spatial_loss 1.28, Flat_loss 0.32, Train_acc 93.62, Test_acc 43.17
2024-08-30 17:11:57,122 [podnet.py] => Task 1, Epoch 5/200 (LR 0.09985) => LSC_loss 0.24, Spatial_loss 1.21, Flat_loss 0.29, Train_acc 94.73, Test_acc 52.88
2024-08-30 17:11:58,843 [podnet.py] => Task 1, Epoch 6/200 (LR 0.09978) => LSC_loss 0.23, Spatial_loss 1.20, Flat_loss 0.29, Train_acc 94.27, Test_acc 49.57
2024-08-30 17:12:00,475 [podnet.py] => Task 1, Epoch 7/200 (LR 0.09970) => LSC_loss 0.17, Spatial_loss 1.05, Flat_loss 0.26, Train_acc 96.62, Test_acc 47.52
2024-08-30 17:12:02,079 [podnet.py] => Task 1, Epoch 8/200 (LR 0.09961) => LSC_loss 0.17, Spatial_loss 1.02, Flat_loss 0.26, Train_acc 96.49, Test_acc 54.33
2024-08-30 17:12:03,515 [podnet.py] => Task 1, Epoch 9/200 (LR 0.09950) => LSC_loss 0.15, Spatial_loss 1.01, Flat_loss 0.25, Train_acc 96.80, Test_acc 55.31
2024-08-30 17:12:04,806 [podnet.py] => Task 1, Epoch 10/200 (LR 0.09938) => LSC_loss 0.17, Spatial_loss 1.05, Flat_loss 0.26, Train_acc 96.09, Test_acc 53.98
2024-08-30 17:12:06,063 [podnet.py] => Task 1, Epoch 11/200 (LR 0.09926) => LSC_loss 0.13, Spatial_loss 0.93, Flat_loss 0.24, Train_acc 97.40, Test_acc 61.05
2024-08-30 17:12:07,382 [podnet.py] => Task 1, Epoch 12/200 (LR 0.09911) => LSC_loss 0.11, Spatial_loss 0.86, Flat_loss 0.22, Train_acc 98.62, Test_acc 64.55
2024-08-30 17:12:08,586 [podnet.py] => Task 1, Epoch 13/200 (LR 0.09896) => LSC_loss 0.10, Spatial_loss 0.82, Flat_loss 0.22, Train_acc 98.56, Test_acc 64.17
2024-08-30 17:12:09,978 [podnet.py] => Task 1, Epoch 14/200 (LR 0.09880) => LSC_loss 0.10, Spatial_loss 0.78, Flat_loss 0.21, Train_acc 98.96, Test_acc 62.05
2024-08-30 17:12:11,345 [podnet.py] => Task 1, Epoch 15/200 (LR 0.09862) => LSC_loss 0.09, Spatial_loss 0.74, Flat_loss 0.21, Train_acc 99.31, Test_acc 58.88
2024-08-30 17:12:12,539 [podnet.py] => Task 1, Epoch 16/200 (LR 0.09843) => LSC_loss 0.08, Spatial_loss 0.78, Flat_loss 0.21, Train_acc 99.16, Test_acc 60.00
2024-08-30 17:12:13,750 [podnet.py] => Task 1, Epoch 17/200 (LR 0.09823) => LSC_loss 0.08, Spatial_loss 0.78, Flat_loss 0.21, Train_acc 99.33, Test_acc 65.10
2024-08-30 17:12:15,113 [podnet.py] => Task 1, Epoch 18/200 (LR 0.09801) => LSC_loss 0.07, Spatial_loss 0.74, Flat_loss 0.20, Train_acc 99.71, Test_acc 59.12
2024-08-30 17:12:16,489 [podnet.py] => Task 1, Epoch 19/200 (LR 0.09779) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.20, Train_acc 99.44, Test_acc 62.90
2024-08-30 17:12:17,924 [podnet.py] => Task 1, Epoch 20/200 (LR 0.09755) => LSC_loss 0.07, Spatial_loss 0.71, Flat_loss 0.19, Train_acc 99.73, Test_acc 62.45
2024-08-30 17:12:19,182 [podnet.py] => Task 1, Epoch 21/200 (LR 0.09730) => LSC_loss 0.07, Spatial_loss 0.68, Flat_loss 0.20, Train_acc 99.51, Test_acc 59.05
2024-08-30 17:12:20,641 [podnet.py] => Task 1, Epoch 22/200 (LR 0.09704) => LSC_loss 0.07, Spatial_loss 0.67, Flat_loss 0.19, Train_acc 99.58, Test_acc 68.69
2024-08-30 17:12:22,001 [podnet.py] => Task 1, Epoch 23/200 (LR 0.09677) => LSC_loss 0.07, Spatial_loss 0.68, Flat_loss 0.19, Train_acc 99.42, Test_acc 57.90
2024-08-30 17:12:23,387 [podnet.py] => Task 1, Epoch 24/200 (LR 0.09649) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.19, Train_acc 99.69, Test_acc 60.67
2024-08-30 17:12:24,763 [podnet.py] => Task 1, Epoch 25/200 (LR 0.09619) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.20, Train_acc 99.44, Test_acc 68.67
2024-08-30 17:12:26,050 [podnet.py] => Task 1, Epoch 26/200 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.19, Train_acc 99.62, Test_acc 58.90
2024-08-30 17:12:27,533 [podnet.py] => Task 1, Epoch 27/200 (LR 0.09557) => LSC_loss 0.07, Spatial_loss 0.70, Flat_loss 0.19, Train_acc 99.40, Test_acc 57.52
2024-08-30 17:12:28,759 [podnet.py] => Task 1, Epoch 28/200 (LR 0.09524) => LSC_loss 0.08, Spatial_loss 0.76, Flat_loss 0.21, Train_acc 98.60, Test_acc 61.90
2024-08-30 17:12:30,355 [podnet.py] => Task 1, Epoch 29/200 (LR 0.09490) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.19, Train_acc 99.73, Test_acc 63.31
2024-08-30 17:12:31,837 [podnet.py] => Task 1, Epoch 30/200 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.19, Train_acc 99.71, Test_acc 68.60
2024-08-30 17:12:33,174 [podnet.py] => Task 1, Epoch 31/200 (LR 0.09419) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.18, Train_acc 99.80, Test_acc 63.12
2024-08-30 17:12:34,545 [podnet.py] => Task 1, Epoch 32/200 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.19, Train_acc 99.82, Test_acc 65.40
2024-08-30 17:12:35,834 [podnet.py] => Task 1, Epoch 33/200 (LR 0.09343) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.18, Train_acc 99.82, Test_acc 65.40
2024-08-30 17:12:37,169 [podnet.py] => Task 1, Epoch 34/200 (LR 0.09304) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.18, Train_acc 99.78, Test_acc 59.60
2024-08-30 17:12:38,502 [podnet.py] => Task 1, Epoch 35/200 (LR 0.09263) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.18, Train_acc 99.82, Test_acc 60.36
2024-08-30 17:12:39,822 [podnet.py] => Task 1, Epoch 36/200 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.17, Train_acc 99.80, Test_acc 64.36
2024-08-30 17:12:41,145 [podnet.py] => Task 1, Epoch 37/200 (LR 0.09179) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.17, Train_acc 99.82, Test_acc 61.69
2024-08-30 17:12:42,657 [podnet.py] => Task 1, Epoch 38/200 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.17, Train_acc 99.87, Test_acc 62.05
2024-08-30 17:12:44,325 [podnet.py] => Task 1, Epoch 39/200 (LR 0.09091) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.17, Train_acc 99.82, Test_acc 58.57
2024-08-30 17:12:45,654 [podnet.py] => Task 1, Epoch 40/200 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.18, Train_acc 99.78, Test_acc 64.71
2024-08-30 17:12:46,990 [podnet.py] => Task 1, Epoch 41/200 (LR 0.08998) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.17, Train_acc 99.76, Test_acc 63.00
2024-08-30 17:12:48,394 [podnet.py] => Task 1, Epoch 42/200 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.17, Train_acc 99.91, Test_acc 62.67
2024-08-30 17:12:49,848 [podnet.py] => Task 1, Epoch 43/200 (LR 0.08902) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.17, Train_acc 99.84, Test_acc 62.38
2024-08-30 17:12:51,439 [podnet.py] => Task 1, Epoch 44/200 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.17, Train_acc 99.93, Test_acc 62.64
2024-08-30 17:12:52,892 [podnet.py] => Task 1, Epoch 45/200 (LR 0.08802) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.17, Train_acc 99.87, Test_acc 62.24
2024-08-30 17:12:54,330 [podnet.py] => Task 1, Epoch 46/200 (LR 0.08751) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.98, Test_acc 62.36
2024-08-30 17:12:55,823 [podnet.py] => Task 1, Epoch 47/200 (LR 0.08698) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.17, Train_acc 99.87, Test_acc 62.93
2024-08-30 17:12:57,184 [podnet.py] => Task 1, Epoch 48/200 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.17, Train_acc 99.89, Test_acc 60.76
2024-08-30 17:12:58,469 [podnet.py] => Task 1, Epoch 49/200 (LR 0.08591) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.17, Train_acc 99.80, Test_acc 61.26
2024-08-30 17:12:59,763 [podnet.py] => Task 1, Epoch 50/200 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.98, Test_acc 66.90
2024-08-30 17:13:01,084 [podnet.py] => Task 1, Epoch 51/200 (LR 0.08480) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.16, Train_acc 99.93, Test_acc 63.52
2024-08-30 17:13:02,455 [podnet.py] => Task 1, Epoch 52/200 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.16, Train_acc 99.93, Test_acc 64.62
2024-08-30 17:13:03,758 [podnet.py] => Task 1, Epoch 53/200 (LR 0.08365) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.16, Train_acc 99.91, Test_acc 65.45
2024-08-30 17:13:05,080 [podnet.py] => Task 1, Epoch 54/200 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.16, Train_acc 99.91, Test_acc 63.26
2024-08-30 17:13:06,357 [podnet.py] => Task 1, Epoch 55/200 (LR 0.08247) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.87, Test_acc 63.48
2024-08-30 17:13:07,633 [podnet.py] => Task 1, Epoch 56/200 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.16, Train_acc 99.96, Test_acc 62.29
2024-08-30 17:13:08,947 [podnet.py] => Task 1, Epoch 57/200 (LR 0.08126) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.16, Train_acc 99.96, Test_acc 63.67
2024-08-30 17:13:10,348 [podnet.py] => Task 1, Epoch 58/200 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.16, Train_acc 99.96, Test_acc 63.71
2024-08-30 17:13:11,645 [podnet.py] => Task 1, Epoch 59/200 (LR 0.08002) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.16, Train_acc 99.93, Test_acc 60.67
2024-08-30 17:13:13,029 [podnet.py] => Task 1, Epoch 60/200 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.16, Train_acc 99.91, Test_acc 64.93
2024-08-30 17:13:14,329 [podnet.py] => Task 1, Epoch 61/200 (LR 0.07875) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.16, Train_acc 99.89, Test_acc 63.76
2024-08-30 17:13:15,655 [podnet.py] => Task 1, Epoch 62/200 (LR 0.07810) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.15, Train_acc 99.93, Test_acc 59.43
2024-08-30 17:13:17,175 [podnet.py] => Task 1, Epoch 63/200 (LR 0.07745) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.98, Test_acc 65.07
2024-08-30 17:13:18,533 [podnet.py] => Task 1, Epoch 64/200 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 100.00, Test_acc 65.17
2024-08-30 17:13:19,797 [podnet.py] => Task 1, Epoch 65/200 (LR 0.07612) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.15, Train_acc 99.93, Test_acc 63.93
2024-08-30 17:13:21,273 [podnet.py] => Task 1, Epoch 66/200 (LR 0.07545) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.16, Train_acc 99.87, Test_acc 61.83
2024-08-30 17:13:22,661 [podnet.py] => Task 1, Epoch 67/200 (LR 0.07477) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.16, Train_acc 99.84, Test_acc 66.90
2024-08-30 17:13:24,195 [podnet.py] => Task 1, Epoch 68/200 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.93, Test_acc 61.74
2024-08-30 17:13:25,645 [podnet.py] => Task 1, Epoch 69/200 (LR 0.07340) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.16, Train_acc 99.93, Test_acc 55.17
2024-08-30 17:13:27,089 [podnet.py] => Task 1, Epoch 70/200 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.16, Train_acc 99.91, Test_acc 61.95
2024-08-30 17:13:28,384 [podnet.py] => Task 1, Epoch 71/200 (LR 0.07200) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.15, Train_acc 99.98, Test_acc 61.07
2024-08-30 17:13:29,675 [podnet.py] => Task 1, Epoch 72/200 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.15, Train_acc 99.98, Test_acc 64.40
2024-08-30 17:13:30,977 [podnet.py] => Task 1, Epoch 73/200 (LR 0.07058) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.93, Test_acc 62.50
2024-08-30 17:13:32,369 [podnet.py] => Task 1, Epoch 74/200 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.15, Train_acc 99.93, Test_acc 61.55
2024-08-30 17:13:33,922 [podnet.py] => Task 1, Epoch 75/200 (LR 0.06913) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.15, Train_acc 99.93, Test_acc 67.60
2024-08-30 17:13:35,334 [podnet.py] => Task 1, Epoch 76/200 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.89, Test_acc 61.52
2024-08-30 17:13:36,731 [podnet.py] => Task 1, Epoch 77/200 (LR 0.06767) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.16, Train_acc 99.44, Test_acc 62.98
2024-08-30 17:13:38,140 [podnet.py] => Task 1, Epoch 78/200 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.91, Test_acc 60.00
2024-08-30 17:13:39,616 [podnet.py] => Task 1, Epoch 79/200 (LR 0.06620) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.15, Train_acc 99.93, Test_acc 63.57
2024-08-30 17:13:41,391 [podnet.py] => Task 1, Epoch 80/200 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.15, Train_acc 99.96, Test_acc 65.64
2024-08-30 17:13:42,872 [podnet.py] => Task 1, Epoch 81/200 (LR 0.06470) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.15, Train_acc 99.93, Test_acc 65.26
2024-08-30 17:13:44,346 [podnet.py] => Task 1, Epoch 82/200 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.79
2024-08-30 17:13:45,745 [podnet.py] => Task 1, Epoch 83/200 (LR 0.06319) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.14, Train_acc 99.93, Test_acc 64.48
2024-08-30 17:13:47,106 [podnet.py] => Task 1, Epoch 84/200 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.60
2024-08-30 17:13:48,434 [podnet.py] => Task 1, Epoch 85/200 (LR 0.06167) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.33
2024-08-30 17:13:49,792 [podnet.py] => Task 1, Epoch 86/200 (LR 0.06091) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.14, Train_acc 99.89, Test_acc 62.21
2024-08-30 17:13:51,211 [podnet.py] => Task 1, Epoch 87/200 (LR 0.06014) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.15, Train_acc 99.84, Test_acc 61.48
2024-08-30 17:13:52,744 [podnet.py] => Task 1, Epoch 88/200 (LR 0.05937) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.15, Train_acc 99.93, Test_acc 64.17
2024-08-30 17:13:54,433 [podnet.py] => Task 1, Epoch 89/200 (LR 0.05860) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.15, Train_acc 99.93, Test_acc 62.31
2024-08-30 17:13:55,904 [podnet.py] => Task 1, Epoch 90/200 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.15, Train_acc 99.96, Test_acc 64.81
2024-08-30 17:13:57,316 [podnet.py] => Task 1, Epoch 91/200 (LR 0.05705) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.14
2024-08-30 17:13:58,562 [podnet.py] => Task 1, Epoch 92/200 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.14, Train_acc 99.98, Test_acc 64.55
2024-08-30 17:13:59,845 [podnet.py] => Task 1, Epoch 93/200 (LR 0.05549) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.14, Train_acc 99.98, Test_acc 62.95
2024-08-30 17:14:01,175 [podnet.py] => Task 1, Epoch 94/200 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.14, Train_acc 100.00, Test_acc 64.48
2024-08-30 17:14:02,355 [podnet.py] => Task 1, Epoch 95/200 (LR 0.05392) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.14, Train_acc 99.98, Test_acc 68.02
2024-08-30 17:14:03,700 [podnet.py] => Task 1, Epoch 96/200 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.50
2024-08-30 17:14:05,247 [podnet.py] => Task 1, Epoch 97/200 (LR 0.05236) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.60
2024-08-30 17:14:06,803 [podnet.py] => Task 1, Epoch 98/200 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.14, Train_acc 99.91, Test_acc 62.83
2024-08-30 17:14:08,276 [podnet.py] => Task 1, Epoch 99/200 (LR 0.05079) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 100.00, Test_acc 64.43
2024-08-30 17:14:09,929 [podnet.py] => Task 1, Epoch 100/200 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.64
2024-08-30 17:14:11,245 [podnet.py] => Task 1, Epoch 101/200 (LR 0.04921) => LSC_loss 0.03, Spatial_loss 0.43, Flat_loss 0.14, Train_acc 99.98, Test_acc 63.71
2024-08-30 17:14:12,862 [podnet.py] => Task 1, Epoch 102/200 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.14, Train_acc 99.98, Test_acc 62.52
2024-08-30 17:14:14,400 [podnet.py] => Task 1, Epoch 103/200 (LR 0.04764) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.98, Test_acc 63.71
2024-08-30 17:14:15,718 [podnet.py] => Task 1, Epoch 104/200 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.98, Test_acc 66.43
2024-08-30 17:14:17,113 [podnet.py] => Task 1, Epoch 105/200 (LR 0.04608) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.91, Test_acc 62.02
2024-08-30 17:14:18,397 [podnet.py] => Task 1, Epoch 106/200 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 100.00, Test_acc 64.19
2024-08-30 17:14:19,681 [podnet.py] => Task 1, Epoch 107/200 (LR 0.04451) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.96, Test_acc 63.67
2024-08-30 17:14:21,066 [podnet.py] => Task 1, Epoch 108/200 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.95
2024-08-30 17:14:22,466 [podnet.py] => Task 1, Epoch 109/200 (LR 0.04295) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.14
2024-08-30 17:14:23,932 [podnet.py] => Task 1, Epoch 110/200 (LR 0.04218) => LSC_loss 0.03, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.17
2024-08-30 17:14:25,590 [podnet.py] => Task 1, Epoch 111/200 (LR 0.04140) => LSC_loss 0.03, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 99.98, Test_acc 62.57
2024-08-30 17:14:27,303 [podnet.py] => Task 1, Epoch 112/200 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.67
2024-08-30 17:14:28,841 [podnet.py] => Task 1, Epoch 113/200 (LR 0.03986) => LSC_loss 0.03, Spatial_loss 0.38, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.90
2024-08-30 17:14:30,228 [podnet.py] => Task 1, Epoch 114/200 (LR 0.03909) => LSC_loss 0.03, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.36
2024-08-30 17:14:31,798 [podnet.py] => Task 1, Epoch 115/200 (LR 0.03833) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.38
2024-08-30 17:14:33,309 [podnet.py] => Task 1, Epoch 116/200 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.71
2024-08-30 17:14:34,823 [podnet.py] => Task 1, Epoch 117/200 (LR 0.03681) => LSC_loss 0.03, Spatial_loss 0.39, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.57
2024-08-30 17:14:36,539 [podnet.py] => Task 1, Epoch 118/200 (LR 0.03605) => LSC_loss 0.03, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 100.00, Test_acc 66.31
2024-08-30 17:14:38,128 [podnet.py] => Task 1, Epoch 119/200 (LR 0.03530) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.69
2024-08-30 17:14:39,520 [podnet.py] => Task 1, Epoch 120/200 (LR 0.03455) => LSC_loss 0.03, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.57
2024-08-30 17:14:40,902 [podnet.py] => Task 1, Epoch 121/200 (LR 0.03380) => LSC_loss 0.03, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.76
2024-08-30 17:14:42,407 [podnet.py] => Task 1, Epoch 122/200 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.33
2024-08-30 17:14:43,884 [podnet.py] => Task 1, Epoch 123/200 (LR 0.03233) => LSC_loss 0.03, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.29
2024-08-30 17:14:45,361 [podnet.py] => Task 1, Epoch 124/200 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.93, Test_acc 66.14
2024-08-30 17:14:46,748 [podnet.py] => Task 1, Epoch 125/200 (LR 0.03087) => LSC_loss 0.03, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.40
2024-08-30 17:14:48,156 [podnet.py] => Task 1, Epoch 126/200 (LR 0.03014) => LSC_loss 0.03, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.26
2024-08-30 17:14:49,520 [podnet.py] => Task 1, Epoch 127/200 (LR 0.02942) => LSC_loss 0.03, Spatial_loss 0.33, Flat_loss 0.13, Train_acc 100.00, Test_acc 66.19
2024-08-30 17:14:51,041 [podnet.py] => Task 1, Epoch 128/200 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.32, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.36
2024-08-30 17:14:52,262 [podnet.py] => Task 1, Epoch 129/200 (LR 0.02800) => LSC_loss 0.03, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.95
2024-08-30 17:14:53,954 [podnet.py] => Task 1, Epoch 130/200 (LR 0.02730) => LSC_loss 0.03, Spatial_loss 0.32, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.64
2024-08-30 17:14:55,739 [podnet.py] => Task 1, Epoch 131/200 (LR 0.02660) => LSC_loss 0.03, Spatial_loss 0.32, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.05
2024-08-30 17:14:57,497 [podnet.py] => Task 1, Epoch 132/200 (LR 0.02591) => LSC_loss 0.03, Spatial_loss 0.32, Flat_loss 0.13, Train_acc 100.00, Test_acc 66.40
2024-08-30 17:14:59,013 [podnet.py] => Task 1, Epoch 133/200 (LR 0.02523) => LSC_loss 0.03, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.31
2024-08-30 17:15:00,502 [podnet.py] => Task 1, Epoch 134/200 (LR 0.02455) => LSC_loss 0.03, Spatial_loss 0.32, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.95
2024-08-30 17:15:01,975 [podnet.py] => Task 1, Epoch 135/200 (LR 0.02388) => LSC_loss 0.03, Spatial_loss 0.31, Flat_loss 0.13, Train_acc 100.00, Test_acc 66.36
2024-08-30 17:15:03,418 [podnet.py] => Task 1, Epoch 136/200 (LR 0.02321) => LSC_loss 0.03, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.57
2024-08-30 17:15:04,822 [podnet.py] => Task 1, Epoch 137/200 (LR 0.02255) => LSC_loss 0.03, Spatial_loss 0.30, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.69
2024-08-30 17:15:06,264 [podnet.py] => Task 1, Epoch 138/200 (LR 0.02190) => LSC_loss 0.03, Spatial_loss 0.30, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.93
2024-08-30 17:15:07,644 [podnet.py] => Task 1, Epoch 139/200 (LR 0.02125) => LSC_loss 0.03, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.52
2024-08-30 17:15:09,024 [podnet.py] => Task 1, Epoch 140/200 (LR 0.02061) => LSC_loss 0.03, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.26
2024-08-30 17:15:10,483 [podnet.py] => Task 1, Epoch 141/200 (LR 0.01998) => LSC_loss 0.03, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 99.96, Test_acc 66.71
2024-08-30 17:15:12,085 [podnet.py] => Task 1, Epoch 142/200 (LR 0.01935) => LSC_loss 0.03, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.90
2024-08-30 17:15:13,539 [podnet.py] => Task 1, Epoch 143/200 (LR 0.01874) => LSC_loss 0.03, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.36
2024-08-30 17:15:14,924 [podnet.py] => Task 1, Epoch 144/200 (LR 0.01813) => LSC_loss 0.03, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.00
2024-08-30 17:15:16,555 [podnet.py] => Task 1, Epoch 145/200 (LR 0.01753) => LSC_loss 0.03, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.90
2024-08-30 17:15:18,044 [podnet.py] => Task 1, Epoch 146/200 (LR 0.01693) => LSC_loss 0.03, Spatial_loss 0.26, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.83
2024-08-30 17:15:19,598 [podnet.py] => Task 1, Epoch 147/200 (LR 0.01635) => LSC_loss 0.03, Spatial_loss 0.27, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.95
2024-08-30 17:15:21,098 [podnet.py] => Task 1, Epoch 148/200 (LR 0.01577) => LSC_loss 0.03, Spatial_loss 0.26, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.60
2024-08-30 17:15:22,785 [podnet.py] => Task 1, Epoch 149/200 (LR 0.01520) => LSC_loss 0.03, Spatial_loss 0.26, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.74
2024-08-30 17:15:24,388 [podnet.py] => Task 1, Epoch 150/200 (LR 0.01464) => LSC_loss 0.03, Spatial_loss 0.25, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.83
2024-08-30 17:15:25,713 [podnet.py] => Task 1, Epoch 151/200 (LR 0.01409) => LSC_loss 0.03, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.05
2024-08-30 17:15:27,136 [podnet.py] => Task 1, Epoch 152/200 (LR 0.01355) => LSC_loss 0.03, Spatial_loss 0.27, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.76
2024-08-30 17:15:28,498 [podnet.py] => Task 1, Epoch 153/200 (LR 0.01302) => LSC_loss 0.03, Spatial_loss 0.26, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.98
2024-08-30 17:15:30,038 [podnet.py] => Task 1, Epoch 154/200 (LR 0.01249) => LSC_loss 0.03, Spatial_loss 0.24, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.12
2024-08-30 17:15:31,775 [podnet.py] => Task 1, Epoch 155/200 (LR 0.01198) => LSC_loss 0.03, Spatial_loss 0.24, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.69
2024-08-30 17:15:33,825 [podnet.py] => Task 1, Epoch 156/200 (LR 0.01147) => LSC_loss 0.03, Spatial_loss 0.26, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.86
2024-08-30 17:15:35,707 [podnet.py] => Task 1, Epoch 157/200 (LR 0.01098) => LSC_loss 0.03, Spatial_loss 0.26, Flat_loss 0.12, Train_acc 100.00, Test_acc 64.36
2024-08-30 17:15:37,027 [podnet.py] => Task 1, Epoch 158/200 (LR 0.01049) => LSC_loss 0.03, Spatial_loss 0.25, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.81
2024-08-30 17:15:38,456 [podnet.py] => Task 1, Epoch 159/200 (LR 0.01002) => LSC_loss 0.03, Spatial_loss 0.24, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.40
2024-08-30 17:15:39,851 [podnet.py] => Task 1, Epoch 160/200 (LR 0.00955) => LSC_loss 0.03, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.67
2024-08-30 17:15:41,174 [podnet.py] => Task 1, Epoch 161/200 (LR 0.00909) => LSC_loss 0.03, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.45
2024-08-30 17:15:42,473 [podnet.py] => Task 1, Epoch 162/200 (LR 0.00865) => LSC_loss 0.03, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.83
2024-08-30 17:15:43,806 [podnet.py] => Task 1, Epoch 163/200 (LR 0.00821) => LSC_loss 0.03, Spatial_loss 0.23, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.00
2024-08-30 17:15:45,237 [podnet.py] => Task 1, Epoch 164/200 (LR 0.00778) => LSC_loss 0.03, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.71
2024-08-30 17:15:46,476 [podnet.py] => Task 1, Epoch 165/200 (LR 0.00737) => LSC_loss 0.03, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.76
2024-08-30 17:15:47,791 [podnet.py] => Task 1, Epoch 166/200 (LR 0.00696) => LSC_loss 0.03, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.74
2024-08-30 17:15:49,405 [podnet.py] => Task 1, Epoch 167/200 (LR 0.00657) => LSC_loss 0.03, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.17
2024-08-30 17:15:50,787 [podnet.py] => Task 1, Epoch 168/200 (LR 0.00618) => LSC_loss 0.03, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.43
2024-08-30 17:15:52,148 [podnet.py] => Task 1, Epoch 169/200 (LR 0.00581) => LSC_loss 0.03, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.95
2024-08-30 17:15:53,479 [podnet.py] => Task 1, Epoch 170/200 (LR 0.00545) => LSC_loss 0.03, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.17
2024-08-30 17:15:54,838 [podnet.py] => Task 1, Epoch 171/200 (LR 0.00510) => LSC_loss 0.03, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.79
2024-08-30 17:15:56,191 [podnet.py] => Task 1, Epoch 172/200 (LR 0.00476) => LSC_loss 0.03, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.88
2024-08-30 17:15:57,606 [podnet.py] => Task 1, Epoch 173/200 (LR 0.00443) => LSC_loss 0.03, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.33
2024-08-30 17:15:58,972 [podnet.py] => Task 1, Epoch 174/200 (LR 0.00411) => LSC_loss 0.03, Spatial_loss 0.19, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.38
2024-08-30 17:16:00,265 [podnet.py] => Task 1, Epoch 175/200 (LR 0.00381) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.88
2024-08-30 17:16:01,642 [podnet.py] => Task 1, Epoch 176/200 (LR 0.00351) => LSC_loss 0.03, Spatial_loss 0.19, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.55
2024-08-30 17:16:03,343 [podnet.py] => Task 1, Epoch 177/200 (LR 0.00323) => LSC_loss 0.03, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.33
2024-08-30 17:16:04,750 [podnet.py] => Task 1, Epoch 178/200 (LR 0.00296) => LSC_loss 0.03, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.24
2024-08-30 17:16:06,058 [podnet.py] => Task 1, Epoch 179/200 (LR 0.00270) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.74
2024-08-30 17:16:07,686 [podnet.py] => Task 1, Epoch 180/200 (LR 0.00245) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.81
2024-08-30 17:16:09,105 [podnet.py] => Task 1, Epoch 181/200 (LR 0.00221) => LSC_loss 0.03, Spatial_loss 0.19, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.81
2024-08-30 17:16:10,530 [podnet.py] => Task 1, Epoch 182/200 (LR 0.00199) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.52
2024-08-30 17:16:11,900 [podnet.py] => Task 1, Epoch 183/200 (LR 0.00177) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.60
2024-08-30 17:16:13,274 [podnet.py] => Task 1, Epoch 184/200 (LR 0.00157) => LSC_loss 0.03, Spatial_loss 0.19, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.93
2024-08-30 17:16:14,666 [podnet.py] => Task 1, Epoch 185/200 (LR 0.00138) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.57
2024-08-30 17:16:15,914 [podnet.py] => Task 1, Epoch 186/200 (LR 0.00120) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.29
2024-08-30 17:16:17,178 [podnet.py] => Task 1, Epoch 187/200 (LR 0.00104) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.69
2024-08-30 17:16:18,672 [podnet.py] => Task 1, Epoch 188/200 (LR 0.00089) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.64
2024-08-30 17:16:19,985 [podnet.py] => Task 1, Epoch 189/200 (LR 0.00074) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.93
2024-08-30 17:16:21,321 [podnet.py] => Task 1, Epoch 190/200 (LR 0.00062) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.43
2024-08-30 17:16:22,640 [podnet.py] => Task 1, Epoch 191/200 (LR 0.00050) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.93
2024-08-30 17:16:24,168 [podnet.py] => Task 1, Epoch 192/200 (LR 0.00039) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.05
2024-08-30 17:16:25,455 [podnet.py] => Task 1, Epoch 193/200 (LR 0.00030) => LSC_loss 0.03, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.95
2024-08-30 17:16:26,808 [podnet.py] => Task 1, Epoch 194/200 (LR 0.00022) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.93
2024-08-30 17:16:28,294 [podnet.py] => Task 1, Epoch 195/200 (LR 0.00015) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.83
2024-08-30 17:16:29,740 [podnet.py] => Task 1, Epoch 196/200 (LR 0.00010) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.74
2024-08-30 17:16:31,095 [podnet.py] => Task 1, Epoch 197/200 (LR 0.00006) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.02
2024-08-30 17:16:32,348 [podnet.py] => Task 1, Epoch 198/200 (LR 0.00002) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.93
2024-08-30 17:16:33,619 [podnet.py] => Task 1, Epoch 199/200 (LR 0.00001) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.86
2024-08-30 17:16:34,918 [podnet.py] => Task 1, Epoch 200/200 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.98
2024-08-30 17:16:35,259 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:16:35,259 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:16:36,336 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:16:38,321 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:16:39,485 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:16:42,017 [podnet.py] => Exemplar size: 497
2024-08-30 17:16:42,017 [trainer.py] => CNN: {'total': 66.98, '00-04': 55.7, '05-06': 95.17, 'old': 55.7, 'new': 95.17}
2024-08-30 17:16:42,017 [trainer.py] => NME: {'total': 73.95, '00-04': 73.03, '05-06': 76.25, 'old': 73.03, 'new': 76.25}
2024-08-30 17:16:42,017 [trainer.py] => CNN top1 curve: [89.63, 66.98]
2024-08-30 17:16:42,017 [trainer.py] => CNN top5 curve: [100.0, 98.26]
2024-08-30 17:16:42,017 [trainer.py] => NME top1 curve: [89.6, 73.95]
2024-08-30 17:16:42,017 [trainer.py] => NME top5 curve: [100.0, 98.26]

2024-08-30 17:16:42,017 [trainer.py] => Average Accuracy (CNN): 78.305
2024-08-30 17:16:42,017 [trainer.py] => Average Accuracy (NME): 81.775
2024-08-30 17:16:42,018 [trainer.py] => All params: 3879745
2024-08-30 17:16:42,018 [trainer.py] => Trainable params: 3879745
2024-08-30 17:16:42,019 [podnet.py] => Learning on 7-9
2024-08-30 17:16:42,049 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 17:16:43,650 [podnet.py] => Task 2, Epoch 1/200 (LR 0.09999) => LSC_loss 1.16, Spatial_loss 1.15, Flat_loss 0.48, Train_acc 79.74, Test_acc 34.17
2024-08-30 17:16:45,021 [podnet.py] => Task 2, Epoch 2/200 (LR 0.09998) => LSC_loss 0.34, Spatial_loss 1.07, Flat_loss 0.31, Train_acc 92.26, Test_acc 42.72
2024-08-30 17:16:46,587 [podnet.py] => Task 2, Epoch 3/200 (LR 0.09994) => LSC_loss 0.25, Spatial_loss 0.91, Flat_loss 0.25, Train_acc 94.95, Test_acc 48.59
2024-08-30 17:16:48,224 [podnet.py] => Task 2, Epoch 4/200 (LR 0.09990) => LSC_loss 0.22, Spatial_loss 0.91, Flat_loss 0.24, Train_acc 95.71, Test_acc 53.30
2024-08-30 17:16:49,688 [podnet.py] => Task 2, Epoch 5/200 (LR 0.09985) => LSC_loss 0.16, Spatial_loss 0.84, Flat_loss 0.23, Train_acc 97.29, Test_acc 54.04
2024-08-30 17:16:51,116 [podnet.py] => Task 2, Epoch 6/200 (LR 0.09978) => LSC_loss 0.13, Spatial_loss 0.74, Flat_loss 0.21, Train_acc 98.69, Test_acc 55.43
2024-08-30 17:16:52,772 [podnet.py] => Task 2, Epoch 7/200 (LR 0.09970) => LSC_loss 0.11, Spatial_loss 0.68, Flat_loss 0.20, Train_acc 99.00, Test_acc 56.76
2024-08-30 17:16:54,185 [podnet.py] => Task 2, Epoch 8/200 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.66, Flat_loss 0.19, Train_acc 99.36, Test_acc 53.33
2024-08-30 17:16:55,569 [podnet.py] => Task 2, Epoch 9/200 (LR 0.09950) => LSC_loss 0.09, Spatial_loss 0.65, Flat_loss 0.19, Train_acc 99.44, Test_acc 54.22
2024-08-30 17:16:57,010 [podnet.py] => Task 2, Epoch 10/200 (LR 0.09938) => LSC_loss 0.08, Spatial_loss 0.64, Flat_loss 0.18, Train_acc 99.69, Test_acc 54.94
2024-08-30 17:16:58,414 [podnet.py] => Task 2, Epoch 11/200 (LR 0.09926) => LSC_loss 0.08, Spatial_loss 0.61, Flat_loss 0.18, Train_acc 99.80, Test_acc 55.80
2024-08-30 17:16:59,799 [podnet.py] => Task 2, Epoch 12/200 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.62, Flat_loss 0.18, Train_acc 99.71, Test_acc 54.41
2024-08-30 17:17:01,176 [podnet.py] => Task 2, Epoch 13/200 (LR 0.09896) => LSC_loss 0.07, Spatial_loss 0.62, Flat_loss 0.18, Train_acc 99.82, Test_acc 57.06
2024-08-30 17:17:02,632 [podnet.py] => Task 2, Epoch 14/200 (LR 0.09880) => LSC_loss 0.07, Spatial_loss 0.60, Flat_loss 0.17, Train_acc 99.87, Test_acc 59.70
2024-08-30 17:17:04,108 [podnet.py] => Task 2, Epoch 15/200 (LR 0.09862) => LSC_loss 0.07, Spatial_loss 0.57, Flat_loss 0.17, Train_acc 99.93, Test_acc 58.04
2024-08-30 17:17:05,493 [podnet.py] => Task 2, Epoch 16/200 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.60, Flat_loss 0.17, Train_acc 99.84, Test_acc 53.41
2024-08-30 17:17:06,882 [podnet.py] => Task 2, Epoch 17/200 (LR 0.09823) => LSC_loss 0.06, Spatial_loss 0.57, Flat_loss 0.16, Train_acc 99.87, Test_acc 57.19
2024-08-30 17:17:08,278 [podnet.py] => Task 2, Epoch 18/200 (LR 0.09801) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.16, Train_acc 99.98, Test_acc 51.24
2024-08-30 17:17:09,728 [podnet.py] => Task 2, Epoch 19/200 (LR 0.09779) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.16, Train_acc 99.69, Test_acc 57.54
2024-08-30 17:17:11,153 [podnet.py] => Task 2, Epoch 20/200 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.87, Test_acc 57.50
2024-08-30 17:17:12,454 [podnet.py] => Task 2, Epoch 21/200 (LR 0.09730) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.89, Test_acc 58.78
2024-08-30 17:17:13,809 [podnet.py] => Task 2, Epoch 22/200 (LR 0.09704) => LSC_loss 0.11, Spatial_loss 0.69, Flat_loss 0.19, Train_acc 98.55, Test_acc 56.98
2024-08-30 17:17:15,526 [podnet.py] => Task 2, Epoch 23/200 (LR 0.09677) => LSC_loss 0.07, Spatial_loss 0.61, Flat_loss 0.17, Train_acc 99.62, Test_acc 58.67
2024-08-30 17:17:16,945 [podnet.py] => Task 2, Epoch 24/200 (LR 0.09649) => LSC_loss 0.06, Spatial_loss 0.59, Flat_loss 0.16, Train_acc 99.87, Test_acc 55.57
2024-08-30 17:17:18,287 [podnet.py] => Task 2, Epoch 25/200 (LR 0.09619) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.82, Test_acc 52.54
2024-08-30 17:17:19,634 [podnet.py] => Task 2, Epoch 26/200 (LR 0.09589) => LSC_loss 0.08, Spatial_loss 0.61, Flat_loss 0.17, Train_acc 99.44, Test_acc 48.04
2024-08-30 17:17:21,189 [podnet.py] => Task 2, Epoch 27/200 (LR 0.09557) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.16, Train_acc 99.96, Test_acc 56.83
2024-08-30 17:17:22,539 [podnet.py] => Task 2, Epoch 28/200 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.98, Test_acc 56.13
2024-08-30 17:17:23,894 [podnet.py] => Task 2, Epoch 29/200 (LR 0.09490) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 100.00, Test_acc 58.89
2024-08-30 17:17:25,311 [podnet.py] => Task 2, Epoch 30/200 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.15, Train_acc 99.98, Test_acc 60.13
2024-08-30 17:17:26,815 [podnet.py] => Task 2, Epoch 31/200 (LR 0.09419) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.14, Train_acc 99.96, Test_acc 56.89
2024-08-30 17:17:28,521 [podnet.py] => Task 2, Epoch 32/200 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.14, Train_acc 99.96, Test_acc 54.76
2024-08-30 17:17:29,931 [podnet.py] => Task 2, Epoch 33/200 (LR 0.09343) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.14, Train_acc 99.96, Test_acc 56.89
2024-08-30 17:17:31,235 [podnet.py] => Task 2, Epoch 34/200 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.15, Train_acc 99.58, Test_acc 53.48
2024-08-30 17:17:32,602 [podnet.py] => Task 2, Epoch 35/200 (LR 0.09263) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.15, Train_acc 99.96, Test_acc 51.00
2024-08-30 17:17:34,045 [podnet.py] => Task 2, Epoch 36/200 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.15, Train_acc 99.93, Test_acc 55.80
2024-08-30 17:17:35,398 [podnet.py] => Task 2, Epoch 37/200 (LR 0.09179) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.14, Train_acc 99.96, Test_acc 53.81
2024-08-30 17:17:36,776 [podnet.py] => Task 2, Epoch 38/200 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.14, Train_acc 99.93, Test_acc 54.39
2024-08-30 17:17:38,107 [podnet.py] => Task 2, Epoch 39/200 (LR 0.09091) => LSC_loss 0.07, Spatial_loss 0.54, Flat_loss 0.15, Train_acc 99.42, Test_acc 54.57
2024-08-30 17:17:39,618 [podnet.py] => Task 2, Epoch 40/200 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.14, Train_acc 99.96, Test_acc 54.15
2024-08-30 17:17:40,977 [podnet.py] => Task 2, Epoch 41/200 (LR 0.08998) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.14, Train_acc 99.96, Test_acc 56.37
2024-08-30 17:17:42,335 [podnet.py] => Task 2, Epoch 42/200 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.14, Train_acc 100.00, Test_acc 53.93
2024-08-30 17:17:43,782 [podnet.py] => Task 2, Epoch 43/200 (LR 0.08902) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.14, Train_acc 99.96, Test_acc 51.78
2024-08-30 17:17:45,197 [podnet.py] => Task 2, Epoch 44/200 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.14, Train_acc 99.96, Test_acc 56.35
2024-08-30 17:17:46,560 [podnet.py] => Task 2, Epoch 45/200 (LR 0.08802) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.13, Train_acc 99.93, Test_acc 55.91
2024-08-30 17:17:47,993 [podnet.py] => Task 2, Epoch 46/200 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.14, Train_acc 99.98, Test_acc 56.41
2024-08-30 17:17:49,618 [podnet.py] => Task 2, Epoch 47/200 (LR 0.08698) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.13, Train_acc 99.98, Test_acc 55.81
2024-08-30 17:17:51,035 [podnet.py] => Task 2, Epoch 48/200 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.13, Train_acc 99.98, Test_acc 53.54
2024-08-30 17:17:52,451 [podnet.py] => Task 2, Epoch 49/200 (LR 0.08591) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.13, Train_acc 99.96, Test_acc 57.91
2024-08-30 17:17:53,784 [podnet.py] => Task 2, Epoch 50/200 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.13, Train_acc 99.98, Test_acc 53.59
2024-08-30 17:17:55,222 [podnet.py] => Task 2, Epoch 51/200 (LR 0.08480) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.13, Train_acc 100.00, Test_acc 52.76
2024-08-30 17:17:56,663 [podnet.py] => Task 2, Epoch 52/200 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.13, Train_acc 100.00, Test_acc 56.41
2024-08-30 17:17:58,085 [podnet.py] => Task 2, Epoch 53/200 (LR 0.08365) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.13, Train_acc 99.93, Test_acc 52.59
2024-08-30 17:17:59,396 [podnet.py] => Task 2, Epoch 54/200 (LR 0.08307) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.14, Train_acc 99.53, Test_acc 54.31
2024-08-30 17:18:00,737 [podnet.py] => Task 2, Epoch 55/200 (LR 0.08247) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.14, Train_acc 99.89, Test_acc 47.85
2024-08-30 17:18:02,224 [podnet.py] => Task 2, Epoch 56/200 (LR 0.08187) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.13, Train_acc 99.98, Test_acc 52.87
2024-08-30 17:18:03,692 [podnet.py] => Task 2, Epoch 57/200 (LR 0.08126) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.13, Train_acc 100.00, Test_acc 59.06
2024-08-30 17:18:05,127 [podnet.py] => Task 2, Epoch 58/200 (LR 0.08065) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.13, Train_acc 99.98, Test_acc 58.24
2024-08-30 17:18:06,459 [podnet.py] => Task 2, Epoch 59/200 (LR 0.08002) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.13, Train_acc 99.96, Test_acc 57.78
2024-08-30 17:18:07,868 [podnet.py] => Task 2, Epoch 60/200 (LR 0.07939) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.98
2024-08-30 17:18:09,394 [podnet.py] => Task 2, Epoch 61/200 (LR 0.07875) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.13, Train_acc 99.96, Test_acc 51.28
2024-08-30 17:18:10,770 [podnet.py] => Task 2, Epoch 62/200 (LR 0.07810) => LSC_loss 0.08, Spatial_loss 0.48, Flat_loss 0.13, Train_acc 99.84, Test_acc 48.06
2024-08-30 17:18:12,179 [podnet.py] => Task 2, Epoch 63/200 (LR 0.07745) => LSC_loss 0.25, Spatial_loss 0.92, Flat_loss 0.23, Train_acc 94.04, Test_acc 51.83
2024-08-30 17:18:13,511 [podnet.py] => Task 2, Epoch 64/200 (LR 0.07679) => LSC_loss 0.14, Spatial_loss 0.81, Flat_loss 0.20, Train_acc 97.04, Test_acc 62.33
2024-08-30 17:18:14,931 [podnet.py] => Task 2, Epoch 65/200 (LR 0.07612) => LSC_loss 0.07, Spatial_loss 0.64, Flat_loss 0.16, Train_acc 99.36, Test_acc 58.37
2024-08-30 17:18:16,381 [podnet.py] => Task 2, Epoch 66/200 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.15, Train_acc 99.96, Test_acc 56.04
2024-08-30 17:18:17,787 [podnet.py] => Task 2, Epoch 67/200 (LR 0.07477) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.14, Train_acc 100.00, Test_acc 54.74
2024-08-30 17:18:19,151 [podnet.py] => Task 2, Epoch 68/200 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.14, Train_acc 99.93, Test_acc 57.59
2024-08-30 17:18:20,546 [podnet.py] => Task 2, Epoch 69/200 (LR 0.07340) => LSC_loss 0.05, Spatial_loss 0.52, Flat_loss 0.14, Train_acc 99.91, Test_acc 52.91
2024-08-30 17:18:21,933 [podnet.py] => Task 2, Epoch 70/200 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.13, Train_acc 99.96, Test_acc 55.00
2024-08-30 17:18:23,322 [podnet.py] => Task 2, Epoch 71/200 (LR 0.07200) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.13, Train_acc 100.00, Test_acc 59.30
2024-08-30 17:18:24,726 [podnet.py] => Task 2, Epoch 72/200 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.26
2024-08-30 17:18:26,154 [podnet.py] => Task 2, Epoch 73/200 (LR 0.07058) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.80
2024-08-30 17:18:27,524 [podnet.py] => Task 2, Epoch 74/200 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.00
2024-08-30 17:18:28,944 [podnet.py] => Task 2, Epoch 75/200 (LR 0.06913) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.81
2024-08-30 17:18:30,322 [podnet.py] => Task 2, Epoch 76/200 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.57
2024-08-30 17:18:31,910 [podnet.py] => Task 2, Epoch 77/200 (LR 0.06767) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.12, Train_acc 99.98, Test_acc 57.04
2024-08-30 17:18:33,406 [podnet.py] => Task 2, Epoch 78/200 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.13, Train_acc 99.96, Test_acc 59.44
2024-08-30 17:18:34,795 [podnet.py] => Task 2, Epoch 79/200 (LR 0.06620) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 100.00, Test_acc 53.02
2024-08-30 17:18:36,203 [podnet.py] => Task 2, Epoch 80/200 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.12, Train_acc 99.98, Test_acc 57.46
2024-08-30 17:18:37,638 [podnet.py] => Task 2, Epoch 81/200 (LR 0.06470) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.12, Train_acc 100.00, Test_acc 53.02
2024-08-30 17:18:39,254 [podnet.py] => Task 2, Epoch 82/200 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.20
2024-08-30 17:18:40,844 [podnet.py] => Task 2, Epoch 83/200 (LR 0.06319) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.52
2024-08-30 17:18:42,173 [podnet.py] => Task 2, Epoch 84/200 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 99.93, Test_acc 60.41
2024-08-30 17:18:43,682 [podnet.py] => Task 2, Epoch 85/200 (LR 0.06167) => LSC_loss 0.06, Spatial_loss 0.44, Flat_loss 0.13, Train_acc 99.73, Test_acc 54.41
2024-08-30 17:18:45,346 [podnet.py] => Task 2, Epoch 86/200 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 99.98, Test_acc 58.98
2024-08-30 17:18:47,113 [podnet.py] => Task 2, Epoch 87/200 (LR 0.06014) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.13, Train_acc 99.98, Test_acc 54.28
2024-08-30 17:18:48,695 [podnet.py] => Task 2, Epoch 88/200 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.83
2024-08-30 17:18:50,412 [podnet.py] => Task 2, Epoch 89/200 (LR 0.05860) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.30
2024-08-30 17:18:51,836 [podnet.py] => Task 2, Epoch 90/200 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 99.98, Test_acc 54.93
2024-08-30 17:18:53,191 [podnet.py] => Task 2, Epoch 91/200 (LR 0.05705) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 100.00, Test_acc 58.57
2024-08-30 17:18:54,691 [podnet.py] => Task 2, Epoch 92/200 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.12, Train_acc 99.98, Test_acc 57.13
2024-08-30 17:18:56,446 [podnet.py] => Task 2, Epoch 93/200 (LR 0.05549) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.26
2024-08-30 17:18:58,097 [podnet.py] => Task 2, Epoch 94/200 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.91
2024-08-30 17:18:59,396 [podnet.py] => Task 2, Epoch 95/200 (LR 0.05392) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.33
2024-08-30 17:19:00,686 [podnet.py] => Task 2, Epoch 96/200 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.85
2024-08-30 17:19:02,165 [podnet.py] => Task 2, Epoch 97/200 (LR 0.05236) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.12, Train_acc 100.00, Test_acc 53.63
2024-08-30 17:19:03,845 [podnet.py] => Task 2, Epoch 98/200 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.46
2024-08-30 17:19:05,225 [podnet.py] => Task 2, Epoch 99/200 (LR 0.05079) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.65
2024-08-30 17:19:06,573 [podnet.py] => Task 2, Epoch 100/200 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.28
2024-08-30 17:19:08,095 [podnet.py] => Task 2, Epoch 101/200 (LR 0.04921) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.12, Train_acc 100.00, Test_acc 54.57
2024-08-30 17:19:09,693 [podnet.py] => Task 2, Epoch 102/200 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.12, Train_acc 100.00, Test_acc 55.57
2024-08-30 17:19:11,153 [podnet.py] => Task 2, Epoch 103/200 (LR 0.04764) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.69
2024-08-30 17:19:12,445 [podnet.py] => Task 2, Epoch 104/200 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.12, Train_acc 99.98, Test_acc 53.11
2024-08-30 17:19:13,755 [podnet.py] => Task 2, Epoch 105/200 (LR 0.04608) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.12, Train_acc 99.98, Test_acc 56.52
2024-08-30 17:19:15,266 [podnet.py] => Task 2, Epoch 106/200 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.11, Train_acc 99.98, Test_acc 56.06
2024-08-30 17:19:16,806 [podnet.py] => Task 2, Epoch 107/200 (LR 0.04451) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.61
2024-08-30 17:19:18,346 [podnet.py] => Task 2, Epoch 108/200 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.11, Train_acc 100.00, Test_acc 54.06
2024-08-30 17:19:19,665 [podnet.py] => Task 2, Epoch 109/200 (LR 0.04295) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.30
2024-08-30 17:19:21,112 [podnet.py] => Task 2, Epoch 110/200 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.02
2024-08-30 17:19:22,545 [podnet.py] => Task 2, Epoch 111/200 (LR 0.04140) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.69
2024-08-30 17:19:24,320 [podnet.py] => Task 2, Epoch 112/200 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.72
2024-08-30 17:19:25,869 [podnet.py] => Task 2, Epoch 113/200 (LR 0.03986) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.87
2024-08-30 17:19:27,152 [podnet.py] => Task 2, Epoch 114/200 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.74
2024-08-30 17:19:28,512 [podnet.py] => Task 2, Epoch 115/200 (LR 0.03833) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.26
2024-08-30 17:19:29,951 [podnet.py] => Task 2, Epoch 116/200 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.43
2024-08-30 17:19:31,343 [podnet.py] => Task 2, Epoch 117/200 (LR 0.03681) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.44
2024-08-30 17:19:32,779 [podnet.py] => Task 2, Epoch 118/200 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.43
2024-08-30 17:19:34,140 [podnet.py] => Task 2, Epoch 119/200 (LR 0.03530) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.89
2024-08-30 17:19:35,641 [podnet.py] => Task 2, Epoch 120/200 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.11, Train_acc 100.00, Test_acc 54.98
2024-08-30 17:19:37,079 [podnet.py] => Task 2, Epoch 121/200 (LR 0.03380) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.76
2024-08-30 17:19:38,423 [podnet.py] => Task 2, Epoch 122/200 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.46
2024-08-30 17:19:39,771 [podnet.py] => Task 2, Epoch 123/200 (LR 0.03233) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.15
2024-08-30 17:19:41,249 [podnet.py] => Task 2, Epoch 124/200 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.54
2024-08-30 17:19:42,804 [podnet.py] => Task 2, Epoch 125/200 (LR 0.03087) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.74
2024-08-30 17:19:44,201 [podnet.py] => Task 2, Epoch 126/200 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.20
2024-08-30 17:19:45,641 [podnet.py] => Task 2, Epoch 127/200 (LR 0.02942) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.20
2024-08-30 17:19:46,983 [podnet.py] => Task 2, Epoch 128/200 (LR 0.02871) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.96
2024-08-30 17:19:48,456 [podnet.py] => Task 2, Epoch 129/200 (LR 0.02800) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.33
2024-08-30 17:19:49,982 [podnet.py] => Task 2, Epoch 130/200 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.81
2024-08-30 17:19:51,485 [podnet.py] => Task 2, Epoch 131/200 (LR 0.02660) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.76
2024-08-30 17:19:52,814 [podnet.py] => Task 2, Epoch 132/200 (LR 0.02591) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.11, Train_acc 100.00, Test_acc 53.94
2024-08-30 17:19:54,127 [podnet.py] => Task 2, Epoch 133/200 (LR 0.02523) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 99.98, Test_acc 54.04
2024-08-30 17:19:55,509 [podnet.py] => Task 2, Epoch 134/200 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 99.98, Test_acc 56.13
2024-08-30 17:19:56,951 [podnet.py] => Task 2, Epoch 135/200 (LR 0.02388) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.20
2024-08-30 17:19:58,314 [podnet.py] => Task 2, Epoch 136/200 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.44
2024-08-30 17:19:59,696 [podnet.py] => Task 2, Epoch 137/200 (LR 0.02255) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.11
2024-08-30 17:20:01,112 [podnet.py] => Task 2, Epoch 138/200 (LR 0.02190) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.11, Train_acc 99.98, Test_acc 56.63
2024-08-30 17:20:02,511 [podnet.py] => Task 2, Epoch 139/200 (LR 0.02125) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.80
2024-08-30 17:20:03,828 [podnet.py] => Task 2, Epoch 140/200 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.89
2024-08-30 17:20:05,206 [podnet.py] => Task 2, Epoch 141/200 (LR 0.01998) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.65
2024-08-30 17:20:06,600 [podnet.py] => Task 2, Epoch 142/200 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.35
2024-08-30 17:20:08,079 [podnet.py] => Task 2, Epoch 143/200 (LR 0.01874) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.00
2024-08-30 17:20:09,586 [podnet.py] => Task 2, Epoch 144/200 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.17
2024-08-30 17:20:10,940 [podnet.py] => Task 2, Epoch 145/200 (LR 0.01753) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.93
2024-08-30 17:20:12,236 [podnet.py] => Task 2, Epoch 146/200 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.63
2024-08-30 17:20:13,718 [podnet.py] => Task 2, Epoch 147/200 (LR 0.01635) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.91
2024-08-30 17:20:15,130 [podnet.py] => Task 2, Epoch 148/200 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.81
2024-08-30 17:20:16,435 [podnet.py] => Task 2, Epoch 149/200 (LR 0.01520) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 99.98, Test_acc 55.50
2024-08-30 17:20:17,699 [podnet.py] => Task 2, Epoch 150/200 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 99.98, Test_acc 57.33
2024-08-30 17:20:19,258 [podnet.py] => Task 2, Epoch 151/200 (LR 0.01409) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.37
2024-08-30 17:20:20,782 [podnet.py] => Task 2, Epoch 152/200 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.52
2024-08-30 17:20:22,199 [podnet.py] => Task 2, Epoch 153/200 (LR 0.01302) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.13
2024-08-30 17:20:23,590 [podnet.py] => Task 2, Epoch 154/200 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.39
2024-08-30 17:20:25,088 [podnet.py] => Task 2, Epoch 155/200 (LR 0.01198) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.41
2024-08-30 17:20:26,825 [podnet.py] => Task 2, Epoch 156/200 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.43
2024-08-30 17:20:28,335 [podnet.py] => Task 2, Epoch 157/200 (LR 0.01098) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.72
2024-08-30 17:20:29,909 [podnet.py] => Task 2, Epoch 158/200 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.11, Train_acc 100.00, Test_acc 54.65
2024-08-30 17:20:31,333 [podnet.py] => Task 2, Epoch 159/200 (LR 0.01002) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.56
2024-08-30 17:20:32,720 [podnet.py] => Task 2, Epoch 160/200 (LR 0.00955) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.91
2024-08-30 17:20:34,202 [podnet.py] => Task 2, Epoch 161/200 (LR 0.00909) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.96
2024-08-30 17:20:35,685 [podnet.py] => Task 2, Epoch 162/200 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.80
2024-08-30 17:20:37,025 [podnet.py] => Task 2, Epoch 163/200 (LR 0.00821) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.20
2024-08-30 17:20:38,416 [podnet.py] => Task 2, Epoch 164/200 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.74
2024-08-30 17:20:39,837 [podnet.py] => Task 2, Epoch 165/200 (LR 0.00737) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 100.00, Test_acc 57.35
2024-08-30 17:20:41,299 [podnet.py] => Task 2, Epoch 166/200 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.35
2024-08-30 17:20:42,926 [podnet.py] => Task 2, Epoch 167/200 (LR 0.00657) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.41
2024-08-30 17:20:44,211 [podnet.py] => Task 2, Epoch 168/200 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.35
2024-08-30 17:20:45,504 [podnet.py] => Task 2, Epoch 169/200 (LR 0.00581) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.76
2024-08-30 17:20:47,074 [podnet.py] => Task 2, Epoch 170/200 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.69
2024-08-30 17:20:48,705 [podnet.py] => Task 2, Epoch 171/200 (LR 0.00510) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.69
2024-08-30 17:20:50,193 [podnet.py] => Task 2, Epoch 172/200 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.50
2024-08-30 17:20:51,709 [podnet.py] => Task 2, Epoch 173/200 (LR 0.00443) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.06
2024-08-30 17:20:53,062 [podnet.py] => Task 2, Epoch 174/200 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.00
2024-08-30 17:20:54,470 [podnet.py] => Task 2, Epoch 175/200 (LR 0.00381) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.52
2024-08-30 17:20:56,014 [podnet.py] => Task 2, Epoch 176/200 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.04
2024-08-30 17:20:57,498 [podnet.py] => Task 2, Epoch 177/200 (LR 0.00323) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.02
2024-08-30 17:20:58,841 [podnet.py] => Task 2, Epoch 178/200 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.78
2024-08-30 17:21:00,279 [podnet.py] => Task 2, Epoch 179/200 (LR 0.00270) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.39
2024-08-30 17:21:01,805 [podnet.py] => Task 2, Epoch 180/200 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.83
2024-08-30 17:21:03,400 [podnet.py] => Task 2, Epoch 181/200 (LR 0.00221) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.50
2024-08-30 17:21:04,854 [podnet.py] => Task 2, Epoch 182/200 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.13
2024-08-30 17:21:06,264 [podnet.py] => Task 2, Epoch 183/200 (LR 0.00177) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.11
2024-08-30 17:21:07,589 [podnet.py] => Task 2, Epoch 184/200 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.00
2024-08-30 17:21:09,123 [podnet.py] => Task 2, Epoch 185/200 (LR 0.00138) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.63
2024-08-30 17:21:10,682 [podnet.py] => Task 2, Epoch 186/200 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.94
2024-08-30 17:21:12,118 [podnet.py] => Task 2, Epoch 187/200 (LR 0.00104) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.13
2024-08-30 17:21:13,418 [podnet.py] => Task 2, Epoch 188/200 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.41
2024-08-30 17:21:14,851 [podnet.py] => Task 2, Epoch 189/200 (LR 0.00074) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.96
2024-08-30 17:21:16,335 [podnet.py] => Task 2, Epoch 190/200 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.74
2024-08-30 17:21:17,696 [podnet.py] => Task 2, Epoch 191/200 (LR 0.00050) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.61
2024-08-30 17:21:19,032 [podnet.py] => Task 2, Epoch 192/200 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.46
2024-08-30 17:21:20,478 [podnet.py] => Task 2, Epoch 193/200 (LR 0.00030) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.26
2024-08-30 17:21:21,785 [podnet.py] => Task 2, Epoch 194/200 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.48
2024-08-30 17:21:23,119 [podnet.py] => Task 2, Epoch 195/200 (LR 0.00015) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.69
2024-08-30 17:21:24,590 [podnet.py] => Task 2, Epoch 196/200 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.70
2024-08-30 17:21:26,065 [podnet.py] => Task 2, Epoch 197/200 (LR 0.00006) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.37
2024-08-30 17:21:27,508 [podnet.py] => Task 2, Epoch 198/200 (LR 0.00002) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.76
2024-08-30 17:21:28,890 [podnet.py] => Task 2, Epoch 199/200 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 56.28
2024-08-30 17:21:30,257 [podnet.py] => Task 2, Epoch 200/200 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.54
2024-08-30 17:21:30,705 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:21:30,705 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:21:32,220 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:21:33,885 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 17:21:35,287 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 17:21:37,693 [podnet.py] => Exemplar size: 495
2024-08-30 17:21:37,694 [trainer.py] => CNN: {'total': 57.54, '00-04': 45.0, '05-06': 48.08, '07-08': 98.33, 'old': 45.88, 'new': 98.33}
2024-08-30 17:21:37,694 [trainer.py] => NME: {'total': 65.39, '00-04': 64.73, '05-06': 43.83, '07-08': 88.58, 'old': 58.76, 'new': 88.58}
2024-08-30 17:21:37,694 [trainer.py] => CNN top1 curve: [89.63, 66.98, 57.54]
2024-08-30 17:21:37,694 [trainer.py] => CNN top5 curve: [100.0, 98.26, 92.76]
2024-08-30 17:21:37,694 [trainer.py] => NME top1 curve: [89.6, 73.95, 65.39]
2024-08-30 17:21:37,694 [trainer.py] => NME top5 curve: [100.0, 98.26, 95.09]

2024-08-30 17:21:37,694 [trainer.py] => Average Accuracy (CNN): 71.38333333333334
2024-08-30 17:21:37,694 [trainer.py] => Average Accuracy (NME): 76.31333333333333
2024-08-30 17:21:37,695 [trainer.py] => Forgetting (CNN): 45.86
