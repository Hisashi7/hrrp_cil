2025-12-27 17:39:57,666 [trainer.py] => config: ./exps/finetune.json
2025-12-27 17:39:57,667 [trainer.py] => prefix: EFCIL
2025-12-27 17:39:57,667 [trainer.py] => dataset: hrrp9
2025-12-27 17:39:57,667 [trainer.py] => memory_size: 0
2025-12-27 17:39:57,667 [trainer.py] => memory_per_class: 20
2025-12-27 17:39:57,667 [trainer.py] => fixed_memory: False
2025-12-27 17:39:57,667 [trainer.py] => shuffle: True
2025-12-27 17:39:57,667 [trainer.py] => init_cls: 5
2025-12-27 17:39:57,667 [trainer.py] => increment: 2
2025-12-27 17:39:57,667 [trainer.py] => model_name: finetune
2025-12-27 17:39:57,667 [trainer.py] => convnet_type: resnet18
2025-12-27 17:39:57,667 [trainer.py] => init_train: False
2025-12-27 17:39:57,667 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_35172.pth
2025-12-27 17:39:57,667 [trainer.py] => fc_path2: checkpoints/init_train/fc_35172.pth
2025-12-27 17:39:57,667 [trainer.py] => seed: 1993
2025-12-27 17:39:57,667 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2025-12-27 17:39:57,667 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2025-12-27 17:39:57,667 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42871.pth
2025-12-27 17:39:57,667 [trainer.py] => fc_path1: checkpoints/init_train/fc_42871.pth
2025-12-27 17:39:57,667 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-27 17:39:57,667 [trainer.py] => seed1: [110]
2025-12-27 17:39:57,668 [trainer.py] => epochs: 150
2025-12-27 17:39:57,668 [trainer.py] => lrate: 0.001
2025-12-27 17:39:57,668 [trainer.py] => milestones: [20, 35, 70]
2025-12-27 17:39:57,668 [trainer.py] => lrate_decay: 0.1
2025-12-27 17:39:57,668 [trainer.py] => momentum: 0.9
2025-12-27 17:39:57,668 [trainer.py] => batch_size: 128
2025-12-27 17:39:57,668 [trainer.py] => weight_decay: 0.0002
2025-12-27 17:39:57,668 [trainer.py] => num_workers: 8
2025-12-27 17:39:58,368 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2025-12-27 17:39:58,730 [trainer.py] => All params: 3843904
2025-12-27 17:39:58,730 [trainer.py] => Trainable params: 3843904
2025-12-27 17:39:58,750 [finetune.py] => Learning on 0-5
2025-12-27 17:39:58,896 [finetune.py] => init_train?---False
2025-12-27 17:39:59,704 [trainer.py] => task:0 training time:0.97s
2025-12-27 17:39:59,704 [trainer.py] => All params: 3846469
2025-12-27 17:40:00,237 [trainer.py] => No NME accuracy.
2025-12-27 17:40:00,237 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2025-12-27 17:40:00,237 [trainer.py] => CNN top1 curve: [89.93]
2025-12-27 17:40:00,237 [trainer.py] => CNN top5 curve: [100.0]

2025-12-27 17:40:00,238 [trainer.py] => Average Accuracy (CNN): 89.93
2025-12-27 17:40:00,238 [trainer.py] => Average Accuracy (CNN): 89.93
2025-12-27 17:40:00,238 [trainer.py] => All params: 3846469
2025-12-27 17:40:00,239 [trainer.py] => Trainable params: 3846469
2025-12-27 17:40:00,241 [finetune.py] => Learning on 5-7
2025-12-27 17:40:02,031 [finetune.py] => Task 1, Epoch 1/150 => Loss 0.650, Train_accy 0.00, Test_accy 62.00
2025-12-27 17:40:07,237 [finetune.py] => Task 1, Epoch 6/150 => Loss 0.258, Train_accy 0.05, Test_accy 61.55
2025-12-27 17:40:12,310 [finetune.py] => Task 1, Epoch 11/150 => Loss 0.123, Train_accy 1.18, Test_accy 61.50
2025-12-27 17:40:17,444 [finetune.py] => Task 1, Epoch 16/150 => Loss 0.078, Train_accy 2.95, Test_accy 61.62
2025-12-27 17:40:22,391 [finetune.py] => Task 1, Epoch 21/150 => Loss 0.055, Train_accy 4.70, Test_accy 61.83
2025-12-27 17:40:27,231 [finetune.py] => Task 1, Epoch 26/150 => Loss 0.055, Train_accy 5.70, Test_accy 61.93
2025-12-27 17:40:32,234 [finetune.py] => Task 1, Epoch 31/150 => Loss 0.054, Train_accy 6.10, Test_accy 61.88
2025-12-27 17:40:37,531 [finetune.py] => Task 1, Epoch 36/150 => Loss 0.053, Train_accy 5.95, Test_accy 62.02
2025-12-27 17:40:42,608 [finetune.py] => Task 1, Epoch 41/150 => Loss 0.052, Train_accy 6.18, Test_accy 61.90
2025-12-27 17:40:47,816 [finetune.py] => Task 1, Epoch 46/150 => Loss 0.051, Train_accy 5.90, Test_accy 61.95
2025-12-27 17:40:52,894 [finetune.py] => Task 1, Epoch 51/150 => Loss 0.054, Train_accy 5.55, Test_accy 61.90
2025-12-27 17:40:57,984 [finetune.py] => Task 1, Epoch 56/150 => Loss 0.053, Train_accy 5.68, Test_accy 61.95
2025-12-27 17:41:02,971 [finetune.py] => Task 1, Epoch 61/150 => Loss 0.050, Train_accy 6.65, Test_accy 61.90
2025-12-27 17:41:08,017 [finetune.py] => Task 1, Epoch 66/150 => Loss 0.051, Train_accy 5.95, Test_accy 62.12
2025-12-27 17:41:13,049 [finetune.py] => Task 1, Epoch 71/150 => Loss 0.052, Train_accy 5.90, Test_accy 61.81
2025-12-27 17:41:17,986 [finetune.py] => Task 1, Epoch 76/150 => Loss 0.047, Train_accy 6.25, Test_accy 61.86
2025-12-27 17:41:22,894 [finetune.py] => Task 1, Epoch 81/150 => Loss 0.050, Train_accy 6.08, Test_accy 61.81
2025-12-27 17:41:27,810 [finetune.py] => Task 1, Epoch 86/150 => Loss 0.052, Train_accy 6.05, Test_accy 61.81
2025-12-27 17:41:32,547 [finetune.py] => Task 1, Epoch 91/150 => Loss 0.049, Train_accy 6.62, Test_accy 62.05
2025-12-27 17:41:37,429 [finetune.py] => Task 1, Epoch 96/150 => Loss 0.054, Train_accy 6.10, Test_accy 62.07
2025-12-27 17:41:42,168 [finetune.py] => Task 1, Epoch 101/150 => Loss 0.050, Train_accy 6.12, Test_accy 61.90
2025-12-27 17:41:46,997 [finetune.py] => Task 1, Epoch 106/150 => Loss 0.048, Train_accy 6.88, Test_accy 62.19
2025-12-27 17:41:51,976 [finetune.py] => Task 1, Epoch 111/150 => Loss 0.050, Train_accy 6.02, Test_accy 61.95
2025-12-27 17:41:56,765 [finetune.py] => Task 1, Epoch 116/150 => Loss 0.049, Train_accy 6.80, Test_accy 61.83
2025-12-27 17:42:01,492 [finetune.py] => Task 1, Epoch 121/150 => Loss 0.049, Train_accy 6.00, Test_accy 61.86
2025-12-27 17:42:06,212 [finetune.py] => Task 1, Epoch 126/150 => Loss 0.050, Train_accy 6.28, Test_accy 61.90
2025-12-27 17:42:11,043 [finetune.py] => Task 1, Epoch 131/150 => Loss 0.051, Train_accy 6.02, Test_accy 61.98
2025-12-27 17:42:15,962 [finetune.py] => Task 1, Epoch 136/150 => Loss 0.050, Train_accy 6.22, Test_accy 61.90
2025-12-27 17:42:20,648 [finetune.py] => Task 1, Epoch 141/150 => Loss 0.051, Train_accy 6.20, Test_accy 61.95
2025-12-27 17:42:25,497 [finetune.py] => Task 1, Epoch 146/150 => Loss 0.051, Train_accy 6.25, Test_accy 62.00
2025-12-27 17:42:28,947 [finetune.py] => Task 1, Epoch 150/150 => Loss 0.049, Train_accy 6.42
2025-12-27 17:42:28,948 [finetune.py] => 100 epoches training time:100.50s
2025-12-27 17:42:28,948 [finetune.py] => Average training time of single epoch:0.88s
2025-12-27 17:42:28,948 [trainer.py] => task:1 training time:148.71s
2025-12-27 17:42:28,949 [trainer.py] => All params: 3847495
2025-12-27 17:42:29,489 [trainer.py] => No NME accuracy.
2025-12-27 17:42:29,489 [trainer.py] => CNN: {'total': 61.83, '00-04': 85.1, '05-06': 3.67, 'old': 85.1, 'new': 3.67}
2025-12-27 17:42:29,489 [trainer.py] => CNN top1 curve: [89.93, 61.83]
2025-12-27 17:42:29,489 [trainer.py] => CNN top5 curve: [100.0, 97.05]

2025-12-27 17:42:29,489 [trainer.py] => Average Accuracy (CNN): 75.88
2025-12-27 17:42:29,490 [trainer.py] => Average Accuracy (CNN): 75.88
2025-12-27 17:42:29,490 [trainer.py] => All params: 3847495
2025-12-27 17:42:29,490 [trainer.py] => Trainable params: 3847495
2025-12-27 17:42:29,492 [finetune.py] => Learning on 7-9
2025-12-27 17:42:31,062 [finetune.py] => Task 2, Epoch 1/150 => Loss 0.563, Train_accy 0.00, Test_accy 45.22
2025-12-27 17:42:36,172 [finetune.py] => Task 2, Epoch 6/150 => Loss 0.126, Train_accy 0.02, Test_accy 43.78
2025-12-27 17:42:41,554 [finetune.py] => Task 2, Epoch 11/150 => Loss 0.069, Train_accy 0.05, Test_accy 43.54
2025-12-27 17:42:46,634 [finetune.py] => Task 2, Epoch 16/150 => Loss 0.049, Train_accy 0.10, Test_accy 43.06
2025-12-27 17:42:51,915 [finetune.py] => Task 2, Epoch 21/150 => Loss 0.036, Train_accy 0.38, Test_accy 42.80
2025-12-27 17:42:57,383 [finetune.py] => Task 2, Epoch 26/150 => Loss 0.041, Train_accy 0.52, Test_accy 43.02
2025-12-27 17:43:02,608 [finetune.py] => Task 2, Epoch 31/150 => Loss 0.034, Train_accy 0.65, Test_accy 42.72
2025-12-27 17:43:08,126 [finetune.py] => Task 2, Epoch 36/150 => Loss 0.036, Train_accy 0.70, Test_accy 42.65
2025-12-27 17:43:13,370 [finetune.py] => Task 2, Epoch 41/150 => Loss 0.035, Train_accy 0.72, Test_accy 42.91
2025-12-27 17:43:18,827 [finetune.py] => Task 2, Epoch 46/150 => Loss 0.036, Train_accy 0.62, Test_accy 42.96
2025-12-27 17:43:24,227 [finetune.py] => Task 2, Epoch 51/150 => Loss 0.037, Train_accy 0.62, Test_accy 42.57
2025-12-27 17:43:29,372 [finetune.py] => Task 2, Epoch 56/150 => Loss 0.036, Train_accy 0.55, Test_accy 42.26
2025-12-27 17:43:34,700 [finetune.py] => Task 2, Epoch 61/150 => Loss 0.035, Train_accy 0.62, Test_accy 42.98
2025-12-27 17:43:39,873 [finetune.py] => Task 2, Epoch 66/150 => Loss 0.034, Train_accy 0.50, Test_accy 42.76
2025-12-27 17:43:45,022 [finetune.py] => Task 2, Epoch 71/150 => Loss 0.036, Train_accy 0.50, Test_accy 43.09
2025-12-27 17:43:50,310 [finetune.py] => Task 2, Epoch 76/150 => Loss 0.037, Train_accy 0.62, Test_accy 42.89
2025-12-27 17:43:55,475 [finetune.py] => Task 2, Epoch 81/150 => Loss 0.035, Train_accy 0.50, Test_accy 42.96
2025-12-27 17:44:00,581 [finetune.py] => Task 2, Epoch 86/150 => Loss 0.035, Train_accy 0.57, Test_accy 42.74
2025-12-27 17:44:05,942 [finetune.py] => Task 2, Epoch 91/150 => Loss 0.036, Train_accy 0.60, Test_accy 42.83
2025-12-27 17:44:11,318 [finetune.py] => Task 2, Epoch 96/150 => Loss 0.035, Train_accy 0.65, Test_accy 42.52
2025-12-27 17:44:16,593 [finetune.py] => Task 2, Epoch 101/150 => Loss 0.035, Train_accy 0.55, Test_accy 42.87
2025-12-27 17:44:21,947 [finetune.py] => Task 2, Epoch 106/150 => Loss 0.038, Train_accy 0.52, Test_accy 42.74
2025-12-27 17:44:27,273 [finetune.py] => Task 2, Epoch 111/150 => Loss 0.039, Train_accy 0.52, Test_accy 42.44
2025-12-27 17:44:32,552 [finetune.py] => Task 2, Epoch 116/150 => Loss 0.035, Train_accy 0.40, Test_accy 42.67
2025-12-27 17:44:37,593 [finetune.py] => Task 2, Epoch 121/150 => Loss 0.034, Train_accy 0.55, Test_accy 42.85
2025-12-27 17:44:42,911 [finetune.py] => Task 2, Epoch 126/150 => Loss 0.039, Train_accy 0.60, Test_accy 42.35
2025-12-27 17:44:48,097 [finetune.py] => Task 2, Epoch 131/150 => Loss 0.033, Train_accy 0.62, Test_accy 42.65
2025-12-27 17:44:53,255 [finetune.py] => Task 2, Epoch 136/150 => Loss 0.036, Train_accy 0.52, Test_accy 42.83
2025-12-27 17:44:58,244 [finetune.py] => Task 2, Epoch 141/150 => Loss 0.035, Train_accy 0.65, Test_accy 43.07
2025-12-27 17:45:03,288 [finetune.py] => Task 2, Epoch 146/150 => Loss 0.038, Train_accy 0.55, Test_accy 42.81
2025-12-27 17:45:06,974 [finetune.py] => Task 2, Epoch 150/150 => Loss 0.034, Train_accy 0.55
2025-12-27 17:45:06,974 [finetune.py] => 100 epoches training time:105.46s
2025-12-27 17:45:06,974 [finetune.py] => Average training time of single epoch:0.92s
2025-12-27 17:45:06,975 [trainer.py] => task:2 training time:157.48s
2025-12-27 17:45:06,975 [trainer.py] => All params: 3848521
2025-12-27 17:45:07,607 [trainer.py] => No NME accuracy.
2025-12-27 17:45:07,607 [trainer.py] => CNN: {'total': 42.72, '00-04': 76.57, '05-06': 0.25, '07-08': 0.58, 'old': 54.76, 'new': 0.58}
2025-12-27 17:45:07,607 [trainer.py] => CNN top1 curve: [89.93, 61.83, 42.72]
2025-12-27 17:45:07,607 [trainer.py] => CNN top5 curve: [100.0, 97.05, 93.44]

2025-12-27 17:45:07,607 [trainer.py] => Average Accuracy (CNN): 64.82666666666667
2025-12-27 17:45:07,607 [trainer.py] => Average Accuracy (CNN): 64.82666666666667
2025-12-27 17:45:07,607 [trainer.py] => Time consumed in all training process:308.88s
2025-12-27 17:45:07,607 [trainer.py] => Average Time consumed in single task:102.39s
2025-12-27 17:45:07,608 [trainer.py] => Accuracy Matrix (CNN):
2025-12-27 17:45:07,608 [trainer.py] => [[89.93 85.1  76.57]
 [ 0.    3.67  0.25]
 [ 0.    0.    0.58]]
2025-12-27 17:45:07,608 [trainer.py] => Forgetting (CNN): 8.390000000000008
