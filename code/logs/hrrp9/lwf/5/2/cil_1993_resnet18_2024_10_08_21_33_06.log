2024-10-08 21:33:06,481 [trainer.py] => config: ./exps/lwf.json
2024-10-08 21:33:06,481 [trainer.py] => prefix: cil
2024-10-08 21:33:06,481 [trainer.py] => dataset: hrrp9
2024-10-08 21:33:06,481 [trainer.py] => memory_size: 500
2024-10-08 21:33:06,481 [trainer.py] => memory_per_class: 20
2024-10-08 21:33:06,482 [trainer.py] => fixed_memory: False
2024-10-08 21:33:06,482 [trainer.py] => shuffle: True
2024-10-08 21:33:06,482 [trainer.py] => init_cls: 5
2024-10-08 21:33:06,482 [trainer.py] => increment: 2
2024-10-08 21:33:06,482 [trainer.py] => model_name: lwf
2024-10-08 21:33:06,482 [trainer.py] => convnet_type: resnet18
2024-10-08 21:33:06,482 [trainer.py] => device: [device(type='cuda', index=1)]
2024-10-08 21:33:06,482 [trainer.py] => init_train: False
2024-10-08 21:33:06,482 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-08 21:33:06,482 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-08 21:33:06,482 [trainer.py] => seed: 1993
2024-10-08 21:33:06,482 [trainer.py] => epochs: 150
2024-10-08 21:33:06,482 [trainer.py] => lrate: 0.1
2024-10-08 21:33:06,482 [trainer.py] => milestones: [30, 50, 80, 120]
2024-10-08 21:33:06,483 [trainer.py] => lrate_decay: 0.1
2024-10-08 21:33:06,483 [trainer.py] => batch_size: 128
2024-10-08 21:33:06,483 [trainer.py] => weight_decay: 0.0002
2024-10-08 21:33:06,483 [trainer.py] => momentum: 0
2024-10-08 21:33:06,483 [trainer.py] => num_workers: 4
2024-10-08 21:33:06,483 [trainer.py] => T: 2
2024-10-08 21:33:06,483 [trainer.py] => lamda: 0.5
2024-10-08 21:33:07,489 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-08 21:33:08,084 [trainer.py] => All params: 3843904
2024-10-08 21:33:08,085 [trainer.py] => Trainable params: 3843904
2024-10-08 21:33:08,089 [lwf.py] => Learning on 0-5
2024-10-08 21:33:08,634 [lwf.py] => init_train?---False
2024-10-08 21:33:09,673 [trainer.py] => All params: 3846469
2024-10-08 21:33:10,213 [trainer.py] => No NME accuracy.
2024-10-08 21:33:10,213 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-08 21:33:10,213 [trainer.py] => CNN top1 curve: [89.93]
2024-10-08 21:33:10,213 [trainer.py] => CNN top5 curve: [100.0]

2024-10-08 21:33:10,213 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-08 21:33:10,214 [trainer.py] => All params: 3846469
2024-10-08 21:33:10,214 [trainer.py] => Trainable params: 3846469
2024-10-08 21:33:10,215 [lwf.py] => Learning on 5-7
2024-10-08 21:33:17,171 [lwf.py] => Task 1, Epoch 5/150 => Loss 0.529, Train_accy 31.78, Test_accy 69.40
2024-10-08 21:33:23,586 [lwf.py] => Task 1, Epoch 10/150 => Loss 0.506, Train_accy 46.55, Test_accy 71.50
2024-10-08 21:33:30,271 [lwf.py] => Task 1, Epoch 15/150 => Loss 0.504, Train_accy 52.90, Test_accy 71.62
2024-10-08 21:33:36,626 [lwf.py] => Task 1, Epoch 20/150 => Loss 0.502, Train_accy 55.62, Test_accy 73.45
2024-10-08 21:33:43,083 [lwf.py] => Task 1, Epoch 25/150 => Loss 0.498, Train_accy 58.40, Test_accy 72.40
2024-10-08 21:33:49,480 [lwf.py] => Task 1, Epoch 30/150 => Loss 0.498, Train_accy 60.35, Test_accy 73.24
2024-10-08 21:33:55,764 [lwf.py] => Task 1, Epoch 35/150 => Loss 0.496, Train_accy 61.08, Test_accy 72.81
2024-10-08 21:34:02,441 [lwf.py] => Task 1, Epoch 40/150 => Loss 0.494, Train_accy 61.22, Test_accy 72.79
2024-10-08 21:34:08,545 [lwf.py] => Task 1, Epoch 45/150 => Loss 0.499, Train_accy 62.22, Test_accy 72.55
2024-10-08 21:34:14,868 [lwf.py] => Task 1, Epoch 50/150 => Loss 0.495, Train_accy 61.70, Test_accy 72.64
2024-10-08 21:34:21,460 [lwf.py] => Task 1, Epoch 55/150 => Loss 0.498, Train_accy 61.95, Test_accy 72.83
2024-10-08 21:34:27,945 [lwf.py] => Task 1, Epoch 60/150 => Loss 0.496, Train_accy 62.35, Test_accy 72.71
2024-10-08 21:34:34,294 [lwf.py] => Task 1, Epoch 65/150 => Loss 0.498, Train_accy 62.30, Test_accy 72.48
2024-10-08 21:34:40,767 [lwf.py] => Task 1, Epoch 70/150 => Loss 0.496, Train_accy 62.08, Test_accy 72.98
2024-10-08 21:34:47,151 [lwf.py] => Task 1, Epoch 75/150 => Loss 0.497, Train_accy 61.92, Test_accy 72.76
2024-10-08 21:34:53,459 [lwf.py] => Task 1, Epoch 80/150 => Loss 0.496, Train_accy 62.35, Test_accy 72.86
2024-10-08 21:35:00,033 [lwf.py] => Task 1, Epoch 85/150 => Loss 0.498, Train_accy 61.68, Test_accy 72.69
2024-10-08 21:35:06,169 [lwf.py] => Task 1, Epoch 90/150 => Loss 0.495, Train_accy 62.45, Test_accy 72.88
2024-10-08 21:35:11,984 [lwf.py] => Task 1, Epoch 95/150 => Loss 0.496, Train_accy 62.68, Test_accy 72.90
2024-10-08 21:35:17,686 [lwf.py] => Task 1, Epoch 100/150 => Loss 0.496, Train_accy 62.55, Test_accy 72.71
2024-10-08 21:35:23,291 [lwf.py] => Task 1, Epoch 105/150 => Loss 0.497, Train_accy 62.28, Test_accy 72.60
2024-10-08 21:35:28,954 [lwf.py] => Task 1, Epoch 110/150 => Loss 0.496, Train_accy 62.18, Test_accy 72.95
2024-10-08 21:35:34,649 [lwf.py] => Task 1, Epoch 115/150 => Loss 0.497, Train_accy 62.12, Test_accy 72.79
2024-10-08 21:35:40,218 [lwf.py] => Task 1, Epoch 120/150 => Loss 0.495, Train_accy 62.08, Test_accy 72.60
2024-10-08 21:35:45,679 [lwf.py] => Task 1, Epoch 125/150 => Loss 0.495, Train_accy 62.70, Test_accy 72.81
2024-10-08 21:35:51,533 [lwf.py] => Task 1, Epoch 130/150 => Loss 0.497, Train_accy 61.45, Test_accy 72.71
2024-10-08 21:35:57,141 [lwf.py] => Task 1, Epoch 135/150 => Loss 0.495, Train_accy 61.75, Test_accy 72.67
2024-10-08 21:36:02,824 [lwf.py] => Task 1, Epoch 140/150 => Loss 0.495, Train_accy 62.58, Test_accy 72.74
2024-10-08 21:36:08,516 [lwf.py] => Task 1, Epoch 145/150 => Loss 0.496, Train_accy 61.75, Test_accy 72.69
2024-10-08 21:36:14,205 [lwf.py] => Task 1, Epoch 150/150 => Loss 0.496, Train_accy 61.85, Test_accy 72.79
2024-10-08 21:36:14,207 [lwf.py] => Task 1, Epoch 150/150 => Loss 0.496, Train_accy 61.85, Test_accy 72.79
2024-10-08 21:36:14,208 [trainer.py] => All params: 3847495
2024-10-08 21:36:14,787 [trainer.py] => No NME accuracy.
2024-10-08 21:36:14,787 [trainer.py] => CNN: {'total': 72.79, '00-04': 86.0, '05-06': 39.75, 'old': 86.0, 'new': 39.75}
2024-10-08 21:36:14,787 [trainer.py] => CNN top1 curve: [89.93, 72.79]
2024-10-08 21:36:14,787 [trainer.py] => CNN top5 curve: [100.0, 99.26]

2024-10-08 21:36:14,787 [trainer.py] => Average Accuracy (CNN): 81.36000000000001
2024-10-08 21:36:14,788 [trainer.py] => All params: 3847495
2024-10-08 21:36:14,788 [trainer.py] => Trainable params: 3847495
2024-10-08 21:36:14,789 [lwf.py] => Learning on 7-9
2024-10-08 21:36:20,476 [lwf.py] => Task 2, Epoch 5/150 => Loss 0.656, Train_accy 20.90, Test_accy 58.35
2024-10-08 21:36:26,264 [lwf.py] => Task 2, Epoch 10/150 => Loss 0.643, Train_accy 30.82, Test_accy 58.81
2024-10-08 21:36:32,313 [lwf.py] => Task 2, Epoch 15/150 => Loss 0.646, Train_accy 35.72, Test_accy 61.87
2024-10-08 21:36:37,919 [lwf.py] => Task 2, Epoch 20/150 => Loss 0.640, Train_accy 40.25, Test_accy 61.20
2024-10-08 21:36:43,707 [lwf.py] => Task 2, Epoch 25/150 => Loss 0.646, Train_accy 41.28, Test_accy 61.65
2024-10-08 21:36:49,434 [lwf.py] => Task 2, Epoch 30/150 => Loss 0.642, Train_accy 44.98, Test_accy 55.65
2024-10-08 21:36:55,461 [lwf.py] => Task 2, Epoch 35/150 => Loss 0.641, Train_accy 41.02, Test_accy 61.87
2024-10-08 21:37:01,302 [lwf.py] => Task 2, Epoch 40/150 => Loss 0.640, Train_accy 43.72, Test_accy 61.76
2024-10-08 21:37:07,208 [lwf.py] => Task 2, Epoch 45/150 => Loss 0.639, Train_accy 44.62, Test_accy 61.78
2024-10-08 21:37:12,840 [lwf.py] => Task 2, Epoch 50/150 => Loss 0.640, Train_accy 44.90, Test_accy 61.26
2024-10-08 21:37:18,609 [lwf.py] => Task 2, Epoch 55/150 => Loss 0.638, Train_accy 45.55, Test_accy 61.24
2024-10-08 21:37:24,538 [lwf.py] => Task 2, Epoch 60/150 => Loss 0.639, Train_accy 45.40, Test_accy 61.30
2024-10-08 21:37:30,343 [lwf.py] => Task 2, Epoch 65/150 => Loss 0.638, Train_accy 45.30, Test_accy 61.65
2024-10-08 21:37:36,010 [lwf.py] => Task 2, Epoch 70/150 => Loss 0.639, Train_accy 45.58, Test_accy 61.52
2024-10-08 21:37:41,921 [lwf.py] => Task 2, Epoch 75/150 => Loss 0.639, Train_accy 46.45, Test_accy 61.44
2024-10-08 21:37:47,687 [lwf.py] => Task 2, Epoch 80/150 => Loss 0.640, Train_accy 46.18, Test_accy 61.33
2024-10-08 21:37:53,427 [lwf.py] => Task 2, Epoch 85/150 => Loss 0.639, Train_accy 45.62, Test_accy 61.63
2024-10-08 21:37:59,441 [lwf.py] => Task 2, Epoch 90/150 => Loss 0.640, Train_accy 45.00, Test_accy 61.09
2024-10-08 21:38:05,443 [lwf.py] => Task 2, Epoch 95/150 => Loss 0.638, Train_accy 46.22, Test_accy 61.65
2024-10-08 21:38:11,461 [lwf.py] => Task 2, Epoch 100/150 => Loss 0.639, Train_accy 45.50, Test_accy 61.50
2024-10-08 21:38:17,256 [lwf.py] => Task 2, Epoch 105/150 => Loss 0.639, Train_accy 46.55, Test_accy 61.48
2024-10-08 21:38:23,279 [lwf.py] => Task 2, Epoch 110/150 => Loss 0.638, Train_accy 45.62, Test_accy 61.54
2024-10-08 21:38:29,066 [lwf.py] => Task 2, Epoch 115/150 => Loss 0.639, Train_accy 45.62, Test_accy 61.57
2024-10-08 21:38:35,201 [lwf.py] => Task 2, Epoch 120/150 => Loss 0.639, Train_accy 46.35, Test_accy 61.78
2024-10-08 21:38:41,029 [lwf.py] => Task 2, Epoch 125/150 => Loss 0.638, Train_accy 46.15, Test_accy 61.93
2024-10-08 21:38:46,784 [lwf.py] => Task 2, Epoch 130/150 => Loss 0.638, Train_accy 45.68, Test_accy 61.30
2024-10-08 21:38:52,839 [lwf.py] => Task 2, Epoch 135/150 => Loss 0.638, Train_accy 46.48, Test_accy 61.48
2024-10-08 21:38:58,666 [lwf.py] => Task 2, Epoch 140/150 => Loss 0.638, Train_accy 46.22, Test_accy 61.94
2024-10-08 21:39:04,400 [lwf.py] => Task 2, Epoch 145/150 => Loss 0.638, Train_accy 46.32, Test_accy 61.83
2024-10-08 21:39:10,327 [lwf.py] => Task 2, Epoch 150/150 => Loss 0.637, Train_accy 45.75, Test_accy 61.37
2024-10-08 21:39:10,329 [lwf.py] => Task 2, Epoch 150/150 => Loss 0.637, Train_accy 45.75, Test_accy 61.37
2024-10-08 21:39:10,330 [trainer.py] => All params: 3848521
2024-10-08 21:39:11,025 [trainer.py] => No NME accuracy.
2024-10-08 21:39:11,025 [trainer.py] => CNN: {'total': 61.37, '00-04': 81.87, '05-06': 30.0, '07-08': 41.5, 'old': 67.05, 'new': 41.5}
2024-10-08 21:39:11,025 [trainer.py] => CNN top1 curve: [89.93, 72.79, 61.37]
2024-10-08 21:39:11,025 [trainer.py] => CNN top5 curve: [100.0, 99.26, 98.33]

2024-10-08 21:39:11,026 [trainer.py] => Average Accuracy (CNN): 74.69666666666667
2024-10-08 21:39:11,026 [trainer.py] => Forgetting (CNN): 8.905000000000001
