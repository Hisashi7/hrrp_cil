2024-08-29 20:36:30,828 [trainer.py] => config: ./exps/lwf.json
2024-08-29 20:36:30,828 [trainer.py] => prefix: reproduce
2024-08-29 20:36:30,828 [trainer.py] => dataset: hrrp9
2024-08-29 20:36:30,828 [trainer.py] => memory_size: 500
2024-08-29 20:36:30,828 [trainer.py] => memory_per_class: 20
2024-08-29 20:36:30,828 [trainer.py] => fixed_memory: False
2024-08-29 20:36:30,828 [trainer.py] => shuffle: True
2024-08-29 20:36:30,828 [trainer.py] => init_cls: 5
2024-08-29 20:36:30,828 [trainer.py] => increment: 2
2024-08-29 20:36:30,829 [trainer.py] => model_name: lwf
2024-08-29 20:36:30,829 [trainer.py] => convnet_type: resnet18
2024-08-29 20:36:30,829 [trainer.py] => device: [device(type='cuda', index=3)]
2024-08-29 20:36:30,829 [trainer.py] => init_train: False
2024-08-29 20:36:30,829 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-29 20:36:30,829 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-29 20:36:30,829 [trainer.py] => seed: 1993
2024-08-29 20:36:31,336 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-29 20:36:33,391 [trainer.py] => All params: 3843904
2024-08-29 20:36:33,392 [trainer.py] => Trainable params: 3843904
2024-08-29 20:36:33,395 [lwf.py] => Learning on 0-5
2024-08-29 20:36:35,173 [lwf.py] => init_train?---False
2024-08-29 20:36:40,022 [trainer.py] => No NME accuracy.
2024-08-29 20:36:40,022 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-08-29 20:36:40,022 [trainer.py] => CNN top1 curve: [89.93]
2024-08-29 20:36:40,022 [trainer.py] => CNN top5 curve: [100.0]

2024-08-29 20:36:40,022 [trainer.py] => Average Accuracy (CNN): 89.93
2024-08-29 20:36:40,022 [trainer.py] => All params: 3846469
2024-08-29 20:36:40,023 [trainer.py] => Trainable params: 3846469
2024-08-29 20:36:40,023 [lwf.py] => Learning on 5-7
2024-08-29 20:36:43,668 [lwf.py] => Task 1, Epoch 1/150 => Loss 3.615, Train_accy 19.38, Test_accy 58.38
2024-08-29 20:36:53,933 [lwf.py] => Task 1, Epoch 6/150 => Loss 3.073, Train_accy 71.92, Test_accy 64.21
2024-08-29 20:37:05,276 [lwf.py] => Task 1, Epoch 11/150 => Loss 3.022, Train_accy 75.10, Test_accy 70.05
2024-08-29 20:37:15,653 [lwf.py] => Task 1, Epoch 16/150 => Loss 3.000, Train_accy 74.30, Test_accy 70.62
2024-08-29 20:37:26,208 [lwf.py] => Task 1, Epoch 21/150 => Loss 3.002, Train_accy 72.30, Test_accy 70.19
2024-08-29 20:37:36,736 [lwf.py] => Task 1, Epoch 26/150 => Loss 2.992, Train_accy 71.62, Test_accy 67.55
2024-08-29 20:37:47,842 [lwf.py] => Task 1, Epoch 31/150 => Loss 2.991, Train_accy 69.47, Test_accy 68.43
2024-08-29 20:37:59,154 [lwf.py] => Task 1, Epoch 36/150 => Loss 2.990, Train_accy 70.00, Test_accy 69.55
2024-08-29 20:38:10,636 [lwf.py] => Task 1, Epoch 41/150 => Loss 2.988, Train_accy 72.55, Test_accy 70.45
2024-08-29 20:38:21,440 [lwf.py] => Task 1, Epoch 46/150 => Loss 2.984, Train_accy 69.18, Test_accy 69.88
2024-08-29 20:38:32,805 [lwf.py] => Task 1, Epoch 51/150 => Loss 2.986, Train_accy 69.70, Test_accy 69.17
2024-08-29 20:38:43,736 [lwf.py] => Task 1, Epoch 56/150 => Loss 2.980, Train_accy 68.92, Test_accy 69.40
2024-08-29 20:38:54,838 [lwf.py] => Task 1, Epoch 61/150 => Loss 2.995, Train_accy 68.80, Test_accy 69.52
2024-08-29 20:39:06,062 [lwf.py] => Task 1, Epoch 66/150 => Loss 2.986, Train_accy 69.50, Test_accy 67.43
2024-08-29 20:39:17,142 [lwf.py] => Task 1, Epoch 71/150 => Loss 2.983, Train_accy 68.70, Test_accy 68.31
2024-08-29 20:39:28,409 [lwf.py] => Task 1, Epoch 76/150 => Loss 2.975, Train_accy 67.12, Test_accy 69.50
2024-08-29 20:39:40,210 [lwf.py] => Task 1, Epoch 81/150 => Loss 2.975, Train_accy 67.62, Test_accy 69.64
2024-08-29 20:39:51,902 [lwf.py] => Task 1, Epoch 86/150 => Loss 2.965, Train_accy 67.55, Test_accy 69.71
2024-08-29 20:40:03,690 [lwf.py] => Task 1, Epoch 91/150 => Loss 2.966, Train_accy 67.70, Test_accy 69.90
2024-08-29 20:40:14,896 [lwf.py] => Task 1, Epoch 96/150 => Loss 2.958, Train_accy 68.20, Test_accy 70.10
2024-08-29 20:40:26,675 [lwf.py] => Task 1, Epoch 101/150 => Loss 2.951, Train_accy 67.72, Test_accy 70.10
2024-08-29 20:40:38,963 [lwf.py] => Task 1, Epoch 106/150 => Loss 2.959, Train_accy 67.82, Test_accy 69.17
2024-08-29 20:40:51,185 [lwf.py] => Task 1, Epoch 111/150 => Loss 2.961, Train_accy 68.30, Test_accy 69.38
2024-08-29 20:41:03,010 [lwf.py] => Task 1, Epoch 116/150 => Loss 2.959, Train_accy 68.68, Test_accy 69.19
2024-08-29 20:41:14,837 [lwf.py] => Task 1, Epoch 121/150 => Loss 2.964, Train_accy 67.45, Test_accy 69.45
2024-08-29 20:41:27,006 [lwf.py] => Task 1, Epoch 126/150 => Loss 2.961, Train_accy 68.32, Test_accy 69.02
2024-08-29 20:41:39,160 [lwf.py] => Task 1, Epoch 131/150 => Loss 2.954, Train_accy 68.05, Test_accy 69.76
2024-08-29 20:41:51,052 [lwf.py] => Task 1, Epoch 136/150 => Loss 2.963, Train_accy 68.47, Test_accy 69.62
2024-08-29 20:42:02,945 [lwf.py] => Task 1, Epoch 141/150 => Loss 2.955, Train_accy 68.32, Test_accy 69.48
2024-08-29 20:42:14,658 [lwf.py] => Task 1, Epoch 146/150 => Loss 2.963, Train_accy 68.03, Test_accy 69.83
2024-08-29 20:42:23,041 [lwf.py] => Task 1, Epoch 150/150 => Loss 2.963, Train_accy 68.53
2024-08-29 20:42:24,747 [trainer.py] => No NME accuracy.
2024-08-29 20:42:24,750 [trainer.py] => CNN: {'total': 68.93, '00-04': 75.07, '05-06': 53.58, 'old': 75.07, 'new': 53.58}
2024-08-29 20:42:24,750 [trainer.py] => CNN top1 curve: [89.93, 68.93]
2024-08-29 20:42:24,750 [trainer.py] => CNN top5 curve: [100.0, 99.02]

2024-08-29 20:42:24,751 [trainer.py] => Average Accuracy (CNN): 79.43
2024-08-29 20:42:24,751 [trainer.py] => All params: 3847495
2024-08-29 20:42:24,751 [trainer.py] => Trainable params: 3847495
2024-08-29 20:42:24,752 [lwf.py] => Learning on 7-9
2024-08-29 20:42:28,574 [lwf.py] => Task 2, Epoch 1/150 => Loss 4.874, Train_accy 25.88, Test_accy 39.44
2024-08-29 20:42:40,365 [lwf.py] => Task 2, Epoch 6/150 => Loss 4.534, Train_accy 86.55, Test_accy 42.74
2024-08-29 20:42:51,507 [lwf.py] => Task 2, Epoch 11/150 => Loss 4.512, Train_accy 88.48, Test_accy 43.57
2024-08-29 20:43:02,850 [lwf.py] => Task 2, Epoch 16/150 => Loss 4.509, Train_accy 87.42, Test_accy 42.56
2024-08-29 20:43:14,238 [lwf.py] => Task 2, Epoch 21/150 => Loss 4.507, Train_accy 88.20, Test_accy 43.30
2024-08-29 20:43:25,029 [lwf.py] => Task 2, Epoch 26/150 => Loss 4.509, Train_accy 86.98, Test_accy 42.78
2024-08-29 20:43:36,545 [lwf.py] => Task 2, Epoch 31/150 => Loss 4.511, Train_accy 89.05, Test_accy 43.69
2024-08-29 20:43:47,885 [lwf.py] => Task 2, Epoch 36/150 => Loss 4.505, Train_accy 86.22, Test_accy 40.76
2024-08-29 20:43:59,375 [lwf.py] => Task 2, Epoch 41/150 => Loss 4.504, Train_accy 86.90, Test_accy 41.44
2024-08-29 20:44:10,634 [lwf.py] => Task 2, Epoch 46/150 => Loss 4.505, Train_accy 86.12, Test_accy 43.48
2024-08-29 20:44:21,482 [lwf.py] => Task 2, Epoch 51/150 => Loss 4.503, Train_accy 85.78, Test_accy 41.02
2024-08-29 20:44:32,474 [lwf.py] => Task 2, Epoch 56/150 => Loss 4.508, Train_accy 85.98, Test_accy 45.06
2024-08-29 20:44:44,222 [lwf.py] => Task 2, Epoch 61/150 => Loss 4.501, Train_accy 85.45, Test_accy 42.76
2024-08-29 20:44:55,421 [lwf.py] => Task 2, Epoch 66/150 => Loss 4.499, Train_accy 86.38, Test_accy 44.22
2024-08-29 20:45:06,651 [lwf.py] => Task 2, Epoch 71/150 => Loss 4.506, Train_accy 85.28, Test_accy 42.07
2024-08-29 20:45:17,963 [lwf.py] => Task 2, Epoch 76/150 => Loss 4.498, Train_accy 85.95, Test_accy 41.04
2024-08-29 20:45:29,458 [lwf.py] => Task 2, Epoch 81/150 => Loss 4.501, Train_accy 85.68, Test_accy 42.69
2024-08-29 20:45:40,977 [lwf.py] => Task 2, Epoch 86/150 => Loss 4.493, Train_accy 86.12, Test_accy 43.46
2024-08-29 20:45:51,664 [lwf.py] => Task 2, Epoch 91/150 => Loss 4.488, Train_accy 86.82, Test_accy 43.06
2024-08-29 20:46:02,878 [lwf.py] => Task 2, Epoch 96/150 => Loss 4.492, Train_accy 85.50, Test_accy 43.74
2024-08-29 20:46:14,478 [lwf.py] => Task 2, Epoch 101/150 => Loss 4.489, Train_accy 86.38, Test_accy 43.50
2024-08-29 20:46:25,319 [lwf.py] => Task 2, Epoch 106/150 => Loss 4.495, Train_accy 86.85, Test_accy 43.19
2024-08-29 20:46:36,533 [lwf.py] => Task 2, Epoch 111/150 => Loss 4.498, Train_accy 86.58, Test_accy 43.11
2024-08-29 20:46:47,571 [lwf.py] => Task 2, Epoch 116/150 => Loss 4.492, Train_accy 85.48, Test_accy 42.91
2024-08-29 20:46:59,129 [lwf.py] => Task 2, Epoch 121/150 => Loss 4.492, Train_accy 86.78, Test_accy 43.63
2024-08-29 20:47:09,929 [lwf.py] => Task 2, Epoch 126/150 => Loss 4.495, Train_accy 86.28, Test_accy 42.85
2024-08-29 20:47:20,793 [lwf.py] => Task 2, Epoch 131/150 => Loss 4.492, Train_accy 87.05, Test_accy 43.04
2024-08-29 20:47:31,696 [lwf.py] => Task 2, Epoch 136/150 => Loss 4.493, Train_accy 86.85, Test_accy 43.48
2024-08-29 20:47:42,809 [lwf.py] => Task 2, Epoch 141/150 => Loss 4.493, Train_accy 86.40, Test_accy 43.69
2024-08-29 20:47:53,673 [lwf.py] => Task 2, Epoch 146/150 => Loss 4.495, Train_accy 85.90, Test_accy 42.96
2024-08-29 20:48:01,241 [lwf.py] => Task 2, Epoch 150/150 => Loss 4.497, Train_accy 86.88
2024-08-29 20:48:02,691 [trainer.py] => No NME accuracy.
2024-08-29 20:48:02,691 [trainer.py] => CNN: {'total': 43.02, '00-04': 36.27, '05-06': 22.25, '07-08': 80.67, 'old': 32.26, 'new': 80.67}
2024-08-29 20:48:02,691 [trainer.py] => CNN top1 curve: [89.93, 68.93, 43.02]
2024-08-29 20:48:02,691 [trainer.py] => CNN top5 curve: [100.0, 99.02, 97.26]

2024-08-29 20:48:02,692 [trainer.py] => Average Accuracy (CNN): 67.29333333333334
2024-08-29 20:48:02,692 [trainer.py] => Forgetting (CNN): 42.495000000000005
