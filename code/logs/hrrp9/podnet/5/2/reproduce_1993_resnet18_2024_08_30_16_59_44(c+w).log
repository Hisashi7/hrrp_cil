2024-08-30 16:59:44,669 [trainer.py] => config: ./exps/podnet.json
2024-08-30 16:59:44,669 [trainer.py] => prefix: reproduce
2024-08-30 16:59:44,669 [trainer.py] => dataset: hrrp9
2024-08-30 16:59:44,669 [trainer.py] => memory_size: 500
2024-08-30 16:59:44,669 [trainer.py] => memory_per_class: 20
2024-08-30 16:59:44,669 [trainer.py] => fixed_memory: False
2024-08-30 16:59:44,669 [trainer.py] => shuffle: True
2024-08-30 16:59:44,669 [trainer.py] => init_cls: 5
2024-08-30 16:59:44,669 [trainer.py] => increment: 2
2024-08-30 16:59:44,669 [trainer.py] => model_name: podnet
2024-08-30 16:59:44,669 [trainer.py] => convnet_type: resnet18
2024-08-30 16:59:44,669 [trainer.py] => init_train: True
2024-08-30 16:59:44,669 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 16:59:44,669 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 16:59:44,669 [trainer.py] => device: [device(type='cuda', index=5)]
2024-08-30 16:59:44,669 [trainer.py] => seed: 1993
2024-08-30 16:59:45,126 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 16:59:45,220 [trainer.py] => All params: 3843904
2024-08-30 16:59:45,221 [trainer.py] => Trainable params: 3843904
2024-08-30 16:59:45,221 [podnet.py] => Learning on 0-5
2024-08-30 16:59:45,255 [podnet.py] => Adaptive factor: 0
2024-08-30 16:59:48,041 [podnet.py] => Task 0, Epoch 1/200 (LR 0.09999) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 16:59:50,740 [podnet.py] => Task 0, Epoch 2/200 (LR 0.09998) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.70, Test_acc 28.10
2024-08-30 16:59:53,977 [podnet.py] => Task 0, Epoch 3/200 (LR 0.09994) => LSC_loss 0.74, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.65, Test_acc 46.50
2024-08-30 16:59:55,880 [podnet.py] => Task 0, Epoch 4/200 (LR 0.09990) => LSC_loss 0.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.48, Test_acc 56.47
2024-08-30 16:59:59,064 [podnet.py] => Task 0, Epoch 5/200 (LR 0.09985) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.35, Test_acc 57.07
2024-08-30 17:00:01,921 [podnet.py] => Task 0, Epoch 6/200 (LR 0.09978) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.01, Test_acc 70.80
2024-08-30 17:00:04,009 [podnet.py] => Task 0, Epoch 7/200 (LR 0.09970) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.09, Test_acc 78.63
2024-08-30 17:00:07,242 [podnet.py] => Task 0, Epoch 8/200 (LR 0.09961) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.03, Test_acc 73.23
2024-08-30 17:00:10,613 [podnet.py] => Task 0, Epoch 9/200 (LR 0.09950) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.07, Test_acc 72.00
2024-08-30 17:00:13,364 [podnet.py] => Task 0, Epoch 10/200 (LR 0.09938) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.89, Test_acc 78.53
2024-08-30 17:00:16,648 [podnet.py] => Task 0, Epoch 11/200 (LR 0.09926) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.73, Test_acc 70.67
2024-08-30 17:00:18,908 [podnet.py] => Task 0, Epoch 12/200 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.93, Test_acc 72.97
2024-08-30 17:00:20,747 [podnet.py] => Task 0, Epoch 13/200 (LR 0.09896) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.33, Test_acc 82.83
2024-08-30 17:00:23,759 [podnet.py] => Task 0, Epoch 14/200 (LR 0.09880) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.98, Test_acc 73.07
2024-08-30 17:00:26,387 [podnet.py] => Task 0, Epoch 15/200 (LR 0.09862) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.00, Test_acc 50.33
2024-08-30 17:00:28,851 [podnet.py] => Task 0, Epoch 16/200 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.34, Test_acc 86.00
2024-08-30 17:00:30,755 [podnet.py] => Task 0, Epoch 17/200 (LR 0.09823) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.15, Test_acc 76.53
2024-08-30 17:00:33,331 [podnet.py] => Task 0, Epoch 18/200 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 84.10
2024-08-30 17:00:36,042 [podnet.py] => Task 0, Epoch 19/200 (LR 0.09779) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 76.60
2024-08-30 17:00:38,290 [podnet.py] => Task 0, Epoch 20/200 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 75.47
2024-08-30 17:00:40,862 [podnet.py] => Task 0, Epoch 21/200 (LR 0.09730) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 79.90
2024-08-30 17:00:43,087 [podnet.py] => Task 0, Epoch 22/200 (LR 0.09704) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.93, Test_acc 81.03
2024-08-30 17:00:46,120 [podnet.py] => Task 0, Epoch 23/200 (LR 0.09677) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 81.97
2024-08-30 17:00:49,114 [podnet.py] => Task 0, Epoch 24/200 (LR 0.09649) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.72, Test_acc 79.50
2024-08-30 17:00:51,547 [podnet.py] => Task 0, Epoch 25/200 (LR 0.09619) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 83.03
2024-08-30 17:00:54,259 [podnet.py] => Task 0, Epoch 26/200 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 84.80
2024-08-30 17:00:56,679 [podnet.py] => Task 0, Epoch 27/200 (LR 0.09557) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 80.00
2024-08-30 17:00:59,712 [podnet.py] => Task 0, Epoch 28/200 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 83.80
2024-08-30 17:01:01,217 [podnet.py] => Task 0, Epoch 29/200 (LR 0.09490) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 84.60
2024-08-30 17:01:04,455 [podnet.py] => Task 0, Epoch 30/200 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.21, Test_acc 77.83
2024-08-30 17:01:07,468 [podnet.py] => Task 0, Epoch 31/200 (LR 0.09419) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 80.53
2024-08-30 17:01:08,988 [podnet.py] => Task 0, Epoch 32/200 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.13
2024-08-30 17:01:11,882 [podnet.py] => Task 0, Epoch 33/200 (LR 0.09343) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.99, Test_acc 82.00
2024-08-30 17:01:14,691 [podnet.py] => Task 0, Epoch 34/200 (LR 0.09304) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 78.77
2024-08-30 17:01:17,281 [podnet.py] => Task 0, Epoch 35/200 (LR 0.09263) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.99, Test_acc 83.30
2024-08-30 17:01:20,477 [podnet.py] => Task 0, Epoch 36/200 (LR 0.09222) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.97, Test_acc 80.60
2024-08-30 17:01:23,236 [podnet.py] => Task 0, Epoch 37/200 (LR 0.09179) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.96, Test_acc 85.33
2024-08-30 17:01:25,099 [podnet.py] => Task 0, Epoch 38/200 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 78.57
2024-08-30 17:01:26,880 [podnet.py] => Task 0, Epoch 39/200 (LR 0.09091) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 87.57
2024-08-30 17:01:29,795 [podnet.py] => Task 0, Epoch 40/200 (LR 0.09045) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.39, Test_acc 86.33
2024-08-30 17:01:32,889 [podnet.py] => Task 0, Epoch 41/200 (LR 0.08998) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 78.50
2024-08-30 17:01:35,640 [podnet.py] => Task 0, Epoch 42/200 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 84.73
2024-08-30 17:01:37,791 [podnet.py] => Task 0, Epoch 43/200 (LR 0.08902) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 75.87
2024-08-30 17:01:40,127 [podnet.py] => Task 0, Epoch 44/200 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 81.53
2024-08-30 17:01:43,194 [podnet.py] => Task 0, Epoch 45/200 (LR 0.08802) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.49, Test_acc 75.03
2024-08-30 17:01:45,148 [podnet.py] => Task 0, Epoch 46/200 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 83.07
2024-08-30 17:01:47,748 [podnet.py] => Task 0, Epoch 47/200 (LR 0.08698) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 84.73
2024-08-30 17:01:50,047 [podnet.py] => Task 0, Epoch 48/200 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.23
2024-08-30 17:01:52,508 [podnet.py] => Task 0, Epoch 49/200 (LR 0.08591) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.91, Test_acc 80.70
2024-08-30 17:01:54,390 [podnet.py] => Task 0, Epoch 50/200 (LR 0.08536) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.64, Test_acc 80.33
2024-08-30 17:01:57,486 [podnet.py] => Task 0, Epoch 51/200 (LR 0.08480) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 86.60
2024-08-30 17:01:59,609 [podnet.py] => Task 0, Epoch 52/200 (LR 0.08423) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.62, Test_acc 81.07
2024-08-30 17:02:02,221 [podnet.py] => Task 0, Epoch 53/200 (LR 0.08365) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.46, Test_acc 81.60
2024-08-30 17:02:05,178 [podnet.py] => Task 0, Epoch 54/200 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 79.40
2024-08-30 17:02:08,306 [podnet.py] => Task 0, Epoch 55/200 (LR 0.08247) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.85, Test_acc 78.10
2024-08-30 17:02:11,382 [podnet.py] => Task 0, Epoch 56/200 (LR 0.08187) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 87.00
2024-08-30 17:02:14,604 [podnet.py] => Task 0, Epoch 57/200 (LR 0.08126) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 82.57
2024-08-30 17:02:17,832 [podnet.py] => Task 0, Epoch 58/200 (LR 0.08065) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 86.30
2024-08-30 17:02:20,422 [podnet.py] => Task 0, Epoch 59/200 (LR 0.08002) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 87.63
2024-08-30 17:02:22,621 [podnet.py] => Task 0, Epoch 60/200 (LR 0.07939) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 85.37
2024-08-30 17:02:25,245 [podnet.py] => Task 0, Epoch 61/200 (LR 0.07875) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.14, Test_acc 78.47
2024-08-30 17:02:28,312 [podnet.py] => Task 0, Epoch 62/200 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.34, Test_acc 82.63
2024-08-30 17:02:30,975 [podnet.py] => Task 0, Epoch 63/200 (LR 0.07745) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 85.27
2024-08-30 17:02:33,816 [podnet.py] => Task 0, Epoch 64/200 (LR 0.07679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 86.43
2024-08-30 17:02:36,671 [podnet.py] => Task 0, Epoch 65/200 (LR 0.07612) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.57
2024-08-30 17:02:39,443 [podnet.py] => Task 0, Epoch 66/200 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.17, Test_acc 84.67
2024-08-30 17:02:41,721 [podnet.py] => Task 0, Epoch 67/200 (LR 0.07477) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 82.73
2024-08-30 17:02:44,269 [podnet.py] => Task 0, Epoch 68/200 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.20
2024-08-30 17:02:46,086 [podnet.py] => Task 0, Epoch 69/200 (LR 0.07340) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.27, Test_acc 83.47
2024-08-30 17:02:48,755 [podnet.py] => Task 0, Epoch 70/200 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.58, Test_acc 76.37
2024-08-30 17:02:51,767 [podnet.py] => Task 0, Epoch 71/200 (LR 0.07200) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.67, Test_acc 83.77
2024-08-30 17:02:54,278 [podnet.py] => Task 0, Epoch 72/200 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 85.97
2024-08-30 17:02:56,821 [podnet.py] => Task 0, Epoch 73/200 (LR 0.07058) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 85.37
2024-08-30 17:02:59,360 [podnet.py] => Task 0, Epoch 74/200 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 87.87
2024-08-30 17:03:02,396 [podnet.py] => Task 0, Epoch 75/200 (LR 0.06913) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 84.93
2024-08-30 17:03:05,271 [podnet.py] => Task 0, Epoch 76/200 (LR 0.06841) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.94, Test_acc 83.30
2024-08-30 17:03:07,806 [podnet.py] => Task 0, Epoch 77/200 (LR 0.06767) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 86.93
2024-08-30 17:03:10,216 [podnet.py] => Task 0, Epoch 78/200 (LR 0.06694) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.18, Test_acc 80.80
2024-08-30 17:03:13,587 [podnet.py] => Task 0, Epoch 79/200 (LR 0.06620) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.43
2024-08-30 17:03:16,849 [podnet.py] => Task 0, Epoch 80/200 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 87.10
2024-08-30 17:03:19,632 [podnet.py] => Task 0, Epoch 81/200 (LR 0.06470) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.77
2024-08-30 17:03:22,911 [podnet.py] => Task 0, Epoch 82/200 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 77.37
2024-08-30 17:03:25,487 [podnet.py] => Task 0, Epoch 83/200 (LR 0.06319) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.58, Test_acc 88.90
2024-08-30 17:03:28,492 [podnet.py] => Task 0, Epoch 84/200 (LR 0.06243) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.37
2024-08-30 17:03:31,612 [podnet.py] => Task 0, Epoch 85/200 (LR 0.06167) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-30 17:03:34,101 [podnet.py] => Task 0, Epoch 86/200 (LR 0.06091) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 91.13
2024-08-30 17:03:36,775 [podnet.py] => Task 0, Epoch 87/200 (LR 0.06014) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.77
2024-08-30 17:03:39,724 [podnet.py] => Task 0, Epoch 88/200 (LR 0.05937) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.80, Test_acc 77.43
2024-08-30 17:03:42,395 [podnet.py] => Task 0, Epoch 89/200 (LR 0.05860) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.27, Test_acc 82.77
2024-08-30 17:03:44,560 [podnet.py] => Task 0, Epoch 90/200 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 84.93
2024-08-30 17:03:47,494 [podnet.py] => Task 0, Epoch 91/200 (LR 0.05705) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.26, Test_acc 87.63
2024-08-30 17:03:50,470 [podnet.py] => Task 0, Epoch 92/200 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 86.30
2024-08-30 17:03:53,040 [podnet.py] => Task 0, Epoch 93/200 (LR 0.05549) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 82.70
2024-08-30 17:03:55,351 [podnet.py] => Task 0, Epoch 94/200 (LR 0.05471) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 86.67
2024-08-30 17:03:58,375 [podnet.py] => Task 0, Epoch 95/200 (LR 0.05392) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.90, Test_acc 88.53
2024-08-30 17:04:01,015 [podnet.py] => Task 0, Epoch 96/200 (LR 0.05314) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.65, Test_acc 75.93
2024-08-30 17:04:02,409 [podnet.py] => Task 0, Epoch 97/200 (LR 0.05236) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.81, Test_acc 86.07
2024-08-30 17:04:05,251 [podnet.py] => Task 0, Epoch 98/200 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.95, Test_acc 83.53
2024-08-30 17:04:08,203 [podnet.py] => Task 0, Epoch 99/200 (LR 0.05079) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 87.07
2024-08-30 17:04:10,120 [podnet.py] => Task 0, Epoch 100/200 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.53, Test_acc 86.03
2024-08-30 17:04:11,478 [podnet.py] => Task 0, Epoch 101/200 (LR 0.04921) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 87.93
2024-08-30 17:04:13,957 [podnet.py] => Task 0, Epoch 102/200 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 88.53
2024-08-30 17:04:17,101 [podnet.py] => Task 0, Epoch 103/200 (LR 0.04764) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 89.13
2024-08-30 17:04:20,416 [podnet.py] => Task 0, Epoch 104/200 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 84.87
2024-08-30 17:04:22,940 [podnet.py] => Task 0, Epoch 105/200 (LR 0.04608) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 88.63
2024-08-30 17:04:26,095 [podnet.py] => Task 0, Epoch 106/200 (LR 0.04529) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 86.70
2024-08-30 17:04:29,067 [podnet.py] => Task 0, Epoch 107/200 (LR 0.04451) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 85.30
2024-08-30 17:04:31,258 [podnet.py] => Task 0, Epoch 108/200 (LR 0.04373) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 86.13
2024-08-30 17:04:34,256 [podnet.py] => Task 0, Epoch 109/200 (LR 0.04295) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 86.57
2024-08-30 17:04:37,485 [podnet.py] => Task 0, Epoch 110/200 (LR 0.04218) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 88.03
2024-08-30 17:04:41,056 [podnet.py] => Task 0, Epoch 111/200 (LR 0.04140) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 88.87
2024-08-30 17:04:44,105 [podnet.py] => Task 0, Epoch 112/200 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 86.90
2024-08-30 17:04:47,374 [podnet.py] => Task 0, Epoch 113/200 (LR 0.03986) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 85.40
2024-08-30 17:04:50,927 [podnet.py] => Task 0, Epoch 114/200 (LR 0.03909) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 86.57
2024-08-30 17:04:54,197 [podnet.py] => Task 0, Epoch 115/200 (LR 0.03833) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 89.27
2024-08-30 17:04:57,204 [podnet.py] => Task 0, Epoch 116/200 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 90.97
2024-08-30 17:04:59,853 [podnet.py] => Task 0, Epoch 117/200 (LR 0.03681) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.73
2024-08-30 17:05:03,036 [podnet.py] => Task 0, Epoch 118/200 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 84.87
2024-08-30 17:05:05,894 [podnet.py] => Task 0, Epoch 119/200 (LR 0.03530) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 87.47
2024-08-30 17:05:09,010 [podnet.py] => Task 0, Epoch 120/200 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 88.33
2024-08-30 17:05:12,109 [podnet.py] => Task 0, Epoch 121/200 (LR 0.03380) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 71.53
2024-08-30 17:05:14,899 [podnet.py] => Task 0, Epoch 122/200 (LR 0.03306) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.85, Test_acc 84.37
2024-08-30 17:05:17,572 [podnet.py] => Task 0, Epoch 123/200 (LR 0.03233) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 87.00
2024-08-30 17:05:20,363 [podnet.py] => Task 0, Epoch 124/200 (LR 0.03159) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.87
2024-08-30 17:05:23,189 [podnet.py] => Task 0, Epoch 125/200 (LR 0.03087) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 87.83
2024-08-30 17:05:26,413 [podnet.py] => Task 0, Epoch 126/200 (LR 0.03014) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 88.53
2024-08-30 17:05:30,063 [podnet.py] => Task 0, Epoch 127/200 (LR 0.02942) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.17
2024-08-30 17:05:33,453 [podnet.py] => Task 0, Epoch 128/200 (LR 0.02871) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.03
2024-08-30 17:05:36,532 [podnet.py] => Task 0, Epoch 129/200 (LR 0.02800) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 87.80
2024-08-30 17:05:39,658 [podnet.py] => Task 0, Epoch 130/200 (LR 0.02730) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.13
2024-08-30 17:05:41,782 [podnet.py] => Task 0, Epoch 131/200 (LR 0.02660) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.50
2024-08-30 17:05:45,021 [podnet.py] => Task 0, Epoch 132/200 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:05:47,996 [podnet.py] => Task 0, Epoch 133/200 (LR 0.02523) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.03
2024-08-30 17:05:51,094 [podnet.py] => Task 0, Epoch 134/200 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:05:53,984 [podnet.py] => Task 0, Epoch 135/200 (LR 0.02388) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:05:56,264 [podnet.py] => Task 0, Epoch 136/200 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 88.10
2024-08-30 17:05:58,694 [podnet.py] => Task 0, Epoch 137/200 (LR 0.02255) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 89.17
2024-08-30 17:06:01,065 [podnet.py] => Task 0, Epoch 138/200 (LR 0.02190) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 89.93
2024-08-30 17:06:03,116 [podnet.py] => Task 0, Epoch 139/200 (LR 0.02125) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:06:04,975 [podnet.py] => Task 0, Epoch 140/200 (LR 0.02061) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-08-30 17:06:06,959 [podnet.py] => Task 0, Epoch 141/200 (LR 0.01998) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 90.00
2024-08-30 17:06:08,944 [podnet.py] => Task 0, Epoch 142/200 (LR 0.01935) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 90.20
2024-08-30 17:06:11,629 [podnet.py] => Task 0, Epoch 143/200 (LR 0.01874) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 86.43
2024-08-30 17:06:13,584 [podnet.py] => Task 0, Epoch 144/200 (LR 0.01813) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 87.17
2024-08-30 17:06:15,801 [podnet.py] => Task 0, Epoch 145/200 (LR 0.01753) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.57
2024-08-30 17:06:17,797 [podnet.py] => Task 0, Epoch 146/200 (LR 0.01693) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.30
2024-08-30 17:06:19,842 [podnet.py] => Task 0, Epoch 147/200 (LR 0.01635) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:06:22,395 [podnet.py] => Task 0, Epoch 148/200 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 17:06:24,817 [podnet.py] => Task 0, Epoch 149/200 (LR 0.01520) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:06:26,788 [podnet.py] => Task 0, Epoch 150/200 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:06:28,601 [podnet.py] => Task 0, Epoch 151/200 (LR 0.01409) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:06:30,481 [podnet.py] => Task 0, Epoch 152/200 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:06:32,199 [podnet.py] => Task 0, Epoch 153/200 (LR 0.01302) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:06:34,129 [podnet.py] => Task 0, Epoch 154/200 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-30 17:06:35,880 [podnet.py] => Task 0, Epoch 155/200 (LR 0.01198) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.60
2024-08-30 17:06:37,603 [podnet.py] => Task 0, Epoch 156/200 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 88.73
2024-08-30 17:06:39,369 [podnet.py] => Task 0, Epoch 157/200 (LR 0.01098) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:06:41,096 [podnet.py] => Task 0, Epoch 158/200 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-30 17:06:42,777 [podnet.py] => Task 0, Epoch 159/200 (LR 0.01002) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.83
2024-08-30 17:06:44,464 [podnet.py] => Task 0, Epoch 160/200 (LR 0.00955) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.47
2024-08-30 17:06:46,187 [podnet.py] => Task 0, Epoch 161/200 (LR 0.00909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.73
2024-08-30 17:06:47,811 [podnet.py] => Task 0, Epoch 162/200 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 17:06:49,483 [podnet.py] => Task 0, Epoch 163/200 (LR 0.00821) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:06:51,200 [podnet.py] => Task 0, Epoch 164/200 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:06:52,876 [podnet.py] => Task 0, Epoch 165/200 (LR 0.00737) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:06:54,570 [podnet.py] => Task 0, Epoch 166/200 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:06:56,657 [podnet.py] => Task 0, Epoch 167/200 (LR 0.00657) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:06:58,711 [podnet.py] => Task 0, Epoch 168/200 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:07:00,737 [podnet.py] => Task 0, Epoch 169/200 (LR 0.00581) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:07:02,625 [podnet.py] => Task 0, Epoch 170/200 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:07:04,333 [podnet.py] => Task 0, Epoch 171/200 (LR 0.00510) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:07:06,064 [podnet.py] => Task 0, Epoch 172/200 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:07:07,859 [podnet.py] => Task 0, Epoch 173/200 (LR 0.00443) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:07:09,582 [podnet.py] => Task 0, Epoch 174/200 (LR 0.00411) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.03
2024-08-30 17:07:11,250 [podnet.py] => Task 0, Epoch 175/200 (LR 0.00381) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.07
2024-08-30 17:07:13,057 [podnet.py] => Task 0, Epoch 176/200 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:07:14,762 [podnet.py] => Task 0, Epoch 177/200 (LR 0.00323) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:07:16,427 [podnet.py] => Task 0, Epoch 178/200 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-30 17:07:18,101 [podnet.py] => Task 0, Epoch 179/200 (LR 0.00270) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.93
2024-08-30 17:07:19,899 [podnet.py] => Task 0, Epoch 180/200 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:07:21,635 [podnet.py] => Task 0, Epoch 181/200 (LR 0.00221) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.47
2024-08-30 17:07:23,254 [podnet.py] => Task 0, Epoch 182/200 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.37
2024-08-30 17:07:24,947 [podnet.py] => Task 0, Epoch 183/200 (LR 0.00177) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:07:26,593 [podnet.py] => Task 0, Epoch 184/200 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.07
2024-08-30 17:07:28,264 [podnet.py] => Task 0, Epoch 185/200 (LR 0.00138) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:07:29,982 [podnet.py] => Task 0, Epoch 186/200 (LR 0.00120) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.43
2024-08-30 17:07:31,677 [podnet.py] => Task 0, Epoch 187/200 (LR 0.00104) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:07:33,534 [podnet.py] => Task 0, Epoch 188/200 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:07:35,248 [podnet.py] => Task 0, Epoch 189/200 (LR 0.00074) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:07:37,139 [podnet.py] => Task 0, Epoch 190/200 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:07:39,015 [podnet.py] => Task 0, Epoch 191/200 (LR 0.00050) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:07:40,632 [podnet.py] => Task 0, Epoch 192/200 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:07:42,313 [podnet.py] => Task 0, Epoch 193/200 (LR 0.00030) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:07:43,909 [podnet.py] => Task 0, Epoch 194/200 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:07:45,540 [podnet.py] => Task 0, Epoch 195/200 (LR 0.00015) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:07:47,257 [podnet.py] => Task 0, Epoch 196/200 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:07:48,979 [podnet.py] => Task 0, Epoch 197/200 (LR 0.00006) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:07:50,771 [podnet.py] => Task 0, Epoch 198/200 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:07:52,556 [podnet.py] => Task 0, Epoch 199/200 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:07:54,395 [podnet.py] => Task 0, Epoch 200/200 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:07:54,741 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:07:54,742 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:08:00,781 [podnet.py] => Exemplar size: 500
2024-08-30 17:08:00,781 [trainer.py] => CNN: {'total': 89.63, '00-04': 89.63, 'old': 0, 'new': 89.63}
2024-08-30 17:08:00,781 [trainer.py] => NME: {'total': 89.6, '00-04': 89.6, 'old': 0, 'new': 89.6}
2024-08-30 17:08:00,781 [trainer.py] => CNN top1 curve: [89.63]
2024-08-30 17:08:00,781 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 17:08:00,781 [trainer.py] => NME top1 curve: [89.6]
2024-08-30 17:08:00,781 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 17:08:00,782 [trainer.py] => Average Accuracy (CNN): 89.63
2024-08-30 17:08:00,782 [trainer.py] => Average Accuracy (NME): 89.6
2024-08-30 17:08:00,782 [trainer.py] => All params: 3869505
2024-08-30 17:08:00,782 [trainer.py] => Trainable params: 3869505
2024-08-30 17:08:00,783 [podnet.py] => Learning on 5-7
2024-08-30 17:08:00,799 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 17:08:02,358 [podnet.py] => Task 1, Epoch 1/200 (LR 0.09999) => LSC_loss 1.04, Spatial_loss 2.51, Flat_loss 0.62, Train_acc 73.51, Test_acc 34.10
2024-08-30 17:08:03,945 [podnet.py] => Task 1, Epoch 2/200 (LR 0.09998) => LSC_loss 0.45, Spatial_loss 1.90, Flat_loss 0.38, Train_acc 89.29, Test_acc 49.24
2024-08-30 17:08:05,390 [podnet.py] => Task 1, Epoch 3/200 (LR 0.09994) => LSC_loss 0.33, Spatial_loss 1.72, Flat_loss 0.32, Train_acc 92.47, Test_acc 47.02
2024-08-30 17:08:06,845 [podnet.py] => Task 1, Epoch 4/200 (LR 0.09990) => LSC_loss 0.24, Spatial_loss 1.40, Flat_loss 0.27, Train_acc 94.84, Test_acc 43.60
2024-08-30 17:08:08,300 [podnet.py] => Task 1, Epoch 5/200 (LR 0.09985) => LSC_loss 0.22, Spatial_loss 1.34, Flat_loss 0.25, Train_acc 94.93, Test_acc 55.95
2024-08-30 17:08:09,761 [podnet.py] => Task 1, Epoch 6/200 (LR 0.09978) => LSC_loss 0.21, Spatial_loss 1.28, Flat_loss 0.24, Train_acc 95.04, Test_acc 54.55
2024-08-30 17:08:11,287 [podnet.py] => Task 1, Epoch 7/200 (LR 0.09970) => LSC_loss 0.16, Spatial_loss 1.14, Flat_loss 0.22, Train_acc 96.82, Test_acc 56.26
2024-08-30 17:08:13,336 [podnet.py] => Task 1, Epoch 8/200 (LR 0.09961) => LSC_loss 0.16, Spatial_loss 1.12, Flat_loss 0.22, Train_acc 96.51, Test_acc 58.21
2024-08-30 17:08:15,154 [podnet.py] => Task 1, Epoch 9/200 (LR 0.09950) => LSC_loss 0.17, Spatial_loss 1.19, Flat_loss 0.23, Train_acc 96.36, Test_acc 55.62
2024-08-30 17:08:17,081 [podnet.py] => Task 1, Epoch 10/200 (LR 0.09938) => LSC_loss 0.16, Spatial_loss 1.22, Flat_loss 0.23, Train_acc 96.58, Test_acc 62.50
2024-08-30 17:08:18,859 [podnet.py] => Task 1, Epoch 11/200 (LR 0.09926) => LSC_loss 0.13, Spatial_loss 1.07, Flat_loss 0.21, Train_acc 97.58, Test_acc 65.50
2024-08-30 17:08:20,449 [podnet.py] => Task 1, Epoch 12/200 (LR 0.09911) => LSC_loss 0.11, Spatial_loss 0.95, Flat_loss 0.19, Train_acc 98.31, Test_acc 63.74
2024-08-30 17:08:22,140 [podnet.py] => Task 1, Epoch 13/200 (LR 0.09896) => LSC_loss 0.10, Spatial_loss 0.95, Flat_loss 0.19, Train_acc 98.64, Test_acc 62.74
2024-08-30 17:08:23,574 [podnet.py] => Task 1, Epoch 14/200 (LR 0.09880) => LSC_loss 0.10, Spatial_loss 0.92, Flat_loss 0.18, Train_acc 98.96, Test_acc 59.12
2024-08-30 17:08:24,963 [podnet.py] => Task 1, Epoch 15/200 (LR 0.09862) => LSC_loss 0.09, Spatial_loss 0.87, Flat_loss 0.18, Train_acc 99.27, Test_acc 61.45
2024-08-30 17:08:26,524 [podnet.py] => Task 1, Epoch 16/200 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 0.88, Flat_loss 0.18, Train_acc 99.07, Test_acc 60.57
2024-08-30 17:08:28,135 [podnet.py] => Task 1, Epoch 17/200 (LR 0.09823) => LSC_loss 0.08, Spatial_loss 0.88, Flat_loss 0.18, Train_acc 99.29, Test_acc 66.40
2024-08-30 17:08:29,820 [podnet.py] => Task 1, Epoch 18/200 (LR 0.09801) => LSC_loss 0.08, Spatial_loss 0.85, Flat_loss 0.17, Train_acc 99.49, Test_acc 59.55
2024-08-30 17:08:31,834 [podnet.py] => Task 1, Epoch 19/200 (LR 0.09779) => LSC_loss 0.08, Spatial_loss 0.86, Flat_loss 0.18, Train_acc 99.29, Test_acc 64.21
2024-08-30 17:08:33,488 [podnet.py] => Task 1, Epoch 20/200 (LR 0.09755) => LSC_loss 0.07, Spatial_loss 0.84, Flat_loss 0.17, Train_acc 99.58, Test_acc 61.38
2024-08-30 17:08:34,987 [podnet.py] => Task 1, Epoch 21/200 (LR 0.09730) => LSC_loss 0.08, Spatial_loss 0.84, Flat_loss 0.17, Train_acc 99.16, Test_acc 60.83
2024-08-30 17:08:36,481 [podnet.py] => Task 1, Epoch 22/200 (LR 0.09704) => LSC_loss 0.07, Spatial_loss 0.82, Flat_loss 0.17, Train_acc 99.58, Test_acc 65.17
2024-08-30 17:08:37,880 [podnet.py] => Task 1, Epoch 23/200 (LR 0.09677) => LSC_loss 0.07, Spatial_loss 0.81, Flat_loss 0.17, Train_acc 99.49, Test_acc 60.14
2024-08-30 17:08:39,397 [podnet.py] => Task 1, Epoch 24/200 (LR 0.09649) => LSC_loss 0.09, Spatial_loss 0.94, Flat_loss 0.18, Train_acc 99.00, Test_acc 59.33
2024-08-30 17:08:40,910 [podnet.py] => Task 1, Epoch 25/200 (LR 0.09619) => LSC_loss 0.08, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 98.93, Test_acc 66.12
2024-08-30 17:08:42,414 [podnet.py] => Task 1, Epoch 26/200 (LR 0.09589) => LSC_loss 0.07, Spatial_loss 0.83, Flat_loss 0.17, Train_acc 99.58, Test_acc 62.24
2024-08-30 17:08:43,947 [podnet.py] => Task 1, Epoch 27/200 (LR 0.09557) => LSC_loss 0.06, Spatial_loss 0.78, Flat_loss 0.16, Train_acc 99.76, Test_acc 61.38
2024-08-30 17:08:45,390 [podnet.py] => Task 1, Epoch 28/200 (LR 0.09524) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.17, Train_acc 99.56, Test_acc 63.86
2024-08-30 17:08:46,860 [podnet.py] => Task 1, Epoch 29/200 (LR 0.09490) => LSC_loss 0.06, Spatial_loss 0.78, Flat_loss 0.16, Train_acc 99.80, Test_acc 62.81
2024-08-30 17:08:48,304 [podnet.py] => Task 1, Epoch 30/200 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 99.89, Test_acc 64.50
2024-08-30 17:08:49,792 [podnet.py] => Task 1, Epoch 31/200 (LR 0.09419) => LSC_loss 0.06, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.69, Test_acc 64.62
2024-08-30 17:08:51,446 [podnet.py] => Task 1, Epoch 32/200 (LR 0.09382) => LSC_loss 0.08, Spatial_loss 0.85, Flat_loss 0.18, Train_acc 98.98, Test_acc 68.38
2024-08-30 17:08:53,123 [podnet.py] => Task 1, Epoch 33/200 (LR 0.09343) => LSC_loss 0.06, Spatial_loss 0.75, Flat_loss 0.16, Train_acc 99.53, Test_acc 65.21
2024-08-30 17:08:54,578 [podnet.py] => Task 1, Epoch 34/200 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 99.62, Test_acc 62.48
2024-08-30 17:08:56,078 [podnet.py] => Task 1, Epoch 35/200 (LR 0.09263) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.15, Train_acc 99.80, Test_acc 64.48
2024-08-30 17:08:57,603 [podnet.py] => Task 1, Epoch 36/200 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.82, Test_acc 67.48
2024-08-30 17:08:59,043 [podnet.py] => Task 1, Epoch 37/200 (LR 0.09179) => LSC_loss 0.05, Spatial_loss 0.73, Flat_loss 0.15, Train_acc 99.71, Test_acc 61.00
2024-08-30 17:09:00,484 [podnet.py] => Task 1, Epoch 38/200 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.93, Test_acc 64.07
2024-08-30 17:09:01,897 [podnet.py] => Task 1, Epoch 39/200 (LR 0.09091) => LSC_loss 0.05, Spatial_loss 0.73, Flat_loss 0.15, Train_acc 99.84, Test_acc 60.76
2024-08-30 17:09:03,415 [podnet.py] => Task 1, Epoch 40/200 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.71, Flat_loss 0.15, Train_acc 99.67, Test_acc 62.86
2024-08-30 17:09:04,985 [podnet.py] => Task 1, Epoch 41/200 (LR 0.08998) => LSC_loss 0.05, Spatial_loss 0.70, Flat_loss 0.15, Train_acc 99.80, Test_acc 65.43
2024-08-30 17:09:06,644 [podnet.py] => Task 1, Epoch 42/200 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.14, Train_acc 99.93, Test_acc 66.24
2024-08-30 17:09:07,982 [podnet.py] => Task 1, Epoch 43/200 (LR 0.08902) => LSC_loss 0.05, Spatial_loss 0.70, Flat_loss 0.15, Train_acc 99.69, Test_acc 63.81
2024-08-30 17:09:09,325 [podnet.py] => Task 1, Epoch 44/200 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.89, Test_acc 64.31
2024-08-30 17:09:10,738 [podnet.py] => Task 1, Epoch 45/200 (LR 0.08802) => LSC_loss 0.05, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.87, Test_acc 63.50
2024-08-30 17:09:12,168 [podnet.py] => Task 1, Epoch 46/200 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.84, Test_acc 64.57
2024-08-30 17:09:13,754 [podnet.py] => Task 1, Epoch 47/200 (LR 0.08698) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.87, Test_acc 63.48
2024-08-30 17:09:15,205 [podnet.py] => Task 1, Epoch 48/200 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.84, Test_acc 60.45
2024-08-30 17:09:16,718 [podnet.py] => Task 1, Epoch 49/200 (LR 0.08591) => LSC_loss 0.07, Spatial_loss 0.83, Flat_loss 0.16, Train_acc 99.31, Test_acc 63.17
2024-08-30 17:09:18,356 [podnet.py] => Task 1, Epoch 50/200 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.72, Flat_loss 0.15, Train_acc 99.89, Test_acc 65.05
2024-08-30 17:09:20,148 [podnet.py] => Task 1, Epoch 51/200 (LR 0.08480) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.76, Test_acc 64.60
2024-08-30 17:09:21,977 [podnet.py] => Task 1, Epoch 52/200 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.82, Test_acc 66.62
2024-08-30 17:09:23,812 [podnet.py] => Task 1, Epoch 53/200 (LR 0.08365) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.14, Train_acc 99.82, Test_acc 63.81
2024-08-30 17:09:25,247 [podnet.py] => Task 1, Epoch 54/200 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.89, Test_acc 64.76
2024-08-30 17:09:26,760 [podnet.py] => Task 1, Epoch 55/200 (LR 0.08247) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.78, Test_acc 63.38
2024-08-30 17:09:28,175 [podnet.py] => Task 1, Epoch 56/200 (LR 0.08187) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.84, Test_acc 63.17
2024-08-30 17:09:29,613 [podnet.py] => Task 1, Epoch 57/200 (LR 0.08126) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.91, Test_acc 65.71
2024-08-30 17:09:30,994 [podnet.py] => Task 1, Epoch 58/200 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.57
2024-08-30 17:09:32,483 [podnet.py] => Task 1, Epoch 59/200 (LR 0.08002) => LSC_loss 0.05, Spatial_loss 0.68, Flat_loss 0.14, Train_acc 99.78, Test_acc 63.74
2024-08-30 17:09:33,987 [podnet.py] => Task 1, Epoch 60/200 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.82, Flat_loss 0.16, Train_acc 99.31, Test_acc 62.33
2024-08-30 17:09:35,598 [podnet.py] => Task 1, Epoch 61/200 (LR 0.07875) => LSC_loss 0.05, Spatial_loss 0.69, Flat_loss 0.14, Train_acc 99.78, Test_acc 64.48
2024-08-30 17:09:36,945 [podnet.py] => Task 1, Epoch 62/200 (LR 0.07810) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.93, Test_acc 62.90
2024-08-30 17:09:38,382 [podnet.py] => Task 1, Epoch 63/200 (LR 0.07745) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.96, Test_acc 63.52
2024-08-30 17:09:39,761 [podnet.py] => Task 1, Epoch 64/200 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.19
2024-08-30 17:09:41,270 [podnet.py] => Task 1, Epoch 65/200 (LR 0.07612) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.89, Test_acc 65.95
2024-08-30 17:09:42,799 [podnet.py] => Task 1, Epoch 66/200 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.14, Train_acc 99.76, Test_acc 65.81
2024-08-30 17:09:44,396 [podnet.py] => Task 1, Epoch 67/200 (LR 0.07477) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.13, Train_acc 99.89, Test_acc 67.60
2024-08-30 17:09:46,454 [podnet.py] => Task 1, Epoch 68/200 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.13, Train_acc 99.91, Test_acc 62.50
2024-08-30 17:09:48,318 [podnet.py] => Task 1, Epoch 69/200 (LR 0.07340) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.14, Train_acc 99.78, Test_acc 62.07
2024-08-30 17:09:49,925 [podnet.py] => Task 1, Epoch 70/200 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.14, Train_acc 99.84, Test_acc 63.43
2024-08-30 17:09:51,341 [podnet.py] => Task 1, Epoch 71/200 (LR 0.07200) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.80, Test_acc 63.29
2024-08-30 17:09:52,950 [podnet.py] => Task 1, Epoch 72/200 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.13, Train_acc 99.69, Test_acc 65.02
2024-08-30 17:09:54,534 [podnet.py] => Task 1, Epoch 73/200 (LR 0.07058) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.93, Test_acc 66.45
2024-08-30 17:09:56,047 [podnet.py] => Task 1, Epoch 74/200 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.63, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.83
2024-08-30 17:09:57,459 [podnet.py] => Task 1, Epoch 75/200 (LR 0.06913) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.87, Test_acc 67.31
2024-08-30 17:09:59,126 [podnet.py] => Task 1, Epoch 76/200 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 100.00, Test_acc 63.74
2024-08-30 17:10:00,645 [podnet.py] => Task 1, Epoch 77/200 (LR 0.06767) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 99.91, Test_acc 62.81
2024-08-30 17:10:02,058 [podnet.py] => Task 1, Epoch 78/200 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 99.93, Test_acc 61.31
2024-08-30 17:10:03,490 [podnet.py] => Task 1, Epoch 79/200 (LR 0.06620) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.71
2024-08-30 17:10:04,918 [podnet.py] => Task 1, Epoch 80/200 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.81
2024-08-30 17:10:06,351 [podnet.py] => Task 1, Epoch 81/200 (LR 0.06470) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.91, Test_acc 64.67
2024-08-30 17:10:07,853 [podnet.py] => Task 1, Epoch 82/200 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.96, Test_acc 67.38
2024-08-30 17:10:09,431 [podnet.py] => Task 1, Epoch 83/200 (LR 0.06319) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.10
2024-08-30 17:10:10,923 [podnet.py] => Task 1, Epoch 84/200 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.93, Test_acc 65.69
2024-08-30 17:10:12,562 [podnet.py] => Task 1, Epoch 85/200 (LR 0.06167) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.96, Test_acc 62.74
2024-08-30 17:10:14,145 [podnet.py] => Task 1, Epoch 86/200 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.91, Test_acc 64.07
2024-08-30 17:10:15,772 [podnet.py] => Task 1, Epoch 87/200 (LR 0.06014) => LSC_loss 0.04, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 99.96, Test_acc 65.38
2024-08-30 17:10:17,482 [podnet.py] => Task 1, Epoch 88/200 (LR 0.05937) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 99.91, Test_acc 65.29
2024-08-30 17:10:18,888 [podnet.py] => Task 1, Epoch 89/200 (LR 0.05860) => LSC_loss 0.04, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.98, Test_acc 61.31
2024-08-30 17:10:20,297 [podnet.py] => Task 1, Epoch 90/200 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.93, Test_acc 65.74
2024-08-30 17:10:21,733 [podnet.py] => Task 1, Epoch 91/200 (LR 0.05705) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.93, Test_acc 67.26
2024-08-30 17:10:23,379 [podnet.py] => Task 1, Epoch 92/200 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.93, Test_acc 66.17
2024-08-30 17:10:25,087 [podnet.py] => Task 1, Epoch 93/200 (LR 0.05549) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.93, Test_acc 66.64
2024-08-30 17:10:26,847 [podnet.py] => Task 1, Epoch 94/200 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.62
2024-08-30 17:10:29,028 [podnet.py] => Task 1, Epoch 95/200 (LR 0.05392) => LSC_loss 0.04, Spatial_loss 0.56, Flat_loss 0.12, Train_acc 99.98, Test_acc 67.36
2024-08-30 17:10:31,066 [podnet.py] => Task 1, Epoch 96/200 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.69
2024-08-30 17:10:32,649 [podnet.py] => Task 1, Epoch 97/200 (LR 0.05236) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.12, Train_acc 99.91, Test_acc 66.26
2024-08-30 17:10:34,735 [podnet.py] => Task 1, Epoch 98/200 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.31
2024-08-30 17:10:36,526 [podnet.py] => Task 1, Epoch 99/200 (LR 0.05079) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.36
2024-08-30 17:10:37,934 [podnet.py] => Task 1, Epoch 100/200 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.02
2024-08-30 17:10:39,384 [podnet.py] => Task 1, Epoch 101/200 (LR 0.04921) => LSC_loss 0.04, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.96, Test_acc 68.40
2024-08-30 17:10:40,835 [podnet.py] => Task 1, Epoch 102/200 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.13, Train_acc 99.71, Test_acc 63.50
2024-08-30 17:10:42,294 [podnet.py] => Task 1, Epoch 103/200 (LR 0.04764) => LSC_loss 0.04, Spatial_loss 0.57, Flat_loss 0.12, Train_acc 99.96, Test_acc 67.81
2024-08-30 17:10:43,761 [podnet.py] => Task 1, Epoch 104/200 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.53, Flat_loss 0.12, Train_acc 99.93, Test_acc 67.69
2024-08-30 17:10:45,335 [podnet.py] => Task 1, Epoch 105/200 (LR 0.04608) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 99.84, Test_acc 64.62
2024-08-30 17:10:46,761 [podnet.py] => Task 1, Epoch 106/200 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 99.96, Test_acc 65.21
2024-08-30 17:10:48,320 [podnet.py] => Task 1, Epoch 107/200 (LR 0.04451) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 99.84, Test_acc 67.40
2024-08-30 17:10:49,889 [podnet.py] => Task 1, Epoch 108/200 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.51, Flat_loss 0.12, Train_acc 99.96, Test_acc 66.76
2024-08-30 17:10:51,418 [podnet.py] => Task 1, Epoch 109/200 (LR 0.04295) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.29
2024-08-30 17:10:52,987 [podnet.py] => Task 1, Epoch 110/200 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 99.96, Test_acc 66.81
2024-08-30 17:10:54,646 [podnet.py] => Task 1, Epoch 111/200 (LR 0.04140) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 99.98, Test_acc 65.43
2024-08-30 17:10:56,234 [podnet.py] => Task 1, Epoch 112/200 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.26
2024-08-30 17:10:57,683 [podnet.py] => Task 1, Epoch 113/200 (LR 0.03986) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 99.98, Test_acc 67.43
2024-08-30 17:10:59,155 [podnet.py] => Task 1, Epoch 114/200 (LR 0.03909) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.29
2024-08-30 17:11:00,574 [podnet.py] => Task 1, Epoch 115/200 (LR 0.03833) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.88
2024-08-30 17:11:02,040 [podnet.py] => Task 1, Epoch 116/200 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.95
2024-08-30 17:11:03,585 [podnet.py] => Task 1, Epoch 117/200 (LR 0.03681) => LSC_loss 0.04, Spatial_loss 0.52, Flat_loss 0.11, Train_acc 100.00, Test_acc 64.86
2024-08-30 17:11:05,114 [podnet.py] => Task 1, Epoch 118/200 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.14
2024-08-30 17:11:06,886 [podnet.py] => Task 1, Epoch 119/200 (LR 0.03530) => LSC_loss 0.04, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.81
2024-08-30 17:11:08,638 [podnet.py] => Task 1, Epoch 120/200 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 99.98, Test_acc 66.74
2024-08-30 17:11:10,454 [podnet.py] => Task 1, Epoch 121/200 (LR 0.03380) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.29
2024-08-30 17:11:12,426 [podnet.py] => Task 1, Epoch 122/200 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 99.98, Test_acc 64.88
2024-08-30 17:11:14,389 [podnet.py] => Task 1, Epoch 123/200 (LR 0.03233) => LSC_loss 0.04, Spatial_loss 0.47, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.33
2024-08-30 17:11:16,164 [podnet.py] => Task 1, Epoch 124/200 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 99.93, Test_acc 68.79
2024-08-30 17:11:17,883 [podnet.py] => Task 1, Epoch 125/200 (LR 0.03087) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.67
2024-08-30 17:11:19,575 [podnet.py] => Task 1, Epoch 126/200 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.90
2024-08-30 17:11:21,041 [podnet.py] => Task 1, Epoch 127/200 (LR 0.02942) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.57
2024-08-30 17:11:22,451 [podnet.py] => Task 1, Epoch 128/200 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.33
2024-08-30 17:11:23,893 [podnet.py] => Task 1, Epoch 129/200 (LR 0.02800) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.57
2024-08-30 17:11:25,475 [podnet.py] => Task 1, Epoch 130/200 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.74
2024-08-30 17:11:27,034 [podnet.py] => Task 1, Epoch 131/200 (LR 0.02660) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.31
2024-08-30 17:11:28,661 [podnet.py] => Task 1, Epoch 132/200 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.29
2024-08-30 17:11:30,353 [podnet.py] => Task 1, Epoch 133/200 (LR 0.02523) => LSC_loss 0.04, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 99.98, Test_acc 67.71
2024-08-30 17:11:31,874 [podnet.py] => Task 1, Epoch 134/200 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.64
2024-08-30 17:11:33,431 [podnet.py] => Task 1, Epoch 135/200 (LR 0.02388) => LSC_loss 0.04, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.19
2024-08-30 17:11:34,832 [podnet.py] => Task 1, Epoch 136/200 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.17
2024-08-30 17:11:36,296 [podnet.py] => Task 1, Epoch 137/200 (LR 0.02255) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.17
2024-08-30 17:11:37,897 [podnet.py] => Task 1, Epoch 138/200 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.40
2024-08-30 17:11:39,537 [podnet.py] => Task 1, Epoch 139/200 (LR 0.02125) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.83
2024-08-30 17:11:41,125 [podnet.py] => Task 1, Epoch 140/200 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.74
2024-08-30 17:11:42,537 [podnet.py] => Task 1, Epoch 141/200 (LR 0.01998) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 99.98, Test_acc 68.60
2024-08-30 17:11:43,973 [podnet.py] => Task 1, Epoch 142/200 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.81
2024-08-30 17:11:45,464 [podnet.py] => Task 1, Epoch 143/200 (LR 0.01874) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.38
2024-08-30 17:11:46,896 [podnet.py] => Task 1, Epoch 144/200 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 99.98, Test_acc 67.57
2024-08-30 17:11:48,449 [podnet.py] => Task 1, Epoch 145/200 (LR 0.01753) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.86
2024-08-30 17:11:50,029 [podnet.py] => Task 1, Epoch 146/200 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.19
2024-08-30 17:11:51,373 [podnet.py] => Task 1, Epoch 147/200 (LR 0.01635) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.36
2024-08-30 17:11:52,874 [podnet.py] => Task 1, Epoch 148/200 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.05
2024-08-30 17:11:54,288 [podnet.py] => Task 1, Epoch 149/200 (LR 0.01520) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.62
2024-08-30 17:11:55,856 [podnet.py] => Task 1, Epoch 150/200 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.29
2024-08-30 17:11:57,206 [podnet.py] => Task 1, Epoch 151/200 (LR 0.01409) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.76
2024-08-30 17:11:58,918 [podnet.py] => Task 1, Epoch 152/200 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.36
2024-08-30 17:12:00,681 [podnet.py] => Task 1, Epoch 153/200 (LR 0.01302) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.93
2024-08-30 17:12:02,411 [podnet.py] => Task 1, Epoch 154/200 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.71
2024-08-30 17:12:03,830 [podnet.py] => Task 1, Epoch 155/200 (LR 0.01198) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.07
2024-08-30 17:12:05,286 [podnet.py] => Task 1, Epoch 156/200 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.12
2024-08-30 17:12:06,709 [podnet.py] => Task 1, Epoch 157/200 (LR 0.01098) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.26
2024-08-30 17:12:08,082 [podnet.py] => Task 1, Epoch 158/200 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 99.98, Test_acc 68.33
2024-08-30 17:12:09,627 [podnet.py] => Task 1, Epoch 159/200 (LR 0.01002) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.52
2024-08-30 17:12:11,144 [podnet.py] => Task 1, Epoch 160/200 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.17
2024-08-30 17:12:12,652 [podnet.py] => Task 1, Epoch 161/200 (LR 0.00909) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.45
2024-08-30 17:12:14,039 [podnet.py] => Task 1, Epoch 162/200 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.52
2024-08-30 17:12:15,509 [podnet.py] => Task 1, Epoch 163/200 (LR 0.00821) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.21
2024-08-30 17:12:17,008 [podnet.py] => Task 1, Epoch 164/200 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.98
2024-08-30 17:12:18,492 [podnet.py] => Task 1, Epoch 165/200 (LR 0.00737) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.69
2024-08-30 17:12:19,961 [podnet.py] => Task 1, Epoch 166/200 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.90
2024-08-30 17:12:21,469 [podnet.py] => Task 1, Epoch 167/200 (LR 0.00657) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.24
2024-08-30 17:12:22,876 [podnet.py] => Task 1, Epoch 168/200 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.31
2024-08-30 17:12:24,282 [podnet.py] => Task 1, Epoch 169/200 (LR 0.00581) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.93
2024-08-30 17:12:25,698 [podnet.py] => Task 1, Epoch 170/200 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 70.57
2024-08-30 17:12:27,261 [podnet.py] => Task 1, Epoch 171/200 (LR 0.00510) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.24
2024-08-30 17:12:29,018 [podnet.py] => Task 1, Epoch 172/200 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.86
2024-08-30 17:12:30,731 [podnet.py] => Task 1, Epoch 173/200 (LR 0.00443) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.43
2024-08-30 17:12:32,337 [podnet.py] => Task 1, Epoch 174/200 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.31
2024-08-30 17:12:33,739 [podnet.py] => Task 1, Epoch 175/200 (LR 0.00381) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.69
2024-08-30 17:12:35,157 [podnet.py] => Task 1, Epoch 176/200 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.31
2024-08-30 17:12:36,565 [podnet.py] => Task 1, Epoch 177/200 (LR 0.00323) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.26
2024-08-30 17:12:37,997 [podnet.py] => Task 1, Epoch 178/200 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.86
2024-08-30 17:12:39,322 [podnet.py] => Task 1, Epoch 179/200 (LR 0.00270) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.19
2024-08-30 17:12:40,702 [podnet.py] => Task 1, Epoch 180/200 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.48
2024-08-30 17:12:42,444 [podnet.py] => Task 1, Epoch 181/200 (LR 0.00221) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.81
2024-08-30 17:12:43,899 [podnet.py] => Task 1, Epoch 182/200 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.64
2024-08-30 17:12:45,459 [podnet.py] => Task 1, Epoch 183/200 (LR 0.00177) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.67
2024-08-30 17:12:46,872 [podnet.py] => Task 1, Epoch 184/200 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.60
2024-08-30 17:12:48,345 [podnet.py] => Task 1, Epoch 185/200 (LR 0.00138) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.12
2024-08-30 17:12:49,859 [podnet.py] => Task 1, Epoch 186/200 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.93
2024-08-30 17:12:51,391 [podnet.py] => Task 1, Epoch 187/200 (LR 0.00104) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.93
2024-08-30 17:12:52,822 [podnet.py] => Task 1, Epoch 188/200 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.62
2024-08-30 17:12:54,285 [podnet.py] => Task 1, Epoch 189/200 (LR 0.00074) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.62
2024-08-30 17:12:55,821 [podnet.py] => Task 1, Epoch 190/200 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.36
2024-08-30 17:12:57,262 [podnet.py] => Task 1, Epoch 191/200 (LR 0.00050) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.81
2024-08-30 17:12:58,667 [podnet.py] => Task 1, Epoch 192/200 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 70.00
2024-08-30 17:13:00,006 [podnet.py] => Task 1, Epoch 193/200 (LR 0.00030) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.55
2024-08-30 17:13:01,385 [podnet.py] => Task 1, Epoch 194/200 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.79
2024-08-30 17:13:02,748 [podnet.py] => Task 1, Epoch 195/200 (LR 0.00015) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.81
2024-08-30 17:13:04,200 [podnet.py] => Task 1, Epoch 196/200 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.57
2024-08-30 17:13:05,548 [podnet.py] => Task 1, Epoch 197/200 (LR 0.00006) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.50
2024-08-30 17:13:06,904 [podnet.py] => Task 1, Epoch 198/200 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.71
2024-08-30 17:13:08,316 [podnet.py] => Task 1, Epoch 199/200 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.83
2024-08-30 17:13:09,710 [podnet.py] => Task 1, Epoch 200/200 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 69.95
2024-08-30 17:13:10,073 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:13:10,073 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:13:11,231 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:13:13,444 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:13:14,741 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:13:17,520 [podnet.py] => Exemplar size: 497
2024-08-30 17:13:17,521 [trainer.py] => CNN: {'total': 69.95, '00-04': 60.17, '05-06': 94.42, 'old': 60.17, 'new': 94.42}
2024-08-30 17:13:17,521 [trainer.py] => NME: {'total': 74.81, '00-04': 77.07, '05-06': 69.17, 'old': 77.07, 'new': 69.17}
2024-08-30 17:13:17,521 [trainer.py] => CNN top1 curve: [89.63, 69.95]
2024-08-30 17:13:17,521 [trainer.py] => CNN top5 curve: [100.0, 98.6]
2024-08-30 17:13:17,521 [trainer.py] => NME top1 curve: [89.6, 74.81]
2024-08-30 17:13:17,521 [trainer.py] => NME top5 curve: [100.0, 98.57]

2024-08-30 17:13:17,521 [trainer.py] => Average Accuracy (CNN): 79.78999999999999
2024-08-30 17:13:17,521 [trainer.py] => Average Accuracy (NME): 82.205
2024-08-30 17:13:17,521 [trainer.py] => All params: 3879745
2024-08-30 17:13:17,522 [trainer.py] => Trainable params: 3879745
2024-08-30 17:13:17,522 [podnet.py] => Learning on 7-9
2024-08-30 17:13:17,540 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 17:13:19,225 [podnet.py] => Task 2, Epoch 1/200 (LR 0.09999) => LSC_loss 1.14, Spatial_loss 1.45, Flat_loss 0.45, Train_acc 79.85, Test_acc 37.28
2024-08-30 17:13:20,819 [podnet.py] => Task 2, Epoch 2/200 (LR 0.09998) => LSC_loss 0.34, Spatial_loss 1.37, Flat_loss 0.29, Train_acc 92.28, Test_acc 42.28
2024-08-30 17:13:22,421 [podnet.py] => Task 2, Epoch 3/200 (LR 0.09994) => LSC_loss 0.24, Spatial_loss 1.17, Flat_loss 0.24, Train_acc 95.15, Test_acc 49.26
2024-08-30 17:13:24,017 [podnet.py] => Task 2, Epoch 4/200 (LR 0.09990) => LSC_loss 0.19, Spatial_loss 1.08, Flat_loss 0.22, Train_acc 96.62, Test_acc 54.89
2024-08-30 17:13:25,640 [podnet.py] => Task 2, Epoch 5/200 (LR 0.09985) => LSC_loss 0.15, Spatial_loss 0.97, Flat_loss 0.20, Train_acc 98.27, Test_acc 52.87
2024-08-30 17:13:27,349 [podnet.py] => Task 2, Epoch 6/200 (LR 0.09978) => LSC_loss 0.12, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 99.04, Test_acc 53.56
2024-08-30 17:13:28,888 [podnet.py] => Task 2, Epoch 7/200 (LR 0.09970) => LSC_loss 0.11, Spatial_loss 0.88, Flat_loss 0.18, Train_acc 99.18, Test_acc 57.07
2024-08-30 17:13:30,375 [podnet.py] => Task 2, Epoch 8/200 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.84, Flat_loss 0.17, Train_acc 99.42, Test_acc 59.06
2024-08-30 17:13:31,818 [podnet.py] => Task 2, Epoch 9/200 (LR 0.09950) => LSC_loss 0.10, Spatial_loss 0.83, Flat_loss 0.17, Train_acc 99.51, Test_acc 56.46
2024-08-30 17:13:33,318 [podnet.py] => Task 2, Epoch 10/200 (LR 0.09938) => LSC_loss 0.10, Spatial_loss 0.83, Flat_loss 0.17, Train_acc 99.44, Test_acc 56.94
2024-08-30 17:13:34,777 [podnet.py] => Task 2, Epoch 11/200 (LR 0.09926) => LSC_loss 0.09, Spatial_loss 0.82, Flat_loss 0.16, Train_acc 99.51, Test_acc 49.63
2024-08-30 17:13:36,204 [podnet.py] => Task 2, Epoch 12/200 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.80, Flat_loss 0.16, Train_acc 99.69, Test_acc 55.94
2024-08-30 17:13:37,691 [podnet.py] => Task 2, Epoch 13/200 (LR 0.09896) => LSC_loss 0.08, Spatial_loss 0.81, Flat_loss 0.16, Train_acc 99.80, Test_acc 59.28
2024-08-30 17:13:39,240 [podnet.py] => Task 2, Epoch 14/200 (LR 0.09880) => LSC_loss 0.08, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 99.76, Test_acc 58.56
2024-08-30 17:13:40,725 [podnet.py] => Task 2, Epoch 15/200 (LR 0.09862) => LSC_loss 0.07, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 99.82, Test_acc 57.43
2024-08-30 17:13:42,174 [podnet.py] => Task 2, Epoch 16/200 (LR 0.09843) => LSC_loss 0.07, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 99.82, Test_acc 56.57
2024-08-30 17:13:43,706 [podnet.py] => Task 2, Epoch 17/200 (LR 0.09823) => LSC_loss 0.07, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 99.82, Test_acc 57.44
2024-08-30 17:13:45,223 [podnet.py] => Task 2, Epoch 18/200 (LR 0.09801) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.84, Test_acc 54.04
2024-08-30 17:13:46,666 [podnet.py] => Task 2, Epoch 19/200 (LR 0.09779) => LSC_loss 0.07, Spatial_loss 0.75, Flat_loss 0.14, Train_acc 99.98, Test_acc 56.44
2024-08-30 17:13:48,112 [podnet.py] => Task 2, Epoch 20/200 (LR 0.09755) => LSC_loss 0.07, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.89, Test_acc 56.26
2024-08-30 17:13:49,576 [podnet.py] => Task 2, Epoch 21/200 (LR 0.09730) => LSC_loss 0.07, Spatial_loss 0.71, Flat_loss 0.14, Train_acc 99.87, Test_acc 57.52
2024-08-30 17:13:50,987 [podnet.py] => Task 2, Epoch 22/200 (LR 0.09704) => LSC_loss 0.07, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.82, Test_acc 55.50
2024-08-30 17:13:52,473 [podnet.py] => Task 2, Epoch 23/200 (LR 0.09677) => LSC_loss 0.07, Spatial_loss 0.71, Flat_loss 0.13, Train_acc 99.89, Test_acc 58.96
2024-08-30 17:13:54,052 [podnet.py] => Task 2, Epoch 24/200 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.87, Test_acc 59.33
2024-08-30 17:13:55,591 [podnet.py] => Task 2, Epoch 25/200 (LR 0.09619) => LSC_loss 0.07, Spatial_loss 0.71, Flat_loss 0.13, Train_acc 99.78, Test_acc 49.56
2024-08-30 17:13:57,152 [podnet.py] => Task 2, Epoch 26/200 (LR 0.09589) => LSC_loss 0.08, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 99.36, Test_acc 56.76
2024-08-30 17:13:58,579 [podnet.py] => Task 2, Epoch 27/200 (LR 0.09557) => LSC_loss 0.07, Spatial_loss 0.73, Flat_loss 0.14, Train_acc 99.76, Test_acc 57.41
2024-08-30 17:14:00,166 [podnet.py] => Task 2, Epoch 28/200 (LR 0.09524) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.02
2024-08-30 17:14:01,730 [podnet.py] => Task 2, Epoch 29/200 (LR 0.09490) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 99.96, Test_acc 53.69
2024-08-30 17:14:03,175 [podnet.py] => Task 2, Epoch 30/200 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.96, Test_acc 57.72
2024-08-30 17:14:04,717 [podnet.py] => Task 2, Epoch 31/200 (LR 0.09419) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.93, Test_acc 57.69
2024-08-30 17:14:06,350 [podnet.py] => Task 2, Epoch 32/200 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.68, Flat_loss 0.13, Train_acc 99.93, Test_acc 57.93
2024-08-30 17:14:07,984 [podnet.py] => Task 2, Epoch 33/200 (LR 0.09343) => LSC_loss 0.07, Spatial_loss 0.74, Flat_loss 0.13, Train_acc 99.47, Test_acc 55.37
2024-08-30 17:14:09,717 [podnet.py] => Task 2, Epoch 34/200 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.96, Test_acc 56.83
2024-08-30 17:14:11,180 [podnet.py] => Task 2, Epoch 35/200 (LR 0.09263) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.98, Test_acc 56.28
2024-08-30 17:14:12,832 [podnet.py] => Task 2, Epoch 36/200 (LR 0.09222) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.13, Train_acc 99.89, Test_acc 59.48
2024-08-30 17:14:14,591 [podnet.py] => Task 2, Epoch 37/200 (LR 0.09179) => LSC_loss 0.06, Spatial_loss 0.71, Flat_loss 0.13, Train_acc 99.76, Test_acc 57.11
2024-08-30 17:14:16,092 [podnet.py] => Task 2, Epoch 38/200 (LR 0.09135) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.96, Test_acc 56.11
2024-08-30 17:14:17,622 [podnet.py] => Task 2, Epoch 39/200 (LR 0.09091) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.98, Test_acc 58.06
2024-08-30 17:14:19,046 [podnet.py] => Task 2, Epoch 40/200 (LR 0.09045) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 100.00, Test_acc 54.87
2024-08-30 17:14:20,484 [podnet.py] => Task 2, Epoch 41/200 (LR 0.08998) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.91, Test_acc 61.15
2024-08-30 17:14:21,977 [podnet.py] => Task 2, Epoch 42/200 (LR 0.08951) => LSC_loss 0.06, Spatial_loss 0.67, Flat_loss 0.12, Train_acc 99.93, Test_acc 55.26
2024-08-30 17:14:23,449 [podnet.py] => Task 2, Epoch 43/200 (LR 0.08902) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.93, Test_acc 56.24
2024-08-30 17:14:25,055 [podnet.py] => Task 2, Epoch 44/200 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 99.98, Test_acc 56.44
2024-08-30 17:14:26,674 [podnet.py] => Task 2, Epoch 45/200 (LR 0.08802) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.12, Train_acc 99.93, Test_acc 55.91
2024-08-30 17:14:28,215 [podnet.py] => Task 2, Epoch 46/200 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.12, Train_acc 99.98, Test_acc 57.74
2024-08-30 17:14:29,807 [podnet.py] => Task 2, Epoch 47/200 (LR 0.08698) => LSC_loss 0.06, Spatial_loss 0.65, Flat_loss 0.12, Train_acc 99.87, Test_acc 58.91
2024-08-30 17:14:31,368 [podnet.py] => Task 2, Epoch 48/200 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.98, Test_acc 55.72
2024-08-30 17:14:32,966 [podnet.py] => Task 2, Epoch 49/200 (LR 0.08591) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.12, Train_acc 99.98, Test_acc 56.61
2024-08-30 17:14:34,713 [podnet.py] => Task 2, Epoch 50/200 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.65, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.20
2024-08-30 17:14:36,498 [podnet.py] => Task 2, Epoch 51/200 (LR 0.08480) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.12, Train_acc 99.96, Test_acc 54.52
2024-08-30 17:14:38,292 [podnet.py] => Task 2, Epoch 52/200 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.12, Train_acc 99.98, Test_acc 57.52
2024-08-30 17:14:39,832 [podnet.py] => Task 2, Epoch 53/200 (LR 0.08365) => LSC_loss 0.06, Spatial_loss 0.66, Flat_loss 0.12, Train_acc 99.93, Test_acc 57.76
2024-08-30 17:14:41,404 [podnet.py] => Task 2, Epoch 54/200 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 0.67, Flat_loss 0.12, Train_acc 99.96, Test_acc 58.65
2024-08-30 17:14:42,849 [podnet.py] => Task 2, Epoch 55/200 (LR 0.08247) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 99.91, Test_acc 54.11
2024-08-30 17:14:44,395 [podnet.py] => Task 2, Epoch 56/200 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.63, Flat_loss 0.12, Train_acc 99.96, Test_acc 52.54
2024-08-30 17:14:45,873 [podnet.py] => Task 2, Epoch 57/200 (LR 0.08126) => LSC_loss 0.05, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 100.00, Test_acc 61.98
2024-08-30 17:14:47,348 [podnet.py] => Task 2, Epoch 58/200 (LR 0.08065) => LSC_loss 0.05, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.96, Test_acc 57.98
2024-08-30 17:14:48,886 [podnet.py] => Task 2, Epoch 59/200 (LR 0.08002) => LSC_loss 0.05, Spatial_loss 0.63, Flat_loss 0.12, Train_acc 99.96, Test_acc 61.72
2024-08-30 17:14:50,367 [podnet.py] => Task 2, Epoch 60/200 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.62, Flat_loss 0.11, Train_acc 99.87, Test_acc 58.11
2024-08-30 17:14:51,904 [podnet.py] => Task 2, Epoch 61/200 (LR 0.07875) => LSC_loss 0.06, Spatial_loss 0.65, Flat_loss 0.12, Train_acc 99.87, Test_acc 53.13
2024-08-30 17:14:53,726 [podnet.py] => Task 2, Epoch 62/200 (LR 0.07810) => LSC_loss 0.09, Spatial_loss 0.67, Flat_loss 0.13, Train_acc 99.87, Test_acc 40.31
2024-08-30 17:14:55,585 [podnet.py] => Task 2, Epoch 63/200 (LR 0.07745) => LSC_loss 0.21, Spatial_loss 1.07, Flat_loss 0.21, Train_acc 95.20, Test_acc 52.07
2024-08-30 17:14:57,558 [podnet.py] => Task 2, Epoch 64/200 (LR 0.07679) => LSC_loss 0.12, Spatial_loss 0.91, Flat_loss 0.17, Train_acc 98.04, Test_acc 59.83
2024-08-30 17:14:59,236 [podnet.py] => Task 2, Epoch 65/200 (LR 0.07612) => LSC_loss 0.07, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.67, Test_acc 56.26
2024-08-30 17:15:00,846 [podnet.py] => Task 2, Epoch 66/200 (LR 0.07545) => LSC_loss 0.06, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.96, Test_acc 57.98
2024-08-30 17:15:02,392 [podnet.py] => Task 2, Epoch 67/200 (LR 0.07477) => LSC_loss 0.06, Spatial_loss 0.65, Flat_loss 0.13, Train_acc 99.93, Test_acc 62.30
2024-08-30 17:15:03,918 [podnet.py] => Task 2, Epoch 68/200 (LR 0.07409) => LSC_loss 0.08, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 99.27, Test_acc 58.80
2024-08-30 17:15:05,391 [podnet.py] => Task 2, Epoch 69/200 (LR 0.07340) => LSC_loss 0.06, Spatial_loss 0.70, Flat_loss 0.13, Train_acc 99.89, Test_acc 59.00
2024-08-30 17:15:06,878 [podnet.py] => Task 2, Epoch 70/200 (LR 0.07270) => LSC_loss 0.07, Spatial_loss 0.69, Flat_loss 0.13, Train_acc 99.64, Test_acc 58.78
2024-08-30 17:15:08,387 [podnet.py] => Task 2, Epoch 71/200 (LR 0.07200) => LSC_loss 0.06, Spatial_loss 0.64, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.87
2024-08-30 17:15:09,871 [podnet.py] => Task 2, Epoch 72/200 (LR 0.07129) => LSC_loss 0.06, Spatial_loss 0.61, Flat_loss 0.12, Train_acc 99.93, Test_acc 59.67
2024-08-30 17:15:11,391 [podnet.py] => Task 2, Epoch 73/200 (LR 0.07058) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 100.00, Test_acc 61.94
2024-08-30 17:15:12,914 [podnet.py] => Task 2, Epoch 74/200 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.12, Train_acc 100.00, Test_acc 57.31
2024-08-30 17:15:14,384 [podnet.py] => Task 2, Epoch 75/200 (LR 0.06913) => LSC_loss 0.05, Spatial_loss 0.62, Flat_loss 0.12, Train_acc 100.00, Test_acc 59.76
2024-08-30 17:15:15,995 [podnet.py] => Task 2, Epoch 76/200 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.60, Flat_loss 0.12, Train_acc 99.98, Test_acc 60.02
2024-08-30 17:15:17,531 [podnet.py] => Task 2, Epoch 77/200 (LR 0.06767) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.33
2024-08-30 17:15:19,101 [podnet.py] => Task 2, Epoch 78/200 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.28
2024-08-30 17:15:20,554 [podnet.py] => Task 2, Epoch 79/200 (LR 0.06620) => LSC_loss 0.05, Spatial_loss 0.59, Flat_loss 0.11, Train_acc 100.00, Test_acc 59.54
2024-08-30 17:15:22,148 [podnet.py] => Task 2, Epoch 80/200 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 99.98, Test_acc 56.43
2024-08-30 17:15:23,681 [podnet.py] => Task 2, Epoch 81/200 (LR 0.06470) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.11, Train_acc 99.98, Test_acc 58.19
2024-08-30 17:15:25,115 [podnet.py] => Task 2, Epoch 82/200 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.58, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.85
2024-08-30 17:15:26,671 [podnet.py] => Task 2, Epoch 83/200 (LR 0.06319) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.44
2024-08-30 17:15:28,112 [podnet.py] => Task 2, Epoch 84/200 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 100.00, Test_acc 61.11
2024-08-30 17:15:29,746 [podnet.py] => Task 2, Epoch 85/200 (LR 0.06167) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.24
2024-08-30 17:15:31,717 [podnet.py] => Task 2, Epoch 86/200 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.31
2024-08-30 17:15:33,858 [podnet.py] => Task 2, Epoch 87/200 (LR 0.06014) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 99.96, Test_acc 55.93
2024-08-30 17:15:35,909 [podnet.py] => Task 2, Epoch 88/200 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 99.98, Test_acc 62.09
2024-08-30 17:15:37,423 [podnet.py] => Task 2, Epoch 89/200 (LR 0.05860) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.94
2024-08-30 17:15:38,839 [podnet.py] => Task 2, Epoch 90/200 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.69
2024-08-30 17:15:40,427 [podnet.py] => Task 2, Epoch 91/200 (LR 0.05705) => LSC_loss 0.06, Spatial_loss 0.58, Flat_loss 0.11, Train_acc 99.96, Test_acc 59.00
2024-08-30 17:15:41,851 [podnet.py] => Task 2, Epoch 92/200 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.56, Flat_loss 0.11, Train_acc 100.00, Test_acc 62.17
2024-08-30 17:15:43,302 [podnet.py] => Task 2, Epoch 93/200 (LR 0.05549) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 99.98, Test_acc 60.63
2024-08-30 17:15:44,833 [podnet.py] => Task 2, Epoch 94/200 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:15:46,321 [podnet.py] => Task 2, Epoch 95/200 (LR 0.05392) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.17
2024-08-30 17:15:47,731 [podnet.py] => Task 2, Epoch 96/200 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.96, Test_acc 58.69
2024-08-30 17:15:49,353 [podnet.py] => Task 2, Epoch 97/200 (LR 0.05236) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.24
2024-08-30 17:15:50,840 [podnet.py] => Task 2, Epoch 98/200 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.67
2024-08-30 17:15:52,375 [podnet.py] => Task 2, Epoch 99/200 (LR 0.05079) => LSC_loss 0.05, Spatial_loss 0.55, Flat_loss 0.11, Train_acc 99.98, Test_acc 57.17
2024-08-30 17:15:53,896 [podnet.py] => Task 2, Epoch 100/200 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 99.98, Test_acc 57.81
2024-08-30 17:15:55,358 [podnet.py] => Task 2, Epoch 101/200 (LR 0.04921) => LSC_loss 0.05, Spatial_loss 0.54, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.50
2024-08-30 17:15:56,803 [podnet.py] => Task 2, Epoch 102/200 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.57
2024-08-30 17:15:58,222 [podnet.py] => Task 2, Epoch 103/200 (LR 0.04764) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.39
2024-08-30 17:15:59,708 [podnet.py] => Task 2, Epoch 104/200 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.51, Flat_loss 0.11, Train_acc 99.98, Test_acc 54.76
2024-08-30 17:16:01,233 [podnet.py] => Task 2, Epoch 105/200 (LR 0.04608) => LSC_loss 0.05, Spatial_loss 0.57, Flat_loss 0.11, Train_acc 99.96, Test_acc 60.37
2024-08-30 17:16:02,978 [podnet.py] => Task 2, Epoch 106/200 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.10, Train_acc 99.98, Test_acc 60.26
2024-08-30 17:16:04,504 [podnet.py] => Task 2, Epoch 107/200 (LR 0.04451) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.10, Train_acc 100.00, Test_acc 58.17
2024-08-30 17:16:05,991 [podnet.py] => Task 2, Epoch 108/200 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.19
2024-08-30 17:16:07,663 [podnet.py] => Task 2, Epoch 109/200 (LR 0.04295) => LSC_loss 0.05, Spatial_loss 0.53, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.48
2024-08-30 17:16:09,160 [podnet.py] => Task 2, Epoch 110/200 (LR 0.04218) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.54
2024-08-30 17:16:10,651 [podnet.py] => Task 2, Epoch 111/200 (LR 0.04140) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.70
2024-08-30 17:16:12,081 [podnet.py] => Task 2, Epoch 112/200 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.11, Train_acc 100.00, Test_acc 58.72
2024-08-30 17:16:13,660 [podnet.py] => Task 2, Epoch 113/200 (LR 0.03986) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 99.98, Test_acc 57.61
2024-08-30 17:16:15,142 [podnet.py] => Task 2, Epoch 114/200 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.10, Train_acc 99.98, Test_acc 58.11
2024-08-30 17:16:16,519 [podnet.py] => Task 2, Epoch 115/200 (LR 0.03833) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.39
2024-08-30 17:16:17,969 [podnet.py] => Task 2, Epoch 116/200 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:16:19,446 [podnet.py] => Task 2, Epoch 117/200 (LR 0.03681) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.72
2024-08-30 17:16:20,847 [podnet.py] => Task 2, Epoch 118/200 (LR 0.03605) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.63
2024-08-30 17:16:22,301 [podnet.py] => Task 2, Epoch 119/200 (LR 0.03530) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.39
2024-08-30 17:16:23,854 [podnet.py] => Task 2, Epoch 120/200 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.26
2024-08-30 17:16:25,336 [podnet.py] => Task 2, Epoch 121/200 (LR 0.03380) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.28
2024-08-30 17:16:26,779 [podnet.py] => Task 2, Epoch 122/200 (LR 0.03306) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 58.74
2024-08-30 17:16:28,295 [podnet.py] => Task 2, Epoch 123/200 (LR 0.03233) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.46
2024-08-30 17:16:29,914 [podnet.py] => Task 2, Epoch 124/200 (LR 0.03159) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 100.00, Test_acc 58.15
2024-08-30 17:16:31,431 [podnet.py] => Task 2, Epoch 125/200 (LR 0.03087) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.10, Train_acc 100.00, Test_acc 57.48
2024-08-30 17:16:32,916 [podnet.py] => Task 2, Epoch 126/200 (LR 0.03014) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.10, Train_acc 100.00, Test_acc 58.56
2024-08-30 17:16:34,349 [podnet.py] => Task 2, Epoch 127/200 (LR 0.02942) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.09
2024-08-30 17:16:35,926 [podnet.py] => Task 2, Epoch 128/200 (LR 0.02871) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 58.83
2024-08-30 17:16:37,453 [podnet.py] => Task 2, Epoch 129/200 (LR 0.02800) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.07
2024-08-30 17:16:39,111 [podnet.py] => Task 2, Epoch 130/200 (LR 0.02730) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.11
2024-08-30 17:16:40,633 [podnet.py] => Task 2, Epoch 131/200 (LR 0.02660) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.26
2024-08-30 17:16:42,051 [podnet.py] => Task 2, Epoch 132/200 (LR 0.02591) => LSC_loss 0.06, Spatial_loss 0.45, Flat_loss 0.10, Train_acc 99.96, Test_acc 56.89
2024-08-30 17:16:43,548 [podnet.py] => Task 2, Epoch 133/200 (LR 0.02523) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.12, Train_acc 99.76, Test_acc 59.37
2024-08-30 17:16:44,971 [podnet.py] => Task 2, Epoch 134/200 (LR 0.02455) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.11, Train_acc 99.93, Test_acc 59.07
2024-08-30 17:16:46,603 [podnet.py] => Task 2, Epoch 135/200 (LR 0.02388) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 58.39
2024-08-30 17:16:48,189 [podnet.py] => Task 2, Epoch 136/200 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.35
2024-08-30 17:16:49,688 [podnet.py] => Task 2, Epoch 137/200 (LR 0.02255) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.10, Train_acc 100.00, Test_acc 63.02
2024-08-30 17:16:51,230 [podnet.py] => Task 2, Epoch 138/200 (LR 0.02190) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.69
2024-08-30 17:16:52,996 [podnet.py] => Task 2, Epoch 139/200 (LR 0.02125) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.11, Train_acc 100.00, Test_acc 60.09
2024-08-30 17:16:54,530 [podnet.py] => Task 2, Epoch 140/200 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.33
2024-08-30 17:16:56,084 [podnet.py] => Task 2, Epoch 141/200 (LR 0.01998) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.56
2024-08-30 17:16:57,557 [podnet.py] => Task 2, Epoch 142/200 (LR 0.01935) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.54
2024-08-30 17:16:59,033 [podnet.py] => Task 2, Epoch 143/200 (LR 0.01874) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.67
2024-08-30 17:17:00,493 [podnet.py] => Task 2, Epoch 144/200 (LR 0.01813) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.33
2024-08-30 17:17:02,035 [podnet.py] => Task 2, Epoch 145/200 (LR 0.01753) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:17:03,454 [podnet.py] => Task 2, Epoch 146/200 (LR 0.01693) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.91
2024-08-30 17:17:04,853 [podnet.py] => Task 2, Epoch 147/200 (LR 0.01635) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.46
2024-08-30 17:17:06,322 [podnet.py] => Task 2, Epoch 148/200 (LR 0.01577) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.89
2024-08-30 17:17:07,758 [podnet.py] => Task 2, Epoch 149/200 (LR 0.01520) => LSC_loss 0.06, Spatial_loss 0.41, Flat_loss 0.10, Train_acc 99.96, Test_acc 60.17
2024-08-30 17:17:09,198 [podnet.py] => Task 2, Epoch 150/200 (LR 0.01464) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 99.93, Test_acc 61.04
2024-08-30 17:17:10,665 [podnet.py] => Task 2, Epoch 151/200 (LR 0.01409) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.70
2024-08-30 17:17:12,125 [podnet.py] => Task 2, Epoch 152/200 (LR 0.01355) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.13
2024-08-30 17:17:13,608 [podnet.py] => Task 2, Epoch 153/200 (LR 0.01302) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.67
2024-08-30 17:17:15,168 [podnet.py] => Task 2, Epoch 154/200 (LR 0.01249) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.33
2024-08-30 17:17:16,597 [podnet.py] => Task 2, Epoch 155/200 (LR 0.01198) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.20
2024-08-30 17:17:18,129 [podnet.py] => Task 2, Epoch 156/200 (LR 0.01147) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.11
2024-08-30 17:17:19,571 [podnet.py] => Task 2, Epoch 157/200 (LR 0.01098) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.93
2024-08-30 17:17:21,300 [podnet.py] => Task 2, Epoch 158/200 (LR 0.01049) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.44
2024-08-30 17:17:22,925 [podnet.py] => Task 2, Epoch 159/200 (LR 0.01002) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.28
2024-08-30 17:17:24,390 [podnet.py] => Task 2, Epoch 160/200 (LR 0.00955) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.80
2024-08-30 17:17:25,835 [podnet.py] => Task 2, Epoch 161/200 (LR 0.00909) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.26
2024-08-30 17:17:27,357 [podnet.py] => Task 2, Epoch 162/200 (LR 0.00865) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.56
2024-08-30 17:17:28,946 [podnet.py] => Task 2, Epoch 163/200 (LR 0.00821) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.74
2024-08-30 17:17:30,446 [podnet.py] => Task 2, Epoch 164/200 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.26
2024-08-30 17:17:31,864 [podnet.py] => Task 2, Epoch 165/200 (LR 0.00737) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.50
2024-08-30 17:17:33,427 [podnet.py] => Task 2, Epoch 166/200 (LR 0.00696) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.56
2024-08-30 17:17:34,909 [podnet.py] => Task 2, Epoch 167/200 (LR 0.00657) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.13
2024-08-30 17:17:36,475 [podnet.py] => Task 2, Epoch 168/200 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.57
2024-08-30 17:17:38,023 [podnet.py] => Task 2, Epoch 169/200 (LR 0.00581) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.67
2024-08-30 17:17:39,693 [podnet.py] => Task 2, Epoch 170/200 (LR 0.00545) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.61
2024-08-30 17:17:41,172 [podnet.py] => Task 2, Epoch 171/200 (LR 0.00510) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.56
2024-08-30 17:17:42,612 [podnet.py] => Task 2, Epoch 172/200 (LR 0.00476) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.44
2024-08-30 17:17:44,062 [podnet.py] => Task 2, Epoch 173/200 (LR 0.00443) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.41
2024-08-30 17:17:45,509 [podnet.py] => Task 2, Epoch 174/200 (LR 0.00411) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.93
2024-08-30 17:17:46,971 [podnet.py] => Task 2, Epoch 175/200 (LR 0.00381) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.31
2024-08-30 17:17:48,445 [podnet.py] => Task 2, Epoch 176/200 (LR 0.00351) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.20
2024-08-30 17:17:49,987 [podnet.py] => Task 2, Epoch 177/200 (LR 0.00323) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.22
2024-08-30 17:17:51,495 [podnet.py] => Task 2, Epoch 178/200 (LR 0.00296) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.33
2024-08-30 17:17:52,936 [podnet.py] => Task 2, Epoch 179/200 (LR 0.00270) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.52
2024-08-30 17:17:54,357 [podnet.py] => Task 2, Epoch 180/200 (LR 0.00245) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 99.98, Test_acc 62.19
2024-08-30 17:17:55,770 [podnet.py] => Task 2, Epoch 181/200 (LR 0.00221) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 63.35
2024-08-30 17:17:57,148 [podnet.py] => Task 2, Epoch 182/200 (LR 0.00199) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.67
2024-08-30 17:17:58,604 [podnet.py] => Task 2, Epoch 183/200 (LR 0.00177) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.81
2024-08-30 17:18:00,004 [podnet.py] => Task 2, Epoch 184/200 (LR 0.00157) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.37
2024-08-30 17:18:01,417 [podnet.py] => Task 2, Epoch 185/200 (LR 0.00138) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.91
2024-08-30 17:18:02,854 [podnet.py] => Task 2, Epoch 186/200 (LR 0.00120) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.80
2024-08-30 17:18:04,301 [podnet.py] => Task 2, Epoch 187/200 (LR 0.00104) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.57
2024-08-30 17:18:05,760 [podnet.py] => Task 2, Epoch 188/200 (LR 0.00089) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.91
2024-08-30 17:18:07,269 [podnet.py] => Task 2, Epoch 189/200 (LR 0.00074) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.41
2024-08-30 17:18:08,703 [podnet.py] => Task 2, Epoch 190/200 (LR 0.00062) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.26
2024-08-30 17:18:10,175 [podnet.py] => Task 2, Epoch 191/200 (LR 0.00050) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.04
2024-08-30 17:18:11,582 [podnet.py] => Task 2, Epoch 192/200 (LR 0.00039) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.83
2024-08-30 17:18:12,985 [podnet.py] => Task 2, Epoch 193/200 (LR 0.00030) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.98
2024-08-30 17:18:14,480 [podnet.py] => Task 2, Epoch 194/200 (LR 0.00022) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.87
2024-08-30 17:18:15,910 [podnet.py] => Task 2, Epoch 195/200 (LR 0.00015) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.06
2024-08-30 17:18:17,375 [podnet.py] => Task 2, Epoch 196/200 (LR 0.00010) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.30
2024-08-30 17:18:18,939 [podnet.py] => Task 2, Epoch 197/200 (LR 0.00006) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.78
2024-08-30 17:18:20,424 [podnet.py] => Task 2, Epoch 198/200 (LR 0.00002) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.85
2024-08-30 17:18:21,856 [podnet.py] => Task 2, Epoch 199/200 (LR 0.00001) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.80
2024-08-30 17:18:23,329 [podnet.py] => Task 2, Epoch 200/200 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.69
2024-08-30 17:18:23,769 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:18:23,769 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:18:25,557 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:18:27,518 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 17:18:29,259 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 17:18:32,009 [podnet.py] => Exemplar size: 495
2024-08-30 17:18:32,009 [trainer.py] => CNN: {'total': 62.69, '00-04': 51.87, '05-06': 55.17, '07-08': 97.25, 'old': 52.81, 'new': 97.25}
2024-08-30 17:18:32,009 [trainer.py] => NME: {'total': 67.31, '00-04': 70.1, '05-06': 44.25, '07-08': 83.42, 'old': 62.71, 'new': 83.42}
2024-08-30 17:18:32,009 [trainer.py] => CNN top1 curve: [89.63, 69.95, 62.69]
2024-08-30 17:18:32,009 [trainer.py] => CNN top5 curve: [100.0, 98.6, 93.48]
2024-08-30 17:18:32,009 [trainer.py] => NME top1 curve: [89.6, 74.81, 67.31]
2024-08-30 17:18:32,009 [trainer.py] => NME top5 curve: [100.0, 98.57, 95.5]

2024-08-30 17:18:32,009 [trainer.py] => Average Accuracy (CNN): 74.08999999999999
2024-08-30 17:18:32,009 [trainer.py] => Average Accuracy (NME): 77.24
2024-08-30 17:18:32,010 [trainer.py] => Forgetting (CNN): 38.504999999999995
