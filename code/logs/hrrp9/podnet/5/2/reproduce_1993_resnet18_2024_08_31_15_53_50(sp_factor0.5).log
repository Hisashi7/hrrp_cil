2024-08-31 15:53:50,707 [trainer.py] => config: ./exps/podnet.json
2024-08-31 15:53:50,707 [trainer.py] => prefix: reproduce
2024-08-31 15:53:50,707 [trainer.py] => dataset: hrrp9
2024-08-31 15:53:50,707 [trainer.py] => memory_size: 500
2024-08-31 15:53:50,707 [trainer.py] => memory_per_class: 20
2024-08-31 15:53:50,707 [trainer.py] => fixed_memory: False
2024-08-31 15:53:50,707 [trainer.py] => shuffle: True
2024-08-31 15:53:50,707 [trainer.py] => init_cls: 5
2024-08-31 15:53:50,707 [trainer.py] => increment: 2
2024-08-31 15:53:50,707 [trainer.py] => model_name: podnet
2024-08-31 15:53:50,707 [trainer.py] => convnet_type: resnet18
2024-08-31 15:53:50,707 [trainer.py] => init_train: True
2024-08-31 15:53:50,707 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-31 15:53:50,707 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-31 15:53:50,707 [trainer.py] => device: [device(type='cuda', index=6)]
2024-08-31 15:53:50,707 [trainer.py] => seed: 1993
2024-08-31 15:53:51,163 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-31 15:53:51,262 [trainer.py] => All params: 3843904
2024-08-31 15:53:51,262 [trainer.py] => Trainable params: 3843904
2024-08-31 15:53:51,263 [podnet.py] => Learning on 0-5
2024-08-31 15:53:51,296 [podnet.py] => Adaptive factor: 0
2024-08-31 15:53:54,004 [podnet.py] => Task 0, Epoch 1/300 (LR 0.10000) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-31 15:53:55,740 [podnet.py] => Task 0, Epoch 2/300 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.86, Test_acc 32.63
2024-08-31 15:53:58,167 [podnet.py] => Task 0, Epoch 3/300 (LR 0.09998) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.47, Test_acc 36.30
2024-08-31 15:53:59,846 [podnet.py] => Task 0, Epoch 4/300 (LR 0.09996) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.33, Test_acc 59.63
2024-08-31 15:54:01,691 [podnet.py] => Task 0, Epoch 5/300 (LR 0.09993) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.73, Test_acc 55.37
2024-08-31 15:54:04,018 [podnet.py] => Task 0, Epoch 6/300 (LR 0.09990) => LSC_loss 0.25, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.02, Test_acc 72.20
2024-08-31 15:54:06,695 [podnet.py] => Task 0, Epoch 7/300 (LR 0.09987) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.42, Test_acc 67.40
2024-08-31 15:54:08,770 [podnet.py] => Task 0, Epoch 8/300 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.66, Test_acc 74.30
2024-08-31 15:54:11,509 [podnet.py] => Task 0, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.07, Test_acc 74.60
2024-08-31 15:54:14,550 [podnet.py] => Task 0, Epoch 10/300 (LR 0.09973) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.47, Test_acc 78.70
2024-08-31 15:54:17,263 [podnet.py] => Task 0, Epoch 11/300 (LR 0.09967) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 70.47
2024-08-31 15:54:19,966 [podnet.py] => Task 0, Epoch 12/300 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 82.27
2024-08-31 15:54:21,727 [podnet.py] => Task 0, Epoch 13/300 (LR 0.09954) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.29, Test_acc 82.53
2024-08-31 15:54:23,043 [podnet.py] => Task 0, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 67.27
2024-08-31 15:54:25,568 [podnet.py] => Task 0, Epoch 15/300 (LR 0.09938) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.94, Test_acc 79.93
2024-08-31 15:54:27,732 [podnet.py] => Task 0, Epoch 16/300 (LR 0.09930) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.08, Test_acc 83.07
2024-08-31 15:54:29,755 [podnet.py] => Task 0, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 72.60
2024-08-31 15:54:32,014 [podnet.py] => Task 0, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.56, Test_acc 80.30
2024-08-31 15:54:34,734 [podnet.py] => Task 0, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 81.57
2024-08-31 15:54:36,701 [podnet.py] => Task 0, Epoch 20/300 (LR 0.09891) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 79.03
2024-08-31 15:54:39,802 [podnet.py] => Task 0, Epoch 21/300 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 83.07
2024-08-31 15:54:41,736 [podnet.py] => Task 0, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 74.17
2024-08-31 15:54:44,496 [podnet.py] => Task 0, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 82.63
2024-08-31 15:54:47,197 [podnet.py] => Task 0, Epoch 24/300 (LR 0.09843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 82.83
2024-08-31 15:54:49,854 [podnet.py] => Task 0, Epoch 25/300 (LR 0.09830) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.27, Test_acc 84.33
2024-08-31 15:54:52,342 [podnet.py] => Task 0, Epoch 26/300 (LR 0.09816) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.83, Test_acc 85.20
2024-08-31 15:54:54,445 [podnet.py] => Task 0, Epoch 27/300 (LR 0.09801) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 85.90
2024-08-31 15:54:57,095 [podnet.py] => Task 0, Epoch 28/300 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 74.20
2024-08-31 15:54:59,735 [podnet.py] => Task 0, Epoch 29/300 (LR 0.09771) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 82.03
2024-08-31 15:55:01,844 [podnet.py] => Task 0, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 76.93
2024-08-31 15:55:04,475 [podnet.py] => Task 0, Epoch 31/300 (LR 0.09739) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 79.40
2024-08-31 15:55:06,604 [podnet.py] => Task 0, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.66, Test_acc 77.10
2024-08-31 15:55:07,943 [podnet.py] => Task 0, Epoch 33/300 (LR 0.09704) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.02, Test_acc 79.23
2024-08-31 15:55:10,823 [podnet.py] => Task 0, Epoch 34/300 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.07, Test_acc 86.00
2024-08-31 15:55:13,491 [podnet.py] => Task 0, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 74.73
2024-08-31 15:55:16,410 [podnet.py] => Task 0, Epoch 36/300 (LR 0.09649) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.14, Test_acc 79.27
2024-08-31 15:55:18,984 [podnet.py] => Task 0, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 84.97
2024-08-31 15:55:21,205 [podnet.py] => Task 0, Epoch 38/300 (LR 0.09609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 84.70
2024-08-31 15:55:22,998 [podnet.py] => Task 0, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 82.83
2024-08-31 15:55:26,032 [podnet.py] => Task 0, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 75.70
2024-08-31 15:55:28,718 [podnet.py] => Task 0, Epoch 41/300 (LR 0.09546) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.97, Test_acc 83.57
2024-08-31 15:55:31,157 [podnet.py] => Task 0, Epoch 42/300 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 82.43
2024-08-31 15:55:33,907 [podnet.py] => Task 0, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 86.70
2024-08-31 15:55:36,499 [podnet.py] => Task 0, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 71.70
2024-08-31 15:55:38,914 [podnet.py] => Task 0, Epoch 45/300 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.30
2024-08-31 15:55:41,660 [podnet.py] => Task 0, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 80.70
2024-08-31 15:55:44,597 [podnet.py] => Task 0, Epoch 47/300 (LR 0.09407) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 84.67
2024-08-31 15:55:47,622 [podnet.py] => Task 0, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 83.63
2024-08-31 15:55:50,529 [podnet.py] => Task 0, Epoch 49/300 (LR 0.09356) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 85.03
2024-08-31 15:55:52,385 [podnet.py] => Task 0, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 82.57
2024-08-31 15:55:55,351 [podnet.py] => Task 0, Epoch 51/300 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 88.17
2024-08-31 15:55:58,243 [podnet.py] => Task 0, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 80.73
2024-08-31 15:56:01,064 [podnet.py] => Task 0, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 78.07
2024-08-31 15:56:02,865 [podnet.py] => Task 0, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.67
2024-08-31 15:56:05,496 [podnet.py] => Task 0, Epoch 55/300 (LR 0.09193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 82.83
2024-08-31 15:56:07,281 [podnet.py] => Task 0, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 82.33
2024-08-31 15:56:09,125 [podnet.py] => Task 0, Epoch 57/300 (LR 0.09135) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 82.83
2024-08-31 15:56:11,543 [podnet.py] => Task 0, Epoch 58/300 (LR 0.09106) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.50, Test_acc 84.40
2024-08-31 15:56:14,412 [podnet.py] => Task 0, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 70.30
2024-08-31 15:56:16,144 [podnet.py] => Task 0, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.83
2024-08-31 15:56:19,312 [podnet.py] => Task 0, Epoch 61/300 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.69, Test_acc 83.20
2024-08-31 15:56:22,074 [podnet.py] => Task 0, Epoch 62/300 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 86.53
2024-08-31 15:56:24,637 [podnet.py] => Task 0, Epoch 63/300 (LR 0.08951) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 84.83
2024-08-31 15:56:27,502 [podnet.py] => Task 0, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 84.63
2024-08-31 15:56:29,316 [podnet.py] => Task 0, Epoch 65/300 (LR 0.08886) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 81.80
2024-08-31 15:56:30,638 [podnet.py] => Task 0, Epoch 66/300 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 81.17
2024-08-31 15:56:33,683 [podnet.py] => Task 0, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 82.20
2024-08-31 15:56:36,029 [podnet.py] => Task 0, Epoch 68/300 (LR 0.08785) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.91, Test_acc 83.43
2024-08-31 15:56:38,819 [podnet.py] => Task 0, Epoch 69/300 (LR 0.08751) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 84.27
2024-08-31 15:56:40,998 [podnet.py] => Task 0, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.47, Test_acc 84.80
2024-08-31 15:56:43,935 [podnet.py] => Task 0, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 88.13
2024-08-31 15:56:46,777 [podnet.py] => Task 0, Epoch 72/300 (LR 0.08645) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 87.83
2024-08-31 15:56:48,950 [podnet.py] => Task 0, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 87.00
2024-08-31 15:56:50,776 [podnet.py] => Task 0, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 87.03
2024-08-31 15:56:53,213 [podnet.py] => Task 0, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.63
2024-08-31 15:56:56,000 [podnet.py] => Task 0, Epoch 76/300 (LR 0.08498) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.71, Test_acc 78.20
2024-08-31 15:56:58,834 [podnet.py] => Task 0, Epoch 77/300 (LR 0.08461) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 84.30
2024-08-31 15:57:01,149 [podnet.py] => Task 0, Epoch 78/300 (LR 0.08423) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 86.53
2024-08-31 15:57:03,968 [podnet.py] => Task 0, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 80.93
2024-08-31 15:57:06,277 [podnet.py] => Task 0, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 83.10
2024-08-31 15:57:08,846 [podnet.py] => Task 0, Epoch 81/300 (LR 0.08307) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.57
2024-08-31 15:57:11,033 [podnet.py] => Task 0, Epoch 82/300 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 86.83
2024-08-31 15:57:14,163 [podnet.py] => Task 0, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 84.97
2024-08-31 15:57:16,161 [podnet.py] => Task 0, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 80.90
2024-08-31 15:57:19,127 [podnet.py] => Task 0, Epoch 85/300 (LR 0.08147) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.01, Test_acc 86.27
2024-08-31 15:57:21,380 [podnet.py] => Task 0, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.85, Test_acc 80.87
2024-08-31 15:57:23,226 [podnet.py] => Task 0, Epoch 87/300 (LR 0.08065) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.98, Test_acc 78.57
2024-08-31 15:57:26,077 [podnet.py] => Task 0, Epoch 88/300 (LR 0.08023) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.55, Test_acc 85.57
2024-08-31 15:57:28,448 [podnet.py] => Task 0, Epoch 89/300 (LR 0.07981) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.87
2024-08-31 15:57:31,209 [podnet.py] => Task 0, Epoch 90/300 (LR 0.07939) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.83
2024-08-31 15:57:34,020 [podnet.py] => Task 0, Epoch 91/300 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 82.47
2024-08-31 15:57:35,619 [podnet.py] => Task 0, Epoch 92/300 (LR 0.07854) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 86.23
2024-08-31 15:57:38,364 [podnet.py] => Task 0, Epoch 93/300 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.07
2024-08-31 15:57:41,034 [podnet.py] => Task 0, Epoch 94/300 (LR 0.07767) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 84.03
2024-08-31 15:57:43,035 [podnet.py] => Task 0, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 80.70
2024-08-31 15:57:46,018 [podnet.py] => Task 0, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 85.07
2024-08-31 15:57:49,063 [podnet.py] => Task 0, Epoch 97/300 (LR 0.07635) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 79.37
2024-08-31 15:57:50,913 [podnet.py] => Task 0, Epoch 98/300 (LR 0.07590) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 87.33
2024-08-31 15:57:52,687 [podnet.py] => Task 0, Epoch 99/300 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.21, Test_acc 84.17
2024-08-31 15:57:54,624 [podnet.py] => Task 0, Epoch 100/300 (LR 0.07500) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 85.00
2024-08-31 15:57:57,667 [podnet.py] => Task 0, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 82.50
2024-08-31 15:58:00,265 [podnet.py] => Task 0, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.40
2024-08-31 15:58:02,842 [podnet.py] => Task 0, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 83.40
2024-08-31 15:58:05,558 [podnet.py] => Task 0, Epoch 104/300 (LR 0.07316) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 80.57
2024-08-31 15:58:08,279 [podnet.py] => Task 0, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 88.03
2024-08-31 15:58:10,847 [podnet.py] => Task 0, Epoch 106/300 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 85.80
2024-08-31 15:58:13,575 [podnet.py] => Task 0, Epoch 107/300 (LR 0.07176) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 80.70
2024-08-31 15:58:16,474 [podnet.py] => Task 0, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 85.73
2024-08-31 15:58:19,006 [podnet.py] => Task 0, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.50
2024-08-31 15:58:20,866 [podnet.py] => Task 0, Epoch 110/300 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.13
2024-08-31 15:58:23,205 [podnet.py] => Task 0, Epoch 111/300 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 87.13
2024-08-31 15:58:26,202 [podnet.py] => Task 0, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.84, Test_acc 84.77
2024-08-31 15:58:28,865 [podnet.py] => Task 0, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 85.40
2024-08-31 15:58:30,756 [podnet.py] => Task 0, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 83.33
2024-08-31 15:58:33,560 [podnet.py] => Task 0, Epoch 115/300 (LR 0.06792) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 84.27
2024-08-31 15:58:36,376 [podnet.py] => Task 0, Epoch 116/300 (LR 0.06743) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.73
2024-08-31 15:58:39,161 [podnet.py] => Task 0, Epoch 117/300 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 79.07
2024-08-31 15:58:40,564 [podnet.py] => Task 0, Epoch 118/300 (LR 0.06644) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 84.63
2024-08-31 15:58:43,156 [podnet.py] => Task 0, Epoch 119/300 (LR 0.06595) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.53
2024-08-31 15:58:45,595 [podnet.py] => Task 0, Epoch 120/300 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 83.80
2024-08-31 15:58:47,332 [podnet.py] => Task 0, Epoch 121/300 (LR 0.06495) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 87.63
2024-08-31 15:58:49,958 [podnet.py] => Task 0, Epoch 122/300 (LR 0.06445) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 83.27
2024-08-31 15:58:52,876 [podnet.py] => Task 0, Epoch 123/300 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.34, Test_acc 82.97
2024-08-31 15:58:55,382 [podnet.py] => Task 0, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 83.40
2024-08-31 15:58:58,283 [podnet.py] => Task 0, Epoch 125/300 (LR 0.06294) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.77
2024-08-31 15:59:01,147 [podnet.py] => Task 0, Epoch 126/300 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 86.57
2024-08-31 15:59:04,064 [podnet.py] => Task 0, Epoch 127/300 (LR 0.06193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.36, Test_acc 86.00
2024-08-31 15:59:06,645 [podnet.py] => Task 0, Epoch 128/300 (LR 0.06142) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 84.60
2024-08-31 15:59:09,504 [podnet.py] => Task 0, Epoch 129/300 (LR 0.06091) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 87.97
2024-08-31 15:59:12,158 [podnet.py] => Task 0, Epoch 130/300 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 82.23
2024-08-31 15:59:14,646 [podnet.py] => Task 0, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.50, Test_acc 86.90
2024-08-31 15:59:17,443 [podnet.py] => Task 0, Epoch 132/300 (LR 0.05937) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.73
2024-08-31 15:59:19,211 [podnet.py] => Task 0, Epoch 133/300 (LR 0.05885) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.57
2024-08-31 15:59:20,571 [podnet.py] => Task 0, Epoch 134/300 (LR 0.05834) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.87
2024-08-31 15:59:23,125 [podnet.py] => Task 0, Epoch 135/300 (LR 0.05782) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 85.53
2024-08-31 15:59:24,962 [podnet.py] => Task 0, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 79.13
2024-08-31 15:59:27,206 [podnet.py] => Task 0, Epoch 137/300 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 88.00
2024-08-31 15:59:29,781 [podnet.py] => Task 0, Epoch 138/300 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.33
2024-08-31 15:59:32,700 [podnet.py] => Task 0, Epoch 139/300 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 88.17
2024-08-31 15:59:34,475 [podnet.py] => Task 0, Epoch 140/300 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 82.47
2024-08-31 15:59:37,580 [podnet.py] => Task 0, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 84.93
2024-08-31 15:59:40,272 [podnet.py] => Task 0, Epoch 142/300 (LR 0.05418) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.70, Test_acc 84.13
2024-08-31 15:59:42,425 [podnet.py] => Task 0, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 84.60
2024-08-31 15:59:44,477 [podnet.py] => Task 0, Epoch 144/300 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 87.83
2024-08-31 15:59:46,742 [podnet.py] => Task 0, Epoch 145/300 (LR 0.05262) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.60
2024-08-31 15:59:49,542 [podnet.py] => Task 0, Epoch 146/300 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 81.50
2024-08-31 15:59:51,445 [podnet.py] => Task 0, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 86.77
2024-08-31 15:59:53,477 [podnet.py] => Task 0, Epoch 148/300 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 84.77
2024-08-31 15:59:56,448 [podnet.py] => Task 0, Epoch 149/300 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 85.70
2024-08-31 15:59:58,857 [podnet.py] => Task 0, Epoch 150/300 (LR 0.05000) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 86.57
2024-08-31 16:00:01,178 [podnet.py] => Task 0, Epoch 151/300 (LR 0.04948) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 84.43
2024-08-31 16:00:04,185 [podnet.py] => Task 0, Epoch 152/300 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 88.60
2024-08-31 16:00:06,924 [podnet.py] => Task 0, Epoch 153/300 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 87.07
2024-08-31 16:00:09,454 [podnet.py] => Task 0, Epoch 154/300 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 87.57
2024-08-31 16:00:12,516 [podnet.py] => Task 0, Epoch 155/300 (LR 0.04738) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 81.10
2024-08-31 16:00:14,687 [podnet.py] => Task 0, Epoch 156/300 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.67, Test_acc 88.10
2024-08-31 16:00:17,562 [podnet.py] => Task 0, Epoch 157/300 (LR 0.04634) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.87
2024-08-31 16:00:20,407 [podnet.py] => Task 0, Epoch 158/300 (LR 0.04582) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.53
2024-08-31 16:00:23,700 [podnet.py] => Task 0, Epoch 159/300 (LR 0.04529) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.43
2024-08-31 16:00:26,600 [podnet.py] => Task 0, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 86.20
2024-08-31 16:00:29,955 [podnet.py] => Task 0, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 84.87
2024-08-31 16:00:32,944 [podnet.py] => Task 0, Epoch 162/300 (LR 0.04373) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 88.27
2024-08-31 16:00:35,498 [podnet.py] => Task 0, Epoch 163/300 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 85.63
2024-08-31 16:00:37,348 [podnet.py] => Task 0, Epoch 164/300 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 83.27
2024-08-31 16:00:39,636 [podnet.py] => Task 0, Epoch 165/300 (LR 0.04218) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 85.37
2024-08-31 16:00:42,757 [podnet.py] => Task 0, Epoch 166/300 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 89.13
2024-08-31 16:00:45,458 [podnet.py] => Task 0, Epoch 167/300 (LR 0.04115) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.80
2024-08-31 16:00:47,969 [podnet.py] => Task 0, Epoch 168/300 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 90.17
2024-08-31 16:00:50,753 [podnet.py] => Task 0, Epoch 169/300 (LR 0.04012) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.03
2024-08-31 16:00:52,918 [podnet.py] => Task 0, Epoch 170/300 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 89.47
2024-08-31 16:00:55,927 [podnet.py] => Task 0, Epoch 171/300 (LR 0.03909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.13
2024-08-31 16:00:58,847 [podnet.py] => Task 0, Epoch 172/300 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 89.47
2024-08-31 16:01:01,955 [podnet.py] => Task 0, Epoch 173/300 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.13
2024-08-31 16:01:05,055 [podnet.py] => Task 0, Epoch 174/300 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 84.43
2024-08-31 16:01:07,878 [podnet.py] => Task 0, Epoch 175/300 (LR 0.03706) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.30
2024-08-31 16:01:10,582 [podnet.py] => Task 0, Epoch 176/300 (LR 0.03655) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 81.37
2024-08-31 16:01:13,487 [podnet.py] => Task 0, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.73, Test_acc 85.23
2024-08-31 16:01:16,443 [podnet.py] => Task 0, Epoch 178/300 (LR 0.03555) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 85.83
2024-08-31 16:01:19,111 [podnet.py] => Task 0, Epoch 179/300 (LR 0.03505) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 83.43
2024-08-31 16:01:22,291 [podnet.py] => Task 0, Epoch 180/300 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 88.90
2024-08-31 16:01:24,106 [podnet.py] => Task 0, Epoch 181/300 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 88.60
2024-08-31 16:01:26,867 [podnet.py] => Task 0, Epoch 182/300 (LR 0.03356) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-08-31 16:01:29,340 [podnet.py] => Task 0, Epoch 183/300 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.73
2024-08-31 16:01:32,048 [podnet.py] => Task 0, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.11, Test_acc 88.17
2024-08-31 16:01:35,091 [podnet.py] => Task 0, Epoch 185/300 (LR 0.03208) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 87.47
2024-08-31 16:01:37,069 [podnet.py] => Task 0, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 79.03
2024-08-31 16:01:39,318 [podnet.py] => Task 0, Epoch 187/300 (LR 0.03111) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.53, Test_acc 84.80
2024-08-31 16:01:42,022 [podnet.py] => Task 0, Epoch 188/300 (LR 0.03062) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 85.53
2024-08-31 16:01:44,797 [podnet.py] => Task 0, Epoch 189/300 (LR 0.03014) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 88.90
2024-08-31 16:01:46,614 [podnet.py] => Task 0, Epoch 190/300 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 86.80
2024-08-31 16:01:49,467 [podnet.py] => Task 0, Epoch 191/300 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.20
2024-08-31 16:01:51,813 [podnet.py] => Task 0, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 86.90
2024-08-31 16:01:54,435 [podnet.py] => Task 0, Epoch 193/300 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 89.30
2024-08-31 16:01:57,412 [podnet.py] => Task 0, Epoch 194/300 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.27
2024-08-31 16:02:00,413 [podnet.py] => Task 0, Epoch 195/300 (LR 0.02730) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.47
2024-08-31 16:02:02,918 [podnet.py] => Task 0, Epoch 196/300 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.73
2024-08-31 16:02:04,620 [podnet.py] => Task 0, Epoch 197/300 (LR 0.02637) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.63
2024-08-31 16:02:06,427 [podnet.py] => Task 0, Epoch 198/300 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.23
2024-08-31 16:02:08,963 [podnet.py] => Task 0, Epoch 199/300 (LR 0.02545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.60
2024-08-31 16:02:11,709 [podnet.py] => Task 0, Epoch 200/300 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.67
2024-08-31 16:02:15,033 [podnet.py] => Task 0, Epoch 201/300 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.67
2024-08-31 16:02:16,890 [podnet.py] => Task 0, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 88.40
2024-08-31 16:02:19,569 [podnet.py] => Task 0, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 88.30
2024-08-31 16:02:22,297 [podnet.py] => Task 0, Epoch 204/300 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 87.87
2024-08-31 16:02:24,388 [podnet.py] => Task 0, Epoch 205/300 (LR 0.02277) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 88.73
2024-08-31 16:02:27,611 [podnet.py] => Task 0, Epoch 206/300 (LR 0.02233) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.10
2024-08-31 16:02:30,398 [podnet.py] => Task 0, Epoch 207/300 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.13
2024-08-31 16:02:32,610 [podnet.py] => Task 0, Epoch 208/300 (LR 0.02146) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 88.40
2024-08-31 16:02:35,624 [podnet.py] => Task 0, Epoch 209/300 (LR 0.02104) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 89.43
2024-08-31 16:02:38,475 [podnet.py] => Task 0, Epoch 210/300 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.40
2024-08-31 16:02:41,119 [podnet.py] => Task 0, Epoch 211/300 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 86.90
2024-08-31 16:02:43,710 [podnet.py] => Task 0, Epoch 212/300 (LR 0.01977) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 86.67
2024-08-31 16:02:45,972 [podnet.py] => Task 0, Epoch 213/300 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-08-31 16:02:48,748 [podnet.py] => Task 0, Epoch 214/300 (LR 0.01894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 89.67
2024-08-31 16:02:51,181 [podnet.py] => Task 0, Epoch 215/300 (LR 0.01853) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 89.77
2024-08-31 16:02:54,340 [podnet.py] => Task 0, Epoch 216/300 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-31 16:02:57,442 [podnet.py] => Task 0, Epoch 217/300 (LR 0.01773) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.83
2024-08-31 16:03:00,410 [podnet.py] => Task 0, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 89.30
2024-08-31 16:03:02,296 [podnet.py] => Task 0, Epoch 219/300 (LR 0.01693) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.13
2024-08-31 16:03:05,642 [podnet.py] => Task 0, Epoch 220/300 (LR 0.01654) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.87
2024-08-31 16:03:08,890 [podnet.py] => Task 0, Epoch 221/300 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-31 16:03:11,108 [podnet.py] => Task 0, Epoch 222/300 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-31 16:03:14,032 [podnet.py] => Task 0, Epoch 223/300 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-31 16:03:16,881 [podnet.py] => Task 0, Epoch 224/300 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-31 16:03:19,678 [podnet.py] => Task 0, Epoch 225/300 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-31 16:03:22,409 [podnet.py] => Task 0, Epoch 226/300 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-31 16:03:25,398 [podnet.py] => Task 0, Epoch 227/300 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-31 16:03:28,539 [podnet.py] => Task 0, Epoch 228/300 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-08-31 16:03:30,462 [podnet.py] => Task 0, Epoch 229/300 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-31 16:03:33,185 [podnet.py] => Task 0, Epoch 230/300 (LR 0.01284) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-08-31 16:03:35,032 [podnet.py] => Task 0, Epoch 231/300 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.00
2024-08-31 16:03:37,238 [podnet.py] => Task 0, Epoch 232/300 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.90
2024-08-31 16:03:39,012 [podnet.py] => Task 0, Epoch 233/300 (LR 0.01181) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.07
2024-08-31 16:03:41,686 [podnet.py] => Task 0, Epoch 234/300 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-08-31 16:03:44,162 [podnet.py] => Task 0, Epoch 235/300 (LR 0.01114) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.03
2024-08-31 16:03:46,557 [podnet.py] => Task 0, Epoch 236/300 (LR 0.01082) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 89.43
2024-08-31 16:03:48,781 [podnet.py] => Task 0, Epoch 237/300 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-31 16:03:51,399 [podnet.py] => Task 0, Epoch 238/300 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-31 16:03:54,315 [podnet.py] => Task 0, Epoch 239/300 (LR 0.00986) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-31 16:03:56,845 [podnet.py] => Task 0, Epoch 240/300 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:03:58,730 [podnet.py] => Task 0, Epoch 241/300 (LR 0.00924) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-31 16:04:01,712 [podnet.py] => Task 0, Epoch 242/300 (LR 0.00894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-31 16:04:03,729 [podnet.py] => Task 0, Epoch 243/300 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-31 16:04:06,347 [podnet.py] => Task 0, Epoch 244/300 (LR 0.00835) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-31 16:04:08,599 [podnet.py] => Task 0, Epoch 245/300 (LR 0.00807) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-31 16:04:11,054 [podnet.py] => Task 0, Epoch 246/300 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-31 16:04:13,792 [podnet.py] => Task 0, Epoch 247/300 (LR 0.00751) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-31 16:04:16,569 [podnet.py] => Task 0, Epoch 248/300 (LR 0.00723) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-31 16:04:19,183 [podnet.py] => Task 0, Epoch 249/300 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-31 16:04:22,229 [podnet.py] => Task 0, Epoch 250/300 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:04:24,958 [podnet.py] => Task 0, Epoch 251/300 (LR 0.00644) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-31 16:04:27,398 [podnet.py] => Task 0, Epoch 252/300 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-31 16:04:28,723 [podnet.py] => Task 0, Epoch 253/300 (LR 0.00593) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-31 16:04:30,066 [podnet.py] => Task 0, Epoch 254/300 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:04:31,911 [podnet.py] => Task 0, Epoch 255/300 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-31 16:04:34,812 [podnet.py] => Task 0, Epoch 256/300 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-31 16:04:36,621 [podnet.py] => Task 0, Epoch 257/300 (LR 0.00498) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.57
2024-08-31 16:04:38,744 [podnet.py] => Task 0, Epoch 258/300 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.87
2024-08-31 16:04:41,434 [podnet.py] => Task 0, Epoch 259/300 (LR 0.00454) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-31 16:04:43,248 [podnet.py] => Task 0, Epoch 260/300 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-31 16:04:45,572 [podnet.py] => Task 0, Epoch 261/300 (LR 0.00411) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-31 16:04:48,174 [podnet.py] => Task 0, Epoch 262/300 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-31 16:04:50,614 [podnet.py] => Task 0, Epoch 263/300 (LR 0.00371) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-31 16:04:53,033 [podnet.py] => Task 0, Epoch 264/300 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-31 16:04:55,944 [podnet.py] => Task 0, Epoch 265/300 (LR 0.00332) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-31 16:04:58,796 [podnet.py] => Task 0, Epoch 266/300 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-31 16:05:01,828 [podnet.py] => Task 0, Epoch 267/300 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-31 16:05:03,963 [podnet.py] => Task 0, Epoch 268/300 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-31 16:05:06,449 [podnet.py] => Task 0, Epoch 269/300 (LR 0.00261) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-31 16:05:09,210 [podnet.py] => Task 0, Epoch 270/300 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-31 16:05:11,605 [podnet.py] => Task 0, Epoch 271/300 (LR 0.00229) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-31 16:05:14,744 [podnet.py] => Task 0, Epoch 272/300 (LR 0.00213) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-31 16:05:16,957 [podnet.py] => Task 0, Epoch 273/300 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-31 16:05:19,093 [podnet.py] => Task 0, Epoch 274/300 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-31 16:05:21,817 [podnet.py] => Task 0, Epoch 275/300 (LR 0.00170) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-31 16:05:24,029 [podnet.py] => Task 0, Epoch 276/300 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-31 16:05:27,176 [podnet.py] => Task 0, Epoch 277/300 (LR 0.00144) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-31 16:05:29,812 [podnet.py] => Task 0, Epoch 278/300 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.70
2024-08-31 16:05:31,188 [podnet.py] => Task 0, Epoch 279/300 (LR 0.00120) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.97
2024-08-31 16:05:33,867 [podnet.py] => Task 0, Epoch 280/300 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:05:36,915 [podnet.py] => Task 0, Epoch 281/300 (LR 0.00099) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-31 16:05:38,970 [podnet.py] => Task 0, Epoch 282/300 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-31 16:05:41,234 [podnet.py] => Task 0, Epoch 283/300 (LR 0.00079) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-31 16:05:43,251 [podnet.py] => Task 0, Epoch 284/300 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-31 16:05:46,222 [podnet.py] => Task 0, Epoch 285/300 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-31 16:05:49,047 [podnet.py] => Task 0, Epoch 286/300 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:05:51,841 [podnet.py] => Task 0, Epoch 287/300 (LR 0.00046) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-31 16:05:53,622 [podnet.py] => Task 0, Epoch 288/300 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:05:56,539 [podnet.py] => Task 0, Epoch 289/300 (LR 0.00033) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-31 16:05:58,759 [podnet.py] => Task 0, Epoch 290/300 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-31 16:06:01,535 [podnet.py] => Task 0, Epoch 291/300 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-31 16:06:04,441 [podnet.py] => Task 0, Epoch 292/300 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:06:06,768 [podnet.py] => Task 0, Epoch 293/300 (LR 0.00013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-31 16:06:09,464 [podnet.py] => Task 0, Epoch 294/300 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:06:12,161 [podnet.py] => Task 0, Epoch 295/300 (LR 0.00007) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-31 16:06:14,503 [podnet.py] => Task 0, Epoch 296/300 (LR 0.00004) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-08-31 16:06:17,286 [podnet.py] => Task 0, Epoch 297/300 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.87
2024-08-31 16:06:19,290 [podnet.py] => Task 0, Epoch 298/300 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-31 16:06:22,179 [podnet.py] => Task 0, Epoch 299/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-31 16:06:24,473 [podnet.py] => Task 0, Epoch 300/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.90
2024-08-31 16:06:24,721 [base.py] => Reducing exemplars...(100 per classes)
2024-08-31 16:06:24,721 [base.py] => Constructing exemplars...(100 per classes)
2024-08-31 16:06:29,145 [podnet.py] => Exemplar size: 500
2024-08-31 16:06:29,146 [trainer.py] => CNN: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-31 16:06:29,146 [trainer.py] => NME: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-31 16:06:29,146 [trainer.py] => CNN top1 curve: [88.9]
2024-08-31 16:06:29,146 [trainer.py] => CNN top5 curve: [100.0]
2024-08-31 16:06:29,146 [trainer.py] => NME top1 curve: [88.9]
2024-08-31 16:06:29,146 [trainer.py] => NME top5 curve: [100.0]

2024-08-31 16:06:29,146 [trainer.py] => Average Accuracy (CNN): 88.9
2024-08-31 16:06:29,146 [trainer.py] => Average Accuracy (NME): 88.9
2024-08-31 16:06:29,146 [trainer.py] => All params: 3869505
2024-08-31 16:06:29,146 [trainer.py] => Trainable params: 3869505
2024-08-31 16:06:29,147 [podnet.py] => Learning on 5-7
2024-08-31 16:06:29,162 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-31 16:06:31,627 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 1.14, Spatial_loss 0.33, Flat_loss 0.77, Train_acc 72.09, Test_acc 28.14
2024-08-31 16:06:33,305 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.62, Spatial_loss 0.28, Flat_loss 0.58, Train_acc 85.33, Test_acc 36.50
2024-08-31 16:06:35,587 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.50, Spatial_loss 0.29, Flat_loss 0.52, Train_acc 87.02, Test_acc 46.86
2024-08-31 16:06:37,786 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.38, Spatial_loss 0.26, Flat_loss 0.46, Train_acc 90.76, Test_acc 30.29
2024-08-31 16:06:39,785 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.29, Spatial_loss 0.24, Flat_loss 0.42, Train_acc 93.53, Test_acc 41.12
2024-08-31 16:06:42,086 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.24, Spatial_loss 0.22, Flat_loss 0.39, Train_acc 94.56, Test_acc 33.71
2024-08-31 16:06:44,263 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.20, Spatial_loss 0.22, Flat_loss 0.37, Train_acc 95.20, Test_acc 51.95
2024-08-31 16:06:46,574 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.16, Spatial_loss 0.21, Flat_loss 0.35, Train_acc 96.33, Test_acc 56.74
2024-08-31 16:06:48,752 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.20, Flat_loss 0.33, Train_acc 96.80, Test_acc 54.98
2024-08-31 16:06:50,767 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.14, Spatial_loss 0.20, Flat_loss 0.34, Train_acc 97.07, Test_acc 56.05
2024-08-31 16:06:52,962 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.12, Spatial_loss 0.20, Flat_loss 0.32, Train_acc 97.49, Test_acc 61.81
2024-08-31 16:06:55,138 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.14, Spatial_loss 0.20, Flat_loss 0.33, Train_acc 96.18, Test_acc 52.67
2024-08-31 16:06:57,226 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.13, Spatial_loss 0.21, Flat_loss 0.33, Train_acc 97.20, Test_acc 45.36
2024-08-31 16:06:59,383 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.09, Spatial_loss 0.19, Flat_loss 0.30, Train_acc 98.60, Test_acc 58.05
2024-08-31 16:07:01,606 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.17, Flat_loss 0.28, Train_acc 99.33, Test_acc 62.19
2024-08-31 16:07:03,836 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.06, Spatial_loss 0.16, Flat_loss 0.26, Train_acc 99.73, Test_acc 58.29
2024-08-31 16:07:06,037 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.06, Spatial_loss 0.15, Flat_loss 0.25, Train_acc 99.73, Test_acc 63.02
2024-08-31 16:07:08,233 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.58, Test_acc 61.71
2024-08-31 16:07:10,344 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 98.51, Test_acc 58.71
2024-08-31 16:07:12,609 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.09, Spatial_loss 0.18, Flat_loss 0.29, Train_acc 98.47, Test_acc 60.10
2024-08-31 16:07:13,958 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.10, Spatial_loss 0.20, Flat_loss 0.31, Train_acc 98.02, Test_acc 51.45
2024-08-31 16:07:16,223 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.19, Flat_loss 0.29, Train_acc 98.44, Test_acc 51.10
2024-08-31 16:07:18,412 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.10, Spatial_loss 0.20, Flat_loss 0.30, Train_acc 97.91, Test_acc 54.50
2024-08-31 16:07:20,729 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.07, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 99.00, Test_acc 51.05
2024-08-31 16:07:22,938 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.25, Train_acc 99.64, Test_acc 63.69
2024-08-31 16:07:25,093 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.25, Train_acc 99.73, Test_acc 65.38
2024-08-31 16:07:27,261 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.22, Train_acc 99.89, Test_acc 62.40
2024-08-31 16:07:29,435 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.56, Test_acc 62.50
2024-08-31 16:07:31,724 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.23, Train_acc 99.71, Test_acc 69.98
2024-08-31 16:07:33,882 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.84, Test_acc 67.31
2024-08-31 16:07:35,582 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.82, Test_acc 65.24
2024-08-31 16:07:37,445 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.76, Test_acc 61.02
2024-08-31 16:07:39,571 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 99.80, Test_acc 64.86
2024-08-31 16:07:41,780 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 99.82, Test_acc 66.79
2024-08-31 16:07:44,080 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.89, Test_acc 66.31
2024-08-31 16:07:46,139 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.93, Test_acc 62.10
2024-08-31 16:07:48,344 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.87, Test_acc 63.19
2024-08-31 16:07:50,416 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.91, Test_acc 60.43
2024-08-31 16:07:51,604 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.89, Test_acc 68.62
2024-08-31 16:07:53,736 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.78, Test_acc 59.48
2024-08-31 16:07:55,802 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.84, Test_acc 64.43
2024-08-31 16:07:57,848 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.76, Test_acc 60.88
2024-08-31 16:07:59,943 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.67, Test_acc 66.02
2024-08-31 16:08:02,239 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.82, Test_acc 69.60
2024-08-31 16:08:04,382 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.69, Test_acc 67.17
2024-08-31 16:08:06,518 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.89, Test_acc 62.98
2024-08-31 16:08:08,595 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.89, Test_acc 66.74
2024-08-31 16:08:10,648 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.04, Test_acc 63.40
2024-08-31 16:08:12,709 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.20, Spatial_loss 0.19, Flat_loss 0.30, Train_acc 95.80, Test_acc 19.24
2024-08-31 16:08:14,551 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.51, Spatial_loss 0.30, Flat_loss 0.50, Train_acc 86.98, Test_acc 23.60
2024-08-31 16:08:16,744 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.19, Spatial_loss 0.24, Flat_loss 0.37, Train_acc 94.91, Test_acc 48.24
2024-08-31 16:08:18,874 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.16, Spatial_loss 0.24, Flat_loss 0.37, Train_acc 95.73, Test_acc 50.95
2024-08-31 16:08:20,995 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.09, Spatial_loss 0.20, Flat_loss 0.30, Train_acc 98.16, Test_acc 62.31
2024-08-31 16:08:23,095 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.08, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 98.80, Test_acc 60.62
2024-08-31 16:08:25,361 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.06, Spatial_loss 0.17, Flat_loss 0.25, Train_acc 99.47, Test_acc 61.24
2024-08-31 16:08:27,489 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 99.87, Test_acc 72.02
2024-08-31 16:08:29,698 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 68.95
2024-08-31 16:08:31,637 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 99.91, Test_acc 66.17
2024-08-31 16:08:33,605 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.98, Test_acc 66.07
2024-08-31 16:08:35,665 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.18, Train_acc 99.98, Test_acc 67.33
2024-08-31 16:08:37,713 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.91, Test_acc 67.88
2024-08-31 16:08:39,786 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.91, Test_acc 66.81
2024-08-31 16:08:41,780 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.93, Test_acc 70.19
2024-08-31 16:08:43,934 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.17, Train_acc 99.93, Test_acc 67.38
2024-08-31 16:08:46,087 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.17, Train_acc 99.93, Test_acc 67.48
2024-08-31 16:08:48,094 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.89, Test_acc 66.36
2024-08-31 16:08:50,196 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.91, Test_acc 68.00
2024-08-31 16:08:52,287 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.93, Test_acc 64.05
2024-08-31 16:08:54,398 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.93, Test_acc 70.83
2024-08-31 16:08:56,414 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.96, Test_acc 69.71
2024-08-31 16:08:58,517 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.93, Test_acc 67.64
2024-08-31 16:09:00,576 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.93, Test_acc 69.40
2024-08-31 16:09:02,787 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 69.12
2024-08-31 16:09:04,868 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.96, Test_acc 65.98
2024-08-31 16:09:07,094 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.98, Test_acc 68.95
2024-08-31 16:09:09,174 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.98, Test_acc 68.17
2024-08-31 16:09:10,764 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.91, Test_acc 68.83
2024-08-31 16:09:12,788 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.98, Test_acc 69.00
2024-08-31 16:09:14,726 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.82, Test_acc 58.45
2024-08-31 16:09:16,809 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.67, Test_acc 66.57
2024-08-31 16:09:19,005 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.93, Test_acc 64.95
2024-08-31 16:09:20,818 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.87, Test_acc 68.24
2024-08-31 16:09:22,715 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.89, Test_acc 69.02
2024-08-31 16:09:24,980 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.15, Train_acc 99.96, Test_acc 69.17
2024-08-31 16:09:26,767 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.14, Train_acc 99.93, Test_acc 66.71
2024-08-31 16:09:28,886 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 69.24
2024-08-31 16:09:31,145 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.93, Test_acc 68.74
2024-08-31 16:09:33,262 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.15, Train_acc 99.98, Test_acc 68.12
2024-08-31 16:09:35,350 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.82, Test_acc 68.90
2024-08-31 16:09:37,550 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.42, Test_acc 68.24
2024-08-31 16:09:39,693 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.09, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 98.33, Test_acc 41.93
2024-08-31 16:09:41,850 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.20, Spatial_loss 0.21, Flat_loss 0.33, Train_acc 94.98, Test_acc 27.38
2024-08-31 16:09:44,132 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.43, Spatial_loss 0.30, Flat_loss 0.49, Train_acc 89.71, Test_acc 43.36
2024-08-31 16:09:46,112 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.25, Spatial_loss 0.25, Flat_loss 0.41, Train_acc 93.20, Test_acc 57.19
2024-08-31 16:09:48,369 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.13, Spatial_loss 0.22, Flat_loss 0.34, Train_acc 96.98, Test_acc 57.98
2024-08-31 16:09:50,526 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.07, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 98.98, Test_acc 65.81
2024-08-31 16:09:52,725 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.07, Spatial_loss 0.17, Flat_loss 0.26, Train_acc 98.84, Test_acc 65.93
2024-08-31 16:09:54,903 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 99.76, Test_acc 66.45
2024-08-31 16:09:57,215 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 99.93, Test_acc 64.86
2024-08-31 16:09:59,359 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.19, Train_acc 99.87, Test_acc 66.38
2024-08-31 16:10:01,252 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.91, Test_acc 68.81
2024-08-31 16:10:03,353 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.87, Test_acc 67.05
2024-08-31 16:10:05,617 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.91, Test_acc 65.57
2024-08-31 16:10:07,866 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.17, Train_acc 99.93, Test_acc 66.12
2024-08-31 16:10:10,018 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.87, Test_acc 72.83
2024-08-31 16:10:12,305 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.17, Train_acc 99.67, Test_acc 68.38
2024-08-31 16:10:14,523 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.93, Test_acc 68.60
2024-08-31 16:10:16,790 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.96, Test_acc 65.67
2024-08-31 16:10:18,969 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 70.33
2024-08-31 16:10:21,039 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 100.00, Test_acc 68.64
2024-08-31 16:10:23,373 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 66.19
2024-08-31 16:10:25,599 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.67, Test_acc 65.55
2024-08-31 16:10:27,941 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.96, Test_acc 69.17
2024-08-31 16:10:30,088 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.98, Test_acc 70.17
2024-08-31 16:10:32,298 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 68.38
2024-08-31 16:10:33,795 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 69.93
2024-08-31 16:10:35,870 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.14, Train_acc 100.00, Test_acc 66.76
2024-08-31 16:10:37,999 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.98, Test_acc 67.74
2024-08-31 16:10:40,166 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.14, Train_acc 99.96, Test_acc 66.98
2024-08-31 16:10:42,236 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 100.00, Test_acc 70.60
2024-08-31 16:10:44,269 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.98, Test_acc 67.31
2024-08-31 16:10:46,165 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.69
2024-08-31 16:10:47,780 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.96, Test_acc 67.71
2024-08-31 16:10:49,958 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.71
2024-08-31 16:10:52,244 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.22, Train_acc 99.02, Test_acc 60.79
2024-08-31 16:10:54,184 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.06, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 98.93, Test_acc 66.45
2024-08-31 16:10:56,328 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.71, Test_acc 69.26
2024-08-31 16:10:58,375 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.89, Test_acc 67.36
2024-08-31 16:11:00,266 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.93, Test_acc 64.45
2024-08-31 16:11:02,315 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.96, Test_acc 66.79
2024-08-31 16:11:03,570 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.14, Train_acc 99.96, Test_acc 69.24
2024-08-31 16:11:05,683 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.14, Train_acc 99.89, Test_acc 64.29
2024-08-31 16:11:07,425 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 99.16, Test_acc 65.48
2024-08-31 16:11:09,549 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.18, Train_acc 99.80, Test_acc 64.71
2024-08-31 16:11:11,102 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.16, Train_acc 99.91, Test_acc 66.14
2024-08-31 16:11:12,624 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.17, Train_acc 99.82, Test_acc 66.50
2024-08-31 16:11:14,007 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.93, Test_acc 67.64
2024-08-31 16:11:15,334 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 100.00, Test_acc 69.10
2024-08-31 16:11:16,625 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 100.00, Test_acc 68.79
2024-08-31 16:11:17,960 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.98, Test_acc 67.95
2024-08-31 16:11:19,967 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.96, Test_acc 69.57
2024-08-31 16:11:21,950 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.38
2024-08-31 16:11:23,867 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.12
2024-08-31 16:11:25,697 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.33
2024-08-31 16:11:27,629 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.18, Train_acc 99.67, Test_acc 63.33
2024-08-31 16:11:29,520 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.15, Train_acc 99.96, Test_acc 68.02
2024-08-31 16:11:30,907 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.93, Test_acc 65.05
2024-08-31 16:11:32,307 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.14, Train_acc 99.98, Test_acc 69.57
2024-08-31 16:11:34,023 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.33
2024-08-31 16:11:35,829 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.86
2024-08-31 16:11:37,664 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.10
2024-08-31 16:11:39,303 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.31
2024-08-31 16:11:41,258 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.48
2024-08-31 16:11:42,916 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.81
2024-08-31 16:11:44,829 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.07
2024-08-31 16:11:46,776 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.31
2024-08-31 16:11:48,792 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.69
2024-08-31 16:11:50,660 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.62
2024-08-31 16:11:52,607 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.74
2024-08-31 16:11:54,544 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.48
2024-08-31 16:11:56,063 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.88
2024-08-31 16:11:57,796 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.98
2024-08-31 16:11:59,856 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.21
2024-08-31 16:12:01,801 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.31
2024-08-31 16:12:03,494 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.55
2024-08-31 16:12:04,998 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.15, Train_acc 99.82, Test_acc 65.48
2024-08-31 16:12:06,970 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.13, Train_acc 99.96, Test_acc 68.86
2024-08-31 16:12:08,859 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.24
2024-08-31 16:12:10,371 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.50
2024-08-31 16:12:12,429 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 99.98, Test_acc 69.79
2024-08-31 16:12:14,416 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 99.98, Test_acc 69.10
2024-08-31 16:12:16,180 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.93
2024-08-31 16:12:18,302 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.13, Train_acc 99.96, Test_acc 68.43
2024-08-31 16:12:20,024 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.64
2024-08-31 16:12:21,523 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 99.98, Test_acc 69.69
2024-08-31 16:12:23,597 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.81
2024-08-31 16:12:25,787 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.98
2024-08-31 16:12:27,950 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.69
2024-08-31 16:12:29,909 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.93
2024-08-31 16:12:31,966 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.95
2024-08-31 16:12:34,130 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.26
2024-08-31 16:12:35,400 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.17
2024-08-31 16:12:36,902 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 99.98, Test_acc 70.45
2024-08-31 16:12:38,726 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.33
2024-08-31 16:12:40,445 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.60
2024-08-31 16:12:42,314 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.14
2024-08-31 16:12:44,291 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 99.96, Test_acc 69.07
2024-08-31 16:12:46,267 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.00
2024-08-31 16:12:47,888 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.50
2024-08-31 16:12:49,565 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.81
2024-08-31 16:12:51,351 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.36
2024-08-31 16:12:53,351 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.81
2024-08-31 16:12:54,553 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.50
2024-08-31 16:12:56,270 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.69
2024-08-31 16:12:58,287 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.60
2024-08-31 16:13:00,236 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 99.98, Test_acc 70.31
2024-08-31 16:13:02,262 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 99.96, Test_acc 69.07
2024-08-31 16:13:04,076 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.74
2024-08-31 16:13:05,651 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.52
2024-08-31 16:13:07,194 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.93
2024-08-31 16:13:09,155 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.14
2024-08-31 16:13:10,982 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.36
2024-08-31 16:13:12,628 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.17
2024-08-31 16:13:14,319 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.24
2024-08-31 16:13:16,088 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 99.96, Test_acc 68.00
2024-08-31 16:13:17,774 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.26
2024-08-31 16:13:19,453 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.48
2024-08-31 16:13:20,854 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.45
2024-08-31 16:13:22,415 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.07
2024-08-31 16:13:23,792 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.00
2024-08-31 16:13:25,327 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.81
2024-08-31 16:13:26,918 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.26
2024-08-31 16:13:28,688 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.57
2024-08-31 16:13:30,360 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.29
2024-08-31 16:13:32,094 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.55
2024-08-31 16:13:33,800 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.33
2024-08-31 16:13:35,720 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.02
2024-08-31 16:13:37,624 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.02
2024-08-31 16:13:39,326 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.33
2024-08-31 16:13:41,139 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.95
2024-08-31 16:13:42,824 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.45
2024-08-31 16:13:44,338 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.07
2024-08-31 16:13:45,970 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.00
2024-08-31 16:13:47,492 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.74
2024-08-31 16:13:49,082 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.55
2024-08-31 16:13:50,999 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.50
2024-08-31 16:13:52,881 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.98
2024-08-31 16:13:54,428 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.81
2024-08-31 16:13:56,182 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.48
2024-08-31 16:13:57,599 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.62
2024-08-31 16:13:59,385 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.17
2024-08-31 16:14:01,349 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.29
2024-08-31 16:14:02,929 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.07
2024-08-31 16:14:04,891 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.26
2024-08-31 16:14:06,460 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.05
2024-08-31 16:14:08,588 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.69
2024-08-31 16:14:10,491 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.57
2024-08-31 16:14:12,546 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.36
2024-08-31 16:14:14,438 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-08-31 16:14:16,426 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-31 16:14:18,188 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.19
2024-08-31 16:14:19,832 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 99.98, Test_acc 71.67
2024-08-31 16:14:21,445 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.05, Flat_loss 0.11, Train_acc 99.93, Test_acc 70.55
2024-08-31 16:14:23,037 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.86
2024-08-31 16:14:24,617 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-08-31 16:14:26,156 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.19
2024-08-31 16:14:27,671 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-31 16:14:29,235 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.52
2024-08-31 16:14:30,819 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-08-31 16:14:32,232 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-08-31 16:14:33,965 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-31 16:14:35,683 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.17
2024-08-31 16:14:37,301 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.19
2024-08-31 16:14:38,837 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.93
2024-08-31 16:14:40,543 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.76
2024-08-31 16:14:42,266 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.98
2024-08-31 16:14:43,857 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-31 16:14:45,175 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-31 16:14:46,714 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.95
2024-08-31 16:14:48,149 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.74
2024-08-31 16:14:49,768 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-31 16:14:51,284 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.79
2024-08-31 16:14:52,900 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.03, Spatial_loss 0.05, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.21
2024-08-31 16:14:54,565 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-31 16:14:56,155 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.38
2024-08-31 16:14:57,846 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.33
2024-08-31 16:14:59,272 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.31
2024-08-31 16:15:01,188 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-31 16:15:03,276 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.10
2024-08-31 16:15:04,910 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-31 16:15:06,680 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.17
2024-08-31 16:15:08,063 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.76
2024-08-31 16:15:09,951 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.00
2024-08-31 16:15:11,721 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.52
2024-08-31 16:15:13,463 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.71
2024-08-31 16:15:15,063 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-31 16:15:16,754 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-08-31 16:15:18,314 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-31 16:15:20,013 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.45
2024-08-31 16:15:21,839 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.69
2024-08-31 16:15:23,894 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.86
2024-08-31 16:15:25,864 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-31 16:15:27,548 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.98
2024-08-31 16:15:29,575 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-31 16:15:31,669 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 99.98, Test_acc 70.45
2024-08-31 16:15:33,690 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.19
2024-08-31 16:15:35,531 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-31 16:15:37,452 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-31 16:15:39,307 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.74
2024-08-31 16:15:41,114 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.79
2024-08-31 16:15:42,485 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.64
2024-08-31 16:15:44,407 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-31 16:15:46,409 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.98
2024-08-31 16:15:48,217 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-31 16:15:50,123 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.86
2024-08-31 16:15:51,927 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.55
2024-08-31 16:15:53,951 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.81
2024-08-31 16:15:55,872 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.69
2024-08-31 16:15:57,781 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-31 16:15:59,455 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.04, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.86
2024-08-31 16:15:59,800 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-31 16:15:59,800 [base.py] => Reducing exemplars...(100 per classes)
2024-08-31 16:16:00,507 [base.py] => Constructing exemplars...(100 per classes)
2024-08-31 16:16:02,160 [base.py] => Reducing exemplars...(71 per classes)
2024-08-31 16:16:02,911 [base.py] => Constructing exemplars...(71 per classes)
2024-08-31 16:16:04,968 [podnet.py] => Exemplar size: 497
2024-08-31 16:16:04,968 [trainer.py] => CNN: {'total': 70.86, '00-04': 61.07, '05-06': 95.33, 'old': 61.07, 'new': 95.33}
2024-08-31 16:16:04,968 [trainer.py] => NME: {'total': 75.57, '00-04': 76.4, '05-06': 73.5, 'old': 76.4, 'new': 73.5}
2024-08-31 16:16:04,968 [trainer.py] => CNN top1 curve: [88.9, 70.86]
2024-08-31 16:16:04,968 [trainer.py] => CNN top5 curve: [100.0, 97.98]
2024-08-31 16:16:04,968 [trainer.py] => NME top1 curve: [88.9, 75.57]
2024-08-31 16:16:04,968 [trainer.py] => NME top5 curve: [100.0, 97.98]

2024-08-31 16:16:04,968 [trainer.py] => Average Accuracy (CNN): 79.88
2024-08-31 16:16:04,968 [trainer.py] => Average Accuracy (NME): 82.235
2024-08-31 16:16:04,969 [trainer.py] => All params: 3879745
2024-08-31 16:16:04,969 [trainer.py] => Trainable params: 3879745
2024-08-31 16:16:04,970 [podnet.py] => Learning on 7-9
2024-08-31 16:16:04,993 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-31 16:16:06,946 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 1.51, Spatial_loss 0.38, Flat_loss 1.17, Train_acc 75.83, Test_acc 17.67
2024-08-31 16:16:08,730 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.74, Spatial_loss 0.34, Flat_loss 0.89, Train_acc 87.26, Test_acc 17.57
2024-08-31 16:16:09,978 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.68, Spatial_loss 0.34, Flat_loss 0.74, Train_acc 87.46, Test_acc 29.22
2024-08-31 16:16:11,987 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.59, Spatial_loss 0.31, Flat_loss 0.65, Train_acc 89.08, Test_acc 26.24
2024-08-31 16:16:13,795 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.57, Spatial_loss 0.30, Flat_loss 0.62, Train_acc 88.64, Test_acc 21.63
2024-08-31 16:16:15,744 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.46, Spatial_loss 0.28, Flat_loss 0.56, Train_acc 90.68, Test_acc 32.54
2024-08-31 16:16:17,358 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.48, Spatial_loss 0.30, Flat_loss 0.55, Train_acc 89.88, Test_acc 21.80
2024-08-31 16:16:18,821 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.39, Spatial_loss 0.28, Flat_loss 0.52, Train_acc 91.75, Test_acc 43.80
2024-08-31 16:16:20,558 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.30, Spatial_loss 0.26, Flat_loss 0.48, Train_acc 93.48, Test_acc 35.87
2024-08-31 16:16:22,321 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.25, Spatial_loss 0.25, Flat_loss 0.46, Train_acc 94.73, Test_acc 40.13
2024-08-31 16:16:23,708 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.22, Spatial_loss 0.25, Flat_loss 0.44, Train_acc 95.17, Test_acc 46.93
2024-08-31 16:16:25,752 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.24, Spatial_loss 0.26, Flat_loss 0.45, Train_acc 94.77, Test_acc 36.96
2024-08-31 16:16:27,765 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.21, Spatial_loss 0.25, Flat_loss 0.44, Train_acc 95.64, Test_acc 49.69
2024-08-31 16:16:29,692 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.21, Spatial_loss 0.26, Flat_loss 0.44, Train_acc 95.77, Test_acc 49.91
2024-08-31 16:16:31,478 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.17, Spatial_loss 0.25, Flat_loss 0.42, Train_acc 96.73, Test_acc 47.69
2024-08-31 16:16:33,177 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.16, Spatial_loss 0.25, Flat_loss 0.41, Train_acc 96.73, Test_acc 49.63
2024-08-31 16:16:34,623 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.13, Spatial_loss 0.24, Flat_loss 0.40, Train_acc 97.71, Test_acc 57.15
2024-08-31 16:16:36,591 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.14, Spatial_loss 0.24, Flat_loss 0.39, Train_acc 97.31, Test_acc 51.39
2024-08-31 16:16:38,546 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.20, Spatial_loss 0.26, Flat_loss 0.43, Train_acc 95.51, Test_acc 49.78
2024-08-31 16:16:40,414 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.13, Spatial_loss 0.24, Flat_loss 0.39, Train_acc 97.53, Test_acc 48.80
2024-08-31 16:16:42,317 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.10, Spatial_loss 0.23, Flat_loss 0.37, Train_acc 98.64, Test_acc 54.30
2024-08-31 16:16:44,334 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.21, Flat_loss 0.35, Train_acc 99.53, Test_acc 58.11
2024-08-31 16:16:46,103 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.20, Flat_loss 0.33, Train_acc 99.87, Test_acc 54.91
2024-08-31 16:16:47,790 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.19, Flat_loss 0.31, Train_acc 99.91, Test_acc 56.89
2024-08-31 16:16:49,420 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.18, Flat_loss 0.31, Train_acc 99.93, Test_acc 56.89
2024-08-31 16:16:51,358 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.07, Spatial_loss 0.19, Flat_loss 0.31, Train_acc 99.69, Test_acc 55.94
2024-08-31 16:16:52,867 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.06, Spatial_loss 0.18, Flat_loss 0.30, Train_acc 99.91, Test_acc 59.80
2024-08-31 16:16:54,926 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.17, Flat_loss 0.29, Train_acc 99.82, Test_acc 56.56
2024-08-31 16:16:57,045 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.29, Train_acc 99.98, Test_acc 57.44
2024-08-31 16:16:58,709 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.28, Train_acc 99.98, Test_acc 57.91
2024-08-31 16:17:00,473 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 100.00, Test_acc 57.00
2024-08-31 16:17:02,160 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 99.96, Test_acc 58.63
2024-08-31 16:17:04,233 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.06, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 99.96, Test_acc 56.96
2024-08-31 16:17:06,085 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.06, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 99.80, Test_acc 58.74
2024-08-31 16:17:08,000 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 99.93, Test_acc 59.20
2024-08-31 16:17:09,670 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.06, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 99.76, Test_acc 57.69
2024-08-31 16:17:11,554 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.15, Spatial_loss 0.21, Flat_loss 0.34, Train_acc 96.80, Test_acc 35.07
2024-08-31 16:17:12,862 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.29, Spatial_loss 0.27, Flat_loss 0.42, Train_acc 93.62, Test_acc 27.67
2024-08-31 16:17:14,644 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.30, Spatial_loss 0.28, Flat_loss 0.43, Train_acc 93.15, Test_acc 44.09
2024-08-31 16:17:16,813 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.15, Spatial_loss 0.24, Flat_loss 0.37, Train_acc 96.93, Test_acc 47.24
2024-08-31 16:17:18,703 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.10, Spatial_loss 0.22, Flat_loss 0.35, Train_acc 98.15, Test_acc 50.20
2024-08-31 16:17:20,301 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.07, Spatial_loss 0.20, Flat_loss 0.31, Train_acc 99.47, Test_acc 57.31
2024-08-31 16:17:21,954 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.29, Train_acc 99.91, Test_acc 58.28
2024-08-31 16:17:23,487 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.28, Train_acc 99.96, Test_acc 56.98
2024-08-31 16:17:25,031 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.27, Train_acc 99.98, Test_acc 59.44
2024-08-31 16:17:26,264 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.26, Train_acc 100.00, Test_acc 60.72
2024-08-31 16:17:27,885 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.26, Train_acc 99.93, Test_acc 59.13
2024-08-31 16:17:29,446 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.25, Train_acc 99.96, Test_acc 59.83
2024-08-31 16:17:31,017 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.25, Train_acc 100.00, Test_acc 60.35
2024-08-31 16:17:32,984 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.24, Train_acc 99.98, Test_acc 60.19
2024-08-31 16:17:34,899 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.24, Train_acc 99.98, Test_acc 59.57
2024-08-31 16:17:36,867 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.24, Train_acc 99.98, Test_acc 60.20
2024-08-31 16:17:38,802 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.24, Train_acc 99.91, Test_acc 61.61
2024-08-31 16:17:40,552 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.24, Train_acc 99.93, Test_acc 58.56
2024-08-31 16:17:42,328 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.23, Train_acc 100.00, Test_acc 59.89
2024-08-31 16:17:44,364 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.23, Train_acc 99.98, Test_acc 60.61
2024-08-31 16:17:46,379 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 100.00, Test_acc 61.09
2024-08-31 16:17:48,331 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.23, Train_acc 99.93, Test_acc 60.35
2024-08-31 16:17:49,979 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.23, Train_acc 99.98, Test_acc 61.07
2024-08-31 16:17:51,842 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.23, Train_acc 99.96, Test_acc 61.67
2024-08-31 16:17:53,899 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.22, Train_acc 99.98, Test_acc 59.31
2024-08-31 16:17:55,765 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.22, Train_acc 99.98, Test_acc 54.56
2024-08-31 16:17:57,846 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.22, Train_acc 99.98, Test_acc 61.07
2024-08-31 16:17:59,903 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.22, Train_acc 100.00, Test_acc 61.11
2024-08-31 16:18:01,923 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 100.00, Test_acc 62.15
2024-08-31 16:18:03,512 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 99.93, Test_acc 62.19
2024-08-31 16:18:04,736 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.22, Train_acc 99.89, Test_acc 55.39
2024-08-31 16:18:06,634 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.24, Train_acc 99.91, Test_acc 61.56
2024-08-31 16:18:08,484 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.22, Train_acc 99.96, Test_acc 58.06
2024-08-31 16:18:10,113 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 100.00, Test_acc 61.37
2024-08-31 16:18:11,924 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.21, Train_acc 99.98, Test_acc 61.46
2024-08-31 16:18:13,493 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.98, Test_acc 59.91
2024-08-31 16:18:15,259 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.21, Train_acc 99.98, Test_acc 59.80
2024-08-31 16:18:17,202 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.96, Test_acc 59.87
2024-08-31 16:18:19,141 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.96, Test_acc 60.93
2024-08-31 16:18:20,962 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 59.89
2024-08-31 16:18:22,500 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.98, Test_acc 58.54
2024-08-31 16:18:24,455 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 99.98, Test_acc 62.22
2024-08-31 16:18:26,383 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.44
2024-08-31 16:18:28,508 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.41
2024-08-31 16:18:30,275 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 58.67
2024-08-31 16:18:31,889 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 100.00, Test_acc 62.09
2024-08-31 16:18:33,628 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.89, Test_acc 59.30
2024-08-31 16:18:35,347 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.14, Spatial_loss 0.15, Flat_loss 0.26, Train_acc 98.13, Test_acc 17.17
2024-08-31 16:18:37,091 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.99, Spatial_loss 0.40, Flat_loss 0.72, Train_acc 84.01, Test_acc 13.02
2024-08-31 16:18:38,628 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.67, Spatial_loss 0.34, Flat_loss 0.55, Train_acc 88.64, Test_acc 15.59
2024-08-31 16:18:40,567 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.52, Spatial_loss 0.31, Flat_loss 0.50, Train_acc 89.50, Test_acc 37.78
2024-08-31 16:18:42,208 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.42, Spatial_loss 0.29, Flat_loss 0.47, Train_acc 91.57, Test_acc 37.46
2024-08-31 16:18:43,812 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.37, Spatial_loss 0.29, Flat_loss 0.45, Train_acc 91.86, Test_acc 22.96
2024-08-31 16:18:45,323 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.31, Spatial_loss 0.28, Flat_loss 0.42, Train_acc 93.40, Test_acc 33.46
2024-08-31 16:18:46,783 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.22, Spatial_loss 0.26, Flat_loss 0.40, Train_acc 95.22, Test_acc 51.80
2024-08-31 16:18:48,263 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.15, Spatial_loss 0.24, Flat_loss 0.36, Train_acc 97.24, Test_acc 49.06
2024-08-31 16:18:49,772 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.13, Spatial_loss 0.24, Flat_loss 0.36, Train_acc 97.62, Test_acc 46.94
2024-08-31 16:18:51,804 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.11, Spatial_loss 0.23, Flat_loss 0.34, Train_acc 98.55, Test_acc 55.70
2024-08-31 16:18:53,611 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.09, Spatial_loss 0.22, Flat_loss 0.33, Train_acc 98.89, Test_acc 52.06
2024-08-31 16:18:55,136 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.07, Spatial_loss 0.20, Flat_loss 0.31, Train_acc 99.58, Test_acc 57.41
2024-08-31 16:18:57,087 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.06, Spatial_loss 0.19, Flat_loss 0.29, Train_acc 99.93, Test_acc 56.96
2024-08-31 16:18:58,889 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.06, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 99.87, Test_acc 56.07
2024-08-31 16:19:01,026 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.07, Spatial_loss 0.18, Flat_loss 0.29, Train_acc 99.62, Test_acc 55.37
2024-08-31 16:19:02,861 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.11, Spatial_loss 0.20, Flat_loss 0.31, Train_acc 98.35, Test_acc 39.89
2024-08-31 16:19:04,898 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.09, Spatial_loss 0.20, Flat_loss 0.31, Train_acc 99.09, Test_acc 53.94
2024-08-31 16:19:06,722 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 99.76, Test_acc 57.11
2024-08-31 16:19:08,545 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.06, Spatial_loss 0.17, Flat_loss 0.27, Train_acc 99.84, Test_acc 55.19
2024-08-31 16:19:10,582 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.06, Spatial_loss 0.17, Flat_loss 0.26, Train_acc 99.91, Test_acc 54.50
2024-08-31 16:19:12,655 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.25, Train_acc 99.87, Test_acc 58.54
2024-08-31 16:19:14,493 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 100.00, Test_acc 57.17
2024-08-31 16:19:16,293 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.06, Spatial_loss 0.16, Flat_loss 0.25, Train_acc 99.87, Test_acc 58.44
2024-08-31 16:19:18,080 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.98, Test_acc 56.02
2024-08-31 16:19:20,060 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.24, Train_acc 99.98, Test_acc 59.93
2024-08-31 16:19:21,794 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.23, Train_acc 99.98, Test_acc 59.19
2024-08-31 16:19:23,640 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.23, Train_acc 100.00, Test_acc 58.69
2024-08-31 16:19:25,823 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.23, Train_acc 99.98, Test_acc 59.33
2024-08-31 16:19:27,039 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 100.00, Test_acc 57.54
2024-08-31 16:19:28,637 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 100.00, Test_acc 58.24
2024-08-31 16:19:30,381 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 99.93, Test_acc 58.28
2024-08-31 16:19:32,200 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 99.96, Test_acc 59.20
2024-08-31 16:19:33,884 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 100.00, Test_acc 58.06
2024-08-31 16:19:35,435 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 100.00, Test_acc 60.26
2024-08-31 16:19:37,188 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.96, Test_acc 60.39
2024-08-31 16:19:38,919 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.96, Test_acc 59.56
2024-08-31 16:19:40,657 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.98, Test_acc 59.85
2024-08-31 16:19:42,787 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.98, Test_acc 60.02
2024-08-31 16:19:44,267 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.98, Test_acc 59.31
2024-08-31 16:19:45,960 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 99.93, Test_acc 43.50
2024-08-31 16:19:47,669 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.45, Spatial_loss 0.28, Flat_loss 0.44, Train_acc 91.02, Test_acc 16.65
2024-08-31 16:19:49,105 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.36, Spatial_loss 0.29, Flat_loss 0.42, Train_acc 92.51, Test_acc 43.69
2024-08-31 16:19:50,740 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.20, Spatial_loss 0.26, Flat_loss 0.37, Train_acc 96.09, Test_acc 55.56
2024-08-31 16:19:52,389 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.12, Spatial_loss 0.23, Flat_loss 0.33, Train_acc 97.53, Test_acc 52.81
2024-08-31 16:19:54,100 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.08, Spatial_loss 0.20, Flat_loss 0.31, Train_acc 99.22, Test_acc 55.89
2024-08-31 16:19:55,576 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.09, Spatial_loss 0.20, Flat_loss 0.30, Train_acc 99.04, Test_acc 35.43
2024-08-31 16:19:57,135 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.12, Spatial_loss 0.23, Flat_loss 0.32, Train_acc 97.91, Test_acc 58.35
2024-08-31 16:19:58,769 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.06, Spatial_loss 0.19, Flat_loss 0.28, Train_acc 99.67, Test_acc 59.43
2024-08-31 16:20:00,155 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.06, Spatial_loss 0.17, Flat_loss 0.25, Train_acc 99.89, Test_acc 58.98
2024-08-31 16:20:02,252 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.07, Spatial_loss 0.17, Flat_loss 0.26, Train_acc 99.69, Test_acc 57.19
2024-08-31 16:20:04,109 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.10, Spatial_loss 0.20, Flat_loss 0.29, Train_acc 98.35, Test_acc 58.72
2024-08-31 16:20:05,844 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.06, Spatial_loss 0.16, Flat_loss 0.26, Train_acc 99.87, Test_acc 59.65
2024-08-31 16:20:07,790 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.98, Test_acc 59.07
2024-08-31 16:20:09,606 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.23, Train_acc 99.98, Test_acc 60.83
2024-08-31 16:20:11,530 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.22, Train_acc 99.98, Test_acc 60.33
2024-08-31 16:20:13,025 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.22, Train_acc 100.00, Test_acc 60.06
2024-08-31 16:20:14,807 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 99.98, Test_acc 59.74
2024-08-31 16:20:16,335 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 99.93, Test_acc 60.33
2024-08-31 16:20:18,384 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 99.98, Test_acc 59.46
2024-08-31 16:20:20,240 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 100.00, Test_acc 60.48
2024-08-31 16:20:21,864 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 100.00, Test_acc 60.22
2024-08-31 16:20:23,702 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.98, Test_acc 60.57
2024-08-31 16:20:25,626 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.50
2024-08-31 16:20:27,440 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 60.35
2024-08-31 16:20:29,052 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.37
2024-08-31 16:20:31,106 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 99.98, Test_acc 59.00
2024-08-31 16:20:32,824 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.24
2024-08-31 16:20:34,739 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.98, Test_acc 60.98
2024-08-31 16:20:36,309 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 58.94
2024-08-31 16:20:38,147 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 62.33
2024-08-31 16:20:40,425 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 60.91
2024-08-31 16:20:42,070 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 60.89
2024-08-31 16:20:43,916 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.02
2024-08-31 16:20:45,792 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 100.00, Test_acc 60.56
2024-08-31 16:20:47,968 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.00
2024-08-31 16:20:49,729 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 99.98, Test_acc 60.67
2024-08-31 16:20:51,062 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 99.98, Test_acc 57.39
2024-08-31 16:20:52,721 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.47, Test_acc 56.19
2024-08-31 16:20:54,599 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.11, Spatial_loss 0.19, Flat_loss 0.30, Train_acc 97.89, Test_acc 52.57
2024-08-31 16:20:56,685 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.09, Spatial_loss 0.19, Flat_loss 0.29, Train_acc 98.87, Test_acc 58.26
2024-08-31 16:20:58,700 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.91, Test_acc 61.52
2024-08-31 16:21:00,311 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 100.00, Test_acc 62.35
2024-08-31 16:21:01,991 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.21, Train_acc 99.98, Test_acc 62.00
2024-08-31 16:21:03,671 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 62.72
2024-08-31 16:21:05,495 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 62.33
2024-08-31 16:21:07,192 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 62.15
2024-08-31 16:21:08,704 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 62.37
2024-08-31 16:21:10,496 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.98, Test_acc 62.61
2024-08-31 16:21:11,804 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.91, Test_acc 60.26
2024-08-31 16:21:13,273 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 62.56
2024-08-31 16:21:14,848 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 100.00, Test_acc 62.20
2024-08-31 16:21:16,681 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.46
2024-08-31 16:21:18,279 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.87
2024-08-31 16:21:20,166 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.39
2024-08-31 16:21:22,090 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.20
2024-08-31 16:21:23,770 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.30
2024-08-31 16:21:25,456 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.41
2024-08-31 16:21:27,326 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.20
2024-08-31 16:21:29,043 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.96
2024-08-31 16:21:30,938 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 99.98, Test_acc 54.81
2024-08-31 16:21:33,030 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.20, Spatial_loss 0.22, Flat_loss 0.33, Train_acc 96.09, Test_acc 47.41
2024-08-31 16:21:34,799 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.14, Spatial_loss 0.22, Flat_loss 0.33, Train_acc 96.93, Test_acc 55.35
2024-08-31 16:21:36,571 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.07, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 98.98, Test_acc 60.30
2024-08-31 16:21:38,658 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.24, Train_acc 99.93, Test_acc 60.63
2024-08-31 16:21:40,155 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.22, Train_acc 99.91, Test_acc 55.50
2024-08-31 16:21:41,941 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.09, Spatial_loss 0.18, Flat_loss 0.28, Train_acc 98.95, Test_acc 58.30
2024-08-31 16:21:43,876 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.23, Train_acc 99.93, Test_acc 61.19
2024-08-31 16:21:45,852 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 100.00, Test_acc 60.52
2024-08-31 16:21:47,467 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.35
2024-08-31 16:21:49,102 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.33
2024-08-31 16:21:50,879 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.31
2024-08-31 16:21:52,985 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.52
2024-08-31 16:21:54,806 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.91
2024-08-31 16:21:56,992 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 62.37
2024-08-31 16:21:58,692 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.44
2024-08-31 16:22:00,621 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.00
2024-08-31 16:22:02,566 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.13
2024-08-31 16:22:04,434 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.19, Train_acc 99.98, Test_acc 61.37
2024-08-31 16:22:06,307 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.13
2024-08-31 16:22:08,294 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.56
2024-08-31 16:22:10,310 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.15
2024-08-31 16:22:12,384 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.74
2024-08-31 16:22:13,881 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.98
2024-08-31 16:22:15,071 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.87
2024-08-31 16:22:17,043 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.19
2024-08-31 16:22:18,798 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.09
2024-08-31 16:22:20,945 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.22
2024-08-31 16:22:22,600 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.54
2024-08-31 16:22:24,278 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.59
2024-08-31 16:22:26,172 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.44
2024-08-31 16:22:27,893 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.28
2024-08-31 16:22:29,337 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.31
2024-08-31 16:22:31,007 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.63
2024-08-31 16:22:33,049 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 99.98, Test_acc 62.56
2024-08-31 16:22:34,737 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 99.98, Test_acc 62.89
2024-08-31 16:22:36,272 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.37
2024-08-31 16:22:37,914 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 99.98, Test_acc 62.30
2024-08-31 16:22:39,616 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:22:41,101 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.83
2024-08-31 16:22:42,737 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:22:44,703 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.52
2024-08-31 16:22:46,676 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.94
2024-08-31 16:22:48,269 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.78
2024-08-31 16:22:50,030 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.28
2024-08-31 16:22:51,943 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.26
2024-08-31 16:22:53,805 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.28
2024-08-31 16:22:55,352 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.44
2024-08-31 16:22:56,852 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.67
2024-08-31 16:22:58,324 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.91
2024-08-31 16:23:00,134 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.50
2024-08-31 16:23:01,418 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.06
2024-08-31 16:23:03,318 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.69
2024-08-31 16:23:04,832 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.89
2024-08-31 16:23:06,395 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.67
2024-08-31 16:23:08,297 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.56
2024-08-31 16:23:10,136 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.94
2024-08-31 16:23:11,705 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.50
2024-08-31 16:23:13,333 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.74
2024-08-31 16:23:15,010 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.30
2024-08-31 16:23:17,053 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.15
2024-08-31 16:23:18,762 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.57
2024-08-31 16:23:20,586 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.81
2024-08-31 16:23:22,445 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.02
2024-08-31 16:23:24,404 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.20
2024-08-31 16:23:26,182 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.65
2024-08-31 16:23:28,036 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.98
2024-08-31 16:23:30,071 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 99.96, Test_acc 61.87
2024-08-31 16:23:31,829 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 99.93, Test_acc 62.48
2024-08-31 16:23:33,703 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.31
2024-08-31 16:23:35,602 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.56
2024-08-31 16:23:37,368 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.65
2024-08-31 16:23:39,300 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.81
2024-08-31 16:23:40,991 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.59
2024-08-31 16:23:42,554 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.59
2024-08-31 16:23:44,262 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.89
2024-08-31 16:23:45,669 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.76
2024-08-31 16:23:47,342 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.31
2024-08-31 16:23:49,143 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:23:50,770 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.91
2024-08-31 16:23:52,456 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.61
2024-08-31 16:23:54,287 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.61
2024-08-31 16:23:56,079 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:23:57,914 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.00
2024-08-31 16:23:59,692 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.74
2024-08-31 16:24:01,653 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.56
2024-08-31 16:24:03,966 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.69
2024-08-31 16:24:05,458 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 99.98, Test_acc 62.93
2024-08-31 16:24:07,441 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.67
2024-08-31 16:24:09,416 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.83
2024-08-31 16:24:11,089 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.67
2024-08-31 16:24:12,809 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.83
2024-08-31 16:24:14,428 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:24:15,846 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.91
2024-08-31 16:24:17,356 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.78
2024-08-31 16:24:19,013 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.17
2024-08-31 16:24:20,560 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.72
2024-08-31 16:24:21,780 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.06
2024-08-31 16:24:23,591 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.65
2024-08-31 16:24:25,470 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.96
2024-08-31 16:24:27,207 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.33
2024-08-31 16:24:28,936 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.07
2024-08-31 16:24:30,393 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.20
2024-08-31 16:24:32,317 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.28
2024-08-31 16:24:33,887 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.76
2024-08-31 16:24:35,863 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.02
2024-08-31 16:24:37,744 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.13
2024-08-31 16:24:39,138 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.87
2024-08-31 16:24:40,822 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.19
2024-08-31 16:24:42,734 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.13
2024-08-31 16:24:44,807 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.17
2024-08-31 16:24:46,856 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.05, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.57
2024-08-31 16:24:48,890 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:24:50,663 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.87
2024-08-31 16:24:52,666 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.26
2024-08-31 16:24:54,380 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.98
2024-08-31 16:24:56,090 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.85
2024-08-31 16:24:56,414 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-31 16:24:56,414 [base.py] => Reducing exemplars...(71 per classes)
2024-08-31 16:24:57,478 [base.py] => Constructing exemplars...(71 per classes)
2024-08-31 16:24:58,949 [base.py] => Reducing exemplars...(55 per classes)
2024-08-31 16:24:59,957 [base.py] => Constructing exemplars...(55 per classes)
2024-08-31 16:25:02,062 [podnet.py] => Exemplar size: 495
2024-08-31 16:25:02,062 [trainer.py] => CNN: {'total': 62.85, '00-04': 55.3, '05-06': 46.5, '07-08': 98.08, 'old': 52.79, 'new': 98.08}
2024-08-31 16:25:02,062 [trainer.py] => NME: {'total': 65.13, '00-04': 68.8, '05-06': 40.42, '07-08': 80.67, 'old': 60.69, 'new': 80.67}
2024-08-31 16:25:02,062 [trainer.py] => CNN top1 curve: [88.9, 70.86, 62.85]
2024-08-31 16:25:02,062 [trainer.py] => CNN top5 curve: [100.0, 97.98, 91.17]
2024-08-31 16:25:02,062 [trainer.py] => NME top1 curve: [88.9, 75.57, 65.13]
2024-08-31 16:25:02,062 [trainer.py] => NME top5 curve: [100.0, 97.98, 93.48]

2024-08-31 16:25:02,062 [trainer.py] => Average Accuracy (CNN): 74.20333333333333
2024-08-31 16:25:02,062 [trainer.py] => Average Accuracy (NME): 76.53333333333333
2024-08-31 16:25:02,063 [trainer.py] => Forgetting (CNN): 41.215
