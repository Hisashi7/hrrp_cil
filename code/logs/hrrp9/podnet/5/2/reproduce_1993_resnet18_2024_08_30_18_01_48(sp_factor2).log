2024-08-30 18:01:48,218 [trainer.py] => config: ./exps/podnet.json
2024-08-30 18:01:48,218 [trainer.py] => prefix: reproduce
2024-08-30 18:01:48,218 [trainer.py] => dataset: hrrp9
2024-08-30 18:01:48,218 [trainer.py] => memory_size: 500
2024-08-30 18:01:48,218 [trainer.py] => memory_per_class: 20
2024-08-30 18:01:48,218 [trainer.py] => fixed_memory: False
2024-08-30 18:01:48,218 [trainer.py] => shuffle: True
2024-08-30 18:01:48,218 [trainer.py] => init_cls: 5
2024-08-30 18:01:48,218 [trainer.py] => increment: 2
2024-08-30 18:01:48,218 [trainer.py] => model_name: podnet
2024-08-30 18:01:48,218 [trainer.py] => convnet_type: resnet18
2024-08-30 18:01:48,218 [trainer.py] => init_train: True
2024-08-30 18:01:48,219 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 18:01:48,219 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 18:01:48,219 [trainer.py] => device: [device(type='cuda', index=8)]
2024-08-30 18:01:48,219 [trainer.py] => seed: 1993
2024-08-30 18:01:48,775 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 18:01:48,911 [trainer.py] => All params: 3843904
2024-08-30 18:01:48,911 [trainer.py] => Trainable params: 3843904
2024-08-30 18:01:48,912 [podnet.py] => Learning on 0-5
2024-08-30 18:01:48,976 [podnet.py] => Adaptive factor: 0
2024-08-30 18:01:52,237 [podnet.py] => Task 0, Epoch 1/300 (LR 0.10000) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 18:01:54,538 [podnet.py] => Task 0, Epoch 2/300 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.86, Test_acc 32.63
2024-08-30 18:01:56,604 [podnet.py] => Task 0, Epoch 3/300 (LR 0.09998) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.47, Test_acc 36.30
2024-08-30 18:01:58,566 [podnet.py] => Task 0, Epoch 4/300 (LR 0.09996) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.33, Test_acc 59.63
2024-08-30 18:02:00,485 [podnet.py] => Task 0, Epoch 5/300 (LR 0.09993) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.73, Test_acc 55.37
2024-08-30 18:02:02,928 [podnet.py] => Task 0, Epoch 6/300 (LR 0.09990) => LSC_loss 0.25, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.02, Test_acc 72.20
2024-08-30 18:02:05,030 [podnet.py] => Task 0, Epoch 7/300 (LR 0.09987) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.42, Test_acc 67.40
2024-08-30 18:02:08,109 [podnet.py] => Task 0, Epoch 8/300 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.66, Test_acc 74.30
2024-08-30 18:02:10,945 [podnet.py] => Task 0, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.07, Test_acc 74.60
2024-08-30 18:02:12,860 [podnet.py] => Task 0, Epoch 10/300 (LR 0.09973) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.47, Test_acc 78.70
2024-08-30 18:02:15,573 [podnet.py] => Task 0, Epoch 11/300 (LR 0.09967) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 70.47
2024-08-30 18:02:18,141 [podnet.py] => Task 0, Epoch 12/300 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 82.27
2024-08-30 18:02:20,349 [podnet.py] => Task 0, Epoch 13/300 (LR 0.09954) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.29, Test_acc 82.53
2024-08-30 18:02:21,905 [podnet.py] => Task 0, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 67.27
2024-08-30 18:02:23,776 [podnet.py] => Task 0, Epoch 15/300 (LR 0.09938) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.94, Test_acc 79.93
2024-08-30 18:02:25,497 [podnet.py] => Task 0, Epoch 16/300 (LR 0.09930) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.08, Test_acc 83.07
2024-08-30 18:02:27,399 [podnet.py] => Task 0, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 72.60
2024-08-30 18:02:29,447 [podnet.py] => Task 0, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.56, Test_acc 80.30
2024-08-30 18:02:31,334 [podnet.py] => Task 0, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 81.57
2024-08-30 18:02:33,238 [podnet.py] => Task 0, Epoch 20/300 (LR 0.09891) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 79.03
2024-08-30 18:02:35,094 [podnet.py] => Task 0, Epoch 21/300 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 83.07
2024-08-30 18:02:36,747 [podnet.py] => Task 0, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 74.17
2024-08-30 18:02:38,734 [podnet.py] => Task 0, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 82.63
2024-08-30 18:02:40,654 [podnet.py] => Task 0, Epoch 24/300 (LR 0.09843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 82.83
2024-08-30 18:02:42,333 [podnet.py] => Task 0, Epoch 25/300 (LR 0.09830) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.27, Test_acc 84.33
2024-08-30 18:02:44,042 [podnet.py] => Task 0, Epoch 26/300 (LR 0.09816) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.83, Test_acc 85.20
2024-08-30 18:02:45,922 [podnet.py] => Task 0, Epoch 27/300 (LR 0.09801) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 85.90
2024-08-30 18:02:48,008 [podnet.py] => Task 0, Epoch 28/300 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 74.20
2024-08-30 18:02:50,038 [podnet.py] => Task 0, Epoch 29/300 (LR 0.09771) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 82.03
2024-08-30 18:02:51,880 [podnet.py] => Task 0, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 76.93
2024-08-30 18:02:53,860 [podnet.py] => Task 0, Epoch 31/300 (LR 0.09739) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 79.40
2024-08-30 18:02:55,948 [podnet.py] => Task 0, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.66, Test_acc 77.10
2024-08-30 18:02:57,773 [podnet.py] => Task 0, Epoch 33/300 (LR 0.09704) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.02, Test_acc 79.23
2024-08-30 18:02:59,919 [podnet.py] => Task 0, Epoch 34/300 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.07, Test_acc 86.00
2024-08-30 18:03:01,870 [podnet.py] => Task 0, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 74.73
2024-08-30 18:03:03,845 [podnet.py] => Task 0, Epoch 36/300 (LR 0.09649) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.14, Test_acc 79.27
2024-08-30 18:03:05,751 [podnet.py] => Task 0, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 84.97
2024-08-30 18:03:07,805 [podnet.py] => Task 0, Epoch 38/300 (LR 0.09609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 84.70
2024-08-30 18:03:09,814 [podnet.py] => Task 0, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 82.83
2024-08-30 18:03:11,983 [podnet.py] => Task 0, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 75.70
2024-08-30 18:03:14,077 [podnet.py] => Task 0, Epoch 41/300 (LR 0.09546) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.97, Test_acc 83.57
2024-08-30 18:03:15,915 [podnet.py] => Task 0, Epoch 42/300 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 82.43
2024-08-30 18:03:17,579 [podnet.py] => Task 0, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 86.70
2024-08-30 18:03:19,413 [podnet.py] => Task 0, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 71.70
2024-08-30 18:03:21,090 [podnet.py] => Task 0, Epoch 45/300 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.30
2024-08-30 18:03:22,893 [podnet.py] => Task 0, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 80.70
2024-08-30 18:03:24,871 [podnet.py] => Task 0, Epoch 47/300 (LR 0.09407) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 84.67
2024-08-30 18:03:26,799 [podnet.py] => Task 0, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 83.63
2024-08-30 18:03:28,731 [podnet.py] => Task 0, Epoch 49/300 (LR 0.09356) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 85.03
2024-08-30 18:03:30,487 [podnet.py] => Task 0, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 82.57
2024-08-30 18:03:32,382 [podnet.py] => Task 0, Epoch 51/300 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 88.17
2024-08-30 18:03:34,164 [podnet.py] => Task 0, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 80.73
2024-08-30 18:03:35,824 [podnet.py] => Task 0, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 78.07
2024-08-30 18:03:37,559 [podnet.py] => Task 0, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.67
2024-08-30 18:03:39,382 [podnet.py] => Task 0, Epoch 55/300 (LR 0.09193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 82.83
2024-08-30 18:03:41,202 [podnet.py] => Task 0, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 82.33
2024-08-30 18:03:43,159 [podnet.py] => Task 0, Epoch 57/300 (LR 0.09135) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 82.83
2024-08-30 18:03:45,183 [podnet.py] => Task 0, Epoch 58/300 (LR 0.09106) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.50, Test_acc 84.40
2024-08-30 18:03:46,976 [podnet.py] => Task 0, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 70.30
2024-08-30 18:03:48,813 [podnet.py] => Task 0, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.83
2024-08-30 18:03:50,900 [podnet.py] => Task 0, Epoch 61/300 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.69, Test_acc 83.20
2024-08-30 18:03:52,935 [podnet.py] => Task 0, Epoch 62/300 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 86.53
2024-08-30 18:03:55,071 [podnet.py] => Task 0, Epoch 63/300 (LR 0.08951) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 84.83
2024-08-30 18:03:57,263 [podnet.py] => Task 0, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 84.63
2024-08-30 18:03:59,463 [podnet.py] => Task 0, Epoch 65/300 (LR 0.08886) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 81.80
2024-08-30 18:04:01,682 [podnet.py] => Task 0, Epoch 66/300 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 81.17
2024-08-30 18:04:03,731 [podnet.py] => Task 0, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 82.20
2024-08-30 18:04:05,798 [podnet.py] => Task 0, Epoch 68/300 (LR 0.08785) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.91, Test_acc 83.43
2024-08-30 18:04:07,627 [podnet.py] => Task 0, Epoch 69/300 (LR 0.08751) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 84.27
2024-08-30 18:04:09,450 [podnet.py] => Task 0, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.47, Test_acc 84.80
2024-08-30 18:04:11,193 [podnet.py] => Task 0, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 88.13
2024-08-30 18:04:13,061 [podnet.py] => Task 0, Epoch 72/300 (LR 0.08645) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 87.83
2024-08-30 18:04:15,008 [podnet.py] => Task 0, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 87.00
2024-08-30 18:04:16,719 [podnet.py] => Task 0, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 87.03
2024-08-30 18:04:18,413 [podnet.py] => Task 0, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.63
2024-08-30 18:04:20,289 [podnet.py] => Task 0, Epoch 76/300 (LR 0.08498) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.71, Test_acc 78.20
2024-08-30 18:04:22,515 [podnet.py] => Task 0, Epoch 77/300 (LR 0.08461) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 84.30
2024-08-30 18:04:24,389 [podnet.py] => Task 0, Epoch 78/300 (LR 0.08423) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 86.53
2024-08-30 18:04:26,149 [podnet.py] => Task 0, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 80.93
2024-08-30 18:04:28,391 [podnet.py] => Task 0, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 83.10
2024-08-30 18:04:30,539 [podnet.py] => Task 0, Epoch 81/300 (LR 0.08307) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.57
2024-08-30 18:04:32,738 [podnet.py] => Task 0, Epoch 82/300 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 86.83
2024-08-30 18:04:35,016 [podnet.py] => Task 0, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 84.97
2024-08-30 18:04:37,071 [podnet.py] => Task 0, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 80.90
2024-08-30 18:04:38,981 [podnet.py] => Task 0, Epoch 85/300 (LR 0.08147) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.01, Test_acc 86.27
2024-08-30 18:04:40,834 [podnet.py] => Task 0, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.85, Test_acc 80.87
2024-08-30 18:04:42,849 [podnet.py] => Task 0, Epoch 87/300 (LR 0.08065) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.98, Test_acc 78.57
2024-08-30 18:04:44,722 [podnet.py] => Task 0, Epoch 88/300 (LR 0.08023) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.55, Test_acc 85.57
2024-08-30 18:04:46,758 [podnet.py] => Task 0, Epoch 89/300 (LR 0.07981) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.87
2024-08-30 18:04:48,723 [podnet.py] => Task 0, Epoch 90/300 (LR 0.07939) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.83
2024-08-30 18:04:50,790 [podnet.py] => Task 0, Epoch 91/300 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 82.47
2024-08-30 18:04:52,841 [podnet.py] => Task 0, Epoch 92/300 (LR 0.07854) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 86.23
2024-08-30 18:04:54,547 [podnet.py] => Task 0, Epoch 93/300 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.07
2024-08-30 18:04:56,166 [podnet.py] => Task 0, Epoch 94/300 (LR 0.07767) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 84.03
2024-08-30 18:04:58,313 [podnet.py] => Task 0, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 80.70
2024-08-30 18:05:00,107 [podnet.py] => Task 0, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 85.07
2024-08-30 18:05:01,786 [podnet.py] => Task 0, Epoch 97/300 (LR 0.07635) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 79.37
2024-08-30 18:05:03,522 [podnet.py] => Task 0, Epoch 98/300 (LR 0.07590) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 87.33
2024-08-30 18:05:05,461 [podnet.py] => Task 0, Epoch 99/300 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.21, Test_acc 84.17
2024-08-30 18:05:07,190 [podnet.py] => Task 0, Epoch 100/300 (LR 0.07500) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 85.00
2024-08-30 18:05:09,033 [podnet.py] => Task 0, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 82.50
2024-08-30 18:05:10,959 [podnet.py] => Task 0, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.40
2024-08-30 18:05:12,951 [podnet.py] => Task 0, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 83.40
2024-08-30 18:05:14,808 [podnet.py] => Task 0, Epoch 104/300 (LR 0.07316) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 80.57
2024-08-30 18:05:16,606 [podnet.py] => Task 0, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 88.03
2024-08-30 18:05:18,505 [podnet.py] => Task 0, Epoch 106/300 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 85.80
2024-08-30 18:05:20,271 [podnet.py] => Task 0, Epoch 107/300 (LR 0.07176) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 80.70
2024-08-30 18:05:22,147 [podnet.py] => Task 0, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 85.73
2024-08-30 18:05:24,006 [podnet.py] => Task 0, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.50
2024-08-30 18:05:25,864 [podnet.py] => Task 0, Epoch 110/300 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.13
2024-08-30 18:05:27,973 [podnet.py] => Task 0, Epoch 111/300 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 87.13
2024-08-30 18:05:29,894 [podnet.py] => Task 0, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.84, Test_acc 84.77
2024-08-30 18:05:31,788 [podnet.py] => Task 0, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 85.40
2024-08-30 18:05:33,907 [podnet.py] => Task 0, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 83.33
2024-08-30 18:05:36,109 [podnet.py] => Task 0, Epoch 115/300 (LR 0.06792) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 84.27
2024-08-30 18:05:37,955 [podnet.py] => Task 0, Epoch 116/300 (LR 0.06743) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.73
2024-08-30 18:05:40,039 [podnet.py] => Task 0, Epoch 117/300 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 79.07
2024-08-30 18:05:42,338 [podnet.py] => Task 0, Epoch 118/300 (LR 0.06644) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 84.63
2024-08-30 18:05:44,564 [podnet.py] => Task 0, Epoch 119/300 (LR 0.06595) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.53
2024-08-30 18:05:46,335 [podnet.py] => Task 0, Epoch 120/300 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 83.80
2024-08-30 18:05:48,169 [podnet.py] => Task 0, Epoch 121/300 (LR 0.06495) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 87.63
2024-08-30 18:05:50,052 [podnet.py] => Task 0, Epoch 122/300 (LR 0.06445) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 83.27
2024-08-30 18:05:51,988 [podnet.py] => Task 0, Epoch 123/300 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.34, Test_acc 82.97
2024-08-30 18:05:53,754 [podnet.py] => Task 0, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 83.40
2024-08-30 18:05:55,574 [podnet.py] => Task 0, Epoch 125/300 (LR 0.06294) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.77
2024-08-30 18:05:57,197 [podnet.py] => Task 0, Epoch 126/300 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 86.57
2024-08-30 18:05:58,938 [podnet.py] => Task 0, Epoch 127/300 (LR 0.06193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.36, Test_acc 86.00
2024-08-30 18:06:00,930 [podnet.py] => Task 0, Epoch 128/300 (LR 0.06142) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 84.60
2024-08-30 18:06:02,642 [podnet.py] => Task 0, Epoch 129/300 (LR 0.06091) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 87.97
2024-08-30 18:06:04,452 [podnet.py] => Task 0, Epoch 130/300 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 82.23
2024-08-30 18:06:06,093 [podnet.py] => Task 0, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.50, Test_acc 86.90
2024-08-30 18:06:07,796 [podnet.py] => Task 0, Epoch 132/300 (LR 0.05937) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.73
2024-08-30 18:06:09,676 [podnet.py] => Task 0, Epoch 133/300 (LR 0.05885) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.57
2024-08-30 18:06:11,591 [podnet.py] => Task 0, Epoch 134/300 (LR 0.05834) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.87
2024-08-30 18:06:13,430 [podnet.py] => Task 0, Epoch 135/300 (LR 0.05782) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 85.53
2024-08-30 18:06:15,678 [podnet.py] => Task 0, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 79.13
2024-08-30 18:06:17,754 [podnet.py] => Task 0, Epoch 137/300 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 88.00
2024-08-30 18:06:19,464 [podnet.py] => Task 0, Epoch 138/300 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.33
2024-08-30 18:06:21,484 [podnet.py] => Task 0, Epoch 139/300 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 88.17
2024-08-30 18:06:24,113 [podnet.py] => Task 0, Epoch 140/300 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 82.47
2024-08-30 18:06:26,147 [podnet.py] => Task 0, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 84.93
2024-08-30 18:06:28,199 [podnet.py] => Task 0, Epoch 142/300 (LR 0.05418) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.70, Test_acc 84.13
2024-08-30 18:06:30,288 [podnet.py] => Task 0, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 84.60
2024-08-30 18:06:32,377 [podnet.py] => Task 0, Epoch 144/300 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 87.83
2024-08-30 18:06:34,794 [podnet.py] => Task 0, Epoch 145/300 (LR 0.05262) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.60
2024-08-30 18:06:36,794 [podnet.py] => Task 0, Epoch 146/300 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 81.50
2024-08-30 18:06:38,448 [podnet.py] => Task 0, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 86.77
2024-08-30 18:06:40,243 [podnet.py] => Task 0, Epoch 148/300 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 84.77
2024-08-30 18:06:42,229 [podnet.py] => Task 0, Epoch 149/300 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 85.70
2024-08-30 18:06:44,147 [podnet.py] => Task 0, Epoch 150/300 (LR 0.05000) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 86.57
2024-08-30 18:06:46,191 [podnet.py] => Task 0, Epoch 151/300 (LR 0.04948) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 84.43
2024-08-30 18:06:48,212 [podnet.py] => Task 0, Epoch 152/300 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 88.60
2024-08-30 18:06:50,143 [podnet.py] => Task 0, Epoch 153/300 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 87.07
2024-08-30 18:06:51,933 [podnet.py] => Task 0, Epoch 154/300 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 87.57
2024-08-30 18:06:53,626 [podnet.py] => Task 0, Epoch 155/300 (LR 0.04738) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 81.10
2024-08-30 18:06:55,465 [podnet.py] => Task 0, Epoch 156/300 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.67, Test_acc 88.10
2024-08-30 18:06:57,017 [podnet.py] => Task 0, Epoch 157/300 (LR 0.04634) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.87
2024-08-30 18:06:58,806 [podnet.py] => Task 0, Epoch 158/300 (LR 0.04582) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.53
2024-08-30 18:07:00,707 [podnet.py] => Task 0, Epoch 159/300 (LR 0.04529) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.43
2024-08-30 18:07:02,740 [podnet.py] => Task 0, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 86.20
2024-08-30 18:07:04,579 [podnet.py] => Task 0, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 84.87
2024-08-30 18:07:06,400 [podnet.py] => Task 0, Epoch 162/300 (LR 0.04373) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 88.27
2024-08-30 18:07:08,394 [podnet.py] => Task 0, Epoch 163/300 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 85.63
2024-08-30 18:07:10,063 [podnet.py] => Task 0, Epoch 164/300 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 83.27
2024-08-30 18:07:11,860 [podnet.py] => Task 0, Epoch 165/300 (LR 0.04218) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 85.37
2024-08-30 18:07:13,641 [podnet.py] => Task 0, Epoch 166/300 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 89.13
2024-08-30 18:07:15,779 [podnet.py] => Task 0, Epoch 167/300 (LR 0.04115) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.80
2024-08-30 18:07:17,623 [podnet.py] => Task 0, Epoch 168/300 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 90.17
2024-08-30 18:07:19,372 [podnet.py] => Task 0, Epoch 169/300 (LR 0.04012) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.03
2024-08-30 18:07:21,305 [podnet.py] => Task 0, Epoch 170/300 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 89.47
2024-08-30 18:07:23,281 [podnet.py] => Task 0, Epoch 171/300 (LR 0.03909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.13
2024-08-30 18:07:25,350 [podnet.py] => Task 0, Epoch 172/300 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 89.47
2024-08-30 18:07:27,186 [podnet.py] => Task 0, Epoch 173/300 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.13
2024-08-30 18:07:28,983 [podnet.py] => Task 0, Epoch 174/300 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 84.43
2024-08-30 18:07:30,614 [podnet.py] => Task 0, Epoch 175/300 (LR 0.03706) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.30
2024-08-30 18:07:32,437 [podnet.py] => Task 0, Epoch 176/300 (LR 0.03655) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 81.37
2024-08-30 18:07:34,113 [podnet.py] => Task 0, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.73, Test_acc 85.23
2024-08-30 18:07:36,050 [podnet.py] => Task 0, Epoch 178/300 (LR 0.03555) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 85.83
2024-08-30 18:07:38,110 [podnet.py] => Task 0, Epoch 179/300 (LR 0.03505) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 83.43
2024-08-30 18:07:40,224 [podnet.py] => Task 0, Epoch 180/300 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 88.90
2024-08-30 18:07:42,170 [podnet.py] => Task 0, Epoch 181/300 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 88.60
2024-08-30 18:07:44,245 [podnet.py] => Task 0, Epoch 182/300 (LR 0.03356) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-08-30 18:07:46,101 [podnet.py] => Task 0, Epoch 183/300 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.73
2024-08-30 18:07:47,822 [podnet.py] => Task 0, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.11, Test_acc 88.17
2024-08-30 18:07:49,617 [podnet.py] => Task 0, Epoch 185/300 (LR 0.03208) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 87.47
2024-08-30 18:07:51,541 [podnet.py] => Task 0, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 79.03
2024-08-30 18:07:53,287 [podnet.py] => Task 0, Epoch 187/300 (LR 0.03111) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.53, Test_acc 84.80
2024-08-30 18:07:55,237 [podnet.py] => Task 0, Epoch 188/300 (LR 0.03062) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 85.53
2024-08-30 18:07:57,217 [podnet.py] => Task 0, Epoch 189/300 (LR 0.03014) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 88.90
2024-08-30 18:07:59,325 [podnet.py] => Task 0, Epoch 190/300 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 86.80
2024-08-30 18:08:01,674 [podnet.py] => Task 0, Epoch 191/300 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.20
2024-08-30 18:08:03,354 [podnet.py] => Task 0, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 86.90
2024-08-30 18:08:05,439 [podnet.py] => Task 0, Epoch 193/300 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 89.30
2024-08-30 18:08:07,370 [podnet.py] => Task 0, Epoch 194/300 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.27
2024-08-30 18:08:09,376 [podnet.py] => Task 0, Epoch 195/300 (LR 0.02730) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.47
2024-08-30 18:08:11,517 [podnet.py] => Task 0, Epoch 196/300 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.73
2024-08-30 18:08:13,585 [podnet.py] => Task 0, Epoch 197/300 (LR 0.02637) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.63
2024-08-30 18:08:15,483 [podnet.py] => Task 0, Epoch 198/300 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.23
2024-08-30 18:08:17,307 [podnet.py] => Task 0, Epoch 199/300 (LR 0.02545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.60
2024-08-30 18:08:19,005 [podnet.py] => Task 0, Epoch 200/300 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.67
2024-08-30 18:08:20,914 [podnet.py] => Task 0, Epoch 201/300 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.67
2024-08-30 18:08:22,799 [podnet.py] => Task 0, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 88.40
2024-08-30 18:08:24,680 [podnet.py] => Task 0, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 88.30
2024-08-30 18:08:26,505 [podnet.py] => Task 0, Epoch 204/300 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 87.87
2024-08-30 18:08:28,507 [podnet.py] => Task 0, Epoch 205/300 (LR 0.02277) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 88.73
2024-08-30 18:08:30,298 [podnet.py] => Task 0, Epoch 206/300 (LR 0.02233) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.10
2024-08-30 18:08:31,912 [podnet.py] => Task 0, Epoch 207/300 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.13
2024-08-30 18:08:33,788 [podnet.py] => Task 0, Epoch 208/300 (LR 0.02146) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 88.40
2024-08-30 18:08:35,904 [podnet.py] => Task 0, Epoch 209/300 (LR 0.02104) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 89.43
2024-08-30 18:08:37,615 [podnet.py] => Task 0, Epoch 210/300 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.40
2024-08-30 18:08:39,318 [podnet.py] => Task 0, Epoch 211/300 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 86.90
2024-08-30 18:08:41,253 [podnet.py] => Task 0, Epoch 212/300 (LR 0.01977) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 86.67
2024-08-30 18:08:42,960 [podnet.py] => Task 0, Epoch 213/300 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-08-30 18:08:44,780 [podnet.py] => Task 0, Epoch 214/300 (LR 0.01894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 89.67
2024-08-30 18:08:46,613 [podnet.py] => Task 0, Epoch 215/300 (LR 0.01853) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 89.77
2024-08-30 18:08:48,477 [podnet.py] => Task 0, Epoch 216/300 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-30 18:08:50,884 [podnet.py] => Task 0, Epoch 217/300 (LR 0.01773) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.83
2024-08-30 18:08:53,213 [podnet.py] => Task 0, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 89.30
2024-08-30 18:08:55,394 [podnet.py] => Task 0, Epoch 219/300 (LR 0.01693) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.13
2024-08-30 18:08:57,468 [podnet.py] => Task 0, Epoch 220/300 (LR 0.01654) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.87
2024-08-30 18:08:59,495 [podnet.py] => Task 0, Epoch 221/300 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-30 18:09:01,304 [podnet.py] => Task 0, Epoch 222/300 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 18:09:03,243 [podnet.py] => Task 0, Epoch 223/300 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-30 18:09:04,890 [podnet.py] => Task 0, Epoch 224/300 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 18:09:06,735 [podnet.py] => Task 0, Epoch 225/300 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 18:09:08,570 [podnet.py] => Task 0, Epoch 226/300 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 18:09:10,629 [podnet.py] => Task 0, Epoch 227/300 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-30 18:09:12,562 [podnet.py] => Task 0, Epoch 228/300 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-08-30 18:09:14,649 [podnet.py] => Task 0, Epoch 229/300 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 18:09:16,547 [podnet.py] => Task 0, Epoch 230/300 (LR 0.01284) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-08-30 18:09:18,165 [podnet.py] => Task 0, Epoch 231/300 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.00
2024-08-30 18:09:19,942 [podnet.py] => Task 0, Epoch 232/300 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.90
2024-08-30 18:09:21,814 [podnet.py] => Task 0, Epoch 233/300 (LR 0.01181) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.07
2024-08-30 18:09:23,687 [podnet.py] => Task 0, Epoch 234/300 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-08-30 18:09:25,414 [podnet.py] => Task 0, Epoch 235/300 (LR 0.01114) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.03
2024-08-30 18:09:27,306 [podnet.py] => Task 0, Epoch 236/300 (LR 0.01082) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 89.43
2024-08-30 18:09:29,155 [podnet.py] => Task 0, Epoch 237/300 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 18:09:30,946 [podnet.py] => Task 0, Epoch 238/300 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 18:09:32,637 [podnet.py] => Task 0, Epoch 239/300 (LR 0.00986) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 18:09:34,665 [podnet.py] => Task 0, Epoch 240/300 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 18:09:36,572 [podnet.py] => Task 0, Epoch 241/300 (LR 0.00924) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 18:09:38,534 [podnet.py] => Task 0, Epoch 242/300 (LR 0.00894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 18:09:40,514 [podnet.py] => Task 0, Epoch 243/300 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 18:09:42,599 [podnet.py] => Task 0, Epoch 244/300 (LR 0.00835) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 18:09:44,707 [podnet.py] => Task 0, Epoch 245/300 (LR 0.00807) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-30 18:09:46,664 [podnet.py] => Task 0, Epoch 246/300 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 18:09:48,757 [podnet.py] => Task 0, Epoch 247/300 (LR 0.00751) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 18:09:50,903 [podnet.py] => Task 0, Epoch 248/300 (LR 0.00723) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 18:09:52,964 [podnet.py] => Task 0, Epoch 249/300 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 18:09:54,779 [podnet.py] => Task 0, Epoch 250/300 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 18:09:56,722 [podnet.py] => Task 0, Epoch 251/300 (LR 0.00644) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 18:09:58,709 [podnet.py] => Task 0, Epoch 252/300 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 18:10:00,549 [podnet.py] => Task 0, Epoch 253/300 (LR 0.00593) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 18:10:02,546 [podnet.py] => Task 0, Epoch 254/300 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 18:10:04,308 [podnet.py] => Task 0, Epoch 255/300 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 18:10:05,955 [podnet.py] => Task 0, Epoch 256/300 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-30 18:10:07,562 [podnet.py] => Task 0, Epoch 257/300 (LR 0.00498) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.57
2024-08-30 18:10:09,377 [podnet.py] => Task 0, Epoch 258/300 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.87
2024-08-30 18:10:11,193 [podnet.py] => Task 0, Epoch 259/300 (LR 0.00454) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-30 18:10:12,977 [podnet.py] => Task 0, Epoch 260/300 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 18:10:14,822 [podnet.py] => Task 0, Epoch 261/300 (LR 0.00411) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 18:10:16,579 [podnet.py] => Task 0, Epoch 262/300 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 18:10:18,229 [podnet.py] => Task 0, Epoch 263/300 (LR 0.00371) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-30 18:10:20,077 [podnet.py] => Task 0, Epoch 264/300 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 18:10:22,152 [podnet.py] => Task 0, Epoch 265/300 (LR 0.00332) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-30 18:10:23,937 [podnet.py] => Task 0, Epoch 266/300 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 18:10:25,910 [podnet.py] => Task 0, Epoch 267/300 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 18:10:27,965 [podnet.py] => Task 0, Epoch 268/300 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 18:10:30,183 [podnet.py] => Task 0, Epoch 269/300 (LR 0.00261) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 18:10:32,150 [podnet.py] => Task 0, Epoch 270/300 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 18:10:34,411 [podnet.py] => Task 0, Epoch 271/300 (LR 0.00229) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 18:10:36,625 [podnet.py] => Task 0, Epoch 272/300 (LR 0.00213) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 18:10:38,392 [podnet.py] => Task 0, Epoch 273/300 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 18:10:40,259 [podnet.py] => Task 0, Epoch 274/300 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 18:10:42,203 [podnet.py] => Task 0, Epoch 275/300 (LR 0.00170) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 18:10:44,078 [podnet.py] => Task 0, Epoch 276/300 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 18:10:45,890 [podnet.py] => Task 0, Epoch 277/300 (LR 0.00144) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 18:10:47,774 [podnet.py] => Task 0, Epoch 278/300 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.70
2024-08-30 18:10:49,607 [podnet.py] => Task 0, Epoch 279/300 (LR 0.00120) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.97
2024-08-30 18:10:51,516 [podnet.py] => Task 0, Epoch 280/300 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 18:10:53,369 [podnet.py] => Task 0, Epoch 281/300 (LR 0.00099) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 18:10:55,189 [podnet.py] => Task 0, Epoch 282/300 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-30 18:10:56,808 [podnet.py] => Task 0, Epoch 283/300 (LR 0.00079) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 18:10:58,825 [podnet.py] => Task 0, Epoch 284/300 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-30 18:11:00,585 [podnet.py] => Task 0, Epoch 285/300 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-30 18:11:02,545 [podnet.py] => Task 0, Epoch 286/300 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 18:11:04,596 [podnet.py] => Task 0, Epoch 287/300 (LR 0.00046) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 18:11:06,555 [podnet.py] => Task 0, Epoch 288/300 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 18:11:08,278 [podnet.py] => Task 0, Epoch 289/300 (LR 0.00033) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 18:11:09,897 [podnet.py] => Task 0, Epoch 290/300 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 18:11:11,680 [podnet.py] => Task 0, Epoch 291/300 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 18:11:13,414 [podnet.py] => Task 0, Epoch 292/300 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 18:11:15,576 [podnet.py] => Task 0, Epoch 293/300 (LR 0.00013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 18:11:17,576 [podnet.py] => Task 0, Epoch 294/300 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 18:11:19,395 [podnet.py] => Task 0, Epoch 295/300 (LR 0.00007) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 18:11:21,329 [podnet.py] => Task 0, Epoch 296/300 (LR 0.00004) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-08-30 18:11:23,126 [podnet.py] => Task 0, Epoch 297/300 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.87
2024-08-30 18:11:25,075 [podnet.py] => Task 0, Epoch 298/300 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 18:11:26,878 [podnet.py] => Task 0, Epoch 299/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 18:11:28,713 [podnet.py] => Task 0, Epoch 300/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.90
2024-08-30 18:11:29,012 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 18:11:29,013 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 18:11:35,108 [podnet.py] => Exemplar size: 500
2024-08-30 18:11:35,108 [trainer.py] => CNN: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-30 18:11:35,108 [trainer.py] => NME: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-30 18:11:35,108 [trainer.py] => CNN top1 curve: [88.9]
2024-08-30 18:11:35,108 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 18:11:35,108 [trainer.py] => NME top1 curve: [88.9]
2024-08-30 18:11:35,108 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 18:11:35,108 [trainer.py] => Average Accuracy (CNN): 88.9
2024-08-30 18:11:35,108 [trainer.py] => Average Accuracy (NME): 88.9
2024-08-30 18:11:35,108 [trainer.py] => All params: 3869505
2024-08-30 18:11:35,109 [trainer.py] => Trainable params: 3869505
2024-08-30 18:11:35,109 [podnet.py] => Learning on 5-7
2024-08-30 18:11:35,128 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 18:11:37,199 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 1.03, Spatial_loss 1.08, Flat_loss 0.72, Train_acc 74.02, Test_acc 37.05
2024-08-30 18:11:38,883 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.49, Spatial_loss 0.89, Flat_loss 0.52, Train_acc 88.27, Test_acc 34.33
2024-08-30 18:11:40,526 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.37, Spatial_loss 0.85, Flat_loss 0.44, Train_acc 90.69, Test_acc 46.31
2024-08-30 18:11:42,079 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.29, Spatial_loss 0.79, Flat_loss 0.40, Train_acc 93.00, Test_acc 39.10
2024-08-30 18:11:43,525 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.23, Spatial_loss 0.71, Flat_loss 0.36, Train_acc 94.31, Test_acc 54.33
2024-08-30 18:11:44,978 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.19, Spatial_loss 0.66, Flat_loss 0.33, Train_acc 95.76, Test_acc 55.52
2024-08-30 18:11:46,557 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.15, Spatial_loss 0.60, Flat_loss 0.30, Train_acc 96.98, Test_acc 55.14
2024-08-30 18:11:48,065 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.13, Spatial_loss 0.55, Flat_loss 0.27, Train_acc 97.40, Test_acc 59.40
2024-08-30 18:11:49,536 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.12, Spatial_loss 0.53, Flat_loss 0.27, Train_acc 97.78, Test_acc 62.19
2024-08-30 18:11:50,990 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.10, Spatial_loss 0.50, Flat_loss 0.25, Train_acc 98.82, Test_acc 63.57
2024-08-30 18:11:52,599 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.09, Spatial_loss 0.52, Flat_loss 0.26, Train_acc 98.69, Test_acc 60.74
2024-08-30 18:11:54,045 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.09, Spatial_loss 0.48, Flat_loss 0.24, Train_acc 99.00, Test_acc 59.29
2024-08-30 18:11:55,681 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.08, Spatial_loss 0.46, Flat_loss 0.23, Train_acc 99.36, Test_acc 62.81
2024-08-30 18:11:57,252 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.08, Spatial_loss 0.46, Flat_loss 0.23, Train_acc 99.04, Test_acc 67.07
2024-08-30 18:11:58,839 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.42, Flat_loss 0.22, Train_acc 99.58, Test_acc 68.88
2024-08-30 18:12:00,303 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.07, Spatial_loss 0.42, Flat_loss 0.22, Train_acc 99.51, Test_acc 63.86
2024-08-30 18:12:01,701 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.08, Spatial_loss 0.46, Flat_loss 0.23, Train_acc 99.11, Test_acc 64.93
2024-08-30 18:12:03,289 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.42, Flat_loss 0.22, Train_acc 99.51, Test_acc 69.50
2024-08-30 18:12:05,329 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.40, Flat_loss 0.21, Train_acc 99.67, Test_acc 69.60
2024-08-30 18:12:06,919 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.06, Spatial_loss 0.38, Flat_loss 0.20, Train_acc 99.76, Test_acc 66.36
2024-08-30 18:12:08,339 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.06, Spatial_loss 0.40, Flat_loss 0.20, Train_acc 99.53, Test_acc 66.50
2024-08-30 18:12:09,894 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.06, Spatial_loss 0.38, Flat_loss 0.20, Train_acc 99.76, Test_acc 62.24
2024-08-30 18:12:11,447 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.38, Flat_loss 0.19, Train_acc 99.78, Test_acc 67.05
2024-08-30 18:12:12,937 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.19, Train_acc 99.71, Test_acc 61.60
2024-08-30 18:12:14,505 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.19, Train_acc 99.69, Test_acc 63.60
2024-08-30 18:12:15,991 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.19, Train_acc 99.82, Test_acc 64.33
2024-08-30 18:12:17,438 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.18, Train_acc 99.78, Test_acc 64.26
2024-08-30 18:12:18,930 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.18, Train_acc 99.91, Test_acc 62.86
2024-08-30 18:12:20,545 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.18, Train_acc 99.73, Test_acc 67.36
2024-08-30 18:12:22,075 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.18, Train_acc 99.80, Test_acc 69.02
2024-08-30 18:12:23,949 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.08, Spatial_loss 0.38, Flat_loss 0.18, Train_acc 98.42, Test_acc 60.31
2024-08-30 18:12:25,682 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.06, Spatial_loss 0.39, Flat_loss 0.19, Train_acc 99.56, Test_acc 61.88
2024-08-30 18:12:27,432 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.18, Train_acc 99.69, Test_acc 66.14
2024-08-30 18:12:29,001 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.18, Train_acc 99.82, Test_acc 67.86
2024-08-30 18:12:30,616 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.17, Train_acc 99.87, Test_acc 68.26
2024-08-30 18:12:32,124 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 99.93, Test_acc 63.86
2024-08-30 18:12:33,683 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.17, Train_acc 99.76, Test_acc 62.40
2024-08-30 18:12:35,164 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.17, Train_acc 99.84, Test_acc 66.79
2024-08-30 18:12:36,717 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 99.89, Test_acc 68.74
2024-08-30 18:12:38,190 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 99.89, Test_acc 60.38
2024-08-30 18:12:39,734 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 99.89, Test_acc 64.12
2024-08-30 18:12:41,419 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 99.82, Test_acc 64.55
2024-08-30 18:12:42,930 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.12, Spatial_loss 0.51, Flat_loss 0.24, Train_acc 97.53, Test_acc 51.90
2024-08-30 18:12:44,448 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.07, Spatial_loss 0.46, Flat_loss 0.22, Train_acc 98.98, Test_acc 68.50
2024-08-30 18:12:45,951 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.37, Flat_loss 0.18, Train_acc 99.56, Test_acc 55.05
2024-08-30 18:12:47,386 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.07, Spatial_loss 0.47, Flat_loss 0.22, Train_acc 99.16, Test_acc 62.52
2024-08-30 18:12:48,847 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.18, Train_acc 99.73, Test_acc 66.10
2024-08-30 18:12:50,345 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.17, Train_acc 99.82, Test_acc 67.14
2024-08-30 18:12:51,836 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 99.93, Test_acc 63.81
2024-08-30 18:12:53,501 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.17, Train_acc 99.87, Test_acc 65.19
2024-08-30 18:12:55,273 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 99.96, Test_acc 67.67
2024-08-30 18:12:56,895 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 99.93, Test_acc 69.14
2024-08-30 18:12:58,543 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 99.89, Test_acc 67.36
2024-08-30 18:13:00,228 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.91, Test_acc 67.48
2024-08-30 18:13:01,836 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.89, Test_acc 65.76
2024-08-30 18:13:03,534 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.96, Test_acc 67.43
2024-08-30 18:13:05,040 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 99.98, Test_acc 65.17
2024-08-30 18:13:06,612 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 99.98, Test_acc 64.38
2024-08-30 18:13:08,175 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.89, Test_acc 67.33
2024-08-30 18:13:09,651 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 99.91, Test_acc 63.31
2024-08-30 18:13:11,189 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 67.21
2024-08-30 18:13:12,875 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.96, Test_acc 66.57
2024-08-30 18:13:14,255 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.14, Train_acc 99.93, Test_acc 64.93
2024-08-30 18:13:15,864 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.14, Train_acc 99.96, Test_acc 63.88
2024-08-30 18:13:17,440 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.07, Spatial_loss 0.41, Flat_loss 0.19, Train_acc 99.13, Test_acc 59.95
2024-08-30 18:13:18,979 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 99.60, Test_acc 65.69
2024-08-30 18:13:20,511 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 99.98, Test_acc 64.24
2024-08-30 18:13:21,974 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.96, Test_acc 61.48
2024-08-30 18:13:23,646 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 99.89, Test_acc 69.64
2024-08-30 18:13:25,214 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.14, Train_acc 99.98, Test_acc 68.36
2024-08-30 18:13:26,629 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.14, Train_acc 99.98, Test_acc 67.81
2024-08-30 18:13:28,055 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 100.00, Test_acc 68.43
2024-08-30 18:13:29,481 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.96, Test_acc 66.33
2024-08-30 18:13:30,926 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.91, Test_acc 66.76
2024-08-30 18:13:32,358 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.93, Test_acc 66.38
2024-08-30 18:13:33,861 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.17, Train_acc 99.64, Test_acc 64.05
2024-08-30 18:13:35,550 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 99.96, Test_acc 66.00
2024-08-30 18:13:37,031 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.14, Train_acc 99.93, Test_acc 67.43
2024-08-30 18:13:38,476 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.98, Test_acc 62.36
2024-08-30 18:13:39,955 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.17, Train_acc 99.62, Test_acc 58.98
2024-08-30 18:13:41,399 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.16, Train_acc 99.80, Test_acc 58.64
2024-08-30 18:13:42,872 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.81
2024-08-30 18:13:44,419 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.14, Train_acc 99.91, Test_acc 67.50
2024-08-30 18:13:45,965 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.96, Test_acc 68.45
2024-08-30 18:13:47,545 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.96, Test_acc 63.86
2024-08-30 18:13:49,180 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.90
2024-08-30 18:13:50,925 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.38
2024-08-30 18:13:52,476 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.48
2024-08-30 18:13:53,948 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.93, Test_acc 66.12
2024-08-30 18:13:55,580 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.98, Test_acc 66.74
2024-08-30 18:13:57,126 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.91, Test_acc 65.71
2024-08-30 18:13:58,713 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.74
2024-08-30 18:14:00,370 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 100.00, Test_acc 65.00
2024-08-30 18:14:01,789 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.96, Test_acc 66.19
2024-08-30 18:14:03,653 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.74
2024-08-30 18:14:05,282 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.10
2024-08-30 18:14:06,837 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.14, Train_acc 99.96, Test_acc 62.12
2024-08-30 18:14:08,482 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.98, Test_acc 66.52
2024-08-30 18:14:10,096 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.13, Train_acc 99.98, Test_acc 62.71
2024-08-30 18:14:11,618 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.93, Test_acc 62.98
2024-08-30 18:14:13,098 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.57
2024-08-30 18:14:14,628 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.02
2024-08-30 18:14:16,135 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.02
2024-08-30 18:14:17,555 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.33
2024-08-30 18:14:18,939 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 99.93, Test_acc 66.60
2024-08-30 18:14:20,407 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.50
2024-08-30 18:14:21,854 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.02
2024-08-30 18:14:23,416 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.13, Train_acc 99.58, Test_acc 62.79
2024-08-30 18:14:24,927 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.69
2024-08-30 18:14:26,337 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.17
2024-08-30 18:14:27,837 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.98, Test_acc 52.76
2024-08-30 18:14:29,258 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.15, Spatial_loss 0.50, Flat_loss 0.24, Train_acc 95.80, Test_acc 49.00
2024-08-30 18:14:30,770 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.19, Spatial_loss 0.62, Flat_loss 0.31, Train_acc 95.18, Test_acc 58.69
2024-08-30 18:14:32,232 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.10, Spatial_loss 0.53, Flat_loss 0.26, Train_acc 97.36, Test_acc 59.79
2024-08-30 18:14:33,666 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.06, Spatial_loss 0.43, Flat_loss 0.21, Train_acc 99.20, Test_acc 65.88
2024-08-30 18:14:35,197 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.19, Train_acc 99.67, Test_acc 64.93
2024-08-30 18:14:37,048 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 99.96, Test_acc 64.76
2024-08-30 18:14:38,797 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 99.91, Test_acc 67.02
2024-08-30 18:14:40,566 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.14, Train_acc 99.98, Test_acc 65.90
2024-08-30 18:14:42,344 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 100.00, Test_acc 69.48
2024-08-30 18:14:44,093 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.93, Test_acc 67.33
2024-08-30 18:14:45,668 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.64
2024-08-30 18:14:47,256 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.74
2024-08-30 18:14:48,691 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.24
2024-08-30 18:14:50,150 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 100.00, Test_acc 70.86
2024-08-30 18:14:51,552 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.21
2024-08-30 18:14:53,047 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 99.98, Test_acc 63.33
2024-08-30 18:14:54,676 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.14, Train_acc 99.87, Test_acc 67.43
2024-08-30 18:14:56,232 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.93, Test_acc 68.38
2024-08-30 18:14:57,744 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.96, Test_acc 66.31
2024-08-30 18:14:59,319 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.36
2024-08-30 18:15:00,960 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.12
2024-08-30 18:15:02,475 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 99.71, Test_acc 66.24
2024-08-30 18:15:03,999 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.98, Test_acc 65.74
2024-08-30 18:15:05,682 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.57
2024-08-30 18:15:07,230 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.14, Train_acc 99.96, Test_acc 66.05
2024-08-30 18:15:08,663 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.90
2024-08-30 18:15:10,229 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.40
2024-08-30 18:15:11,823 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.62
2024-08-30 18:15:13,370 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.79
2024-08-30 18:15:14,732 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.98
2024-08-30 18:15:16,104 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.12, Train_acc 99.98, Test_acc 67.83
2024-08-30 18:15:17,716 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.50
2024-08-30 18:15:19,324 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.13, Train_acc 99.98, Test_acc 61.95
2024-08-30 18:15:20,903 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.16, Train_acc 99.49, Test_acc 67.12
2024-08-30 18:15:22,633 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.14, Train_acc 99.76, Test_acc 64.21
2024-08-30 18:15:24,409 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 99.89, Test_acc 64.48
2024-08-30 18:15:26,411 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.67
2024-08-30 18:15:28,274 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.36
2024-08-30 18:15:29,961 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.95
2024-08-30 18:15:31,886 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.43
2024-08-30 18:15:33,376 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.45
2024-08-30 18:15:34,888 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.98
2024-08-30 18:15:36,366 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.31
2024-08-30 18:15:37,874 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.71
2024-08-30 18:15:39,377 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.12
2024-08-30 18:15:40,965 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 99.96, Test_acc 69.83
2024-08-30 18:15:42,522 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.40
2024-08-30 18:15:44,076 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.33
2024-08-30 18:15:45,579 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 99.98, Test_acc 64.76
2024-08-30 18:15:47,079 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 99.98, Test_acc 69.71
2024-08-30 18:15:48,576 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.00
2024-08-30 18:15:50,077 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.52
2024-08-30 18:15:51,663 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.71
2024-08-30 18:15:53,185 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.83
2024-08-30 18:15:54,603 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.00
2024-08-30 18:15:56,065 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.12, Train_acc 99.98, Test_acc 67.83
2024-08-30 18:15:57,498 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 65.88
2024-08-30 18:15:59,067 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 99.98, Test_acc 67.76
2024-08-30 18:16:00,622 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.83
2024-08-30 18:16:02,163 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.26
2024-08-30 18:16:03,777 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.64
2024-08-30 18:16:05,316 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.88
2024-08-30 18:16:06,864 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.12
2024-08-30 18:16:08,565 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.36
2024-08-30 18:16:09,994 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.69
2024-08-30 18:16:11,415 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.24
2024-08-30 18:16:12,818 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.48
2024-08-30 18:16:14,534 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.02
2024-08-30 18:16:16,556 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.07
2024-08-30 18:16:18,078 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.76
2024-08-30 18:16:19,960 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.81
2024-08-30 18:16:21,788 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.38
2024-08-30 18:16:23,346 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.12
2024-08-30 18:16:24,832 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 99.93, Test_acc 66.62
2024-08-30 18:16:26,309 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.57
2024-08-30 18:16:27,804 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 99.93, Test_acc 67.19
2024-08-30 18:16:29,236 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.64
2024-08-30 18:16:30,900 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.14
2024-08-30 18:16:32,374 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.24
2024-08-30 18:16:34,068 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.17
2024-08-30 18:16:35,627 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.52
2024-08-30 18:16:37,072 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.14
2024-08-30 18:16:38,616 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.43
2024-08-30 18:16:40,065 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.60
2024-08-30 18:16:41,514 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.26
2024-08-30 18:16:43,014 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 99.96, Test_acc 67.62
2024-08-30 18:16:44,493 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.17
2024-08-30 18:16:45,971 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.21
2024-08-30 18:16:47,341 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.10
2024-08-30 18:16:48,742 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.31
2024-08-30 18:16:50,279 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.71
2024-08-30 18:16:51,859 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.52
2024-08-30 18:16:53,332 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.48
2024-08-30 18:16:54,979 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.24
2024-08-30 18:16:56,516 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.02
2024-08-30 18:16:58,021 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.26
2024-08-30 18:16:59,513 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.19
2024-08-30 18:17:00,951 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.74
2024-08-30 18:17:02,517 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 99.98, Test_acc 67.62
2024-08-30 18:17:04,089 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.64
2024-08-30 18:17:05,521 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.74
2024-08-30 18:17:07,028 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.76
2024-08-30 18:17:08,783 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.12
2024-08-30 18:17:10,216 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.50
2024-08-30 18:17:11,842 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.00
2024-08-30 18:17:13,295 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.33
2024-08-30 18:17:14,883 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.83
2024-08-30 18:17:16,418 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.48
2024-08-30 18:17:17,923 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.62
2024-08-30 18:17:19,462 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.67
2024-08-30 18:17:20,910 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.03, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.67
2024-08-30 18:17:22,398 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 99.98, Test_acc 68.29
2024-08-30 18:17:24,099 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.55
2024-08-30 18:17:25,622 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.69
2024-08-30 18:17:27,129 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.86
2024-08-30 18:17:28,687 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.76
2024-08-30 18:17:30,240 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.48
2024-08-30 18:17:31,996 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.03, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.38
2024-08-30 18:17:33,862 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.19
2024-08-30 18:17:35,451 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.98
2024-08-30 18:17:36,999 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.21
2024-08-30 18:17:38,479 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.17
2024-08-30 18:17:39,941 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.55
2024-08-30 18:17:41,652 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.26
2024-08-30 18:17:43,157 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.33
2024-08-30 18:17:44,724 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.62
2024-08-30 18:17:46,356 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.40
2024-08-30 18:17:48,010 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.62
2024-08-30 18:17:49,682 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.33
2024-08-30 18:17:51,291 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.40
2024-08-30 18:17:52,850 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.40
2024-08-30 18:17:54,265 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.48
2024-08-30 18:17:55,833 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.29
2024-08-30 18:17:57,530 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.03, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-30 18:17:59,234 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.60
2024-08-30 18:18:00,631 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.45
2024-08-30 18:18:02,170 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.52
2024-08-30 18:18:03,753 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-08-30 18:18:05,389 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.81
2024-08-30 18:18:06,868 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-30 18:18:08,306 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.74
2024-08-30 18:18:09,749 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-08-30 18:18:11,321 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.29
2024-08-30 18:18:12,795 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.00
2024-08-30 18:18:14,219 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.17
2024-08-30 18:18:15,572 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.29
2024-08-30 18:18:17,171 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.71
2024-08-30 18:18:18,865 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.45
2024-08-30 18:18:20,813 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.90
2024-08-30 18:18:22,537 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.48
2024-08-30 18:18:24,222 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.19
2024-08-30 18:18:25,737 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.03, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.88
2024-08-30 18:18:27,213 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.88
2024-08-30 18:18:28,630 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.88
2024-08-30 18:18:29,976 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.76
2024-08-30 18:18:31,404 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.88
2024-08-30 18:18:32,865 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.48
2024-08-30 18:18:34,521 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.81
2024-08-30 18:18:36,045 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-30 18:18:37,594 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-30 18:18:39,043 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.38
2024-08-30 18:18:40,605 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.45
2024-08-30 18:18:42,063 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.88
2024-08-30 18:18:43,529 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.93
2024-08-30 18:18:44,962 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-08-30 18:18:46,418 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.19
2024-08-30 18:18:47,885 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.07
2024-08-30 18:18:49,481 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-30 18:18:51,233 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.93
2024-08-30 18:18:52,822 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.52
2024-08-30 18:18:54,242 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-08-30 18:18:55,816 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-30 18:18:57,265 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.21
2024-08-30 18:18:58,786 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 99.98, Test_acc 70.21
2024-08-30 18:19:00,383 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.74
2024-08-30 18:19:02,242 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.26
2024-08-30 18:19:03,881 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.14
2024-08-30 18:19:05,355 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.31
2024-08-30 18:19:06,954 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.40
2024-08-30 18:19:08,471 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.03, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.55
2024-08-30 18:19:09,985 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.26
2024-08-30 18:19:11,425 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.69
2024-08-30 18:19:12,945 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-30 18:19:14,460 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.71
2024-08-30 18:19:15,932 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.31
2024-08-30 18:19:17,367 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.81
2024-08-30 18:19:18,788 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.88
2024-08-30 18:19:20,215 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.31
2024-08-30 18:19:21,655 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-30 18:19:22,006 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 18:19:22,006 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 18:19:23,112 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 18:19:25,147 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 18:19:26,385 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 18:19:29,117 [podnet.py] => Exemplar size: 497
2024-08-30 18:19:29,118 [trainer.py] => CNN: {'total': 70.67, '00-04': 60.63, '05-06': 95.75, 'old': 60.63, 'new': 95.75}
2024-08-30 18:19:29,118 [trainer.py] => NME: {'total': 76.95, '00-04': 77.73, '05-06': 75.0, 'old': 77.73, 'new': 75.0}
2024-08-30 18:19:29,119 [trainer.py] => CNN top1 curve: [88.9, 70.67]
2024-08-30 18:19:29,119 [trainer.py] => CNN top5 curve: [100.0, 98.07]
2024-08-30 18:19:29,119 [trainer.py] => NME top1 curve: [88.9, 76.95]
2024-08-30 18:19:29,119 [trainer.py] => NME top5 curve: [100.0, 98.19]

2024-08-30 18:19:29,120 [trainer.py] => Average Accuracy (CNN): 79.785
2024-08-30 18:19:29,120 [trainer.py] => Average Accuracy (NME): 82.92500000000001
2024-08-30 18:19:29,121 [trainer.py] => All params: 3879745
2024-08-30 18:19:29,121 [trainer.py] => Trainable params: 3879745
2024-08-30 18:19:29,122 [podnet.py] => Learning on 7-9
2024-08-30 18:19:29,152 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 18:19:30,961 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 1.35, Spatial_loss 0.94, Flat_loss 0.93, Train_acc 79.45, Test_acc 25.09
2024-08-30 18:19:32,538 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.58, Spatial_loss 0.88, Flat_loss 0.66, Train_acc 88.59, Test_acc 25.17
2024-08-30 18:19:34,120 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.45, Spatial_loss 0.81, Flat_loss 0.54, Train_acc 90.77, Test_acc 30.30
2024-08-30 18:19:35,692 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.35, Spatial_loss 0.75, Flat_loss 0.49, Train_acc 92.04, Test_acc 37.17
2024-08-30 18:19:37,343 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.31, Spatial_loss 0.73, Flat_loss 0.46, Train_acc 93.40, Test_acc 41.50
2024-08-30 18:19:38,927 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.22, Spatial_loss 0.66, Flat_loss 0.43, Train_acc 95.40, Test_acc 38.28
2024-08-30 18:19:40,583 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.19, Spatial_loss 0.65, Flat_loss 0.40, Train_acc 96.42, Test_acc 50.28
2024-08-30 18:19:42,247 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.14, Spatial_loss 0.59, Flat_loss 0.38, Train_acc 97.87, Test_acc 45.50
2024-08-30 18:19:44,193 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.18, Spatial_loss 0.68, Flat_loss 0.40, Train_acc 96.38, Test_acc 47.65
2024-08-30 18:19:45,634 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.11, Spatial_loss 0.57, Flat_loss 0.35, Train_acc 98.55, Test_acc 55.57
2024-08-30 18:19:47,502 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.08, Spatial_loss 0.50, Flat_loss 0.32, Train_acc 99.56, Test_acc 55.85
2024-08-30 18:19:49,196 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.07, Spatial_loss 0.46, Flat_loss 0.31, Train_acc 99.82, Test_acc 59.61
2024-08-30 18:19:51,447 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.07, Spatial_loss 0.45, Flat_loss 0.29, Train_acc 99.84, Test_acc 61.50
2024-08-30 18:19:53,168 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.44, Flat_loss 0.29, Train_acc 99.71, Test_acc 57.52
2024-08-30 18:19:55,208 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.43, Flat_loss 0.28, Train_acc 99.62, Test_acc 61.48
2024-08-30 18:19:57,474 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.07, Spatial_loss 0.42, Flat_loss 0.28, Train_acc 99.82, Test_acc 53.80
2024-08-30 18:19:59,322 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.06, Spatial_loss 0.39, Flat_loss 0.27, Train_acc 99.96, Test_acc 59.74
2024-08-30 18:20:01,172 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.39, Flat_loss 0.26, Train_acc 99.78, Test_acc 61.35
2024-08-30 18:20:03,310 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.07, Spatial_loss 0.40, Flat_loss 0.27, Train_acc 99.40, Test_acc 57.46
2024-08-30 18:20:05,147 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.06, Spatial_loss 0.38, Flat_loss 0.26, Train_acc 99.98, Test_acc 59.48
2024-08-30 18:20:06,682 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.06, Spatial_loss 0.37, Flat_loss 0.25, Train_acc 99.91, Test_acc 59.06
2024-08-30 18:20:08,341 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.06, Spatial_loss 0.37, Flat_loss 0.25, Train_acc 99.84, Test_acc 60.76
2024-08-30 18:20:10,432 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.35, Flat_loss 0.24, Train_acc 100.00, Test_acc 57.24
2024-08-30 18:20:11,940 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.06, Spatial_loss 0.38, Flat_loss 0.24, Train_acc 99.89, Test_acc 60.80
2024-08-30 18:20:14,020 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.36, Flat_loss 0.24, Train_acc 99.84, Test_acc 52.43
2024-08-30 18:20:16,336 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.10, Spatial_loss 0.48, Flat_loss 0.28, Train_acc 98.15, Test_acc 53.65
2024-08-30 18:20:18,723 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.09, Spatial_loss 0.46, Flat_loss 0.28, Train_acc 98.82, Test_acc 53.91
2024-08-30 18:20:20,562 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.40, Flat_loss 0.25, Train_acc 99.87, Test_acc 60.24
2024-08-30 18:20:22,689 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.24, Train_acc 99.89, Test_acc 58.81
2024-08-30 18:20:24,157 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.23, Train_acc 100.00, Test_acc 57.96
2024-08-30 18:20:25,732 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.22, Train_acc 99.98, Test_acc 61.37
2024-08-30 18:20:27,404 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.22, Train_acc 100.00, Test_acc 62.44
2024-08-30 18:20:29,093 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.22, Train_acc 99.96, Test_acc 62.33
2024-08-30 18:20:30,653 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.23, Train_acc 99.82, Test_acc 61.00
2024-08-30 18:20:32,208 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.22, Train_acc 99.93, Test_acc 58.41
2024-08-30 18:20:33,798 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.06, Spatial_loss 0.36, Flat_loss 0.23, Train_acc 99.84, Test_acc 63.33
2024-08-30 18:20:35,401 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.07, Spatial_loss 0.40, Flat_loss 0.25, Train_acc 99.49, Test_acc 57.31
2024-08-30 18:20:37,177 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.06, Spatial_loss 0.36, Flat_loss 0.22, Train_acc 99.89, Test_acc 58.48
2024-08-30 18:20:39,070 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.22, Train_acc 99.98, Test_acc 62.04
2024-08-30 18:20:40,785 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.21, Train_acc 100.00, Test_acc 59.89
2024-08-30 18:20:42,580 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.21, Train_acc 99.96, Test_acc 59.70
2024-08-30 18:20:44,193 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.21, Train_acc 100.00, Test_acc 58.52
2024-08-30 18:20:45,964 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.21, Train_acc 99.91, Test_acc 58.43
2024-08-30 18:20:47,552 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.21, Train_acc 100.00, Test_acc 57.39
2024-08-30 18:20:49,152 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.21, Train_acc 99.91, Test_acc 63.11
2024-08-30 18:20:50,624 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.21, Train_acc 100.00, Test_acc 61.44
2024-08-30 18:20:52,183 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.20, Train_acc 99.98, Test_acc 57.00
2024-08-30 18:20:53,720 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.20, Train_acc 100.00, Test_acc 61.76
2024-08-30 18:20:55,180 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.20, Train_acc 99.98, Test_acc 62.30
2024-08-30 18:20:56,732 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.20, Train_acc 99.96, Test_acc 60.07
2024-08-30 18:20:58,382 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.20, Train_acc 99.96, Test_acc 60.31
2024-08-30 18:20:59,922 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.20, Train_acc 100.00, Test_acc 60.96
2024-08-30 18:21:01,433 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.20, Train_acc 100.00, Test_acc 60.30
2024-08-30 18:21:03,182 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.19, Train_acc 100.00, Test_acc 60.33
2024-08-30 18:21:04,973 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.19, Train_acc 100.00, Test_acc 59.26
2024-08-30 18:21:06,789 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.19, Train_acc 99.98, Test_acc 59.02
2024-08-30 18:21:08,605 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.19, Train_acc 100.00, Test_acc 62.37
2024-08-30 18:21:10,402 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.20, Train_acc 99.91, Test_acc 56.69
2024-08-30 18:21:12,562 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 100.00, Test_acc 59.87
2024-08-30 18:21:14,189 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.19, Train_acc 99.98, Test_acc 63.17
2024-08-30 18:21:15,741 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 99.98, Test_acc 59.80
2024-08-30 18:21:17,789 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.19, Train_acc 100.00, Test_acc 58.41
2024-08-30 18:21:19,883 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 100.00, Test_acc 57.56
2024-08-30 18:21:21,693 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.19, Train_acc 99.98, Test_acc 57.02
2024-08-30 18:21:23,627 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.54
2024-08-30 18:21:25,200 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 100.00, Test_acc 56.09
2024-08-30 18:21:27,000 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.04
2024-08-30 18:21:28,736 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 99.98, Test_acc 62.04
2024-08-30 18:21:30,394 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 100.00, Test_acc 56.00
2024-08-30 18:21:32,066 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.19, Train_acc 100.00, Test_acc 59.28
2024-08-30 18:21:33,645 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.70
2024-08-30 18:21:35,191 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.20
2024-08-30 18:21:36,779 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.18, Train_acc 99.98, Test_acc 55.20
2024-08-30 18:21:38,429 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.18, Train_acc 99.96, Test_acc 60.46
2024-08-30 18:21:40,160 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.18, Train_acc 100.00, Test_acc 55.24
2024-08-30 18:21:42,077 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.69
2024-08-30 18:21:43,753 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.18, Train_acc 100.00, Test_acc 58.57
2024-08-30 18:21:45,311 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.61
2024-08-30 18:21:46,849 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 99.98, Test_acc 59.43
2024-08-30 18:21:48,403 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 99.91, Test_acc 58.17
2024-08-30 18:21:50,141 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.06, Spatial_loss 0.33, Flat_loss 0.19, Train_acc 99.76, Test_acc 47.39
2024-08-30 18:21:51,714 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.06, Spatial_loss 0.36, Flat_loss 0.20, Train_acc 99.51, Test_acc 55.41
2024-08-30 18:21:53,267 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.22, Spatial_loss 0.55, Flat_loss 0.30, Train_acc 95.40, Test_acc 32.63
2024-08-30 18:21:55,005 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.43, Spatial_loss 0.80, Flat_loss 0.41, Train_acc 91.48, Test_acc 49.89
2024-08-30 18:21:57,059 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.24, Spatial_loss 0.69, Flat_loss 0.36, Train_acc 93.97, Test_acc 55.39
2024-08-30 18:21:58,841 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.09, Spatial_loss 0.54, Flat_loss 0.28, Train_acc 98.69, Test_acc 59.00
2024-08-30 18:22:00,436 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.06, Spatial_loss 0.44, Flat_loss 0.24, Train_acc 99.56, Test_acc 61.65
2024-08-30 18:22:02,222 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.23, Train_acc 99.96, Test_acc 62.22
2024-08-30 18:22:03,727 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.21, Train_acc 99.98, Test_acc 60.89
2024-08-30 18:22:05,469 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.20, Train_acc 99.98, Test_acc 61.91
2024-08-30 18:22:06,874 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.20, Train_acc 100.00, Test_acc 59.81
2024-08-30 18:22:08,986 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.19, Train_acc 99.93, Test_acc 60.76
2024-08-30 18:22:10,437 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.35, Flat_loss 0.21, Train_acc 99.76, Test_acc 62.11
2024-08-30 18:22:12,591 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.20, Train_acc 99.96, Test_acc 61.43
2024-08-30 18:22:14,025 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 99.98, Test_acc 62.24
2024-08-30 18:22:15,612 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.50
2024-08-30 18:22:17,321 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 99.98, Test_acc 62.96
2024-08-30 18:22:19,091 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.06, Spatial_loss 0.32, Flat_loss 0.20, Train_acc 99.78, Test_acc 63.56
2024-08-30 18:22:20,731 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.20, Train_acc 99.78, Test_acc 59.61
2024-08-30 18:22:22,373 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.19, Train_acc 99.98, Test_acc 61.57
2024-08-30 18:22:24,099 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.39
2024-08-30 18:22:25,991 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.85
2024-08-30 18:22:27,831 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.18, Train_acc 100.00, Test_acc 58.37
2024-08-30 18:22:29,870 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.29, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.06
2024-08-30 18:22:31,487 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.26
2024-08-30 18:22:33,572 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.18, Train_acc 100.00, Test_acc 62.07
2024-08-30 18:22:35,770 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.18, Train_acc 100.00, Test_acc 58.83
2024-08-30 18:22:37,943 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.91
2024-08-30 18:22:39,534 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.18, Train_acc 99.98, Test_acc 62.11
2024-08-30 18:22:41,233 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.98
2024-08-30 18:22:42,792 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 61.19
2024-08-30 18:22:44,221 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.48
2024-08-30 18:22:45,779 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.20
2024-08-30 18:22:47,446 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.98
2024-08-30 18:22:49,164 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.78
2024-08-30 18:22:51,028 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 64.24
2024-08-30 18:22:52,674 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.17
2024-08-30 18:22:54,319 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.57
2024-08-30 18:22:55,817 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.30
2024-08-30 18:22:57,351 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.54
2024-08-30 18:22:59,034 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.17, Train_acc 100.00, Test_acc 62.02
2024-08-30 18:23:00,925 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 99.98, Test_acc 61.76
2024-08-30 18:23:02,566 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 99.98, Test_acc 61.17
2024-08-30 18:23:04,254 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 99.98, Test_acc 55.24
2024-08-30 18:23:06,158 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.06, Spatial_loss 0.32, Flat_loss 0.20, Train_acc 99.78, Test_acc 63.00
2024-08-30 18:23:07,977 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.28, Flat_loss 0.18, Train_acc 99.98, Test_acc 58.91
2024-08-30 18:23:09,472 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.17, Train_acc 99.98, Test_acc 61.17
2024-08-30 18:23:11,210 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.06, Spatial_loss 0.28, Flat_loss 0.18, Train_acc 99.60, Test_acc 58.85
2024-08-30 18:23:13,198 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 99.98, Test_acc 56.00
2024-08-30 18:23:15,225 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.07, Spatial_loss 0.40, Flat_loss 0.22, Train_acc 99.51, Test_acc 59.63
2024-08-30 18:23:17,200 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.09
2024-08-30 18:23:19,257 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.18, Train_acc 99.96, Test_acc 60.81
2024-08-30 18:23:21,216 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.96
2024-08-30 18:23:23,188 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.44
2024-08-30 18:23:25,296 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.46
2024-08-30 18:23:27,135 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 58.78
2024-08-30 18:23:28,712 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.05, Spatial_loss 0.27, Flat_loss 0.17, Train_acc 100.00, Test_acc 58.91
2024-08-30 18:23:30,318 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 61.02
2024-08-30 18:23:32,047 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 99.98, Test_acc 61.93
2024-08-30 18:23:33,568 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 61.87
2024-08-30 18:23:35,043 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 61.98
2024-08-30 18:23:36,548 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.06
2024-08-30 18:23:38,070 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.83
2024-08-30 18:23:39,541 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.17
2024-08-30 18:23:41,134 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.41
2024-08-30 18:23:42,728 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 99.98, Test_acc 60.44
2024-08-30 18:23:44,314 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 99.98, Test_acc 61.48
2024-08-30 18:23:45,953 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.98
2024-08-30 18:23:47,482 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.81
2024-08-30 18:23:48,917 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.30
2024-08-30 18:23:50,419 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.00
2024-08-30 18:23:52,026 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.91
2024-08-30 18:23:53,668 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.87
2024-08-30 18:23:55,317 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.22
2024-08-30 18:23:56,947 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.05, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 58.11
2024-08-30 18:23:58,587 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.19
2024-08-30 18:24:00,218 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.96
2024-08-30 18:24:02,092 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.96
2024-08-30 18:24:03,708 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.16, Train_acc 100.00, Test_acc 65.13
2024-08-30 18:24:05,187 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.48
2024-08-30 18:24:06,653 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.89
2024-08-30 18:24:08,248 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 99.93, Test_acc 62.98
2024-08-30 18:24:09,862 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.06, Spatial_loss 0.30, Flat_loss 0.20, Train_acc 99.71, Test_acc 62.33
2024-08-30 18:24:11,589 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.17, Train_acc 100.00, Test_acc 58.69
2024-08-30 18:24:13,625 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.93
2024-08-30 18:24:15,466 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.09
2024-08-30 18:24:17,436 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.09
2024-08-30 18:24:19,408 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.37
2024-08-30 18:24:21,136 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.37
2024-08-30 18:24:22,810 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.05, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.22
2024-08-30 18:24:24,458 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 99.98, Test_acc 63.04
2024-08-30 18:24:25,913 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.65
2024-08-30 18:24:27,460 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.15
2024-08-30 18:24:29,105 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.87
2024-08-30 18:24:30,582 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.39
2024-08-30 18:24:32,156 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.11
2024-08-30 18:24:33,761 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.17
2024-08-30 18:24:35,391 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.83
2024-08-30 18:24:36,987 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.93
2024-08-30 18:24:38,606 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.89
2024-08-30 18:24:40,185 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.63
2024-08-30 18:24:42,298 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.44
2024-08-30 18:24:43,898 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.74
2024-08-30 18:24:46,091 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.85
2024-08-30 18:24:48,077 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.35
2024-08-30 18:24:49,760 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.22
2024-08-30 18:24:51,896 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.28
2024-08-30 18:24:53,799 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.72
2024-08-30 18:24:55,221 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 99.96, Test_acc 59.78
2024-08-30 18:24:56,936 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.19, Train_acc 99.82, Test_acc 60.11
2024-08-30 18:24:58,708 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.05, Spatial_loss 0.24, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.59
2024-08-30 18:25:00,477 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.41
2024-08-30 18:25:02,150 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.78
2024-08-30 18:25:03,903 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.06
2024-08-30 18:25:05,521 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.16, Train_acc 99.98, Test_acc 60.74
2024-08-30 18:25:07,318 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.39
2024-08-30 18:25:08,950 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.56
2024-08-30 18:25:10,574 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.74
2024-08-30 18:25:12,198 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.00
2024-08-30 18:25:13,801 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.33
2024-08-30 18:25:15,385 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 99.98, Test_acc 60.96
2024-08-30 18:25:16,886 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.67
2024-08-30 18:25:18,375 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.63
2024-08-30 18:25:19,921 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.70
2024-08-30 18:25:21,540 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 64.02
2024-08-30 18:25:23,068 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.24
2024-08-30 18:25:24,667 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.35
2024-08-30 18:25:26,218 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.96
2024-08-30 18:25:27,756 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.78
2024-08-30 18:25:29,195 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.35
2024-08-30 18:25:30,664 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.28
2024-08-30 18:25:32,158 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.56
2024-08-30 18:25:33,752 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.94
2024-08-30 18:25:35,353 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.02
2024-08-30 18:25:36,966 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.35
2024-08-30 18:25:38,643 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.15
2024-08-30 18:25:40,264 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.09
2024-08-30 18:25:41,841 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.69
2024-08-30 18:25:43,659 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.26
2024-08-30 18:25:45,681 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.46
2024-08-30 18:25:47,424 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.70
2024-08-30 18:25:49,177 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.78
2024-08-30 18:25:50,987 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.69
2024-08-30 18:25:52,709 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.72
2024-08-30 18:25:54,337 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.11
2024-08-30 18:25:55,943 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.69
2024-08-30 18:25:57,475 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.78
2024-08-30 18:25:58,981 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.89
2024-08-30 18:26:00,615 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.39
2024-08-30 18:26:02,299 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.94
2024-08-30 18:26:03,898 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.39
2024-08-30 18:26:05,515 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.37
2024-08-30 18:26:07,085 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.02
2024-08-30 18:26:08,726 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.35
2024-08-30 18:26:10,421 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.13
2024-08-30 18:26:12,077 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.76
2024-08-30 18:26:13,727 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.24
2024-08-30 18:26:15,409 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.63
2024-08-30 18:26:17,176 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.37
2024-08-30 18:26:18,829 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.35
2024-08-30 18:26:20,410 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.11
2024-08-30 18:26:22,040 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.07
2024-08-30 18:26:23,868 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.76
2024-08-30 18:26:25,727 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.00
2024-08-30 18:26:27,462 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.19
2024-08-30 18:26:29,440 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 63.20
2024-08-30 18:26:31,153 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.50
2024-08-30 18:26:32,655 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.76
2024-08-30 18:26:34,368 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.85
2024-08-30 18:26:36,138 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.91
2024-08-30 18:26:37,891 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.17
2024-08-30 18:26:39,536 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 99.98, Test_acc 63.44
2024-08-30 18:26:41,383 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.87
2024-08-30 18:26:43,056 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.17
2024-08-30 18:26:44,662 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.37
2024-08-30 18:26:46,297 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.00
2024-08-30 18:26:47,797 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.54
2024-08-30 18:26:49,415 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.09
2024-08-30 18:26:50,979 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.96
2024-08-30 18:26:52,624 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.52
2024-08-30 18:26:54,183 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.43
2024-08-30 18:26:55,689 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.17
2024-08-30 18:26:57,219 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.93
2024-08-30 18:26:58,804 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.02
2024-08-30 18:27:00,256 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.07
2024-08-30 18:27:01,781 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.28
2024-08-30 18:27:03,387 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.09
2024-08-30 18:27:04,992 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.81
2024-08-30 18:27:06,647 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.48
2024-08-30 18:27:08,081 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.63
2024-08-30 18:27:09,680 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.67
2024-08-30 18:27:11,337 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.30
2024-08-30 18:27:12,799 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.02
2024-08-30 18:27:14,417 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.52
2024-08-30 18:27:16,003 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.85
2024-08-30 18:27:17,613 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.00
2024-08-30 18:27:19,226 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.96
2024-08-30 18:27:20,797 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.72
2024-08-30 18:27:22,407 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.83
2024-08-30 18:27:24,046 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.06
2024-08-30 18:27:25,616 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.30
2024-08-30 18:27:27,191 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.83
2024-08-30 18:27:28,833 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.31
2024-08-30 18:27:30,419 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.80
2024-08-30 18:27:31,885 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.48
2024-08-30 18:27:33,426 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.37
2024-08-30 18:27:34,982 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.63
2024-08-30 18:27:36,618 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.17
2024-08-30 18:27:38,216 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.20
2024-08-30 18:27:39,843 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.48
2024-08-30 18:27:41,431 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.89
2024-08-30 18:27:43,114 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.07
2024-08-30 18:27:44,863 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.39
2024-08-30 18:27:46,614 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.41
2024-08-30 18:27:48,143 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.83
2024-08-30 18:27:49,724 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.15
2024-08-30 18:27:51,282 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.28
2024-08-30 18:27:52,839 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.72
2024-08-30 18:27:54,398 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.09
2024-08-30 18:27:55,876 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.24
2024-08-30 18:27:56,331 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 18:27:56,332 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 18:27:58,091 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 18:28:00,062 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 18:28:01,759 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 18:28:04,468 [podnet.py] => Exemplar size: 495
2024-08-30 18:28:04,469 [trainer.py] => CNN: {'total': 63.24, '00-04': 55.83, '05-06': 46.83, '07-08': 98.17, 'old': 53.26, 'new': 98.17}
2024-08-30 18:28:04,469 [trainer.py] => NME: {'total': 67.04, '00-04': 70.4, '05-06': 41.5, '07-08': 84.17, 'old': 62.14, 'new': 84.17}
2024-08-30 18:28:04,469 [trainer.py] => CNN top1 curve: [88.9, 70.67, 63.24]
2024-08-30 18:28:04,469 [trainer.py] => CNN top5 curve: [100.0, 98.07, 93.39]
2024-08-30 18:28:04,469 [trainer.py] => NME top1 curve: [88.9, 76.95, 67.04]
2024-08-30 18:28:04,469 [trainer.py] => NME top5 curve: [100.0, 98.19, 95.2]

2024-08-30 18:28:04,469 [trainer.py] => Average Accuracy (CNN): 74.27
2024-08-30 18:28:04,469 [trainer.py] => Average Accuracy (NME): 77.63000000000001
2024-08-30 18:28:04,470 [trainer.py] => Forgetting (CNN): 40.995000000000005
