2024-08-30 17:27:30,603 [trainer.py] => config: ./exps/podnet.json
2024-08-30 17:27:30,603 [trainer.py] => prefix: reproduce
2024-08-30 17:27:30,603 [trainer.py] => dataset: hrrp9
2024-08-30 17:27:30,603 [trainer.py] => memory_size: 500
2024-08-30 17:27:30,603 [trainer.py] => memory_per_class: 20
2024-08-30 17:27:30,603 [trainer.py] => fixed_memory: False
2024-08-30 17:27:30,603 [trainer.py] => shuffle: True
2024-08-30 17:27:30,603 [trainer.py] => init_cls: 5
2024-08-30 17:27:30,603 [trainer.py] => increment: 2
2024-08-30 17:27:30,603 [trainer.py] => model_name: podnet
2024-08-30 17:27:30,603 [trainer.py] => convnet_type: resnet18
2024-08-30 17:27:30,603 [trainer.py] => init_train: True
2024-08-30 17:27:30,604 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-30 17:27:30,604 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-30 17:27:30,604 [trainer.py] => device: [device(type='cuda', index=8)]
2024-08-30 17:27:30,604 [trainer.py] => seed: 1993
2024-08-30 17:27:31,105 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-30 17:27:31,223 [trainer.py] => All params: 3843904
2024-08-30 17:27:31,223 [trainer.py] => Trainable params: 3843904
2024-08-30 17:27:31,223 [podnet.py] => Learning on 0-5
2024-08-30 17:27:31,263 [podnet.py] => Adaptive factor: 0
2024-08-30 17:27:33,737 [podnet.py] => Task 0, Epoch 1/300 (LR 0.10000) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-30 17:27:35,175 [podnet.py] => Task 0, Epoch 2/300 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.86, Test_acc 32.63
2024-08-30 17:27:36,737 [podnet.py] => Task 0, Epoch 3/300 (LR 0.09998) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.47, Test_acc 36.30
2024-08-30 17:27:38,433 [podnet.py] => Task 0, Epoch 4/300 (LR 0.09996) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.33, Test_acc 59.63
2024-08-30 17:27:40,211 [podnet.py] => Task 0, Epoch 5/300 (LR 0.09993) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.73, Test_acc 55.37
2024-08-30 17:27:41,710 [podnet.py] => Task 0, Epoch 6/300 (LR 0.09990) => LSC_loss 0.25, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.02, Test_acc 72.20
2024-08-30 17:27:43,173 [podnet.py] => Task 0, Epoch 7/300 (LR 0.09987) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.42, Test_acc 67.40
2024-08-30 17:27:44,941 [podnet.py] => Task 0, Epoch 8/300 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.66, Test_acc 74.30
2024-08-30 17:27:46,499 [podnet.py] => Task 0, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.07, Test_acc 74.60
2024-08-30 17:27:48,024 [podnet.py] => Task 0, Epoch 10/300 (LR 0.09973) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.47, Test_acc 78.70
2024-08-30 17:27:49,568 [podnet.py] => Task 0, Epoch 11/300 (LR 0.09967) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 70.47
2024-08-30 17:27:51,054 [podnet.py] => Task 0, Epoch 12/300 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 82.27
2024-08-30 17:27:52,727 [podnet.py] => Task 0, Epoch 13/300 (LR 0.09954) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.29, Test_acc 82.53
2024-08-30 17:27:54,270 [podnet.py] => Task 0, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 67.27
2024-08-30 17:27:55,810 [podnet.py] => Task 0, Epoch 15/300 (LR 0.09938) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.94, Test_acc 79.93
2024-08-30 17:27:57,455 [podnet.py] => Task 0, Epoch 16/300 (LR 0.09930) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.08, Test_acc 83.07
2024-08-30 17:27:59,194 [podnet.py] => Task 0, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 72.60
2024-08-30 17:28:00,820 [podnet.py] => Task 0, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.56, Test_acc 80.30
2024-08-30 17:28:02,343 [podnet.py] => Task 0, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 81.57
2024-08-30 17:28:03,881 [podnet.py] => Task 0, Epoch 20/300 (LR 0.09891) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 79.03
2024-08-30 17:28:05,457 [podnet.py] => Task 0, Epoch 21/300 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 83.07
2024-08-30 17:28:07,098 [podnet.py] => Task 0, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 74.17
2024-08-30 17:28:08,748 [podnet.py] => Task 0, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 82.63
2024-08-30 17:28:10,591 [podnet.py] => Task 0, Epoch 24/300 (LR 0.09843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 82.83
2024-08-30 17:28:12,411 [podnet.py] => Task 0, Epoch 25/300 (LR 0.09830) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.27, Test_acc 84.33
2024-08-30 17:28:14,105 [podnet.py] => Task 0, Epoch 26/300 (LR 0.09816) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.83, Test_acc 85.20
2024-08-30 17:28:15,650 [podnet.py] => Task 0, Epoch 27/300 (LR 0.09801) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 85.90
2024-08-30 17:28:17,286 [podnet.py] => Task 0, Epoch 28/300 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 74.20
2024-08-30 17:28:18,788 [podnet.py] => Task 0, Epoch 29/300 (LR 0.09771) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 82.03
2024-08-30 17:28:20,387 [podnet.py] => Task 0, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 76.93
2024-08-30 17:28:22,002 [podnet.py] => Task 0, Epoch 31/300 (LR 0.09739) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 79.40
2024-08-30 17:28:23,557 [podnet.py] => Task 0, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.66, Test_acc 77.10
2024-08-30 17:28:25,409 [podnet.py] => Task 0, Epoch 33/300 (LR 0.09704) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.02, Test_acc 79.23
2024-08-30 17:28:27,017 [podnet.py] => Task 0, Epoch 34/300 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.07, Test_acc 86.00
2024-08-30 17:28:28,918 [podnet.py] => Task 0, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 74.73
2024-08-30 17:28:30,685 [podnet.py] => Task 0, Epoch 36/300 (LR 0.09649) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.14, Test_acc 79.27
2024-08-30 17:28:32,490 [podnet.py] => Task 0, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 84.97
2024-08-30 17:28:34,075 [podnet.py] => Task 0, Epoch 38/300 (LR 0.09609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 84.70
2024-08-30 17:28:35,580 [podnet.py] => Task 0, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 82.83
2024-08-30 17:28:37,153 [podnet.py] => Task 0, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 75.70
2024-08-30 17:28:38,771 [podnet.py] => Task 0, Epoch 41/300 (LR 0.09546) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.97, Test_acc 83.57
2024-08-30 17:28:40,450 [podnet.py] => Task 0, Epoch 42/300 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 82.43
2024-08-30 17:28:42,073 [podnet.py] => Task 0, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 86.70
2024-08-30 17:28:43,632 [podnet.py] => Task 0, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 71.70
2024-08-30 17:28:45,291 [podnet.py] => Task 0, Epoch 45/300 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.30
2024-08-30 17:28:46,785 [podnet.py] => Task 0, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 80.70
2024-08-30 17:28:48,350 [podnet.py] => Task 0, Epoch 47/300 (LR 0.09407) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 84.67
2024-08-30 17:28:49,799 [podnet.py] => Task 0, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 83.63
2024-08-30 17:28:51,523 [podnet.py] => Task 0, Epoch 49/300 (LR 0.09356) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 85.03
2024-08-30 17:28:53,128 [podnet.py] => Task 0, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 82.57
2024-08-30 17:28:55,083 [podnet.py] => Task 0, Epoch 51/300 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 88.17
2024-08-30 17:28:56,872 [podnet.py] => Task 0, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 80.73
2024-08-30 17:28:58,403 [podnet.py] => Task 0, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 78.07
2024-08-30 17:29:00,005 [podnet.py] => Task 0, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.67
2024-08-30 17:29:01,844 [podnet.py] => Task 0, Epoch 55/300 (LR 0.09193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 82.83
2024-08-30 17:29:03,349 [podnet.py] => Task 0, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 82.33
2024-08-30 17:29:04,868 [podnet.py] => Task 0, Epoch 57/300 (LR 0.09135) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 82.83
2024-08-30 17:29:06,364 [podnet.py] => Task 0, Epoch 58/300 (LR 0.09106) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.50, Test_acc 84.40
2024-08-30 17:29:07,924 [podnet.py] => Task 0, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 70.30
2024-08-30 17:29:09,685 [podnet.py] => Task 0, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.83
2024-08-30 17:29:11,298 [podnet.py] => Task 0, Epoch 61/300 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.69, Test_acc 83.20
2024-08-30 17:29:13,139 [podnet.py] => Task 0, Epoch 62/300 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 86.53
2024-08-30 17:29:14,921 [podnet.py] => Task 0, Epoch 63/300 (LR 0.08951) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 84.83
2024-08-30 17:29:16,548 [podnet.py] => Task 0, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 84.63
2024-08-30 17:29:18,137 [podnet.py] => Task 0, Epoch 65/300 (LR 0.08886) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 81.80
2024-08-30 17:29:19,665 [podnet.py] => Task 0, Epoch 66/300 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 81.17
2024-08-30 17:29:21,209 [podnet.py] => Task 0, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 82.20
2024-08-30 17:29:22,783 [podnet.py] => Task 0, Epoch 68/300 (LR 0.08785) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.91, Test_acc 83.43
2024-08-30 17:29:24,533 [podnet.py] => Task 0, Epoch 69/300 (LR 0.08751) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 84.27
2024-08-30 17:29:26,105 [podnet.py] => Task 0, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.47, Test_acc 84.80
2024-08-30 17:29:27,713 [podnet.py] => Task 0, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 88.13
2024-08-30 17:29:29,271 [podnet.py] => Task 0, Epoch 72/300 (LR 0.08645) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 87.83
2024-08-30 17:29:30,942 [podnet.py] => Task 0, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 87.00
2024-08-30 17:29:32,450 [podnet.py] => Task 0, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 87.03
2024-08-30 17:29:34,076 [podnet.py] => Task 0, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.63
2024-08-30 17:29:35,647 [podnet.py] => Task 0, Epoch 76/300 (LR 0.08498) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.71, Test_acc 78.20
2024-08-30 17:29:37,280 [podnet.py] => Task 0, Epoch 77/300 (LR 0.08461) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 84.30
2024-08-30 17:29:39,016 [podnet.py] => Task 0, Epoch 78/300 (LR 0.08423) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 86.53
2024-08-30 17:29:40,610 [podnet.py] => Task 0, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 80.93
2024-08-30 17:29:42,311 [podnet.py] => Task 0, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 83.10
2024-08-30 17:29:43,982 [podnet.py] => Task 0, Epoch 81/300 (LR 0.08307) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.57
2024-08-30 17:29:45,577 [podnet.py] => Task 0, Epoch 82/300 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 86.83
2024-08-30 17:29:47,083 [podnet.py] => Task 0, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 84.97
2024-08-30 17:29:48,603 [podnet.py] => Task 0, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 80.90
2024-08-30 17:29:50,167 [podnet.py] => Task 0, Epoch 85/300 (LR 0.08147) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.01, Test_acc 86.27
2024-08-30 17:29:51,952 [podnet.py] => Task 0, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.85, Test_acc 80.87
2024-08-30 17:29:53,731 [podnet.py] => Task 0, Epoch 87/300 (LR 0.08065) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.98, Test_acc 78.57
2024-08-30 17:29:55,388 [podnet.py] => Task 0, Epoch 88/300 (LR 0.08023) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.55, Test_acc 85.57
2024-08-30 17:29:57,146 [podnet.py] => Task 0, Epoch 89/300 (LR 0.07981) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.87
2024-08-30 17:29:58,878 [podnet.py] => Task 0, Epoch 90/300 (LR 0.07939) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.83
2024-08-30 17:30:00,513 [podnet.py] => Task 0, Epoch 91/300 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 82.47
2024-08-30 17:30:02,102 [podnet.py] => Task 0, Epoch 92/300 (LR 0.07854) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 86.23
2024-08-30 17:30:03,857 [podnet.py] => Task 0, Epoch 93/300 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.07
2024-08-30 17:30:05,655 [podnet.py] => Task 0, Epoch 94/300 (LR 0.07767) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 84.03
2024-08-30 17:30:07,348 [podnet.py] => Task 0, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 80.70
2024-08-30 17:30:09,031 [podnet.py] => Task 0, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 85.07
2024-08-30 17:30:10,885 [podnet.py] => Task 0, Epoch 97/300 (LR 0.07635) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 79.37
2024-08-30 17:30:12,757 [podnet.py] => Task 0, Epoch 98/300 (LR 0.07590) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 87.33
2024-08-30 17:30:14,392 [podnet.py] => Task 0, Epoch 99/300 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.21, Test_acc 84.17
2024-08-30 17:30:15,953 [podnet.py] => Task 0, Epoch 100/300 (LR 0.07500) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 85.00
2024-08-30 17:30:17,415 [podnet.py] => Task 0, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 82.50
2024-08-30 17:30:18,942 [podnet.py] => Task 0, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.40
2024-08-30 17:30:20,582 [podnet.py] => Task 0, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 83.40
2024-08-30 17:30:22,076 [podnet.py] => Task 0, Epoch 104/300 (LR 0.07316) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 80.57
2024-08-30 17:30:23,674 [podnet.py] => Task 0, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 88.03
2024-08-30 17:30:25,207 [podnet.py] => Task 0, Epoch 106/300 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 85.80
2024-08-30 17:30:26,764 [podnet.py] => Task 0, Epoch 107/300 (LR 0.07176) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 80.70
2024-08-30 17:30:28,376 [podnet.py] => Task 0, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 85.73
2024-08-30 17:30:29,991 [podnet.py] => Task 0, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.50
2024-08-30 17:30:31,803 [podnet.py] => Task 0, Epoch 110/300 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.13
2024-08-30 17:30:33,773 [podnet.py] => Task 0, Epoch 111/300 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 87.13
2024-08-30 17:30:35,744 [podnet.py] => Task 0, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.84, Test_acc 84.77
2024-08-30 17:30:37,424 [podnet.py] => Task 0, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 85.40
2024-08-30 17:30:39,107 [podnet.py] => Task 0, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 83.33
2024-08-30 17:30:40,635 [podnet.py] => Task 0, Epoch 115/300 (LR 0.06792) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 84.27
2024-08-30 17:30:42,140 [podnet.py] => Task 0, Epoch 116/300 (LR 0.06743) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.73
2024-08-30 17:30:43,601 [podnet.py] => Task 0, Epoch 117/300 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 79.07
2024-08-30 17:30:45,158 [podnet.py] => Task 0, Epoch 118/300 (LR 0.06644) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 84.63
2024-08-30 17:30:46,687 [podnet.py] => Task 0, Epoch 119/300 (LR 0.06595) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.53
2024-08-30 17:30:48,195 [podnet.py] => Task 0, Epoch 120/300 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 83.80
2024-08-30 17:30:49,714 [podnet.py] => Task 0, Epoch 121/300 (LR 0.06495) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 87.63
2024-08-30 17:30:51,425 [podnet.py] => Task 0, Epoch 122/300 (LR 0.06445) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 83.27
2024-08-30 17:30:53,055 [podnet.py] => Task 0, Epoch 123/300 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.34, Test_acc 82.97
2024-08-30 17:30:54,741 [podnet.py] => Task 0, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 83.40
2024-08-30 17:30:56,468 [podnet.py] => Task 0, Epoch 125/300 (LR 0.06294) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.77
2024-08-30 17:30:58,025 [podnet.py] => Task 0, Epoch 126/300 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 86.57
2024-08-30 17:30:59,588 [podnet.py] => Task 0, Epoch 127/300 (LR 0.06193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.36, Test_acc 86.00
2024-08-30 17:31:01,114 [podnet.py] => Task 0, Epoch 128/300 (LR 0.06142) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 84.60
2024-08-30 17:31:02,637 [podnet.py] => Task 0, Epoch 129/300 (LR 0.06091) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 87.97
2024-08-30 17:31:04,165 [podnet.py] => Task 0, Epoch 130/300 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 82.23
2024-08-30 17:31:05,836 [podnet.py] => Task 0, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.50, Test_acc 86.90
2024-08-30 17:31:07,534 [podnet.py] => Task 0, Epoch 132/300 (LR 0.05937) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.73
2024-08-30 17:31:09,220 [podnet.py] => Task 0, Epoch 133/300 (LR 0.05885) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.57
2024-08-30 17:31:10,793 [podnet.py] => Task 0, Epoch 134/300 (LR 0.05834) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.87
2024-08-30 17:31:12,276 [podnet.py] => Task 0, Epoch 135/300 (LR 0.05782) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 85.53
2024-08-30 17:31:13,823 [podnet.py] => Task 0, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 79.13
2024-08-30 17:31:15,350 [podnet.py] => Task 0, Epoch 137/300 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 88.00
2024-08-30 17:31:16,854 [podnet.py] => Task 0, Epoch 138/300 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.33
2024-08-30 17:31:18,453 [podnet.py] => Task 0, Epoch 139/300 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 88.17
2024-08-30 17:31:19,928 [podnet.py] => Task 0, Epoch 140/300 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 82.47
2024-08-30 17:31:21,429 [podnet.py] => Task 0, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 84.93
2024-08-30 17:31:23,083 [podnet.py] => Task 0, Epoch 142/300 (LR 0.05418) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.70, Test_acc 84.13
2024-08-30 17:31:24,819 [podnet.py] => Task 0, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 84.60
2024-08-30 17:31:26,389 [podnet.py] => Task 0, Epoch 144/300 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 87.83
2024-08-30 17:31:27,994 [podnet.py] => Task 0, Epoch 145/300 (LR 0.05262) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.60
2024-08-30 17:31:29,526 [podnet.py] => Task 0, Epoch 146/300 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 81.50
2024-08-30 17:31:31,084 [podnet.py] => Task 0, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 86.77
2024-08-30 17:31:32,624 [podnet.py] => Task 0, Epoch 148/300 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 84.77
2024-08-30 17:31:34,159 [podnet.py] => Task 0, Epoch 149/300 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 85.70
2024-08-30 17:31:36,051 [podnet.py] => Task 0, Epoch 150/300 (LR 0.05000) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 86.57
2024-08-30 17:31:37,761 [podnet.py] => Task 0, Epoch 151/300 (LR 0.04948) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 84.43
2024-08-30 17:31:39,270 [podnet.py] => Task 0, Epoch 152/300 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 88.60
2024-08-30 17:31:40,816 [podnet.py] => Task 0, Epoch 153/300 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 87.07
2024-08-30 17:31:42,366 [podnet.py] => Task 0, Epoch 154/300 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 87.57
2024-08-30 17:31:44,072 [podnet.py] => Task 0, Epoch 155/300 (LR 0.04738) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 81.10
2024-08-30 17:31:45,865 [podnet.py] => Task 0, Epoch 156/300 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.67, Test_acc 88.10
2024-08-30 17:31:47,458 [podnet.py] => Task 0, Epoch 157/300 (LR 0.04634) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.87
2024-08-30 17:31:49,081 [podnet.py] => Task 0, Epoch 158/300 (LR 0.04582) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.53
2024-08-30 17:31:50,838 [podnet.py] => Task 0, Epoch 159/300 (LR 0.04529) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.43
2024-08-30 17:31:52,338 [podnet.py] => Task 0, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 86.20
2024-08-30 17:31:53,875 [podnet.py] => Task 0, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 84.87
2024-08-30 17:31:55,582 [podnet.py] => Task 0, Epoch 162/300 (LR 0.04373) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 88.27
2024-08-30 17:31:57,278 [podnet.py] => Task 0, Epoch 163/300 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 85.63
2024-08-30 17:31:58,965 [podnet.py] => Task 0, Epoch 164/300 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 83.27
2024-08-30 17:32:00,473 [podnet.py] => Task 0, Epoch 165/300 (LR 0.04218) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 85.37
2024-08-30 17:32:02,327 [podnet.py] => Task 0, Epoch 166/300 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 89.13
2024-08-30 17:32:03,881 [podnet.py] => Task 0, Epoch 167/300 (LR 0.04115) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.80
2024-08-30 17:32:05,452 [podnet.py] => Task 0, Epoch 168/300 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 90.17
2024-08-30 17:32:07,219 [podnet.py] => Task 0, Epoch 169/300 (LR 0.04012) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.03
2024-08-30 17:32:08,887 [podnet.py] => Task 0, Epoch 170/300 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 89.47
2024-08-30 17:32:10,532 [podnet.py] => Task 0, Epoch 171/300 (LR 0.03909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.13
2024-08-30 17:32:12,058 [podnet.py] => Task 0, Epoch 172/300 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 89.47
2024-08-30 17:32:13,691 [podnet.py] => Task 0, Epoch 173/300 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.13
2024-08-30 17:32:15,161 [podnet.py] => Task 0, Epoch 174/300 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 84.43
2024-08-30 17:32:16,906 [podnet.py] => Task 0, Epoch 175/300 (LR 0.03706) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.30
2024-08-30 17:32:18,721 [podnet.py] => Task 0, Epoch 176/300 (LR 0.03655) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 81.37
2024-08-30 17:32:20,642 [podnet.py] => Task 0, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.73, Test_acc 85.23
2024-08-30 17:32:22,469 [podnet.py] => Task 0, Epoch 178/300 (LR 0.03555) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 85.83
2024-08-30 17:32:24,033 [podnet.py] => Task 0, Epoch 179/300 (LR 0.03505) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 83.43
2024-08-30 17:32:25,624 [podnet.py] => Task 0, Epoch 180/300 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 88.90
2024-08-30 17:32:27,241 [podnet.py] => Task 0, Epoch 181/300 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 88.60
2024-08-30 17:32:28,700 [podnet.py] => Task 0, Epoch 182/300 (LR 0.03356) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-08-30 17:32:30,178 [podnet.py] => Task 0, Epoch 183/300 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.73
2024-08-30 17:32:31,988 [podnet.py] => Task 0, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.11, Test_acc 88.17
2024-08-30 17:32:33,835 [podnet.py] => Task 0, Epoch 185/300 (LR 0.03208) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 87.47
2024-08-30 17:32:35,358 [podnet.py] => Task 0, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 79.03
2024-08-30 17:32:37,032 [podnet.py] => Task 0, Epoch 187/300 (LR 0.03111) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.53, Test_acc 84.80
2024-08-30 17:32:38,600 [podnet.py] => Task 0, Epoch 188/300 (LR 0.03062) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 85.53
2024-08-30 17:32:40,166 [podnet.py] => Task 0, Epoch 189/300 (LR 0.03014) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 88.90
2024-08-30 17:32:41,652 [podnet.py] => Task 0, Epoch 190/300 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 86.80
2024-08-30 17:32:43,302 [podnet.py] => Task 0, Epoch 191/300 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.20
2024-08-30 17:32:45,057 [podnet.py] => Task 0, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 86.90
2024-08-30 17:32:46,754 [podnet.py] => Task 0, Epoch 193/300 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 89.30
2024-08-30 17:32:48,597 [podnet.py] => Task 0, Epoch 194/300 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.27
2024-08-30 17:32:50,174 [podnet.py] => Task 0, Epoch 195/300 (LR 0.02730) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.47
2024-08-30 17:32:51,790 [podnet.py] => Task 0, Epoch 196/300 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.73
2024-08-30 17:32:53,312 [podnet.py] => Task 0, Epoch 197/300 (LR 0.02637) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.63
2024-08-30 17:32:54,874 [podnet.py] => Task 0, Epoch 198/300 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.23
2024-08-30 17:32:56,587 [podnet.py] => Task 0, Epoch 199/300 (LR 0.02545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.60
2024-08-30 17:32:58,208 [podnet.py] => Task 0, Epoch 200/300 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.67
2024-08-30 17:33:00,133 [podnet.py] => Task 0, Epoch 201/300 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.67
2024-08-30 17:33:01,752 [podnet.py] => Task 0, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 88.40
2024-08-30 17:33:03,248 [podnet.py] => Task 0, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 88.30
2024-08-30 17:33:04,921 [podnet.py] => Task 0, Epoch 204/300 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 87.87
2024-08-30 17:33:06,577 [podnet.py] => Task 0, Epoch 205/300 (LR 0.02277) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 88.73
2024-08-30 17:33:08,155 [podnet.py] => Task 0, Epoch 206/300 (LR 0.02233) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.10
2024-08-30 17:33:09,672 [podnet.py] => Task 0, Epoch 207/300 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.13
2024-08-30 17:33:11,416 [podnet.py] => Task 0, Epoch 208/300 (LR 0.02146) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 88.40
2024-08-30 17:33:12,996 [podnet.py] => Task 0, Epoch 209/300 (LR 0.02104) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 89.43
2024-08-30 17:33:14,542 [podnet.py] => Task 0, Epoch 210/300 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.40
2024-08-30 17:33:16,117 [podnet.py] => Task 0, Epoch 211/300 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 86.90
2024-08-30 17:33:17,829 [podnet.py] => Task 0, Epoch 212/300 (LR 0.01977) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 86.67
2024-08-30 17:33:19,333 [podnet.py] => Task 0, Epoch 213/300 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-08-30 17:33:20,891 [podnet.py] => Task 0, Epoch 214/300 (LR 0.01894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 89.67
2024-08-30 17:33:22,422 [podnet.py] => Task 0, Epoch 215/300 (LR 0.01853) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 89.77
2024-08-30 17:33:23,909 [podnet.py] => Task 0, Epoch 216/300 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-30 17:33:25,486 [podnet.py] => Task 0, Epoch 217/300 (LR 0.01773) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.83
2024-08-30 17:33:27,074 [podnet.py] => Task 0, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 89.30
2024-08-30 17:33:29,031 [podnet.py] => Task 0, Epoch 219/300 (LR 0.01693) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.13
2024-08-30 17:33:30,609 [podnet.py] => Task 0, Epoch 220/300 (LR 0.01654) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.87
2024-08-30 17:33:32,256 [podnet.py] => Task 0, Epoch 221/300 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-30 17:33:33,907 [podnet.py] => Task 0, Epoch 222/300 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 17:33:35,456 [podnet.py] => Task 0, Epoch 223/300 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-30 17:33:36,986 [podnet.py] => Task 0, Epoch 224/300 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:33:38,577 [podnet.py] => Task 0, Epoch 225/300 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 17:33:40,163 [podnet.py] => Task 0, Epoch 226/300 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:33:41,914 [podnet.py] => Task 0, Epoch 227/300 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-30 17:33:43,491 [podnet.py] => Task 0, Epoch 228/300 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-08-30 17:33:45,265 [podnet.py] => Task 0, Epoch 229/300 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:33:46,844 [podnet.py] => Task 0, Epoch 230/300 (LR 0.01284) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-08-30 17:33:48,397 [podnet.py] => Task 0, Epoch 231/300 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.00
2024-08-30 17:33:49,931 [podnet.py] => Task 0, Epoch 232/300 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.90
2024-08-30 17:33:51,525 [podnet.py] => Task 0, Epoch 233/300 (LR 0.01181) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.07
2024-08-30 17:33:53,502 [podnet.py] => Task 0, Epoch 234/300 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-08-30 17:33:55,276 [podnet.py] => Task 0, Epoch 235/300 (LR 0.01114) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.03
2024-08-30 17:33:57,026 [podnet.py] => Task 0, Epoch 236/300 (LR 0.01082) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 89.43
2024-08-30 17:33:58,575 [podnet.py] => Task 0, Epoch 237/300 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:34:00,251 [podnet.py] => Task 0, Epoch 238/300 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:34:01,741 [podnet.py] => Task 0, Epoch 239/300 (LR 0.00986) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-30 17:34:03,339 [podnet.py] => Task 0, Epoch 240/300 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:34:05,016 [podnet.py] => Task 0, Epoch 241/300 (LR 0.00924) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:34:06,529 [podnet.py] => Task 0, Epoch 242/300 (LR 0.00894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-30 17:34:08,216 [podnet.py] => Task 0, Epoch 243/300 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:34:09,778 [podnet.py] => Task 0, Epoch 244/300 (LR 0.00835) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:34:11,250 [podnet.py] => Task 0, Epoch 245/300 (LR 0.00807) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-30 17:34:12,909 [podnet.py] => Task 0, Epoch 246/300 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:34:14,418 [podnet.py] => Task 0, Epoch 247/300 (LR 0.00751) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 17:34:15,957 [podnet.py] => Task 0, Epoch 248/300 (LR 0.00723) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-30 17:34:17,456 [podnet.py] => Task 0, Epoch 249/300 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-30 17:34:19,028 [podnet.py] => Task 0, Epoch 250/300 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:34:20,929 [podnet.py] => Task 0, Epoch 251/300 (LR 0.00644) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-30 17:34:22,594 [podnet.py] => Task 0, Epoch 252/300 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 17:34:24,089 [podnet.py] => Task 0, Epoch 253/300 (LR 0.00593) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-30 17:34:25,661 [podnet.py] => Task 0, Epoch 254/300 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:34:27,193 [podnet.py] => Task 0, Epoch 255/300 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:34:28,752 [podnet.py] => Task 0, Epoch 256/300 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-30 17:34:30,240 [podnet.py] => Task 0, Epoch 257/300 (LR 0.00498) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.57
2024-08-30 17:34:31,883 [podnet.py] => Task 0, Epoch 258/300 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.87
2024-08-30 17:34:33,618 [podnet.py] => Task 0, Epoch 259/300 (LR 0.00454) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-30 17:34:35,296 [podnet.py] => Task 0, Epoch 260/300 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-30 17:34:36,884 [podnet.py] => Task 0, Epoch 261/300 (LR 0.00411) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-30 17:34:38,476 [podnet.py] => Task 0, Epoch 262/300 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 17:34:40,002 [podnet.py] => Task 0, Epoch 263/300 (LR 0.00371) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-30 17:34:41,542 [podnet.py] => Task 0, Epoch 264/300 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:34:43,100 [podnet.py] => Task 0, Epoch 265/300 (LR 0.00332) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-30 17:34:44,990 [podnet.py] => Task 0, Epoch 266/300 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-30 17:34:46,840 [podnet.py] => Task 0, Epoch 267/300 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:34:48,390 [podnet.py] => Task 0, Epoch 268/300 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:34:49,937 [podnet.py] => Task 0, Epoch 269/300 (LR 0.00261) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:34:51,537 [podnet.py] => Task 0, Epoch 270/300 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-30 17:34:53,035 [podnet.py] => Task 0, Epoch 271/300 (LR 0.00229) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 17:34:54,594 [podnet.py] => Task 0, Epoch 272/300 (LR 0.00213) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-30 17:34:56,131 [podnet.py] => Task 0, Epoch 273/300 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-30 17:34:57,846 [podnet.py] => Task 0, Epoch 274/300 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-30 17:34:59,783 [podnet.py] => Task 0, Epoch 275/300 (LR 0.00170) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:35:01,462 [podnet.py] => Task 0, Epoch 276/300 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-30 17:35:03,086 [podnet.py] => Task 0, Epoch 277/300 (LR 0.00144) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:35:04,619 [podnet.py] => Task 0, Epoch 278/300 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.70
2024-08-30 17:35:06,179 [podnet.py] => Task 0, Epoch 279/300 (LR 0.00120) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.97
2024-08-30 17:35:07,921 [podnet.py] => Task 0, Epoch 280/300 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:35:09,706 [podnet.py] => Task 0, Epoch 281/300 (LR 0.00099) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:35:11,406 [podnet.py] => Task 0, Epoch 282/300 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-30 17:35:12,917 [podnet.py] => Task 0, Epoch 283/300 (LR 0.00079) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-30 17:35:14,409 [podnet.py] => Task 0, Epoch 284/300 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-30 17:35:15,918 [podnet.py] => Task 0, Epoch 285/300 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-30 17:35:17,386 [podnet.py] => Task 0, Epoch 286/300 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-30 17:35:18,960 [podnet.py] => Task 0, Epoch 287/300 (LR 0.00046) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-30 17:35:20,660 [podnet.py] => Task 0, Epoch 288/300 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:35:22,414 [podnet.py] => Task 0, Epoch 289/300 (LR 0.00033) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-30 17:35:24,061 [podnet.py] => Task 0, Epoch 290/300 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:35:25,635 [podnet.py] => Task 0, Epoch 291/300 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-30 17:35:27,179 [podnet.py] => Task 0, Epoch 292/300 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:35:28,732 [podnet.py] => Task 0, Epoch 293/300 (LR 0.00013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-30 17:35:30,262 [podnet.py] => Task 0, Epoch 294/300 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-30 17:35:31,890 [podnet.py] => Task 0, Epoch 295/300 (LR 0.00007) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:35:33,688 [podnet.py] => Task 0, Epoch 296/300 (LR 0.00004) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-08-30 17:35:35,231 [podnet.py] => Task 0, Epoch 297/300 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.87
2024-08-30 17:35:36,789 [podnet.py] => Task 0, Epoch 298/300 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-30 17:35:38,295 [podnet.py] => Task 0, Epoch 299/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-30 17:35:39,818 [podnet.py] => Task 0, Epoch 300/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.90
2024-08-30 17:35:40,114 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:35:40,115 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:35:45,730 [podnet.py] => Exemplar size: 500
2024-08-30 17:35:45,730 [trainer.py] => CNN: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-30 17:35:45,730 [trainer.py] => NME: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-30 17:35:45,730 [trainer.py] => CNN top1 curve: [88.9]
2024-08-30 17:35:45,730 [trainer.py] => CNN top5 curve: [100.0]
2024-08-30 17:35:45,730 [trainer.py] => NME top1 curve: [88.9]
2024-08-30 17:35:45,730 [trainer.py] => NME top5 curve: [100.0]

2024-08-30 17:35:45,731 [trainer.py] => Average Accuracy (CNN): 88.9
2024-08-30 17:35:45,731 [trainer.py] => Average Accuracy (NME): 88.9
2024-08-30 17:35:45,731 [trainer.py] => All params: 3869505
2024-08-30 17:35:45,731 [trainer.py] => Trainable params: 3869505
2024-08-30 17:35:45,732 [podnet.py] => Learning on 5-7
2024-08-30 17:35:45,752 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-30 17:35:47,296 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 1.02, Spatial_loss 1.55, Flat_loss 0.72, Train_acc 74.38, Test_acc 16.50
2024-08-30 17:35:48,613 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.46, Spatial_loss 1.27, Flat_loss 0.50, Train_acc 89.07, Test_acc 32.88
2024-08-30 17:35:50,095 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.33, Spatial_loss 1.12, Flat_loss 0.41, Train_acc 92.20, Test_acc 44.83
2024-08-30 17:35:51,487 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.28, Spatial_loss 1.08, Flat_loss 0.38, Train_acc 93.07, Test_acc 46.76
2024-08-30 17:35:52,814 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.20, Spatial_loss 0.93, Flat_loss 0.32, Train_acc 95.60, Test_acc 50.81
2024-08-30 17:35:54,165 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.17, Spatial_loss 0.84, Flat_loss 0.29, Train_acc 96.47, Test_acc 56.24
2024-08-30 17:35:55,507 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.14, Spatial_loss 0.82, Flat_loss 0.27, Train_acc 97.40, Test_acc 53.05
2024-08-30 17:35:56,897 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.13, Spatial_loss 0.73, Flat_loss 0.26, Train_acc 97.67, Test_acc 58.81
2024-08-30 17:35:58,233 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.12, Spatial_loss 0.70, Flat_loss 0.25, Train_acc 97.91, Test_acc 63.88
2024-08-30 17:35:59,611 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.11, Spatial_loss 0.67, Flat_loss 0.24, Train_acc 98.69, Test_acc 63.14
2024-08-30 17:36:01,003 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.10, Spatial_loss 0.68, Flat_loss 0.24, Train_acc 98.60, Test_acc 61.62
2024-08-30 17:36:02,362 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.09, Spatial_loss 0.67, Flat_loss 0.23, Train_acc 98.89, Test_acc 58.43
2024-08-30 17:36:03,708 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.08, Spatial_loss 0.64, Flat_loss 0.22, Train_acc 99.20, Test_acc 60.17
2024-08-30 17:36:05,077 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.08, Spatial_loss 0.60, Flat_loss 0.21, Train_acc 99.07, Test_acc 68.17
2024-08-30 17:36:06,562 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.56, Flat_loss 0.21, Train_acc 99.69, Test_acc 66.98
2024-08-30 17:36:07,790 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.07, Spatial_loss 0.56, Flat_loss 0.21, Train_acc 99.53, Test_acc 64.71
2024-08-30 17:36:09,129 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.08, Spatial_loss 0.60, Flat_loss 0.22, Train_acc 99.29, Test_acc 65.14
2024-08-30 17:36:10,574 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.57, Flat_loss 0.21, Train_acc 99.33, Test_acc 60.33
2024-08-30 17:36:12,022 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.09, Spatial_loss 0.65, Flat_loss 0.22, Train_acc 98.76, Test_acc 62.71
2024-08-30 17:36:13,372 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.07, Spatial_loss 0.57, Flat_loss 0.21, Train_acc 99.29, Test_acc 64.52
2024-08-30 17:36:14,863 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.20, Train_acc 99.67, Test_acc 66.88
2024-08-30 17:36:16,390 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.19, Train_acc 99.71, Test_acc 60.31
2024-08-30 17:36:17,684 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.11, Spatial_loss 0.72, Flat_loss 0.24, Train_acc 97.73, Test_acc 67.19
2024-08-30 17:36:18,990 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.07, Spatial_loss 0.59, Flat_loss 0.21, Train_acc 99.16, Test_acc 60.83
2024-08-30 17:36:20,329 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.54, Flat_loss 0.20, Train_acc 99.64, Test_acc 65.48
2024-08-30 17:36:21,876 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.06, Spatial_loss 0.51, Flat_loss 0.19, Train_acc 99.82, Test_acc 62.29
2024-08-30 17:36:23,524 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.51, Flat_loss 0.18, Train_acc 99.73, Test_acc 60.79
2024-08-30 17:36:24,855 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.49, Flat_loss 0.18, Train_acc 99.73, Test_acc 59.71
2024-08-30 17:36:26,222 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.18, Train_acc 99.87, Test_acc 68.88
2024-08-30 17:36:27,523 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.17, Train_acc 99.84, Test_acc 68.76
2024-08-30 17:36:28,917 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.06, Spatial_loss 0.48, Flat_loss 0.17, Train_acc 99.73, Test_acc 63.81
2024-08-30 17:36:30,235 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.17, Train_acc 99.89, Test_acc 63.45
2024-08-30 17:36:31,529 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.17, Train_acc 99.87, Test_acc 69.07
2024-08-30 17:36:32,892 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.17, Train_acc 99.76, Test_acc 66.95
2024-08-30 17:36:34,264 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.16, Train_acc 99.93, Test_acc 65.64
2024-08-30 17:36:35,638 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.16, Train_acc 99.93, Test_acc 65.29
2024-08-30 17:36:37,019 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.16, Train_acc 99.87, Test_acc 62.07
2024-08-30 17:36:38,371 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.16, Train_acc 99.84, Test_acc 65.31
2024-08-30 17:36:39,669 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.48, Flat_loss 0.18, Train_acc 99.69, Test_acc 64.93
2024-08-30 17:36:40,986 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.17, Train_acc 99.87, Test_acc 61.19
2024-08-30 17:36:42,359 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.16, Train_acc 99.80, Test_acc 61.52
2024-08-30 17:36:43,812 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.16, Train_acc 99.73, Test_acc 64.17
2024-08-30 17:36:45,154 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.06, Spatial_loss 0.53, Flat_loss 0.18, Train_acc 99.51, Test_acc 63.67
2024-08-30 17:36:46,444 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.16, Train_acc 99.84, Test_acc 71.05
2024-08-30 17:36:47,881 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.16, Train_acc 99.78, Test_acc 64.50
2024-08-30 17:36:49,552 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.16, Train_acc 99.82, Test_acc 64.36
2024-08-30 17:36:50,972 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.15, Train_acc 99.87, Test_acc 67.67
2024-08-30 17:36:52,269 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.89, Test_acc 65.36
2024-08-30 17:36:53,642 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.91, Test_acc 65.29
2024-08-30 17:36:55,221 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.15, Train_acc 99.98, Test_acc 67.71
2024-08-30 17:36:56,842 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.15, Train_acc 99.91, Test_acc 63.12
2024-08-30 17:36:58,190 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.15, Train_acc 99.89, Test_acc 64.98
2024-08-30 17:36:59,586 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.04, Spatial_loss 0.45, Flat_loss 0.15, Train_acc 99.98, Test_acc 68.10
2024-08-30 17:37:01,213 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.15, Train_acc 99.89, Test_acc 67.50
2024-08-30 17:37:02,908 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.78, Test_acc 65.57
2024-08-30 17:37:04,315 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.71, Test_acc 61.71
2024-08-30 17:37:05,652 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.15, Train_acc 99.91, Test_acc 66.55
2024-08-30 17:37:06,920 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.84, Test_acc 66.67
2024-08-30 17:37:08,419 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.89, Test_acc 64.69
2024-08-30 17:37:09,800 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.67
2024-08-30 17:37:11,181 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.96, Test_acc 66.88
2024-08-30 17:37:12,489 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.98, Test_acc 64.38
2024-08-30 17:37:14,055 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 100.00, Test_acc 67.02
2024-08-30 17:37:15,368 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.76
2024-08-30 17:37:16,695 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.07, Spatial_loss 0.54, Flat_loss 0.18, Train_acc 98.76, Test_acc 65.10
2024-08-30 17:37:18,050 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.16, Train_acc 99.71, Test_acc 66.52
2024-08-30 17:37:19,391 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.04, Spatial_loss 0.42, Flat_loss 0.15, Train_acc 99.93, Test_acc 66.48
2024-08-30 17:37:20,675 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.15, Train_acc 99.91, Test_acc 64.62
2024-08-30 17:37:22,025 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.15, Train_acc 99.93, Test_acc 70.64
2024-08-30 17:37:23,378 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.98, Test_acc 66.90
2024-08-30 17:37:24,699 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.91, Test_acc 67.19
2024-08-30 17:37:26,041 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.89, Test_acc 70.00
2024-08-30 17:37:27,568 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.89, Test_acc 67.12
2024-08-30 17:37:28,966 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.91, Test_acc 69.98
2024-08-30 17:37:30,287 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.96, Test_acc 66.67
2024-08-30 17:37:31,804 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.14, Train_acc 99.98, Test_acc 65.10
2024-08-30 17:37:33,409 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.98, Test_acc 66.14
2024-08-30 17:37:34,751 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.96, Test_acc 67.52
2024-08-30 17:37:36,080 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.91, Test_acc 64.43
2024-08-30 17:37:37,676 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.16, Train_acc 99.76, Test_acc 62.74
2024-08-30 17:37:39,173 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.38
2024-08-30 17:37:40,625 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.05
2024-08-30 17:37:41,930 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.90
2024-08-30 17:37:43,231 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.90
2024-08-30 17:37:44,501 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.13, Train_acc 99.93, Test_acc 66.05
2024-08-30 17:37:45,873 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.98, Test_acc 66.86
2024-08-30 17:37:47,171 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.13, Train_acc 99.96, Test_acc 60.48
2024-08-30 17:37:48,519 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.07, Spatial_loss 0.51, Flat_loss 0.17, Train_acc 98.87, Test_acc 55.40
2024-08-30 17:37:49,921 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.16, Spatial_loss 0.71, Flat_loss 0.24, Train_acc 96.16, Test_acc 56.69
2024-08-30 17:37:51,230 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.12, Spatial_loss 0.71, Flat_loss 0.24, Train_acc 97.22, Test_acc 62.71
2024-08-30 17:37:52,559 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.20, Train_acc 99.31, Test_acc 64.36
2024-08-30 17:37:53,860 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.17, Train_acc 99.69, Test_acc 66.40
2024-08-30 17:37:55,177 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.04, Spatial_loss 0.43, Flat_loss 0.16, Train_acc 99.93, Test_acc 65.98
2024-08-30 17:37:56,573 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.15, Train_acc 100.00, Test_acc 66.52
2024-08-30 17:37:58,211 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.15, Train_acc 99.96, Test_acc 65.26
2024-08-30 17:37:59,532 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.89, Test_acc 66.14
2024-08-30 17:38:00,923 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.98, Test_acc 63.10
2024-08-30 17:38:02,264 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.14, Train_acc 99.93, Test_acc 67.93
2024-08-30 17:38:03,771 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.14, Train_acc 99.93, Test_acc 65.86
2024-08-30 17:38:05,209 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 99.93, Test_acc 64.69
2024-08-30 17:38:06,576 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.00
2024-08-30 17:38:07,985 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.14, Train_acc 99.93, Test_acc 67.07
2024-08-30 17:38:09,481 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.96, Test_acc 65.36
2024-08-30 17:38:10,746 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 99.98, Test_acc 65.29
2024-08-30 17:38:12,061 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.93, Test_acc 66.29
2024-08-30 17:38:13,443 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.10
2024-08-30 17:38:14,849 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.15, Train_acc 99.80, Test_acc 67.26
2024-08-30 17:38:16,169 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.39, Flat_loss 0.14, Train_acc 99.98, Test_acc 63.88
2024-08-30 17:38:17,557 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.83
2024-08-30 17:38:18,895 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.45
2024-08-30 17:38:20,268 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.98, Test_acc 60.67
2024-08-30 17:38:21,588 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.15, Train_acc 99.78, Test_acc 69.88
2024-08-30 17:38:23,112 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.14, Train_acc 99.98, Test_acc 65.36
2024-08-30 17:38:24,472 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.17
2024-08-30 17:38:25,947 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.74
2024-08-30 17:38:27,306 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.13, Train_acc 99.96, Test_acc 66.60
2024-08-30 17:38:28,810 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.13, Train_acc 100.00, Test_acc 64.60
2024-08-30 17:38:30,226 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.67
2024-08-30 17:38:31,570 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.67
2024-08-30 17:38:32,981 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.57
2024-08-30 17:38:34,391 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.12, Train_acc 99.96, Test_acc 66.21
2024-08-30 17:38:35,821 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 99.98, Test_acc 64.10
2024-08-30 17:38:37,163 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.52
2024-08-30 17:38:38,578 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.48
2024-08-30 17:38:39,998 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.13, Train_acc 99.93, Test_acc 67.10
2024-08-30 17:38:41,326 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.13, Train_acc 99.96, Test_acc 66.29
2024-08-30 17:38:42,626 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.96, Test_acc 61.14
2024-08-30 17:38:43,977 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.15, Train_acc 99.60, Test_acc 63.62
2024-08-30 17:38:45,288 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.14, Train_acc 99.96, Test_acc 64.14
2024-08-30 17:38:46,724 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.13, Train_acc 99.96, Test_acc 65.45
2024-08-30 17:38:48,169 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.88
2024-08-30 17:38:49,564 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.89, Test_acc 61.90
2024-08-30 17:38:51,033 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.16, Train_acc 99.51, Test_acc 65.48
2024-08-30 17:38:52,571 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.04, Spatial_loss 0.41, Flat_loss 0.14, Train_acc 99.82, Test_acc 64.74
2024-08-30 17:38:54,046 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.96, Test_acc 64.40
2024-08-30 17:38:55,928 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.43
2024-08-30 17:38:57,791 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.96, Test_acc 68.07
2024-08-30 17:38:59,305 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.55
2024-08-30 17:39:01,225 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.17
2024-08-30 17:39:02,804 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.33
2024-08-30 17:39:04,184 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.64
2024-08-30 17:39:05,559 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.05
2024-08-30 17:39:06,947 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 100.00, Test_acc 63.83
2024-08-30 17:39:08,388 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.13, Train_acc 99.96, Test_acc 60.36
2024-08-30 17:39:09,864 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.08, Spatial_loss 0.60, Flat_loss 0.19, Train_acc 98.31, Test_acc 63.67
2024-08-30 17:39:11,262 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.16, Train_acc 99.51, Test_acc 63.07
2024-08-30 17:39:12,721 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.14, Train_acc 99.93, Test_acc 63.19
2024-08-30 17:39:14,127 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.13, Train_acc 99.96, Test_acc 67.79
2024-08-30 17:39:15,490 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.93, Test_acc 67.57
2024-08-30 17:39:16,951 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.13, Train_acc 99.98, Test_acc 64.62
2024-08-30 17:39:18,490 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.13, Train_acc 99.96, Test_acc 65.67
2024-08-30 17:39:20,405 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.83
2024-08-30 17:39:22,439 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.02
2024-08-30 17:39:23,812 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.21
2024-08-30 17:39:25,275 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.14
2024-08-30 17:39:26,771 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.45
2024-08-30 17:39:28,211 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.74
2024-08-30 17:39:29,589 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.98
2024-08-30 17:39:31,045 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.36
2024-08-30 17:39:32,444 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.12, Train_acc 99.96, Test_acc 66.19
2024-08-30 17:39:33,979 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.74
2024-08-30 17:39:35,428 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.12
2024-08-30 17:39:36,798 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.83
2024-08-30 17:39:38,266 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.12
2024-08-30 17:39:39,965 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.60
2024-08-30 17:39:41,414 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.13, Train_acc 99.96, Test_acc 70.17
2024-08-30 17:39:43,186 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.31
2024-08-30 17:39:44,612 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.12, Train_acc 99.96, Test_acc 65.43
2024-08-30 17:39:46,172 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 99.98, Test_acc 67.50
2024-08-30 17:39:47,778 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 99.98, Test_acc 66.88
2024-08-30 17:39:49,217 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.36
2024-08-30 17:39:50,621 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.98
2024-08-30 17:39:52,196 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.69
2024-08-30 17:39:53,619 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.00
2024-08-30 17:39:55,000 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.67
2024-08-30 17:39:56,347 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.48
2024-08-30 17:39:58,391 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.88
2024-08-30 17:40:00,024 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.83
2024-08-30 17:40:01,637 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.05
2024-08-30 17:40:02,972 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.55
2024-08-30 17:40:04,329 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.90
2024-08-30 17:40:05,607 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.12, Train_acc 99.98, Test_acc 67.24
2024-08-30 17:40:07,103 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.83
2024-08-30 17:40:08,797 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.05
2024-08-30 17:40:10,527 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 99.93, Test_acc 66.17
2024-08-30 17:40:12,163 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.48
2024-08-30 17:40:13,869 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.43
2024-08-30 17:40:15,359 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 99.98, Test_acc 67.00
2024-08-30 17:40:16,698 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.69
2024-08-30 17:40:18,095 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.83
2024-08-30 17:40:19,749 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.12
2024-08-30 17:40:21,347 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.50
2024-08-30 17:40:23,133 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.86
2024-08-30 17:40:24,937 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.29
2024-08-30 17:40:26,650 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.12
2024-08-30 17:40:28,001 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 99.98, Test_acc 70.26
2024-08-30 17:40:29,362 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.07
2024-08-30 17:40:30,774 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.83
2024-08-30 17:40:32,313 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.40
2024-08-30 17:40:33,800 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.05
2024-08-30 17:40:35,232 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.69
2024-08-30 17:40:36,940 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.36
2024-08-30 17:40:38,458 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.64
2024-08-30 17:40:39,891 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.83
2024-08-30 17:40:41,286 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 99.96, Test_acc 66.71
2024-08-30 17:40:42,817 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.31
2024-08-30 17:40:44,223 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.64
2024-08-30 17:40:45,591 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.00
2024-08-30 17:40:46,881 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.14
2024-08-30 17:40:48,592 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 99.98, Test_acc 67.29
2024-08-30 17:40:50,239 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.52
2024-08-30 17:40:51,879 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.48
2024-08-30 17:40:53,384 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.38
2024-08-30 17:40:55,005 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.07
2024-08-30 17:40:56,404 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.07
2024-08-30 17:40:57,704 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.17
2024-08-30 17:40:59,172 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.79
2024-08-30 17:41:00,538 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.31
2024-08-30 17:41:01,829 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.05
2024-08-30 17:41:03,279 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.43
2024-08-30 17:41:05,012 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.43
2024-08-30 17:41:06,593 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.69
2024-08-30 17:41:08,056 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.67
2024-08-30 17:41:09,554 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.81
2024-08-30 17:41:11,056 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.31
2024-08-30 17:41:12,683 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.33
2024-08-30 17:41:14,256 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.48
2024-08-30 17:41:15,877 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.36
2024-08-30 17:41:17,371 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.03, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.90
2024-08-30 17:41:18,858 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.79
2024-08-30 17:41:20,263 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.17
2024-08-30 17:41:21,714 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.93
2024-08-30 17:41:23,212 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.57
2024-08-30 17:41:24,672 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.26
2024-08-30 17:41:26,298 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.62
2024-08-30 17:41:27,979 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.40
2024-08-30 17:41:29,710 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.17
2024-08-30 17:41:31,072 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.98
2024-08-30 17:41:32,559 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.24
2024-08-30 17:41:34,119 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.17
2024-08-30 17:41:35,864 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.71
2024-08-30 17:41:37,336 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.48
2024-08-30 17:41:38,849 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.10
2024-08-30 17:41:40,650 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.98
2024-08-30 17:41:42,326 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.43
2024-08-30 17:41:44,271 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.74
2024-08-30 17:41:45,868 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.90
2024-08-30 17:41:47,525 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-08-30 17:41:49,172 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.02
2024-08-30 17:41:51,155 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.62
2024-08-30 17:41:53,071 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.88
2024-08-30 17:41:55,245 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.00
2024-08-30 17:41:57,237 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-08-30 17:41:58,533 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.43
2024-08-30 17:41:59,936 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.81
2024-08-30 17:42:01,325 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.21
2024-08-30 17:42:02,803 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.07
2024-08-30 17:42:04,285 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.07
2024-08-30 17:42:05,668 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.55
2024-08-30 17:42:07,218 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-30 17:42:08,677 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.26
2024-08-30 17:42:10,176 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.57
2024-08-30 17:42:11,759 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.19, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.17
2024-08-30 17:42:13,389 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.38
2024-08-30 17:42:14,870 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.79
2024-08-30 17:42:16,282 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.38
2024-08-30 17:42:17,952 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.05
2024-08-30 17:42:19,611 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.29
2024-08-30 17:42:21,528 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.88
2024-08-30 17:42:23,374 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-30 17:42:24,991 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.38
2024-08-30 17:42:26,560 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.10
2024-08-30 17:42:28,143 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.79
2024-08-30 17:42:29,771 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.40
2024-08-30 17:42:31,441 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-30 17:42:33,188 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-30 17:42:34,806 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.31
2024-08-30 17:42:36,559 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-30 17:42:37,993 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-30 17:42:39,515 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.93
2024-08-30 17:42:40,846 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.88
2024-08-30 17:42:42,322 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.24
2024-08-30 17:42:43,987 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.64
2024-08-30 17:42:45,824 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.50
2024-08-30 17:42:47,798 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.45
2024-08-30 17:42:49,551 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.29
2024-08-30 17:42:50,967 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.86
2024-08-30 17:42:52,326 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.71
2024-08-30 17:42:53,762 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.00
2024-08-30 17:42:55,245 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.64
2024-08-30 17:42:56,690 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.69
2024-08-30 17:42:58,256 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-30 17:42:59,748 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.90
2024-08-30 17:43:01,265 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.64
2024-08-30 17:43:02,649 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.07
2024-08-30 17:43:04,044 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-30 17:43:05,372 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.93
2024-08-30 17:43:06,812 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.90
2024-08-30 17:43:08,086 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.69
2024-08-30 17:43:09,784 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.00
2024-08-30 17:43:10,117 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:43:10,118 [base.py] => Reducing exemplars...(100 per classes)
2024-08-30 17:43:11,242 [base.py] => Constructing exemplars...(100 per classes)
2024-08-30 17:43:13,155 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:43:14,213 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:43:16,676 [podnet.py] => Exemplar size: 497
2024-08-30 17:43:16,676 [trainer.py] => CNN: {'total': 71.0, '00-04': 61.03, '05-06': 95.92, 'old': 61.03, 'new': 95.92}
2024-08-30 17:43:16,677 [trainer.py] => NME: {'total': 77.64, '00-04': 79.77, '05-06': 72.33, 'old': 79.77, 'new': 72.33}
2024-08-30 17:43:16,677 [trainer.py] => CNN top1 curve: [88.9, 71.0]
2024-08-30 17:43:16,677 [trainer.py] => CNN top5 curve: [100.0, 98.0]
2024-08-30 17:43:16,677 [trainer.py] => NME top1 curve: [88.9, 77.64]
2024-08-30 17:43:16,677 [trainer.py] => NME top5 curve: [100.0, 98.07]

2024-08-30 17:43:16,677 [trainer.py] => Average Accuracy (CNN): 79.95
2024-08-30 17:43:16,677 [trainer.py] => Average Accuracy (NME): 83.27000000000001
2024-08-30 17:43:16,677 [trainer.py] => All params: 3879745
2024-08-30 17:43:16,677 [trainer.py] => Trainable params: 3879745
2024-08-30 17:43:16,678 [podnet.py] => Learning on 7-9
2024-08-30 17:43:16,715 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-30 17:43:18,305 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 1.39, Spatial_loss 1.22, Flat_loss 0.78, Train_acc 78.72, Test_acc 19.17
2024-08-30 17:43:19,747 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.59, Spatial_loss 1.15, Flat_loss 0.57, Train_acc 88.86, Test_acc 28.07
2024-08-30 17:43:21,292 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.41, Spatial_loss 1.03, Flat_loss 0.47, Train_acc 91.35, Test_acc 35.89
2024-08-30 17:43:22,772 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.30, Spatial_loss 0.97, Flat_loss 0.43, Train_acc 93.13, Test_acc 37.83
2024-08-30 17:43:24,178 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.22, Spatial_loss 0.84, Flat_loss 0.39, Train_acc 94.80, Test_acc 48.13
2024-08-30 17:43:25,700 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.17, Spatial_loss 0.78, Flat_loss 0.37, Train_acc 96.75, Test_acc 47.28
2024-08-30 17:43:27,105 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.16, Spatial_loss 0.78, Flat_loss 0.36, Train_acc 97.31, Test_acc 54.72
2024-08-30 17:43:28,591 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.12, Spatial_loss 0.73, Flat_loss 0.34, Train_acc 98.11, Test_acc 53.43
2024-08-30 17:43:30,126 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.09, Spatial_loss 0.68, Flat_loss 0.32, Train_acc 99.24, Test_acc 56.98
2024-08-30 17:43:31,558 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.09, Spatial_loss 0.63, Flat_loss 0.31, Train_acc 99.42, Test_acc 59.20
2024-08-30 17:43:33,072 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.08, Spatial_loss 0.59, Flat_loss 0.30, Train_acc 99.64, Test_acc 54.39
2024-08-30 17:43:34,682 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.07, Spatial_loss 0.57, Flat_loss 0.29, Train_acc 99.67, Test_acc 59.50
2024-08-30 17:43:36,530 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.07, Spatial_loss 0.57, Flat_loss 0.28, Train_acc 99.89, Test_acc 59.74
2024-08-30 17:43:38,307 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.55, Flat_loss 0.28, Train_acc 99.69, Test_acc 55.81
2024-08-30 17:43:39,897 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.56, Flat_loss 0.28, Train_acc 99.80, Test_acc 58.89
2024-08-30 17:43:41,358 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.06, Spatial_loss 0.55, Flat_loss 0.27, Train_acc 99.78, Test_acc 55.06
2024-08-30 17:43:42,827 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.26, Train_acc 99.91, Test_acc 53.91
2024-08-30 17:43:44,481 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.07, Spatial_loss 0.54, Flat_loss 0.27, Train_acc 99.80, Test_acc 56.11
2024-08-30 17:43:46,032 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.26, Train_acc 99.82, Test_acc 61.11
2024-08-30 17:43:47,495 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.05, Spatial_loss 0.49, Flat_loss 0.26, Train_acc 99.96, Test_acc 56.67
2024-08-30 17:43:48,860 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.06, Spatial_loss 0.49, Flat_loss 0.25, Train_acc 99.98, Test_acc 54.63
2024-08-30 17:43:50,739 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.09, Spatial_loss 0.61, Flat_loss 0.28, Train_acc 98.73, Test_acc 59.59
2024-08-30 17:43:52,498 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.07, Spatial_loss 0.56, Flat_loss 0.27, Train_acc 99.38, Test_acc 55.54
2024-08-30 17:43:54,177 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.08, Spatial_loss 0.60, Flat_loss 0.27, Train_acc 99.20, Test_acc 51.17
2024-08-30 17:43:55,637 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.52, Flat_loss 0.26, Train_acc 99.82, Test_acc 49.52
2024-08-30 17:43:57,223 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.07, Spatial_loss 0.59, Flat_loss 0.27, Train_acc 99.47, Test_acc 58.11
2024-08-30 17:43:58,757 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.06, Spatial_loss 0.51, Flat_loss 0.25, Train_acc 99.96, Test_acc 57.06
2024-08-30 17:44:00,410 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.24, Train_acc 99.82, Test_acc 59.70
2024-08-30 17:44:02,078 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.24, Train_acc 99.98, Test_acc 57.33
2024-08-30 17:44:03,616 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.23, Train_acc 100.00, Test_acc 55.54
2024-08-30 17:44:05,089 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.05, Spatial_loss 0.46, Flat_loss 0.23, Train_acc 99.98, Test_acc 59.61
2024-08-30 17:44:06,571 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.23, Train_acc 99.91, Test_acc 62.37
2024-08-30 17:44:08,152 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.06, Spatial_loss 0.48, Flat_loss 0.23, Train_acc 99.84, Test_acc 62.06
2024-08-30 17:44:09,603 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.23, Train_acc 99.89, Test_acc 61.76
2024-08-30 17:44:10,992 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.23, Train_acc 99.96, Test_acc 59.65
2024-08-30 17:44:12,500 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.06, Spatial_loss 0.49, Flat_loss 0.24, Train_acc 99.71, Test_acc 57.56
2024-08-30 17:44:14,017 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.22, Train_acc 99.87, Test_acc 56.80
2024-08-30 17:44:15,617 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.21, Train_acc 99.98, Test_acc 55.20
2024-08-30 17:44:17,163 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.21, Train_acc 100.00, Test_acc 59.37
2024-08-30 17:44:18,950 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.21, Train_acc 99.98, Test_acc 59.94
2024-08-30 17:44:20,302 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.21, Train_acc 100.00, Test_acc 58.13
2024-08-30 17:44:21,689 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.20, Train_acc 99.98, Test_acc 58.02
2024-08-30 17:44:23,265 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.20, Train_acc 99.96, Test_acc 56.50
2024-08-30 17:44:24,988 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.20, Train_acc 99.98, Test_acc 59.83
2024-08-30 17:44:26,437 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.20, Train_acc 99.98, Test_acc 61.20
2024-08-30 17:44:28,063 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.20, Train_acc 100.00, Test_acc 58.56
2024-08-30 17:44:29,556 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.20, Train_acc 99.96, Test_acc 56.78
2024-08-30 17:44:31,150 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.20, Train_acc 99.96, Test_acc 60.87
2024-08-30 17:44:32,910 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.20, Train_acc 99.93, Test_acc 59.20
2024-08-30 17:44:34,705 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.20, Train_acc 99.96, Test_acc 56.87
2024-08-30 17:44:36,356 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.19, Train_acc 99.98, Test_acc 57.35
2024-08-30 17:44:37,996 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.19, Train_acc 100.00, Test_acc 61.33
2024-08-30 17:44:39,808 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.19, Train_acc 99.93, Test_acc 56.63
2024-08-30 17:44:41,450 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.19, Train_acc 100.00, Test_acc 57.78
2024-08-30 17:44:42,973 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.19, Train_acc 99.98, Test_acc 57.04
2024-08-30 17:44:44,413 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.19, Train_acc 99.98, Test_acc 57.54
2024-08-30 17:44:45,990 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.19, Train_acc 100.00, Test_acc 63.78
2024-08-30 17:44:47,685 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.19, Train_acc 99.91, Test_acc 55.28
2024-08-30 17:44:49,261 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.19, Train_acc 99.93, Test_acc 59.85
2024-08-30 17:44:50,930 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.19, Train_acc 100.00, Test_acc 58.48
2024-08-30 17:44:52,608 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.19, Train_acc 99.98, Test_acc 59.74
2024-08-30 17:44:54,135 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.18, Train_acc 100.00, Test_acc 52.44
2024-08-30 17:44:55,749 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.18, Train_acc 99.96, Test_acc 54.67
2024-08-30 17:44:57,308 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.18, Train_acc 100.00, Test_acc 57.00
2024-08-30 17:44:58,748 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.89
2024-08-30 17:45:00,480 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.19, Train_acc 100.00, Test_acc 58.00
2024-08-30 17:45:02,121 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 99.98, Test_acc 58.48
2024-08-30 17:45:03,531 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.42, Flat_loss 0.18, Train_acc 99.98, Test_acc 52.59
2024-08-30 17:45:05,148 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 100.00, Test_acc 58.24
2024-08-30 17:45:06,627 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.31
2024-08-30 17:45:08,348 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 100.00, Test_acc 58.11
2024-08-30 17:45:09,836 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 99.98, Test_acc 57.61
2024-08-30 17:45:11,211 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 100.00, Test_acc 55.85
2024-08-30 17:45:12,733 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.20
2024-08-30 17:45:14,494 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.18, Train_acc 100.00, Test_acc 54.41
2024-08-30 17:45:16,342 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.18, Train_acc 99.98, Test_acc 57.65
2024-08-30 17:45:17,897 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 100.00, Test_acc 56.65
2024-08-30 17:45:19,321 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 100.00, Test_acc 58.46
2024-08-30 17:45:20,912 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 99.96, Test_acc 57.63
2024-08-30 17:45:22,441 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 99.89, Test_acc 59.20
2024-08-30 17:45:24,004 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 0.45, Flat_loss 0.19, Train_acc 99.82, Test_acc 58.96
2024-08-30 17:45:25,604 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.19, Train_acc 99.78, Test_acc 55.06
2024-08-30 17:45:27,309 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.47, Flat_loss 0.20, Train_acc 99.93, Test_acc 61.07
2024-08-30 17:45:28,779 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.19, Train_acc 99.87, Test_acc 58.20
2024-08-30 17:45:30,561 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.17, Spatial_loss 0.64, Flat_loss 0.26, Train_acc 97.04, Test_acc 52.54
2024-08-30 17:45:32,057 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.22, Spatial_loss 0.82, Flat_loss 0.30, Train_acc 95.60, Test_acc 60.48
2024-08-30 17:45:33,701 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.12, Spatial_loss 0.71, Flat_loss 0.27, Train_acc 97.80, Test_acc 57.44
2024-08-30 17:45:35,228 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.06, Spatial_loss 0.56, Flat_loss 0.23, Train_acc 99.78, Test_acc 60.98
2024-08-30 17:45:36,960 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.05, Spatial_loss 0.50, Flat_loss 0.21, Train_acc 99.89, Test_acc 58.19
2024-08-30 17:45:38,700 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.19, Train_acc 99.98, Test_acc 60.19
2024-08-30 17:45:40,384 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.05, Spatial_loss 0.44, Flat_loss 0.19, Train_acc 100.00, Test_acc 59.65
2024-08-30 17:45:42,061 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.19, Train_acc 99.96, Test_acc 59.31
2024-08-30 17:45:43,904 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.47, Flat_loss 0.21, Train_acc 99.67, Test_acc 58.22
2024-08-30 17:45:45,742 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.05, Spatial_loss 0.48, Flat_loss 0.20, Train_acc 99.87, Test_acc 58.09
2024-08-30 17:45:47,338 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.19, Train_acc 99.98, Test_acc 56.89
2024-08-30 17:45:48,937 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 100.00, Test_acc 59.44
2024-08-30 17:45:51,057 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.93
2024-08-30 17:45:52,570 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.19, Train_acc 99.96, Test_acc 62.17
2024-08-30 17:45:54,147 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 99.96, Test_acc 58.78
2024-08-30 17:45:55,706 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.57
2024-08-30 17:45:57,235 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.18, Train_acc 100.00, Test_acc 60.67
2024-08-30 17:45:58,778 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 99.96, Test_acc 60.35
2024-08-30 17:46:00,461 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.18, Train_acc 99.98, Test_acc 60.19
2024-08-30 17:46:01,980 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.18, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:46:03,355 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.18, Train_acc 100.00, Test_acc 57.59
2024-08-30 17:46:05,478 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 100.00, Test_acc 56.98
2024-08-30 17:46:07,373 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.18, Train_acc 99.98, Test_acc 53.52
2024-08-30 17:46:08,993 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 99.98, Test_acc 59.22
2024-08-30 17:46:10,544 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.07
2024-08-30 17:46:12,737 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.52
2024-08-30 17:46:14,488 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.50
2024-08-30 17:46:16,155 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.17, Train_acc 99.98, Test_acc 59.33
2024-08-30 17:46:17,860 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.18, Train_acc 99.96, Test_acc 56.67
2024-08-30 17:46:19,779 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.18, Train_acc 100.00, Test_acc 57.76
2024-08-30 17:46:21,486 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.17, Train_acc 99.98, Test_acc 59.67
2024-08-30 17:46:22,972 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.54
2024-08-30 17:46:24,383 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.17, Train_acc 99.98, Test_acc 61.91
2024-08-30 17:46:25,880 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.87
2024-08-30 17:46:27,400 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.30
2024-08-30 17:46:28,898 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 99.98, Test_acc 57.30
2024-08-30 17:46:30,466 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.04, Spatial_loss 0.40, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.67
2024-08-30 17:46:32,169 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.38, Flat_loss 0.17, Train_acc 99.98, Test_acc 60.26
2024-08-30 17:46:34,049 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.59
2024-08-30 17:46:35,880 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 99.98, Test_acc 52.65
2024-08-30 17:46:37,611 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.18, Train_acc 99.98, Test_acc 59.94
2024-08-30 17:46:39,226 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.39
2024-08-30 17:46:41,094 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.05, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.41
2024-08-30 17:46:42,876 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.18, Train_acc 99.96, Test_acc 60.15
2024-08-30 17:46:44,419 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 99.98, Test_acc 56.70
2024-08-30 17:46:45,912 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.06, Spatial_loss 0.47, Flat_loss 0.20, Train_acc 99.56, Test_acc 43.46
2024-08-30 17:46:47,454 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.09, Spatial_loss 0.53, Flat_loss 0.21, Train_acc 98.91, Test_acc 57.93
2024-08-30 17:46:49,112 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.41, Flat_loss 0.19, Train_acc 99.84, Test_acc 57.13
2024-08-30 17:46:50,873 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.39, Flat_loss 0.18, Train_acc 100.00, Test_acc 55.41
2024-08-30 17:46:52,362 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.40, Flat_loss 0.18, Train_acc 99.96, Test_acc 57.70
2024-08-30 17:46:53,893 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.18, Train_acc 99.98, Test_acc 55.76
2024-08-30 17:46:55,513 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.38, Flat_loss 0.17, Train_acc 100.00, Test_acc 55.78
2024-08-30 17:46:57,020 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.87
2024-08-30 17:46:58,434 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.22
2024-08-30 17:46:59,998 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.80
2024-08-30 17:47:01,922 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.83
2024-08-30 17:47:03,792 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.24
2024-08-30 17:47:05,796 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.20
2024-08-30 17:47:07,519 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.78
2024-08-30 17:47:09,408 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.83
2024-08-30 17:47:11,577 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 100.00, Test_acc 58.91
2024-08-30 17:47:13,620 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.17, Train_acc 100.00, Test_acc 58.56
2024-08-30 17:47:15,871 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 58.56
2024-08-30 17:47:17,630 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.63
2024-08-30 17:47:19,642 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.72
2024-08-30 17:47:21,866 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.16, Train_acc 99.98, Test_acc 59.67
2024-08-30 17:47:24,240 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.91
2024-08-30 17:47:26,562 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 57.06
2024-08-30 17:47:28,517 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 59.54
2024-08-30 17:47:30,238 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.37, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.33
2024-08-30 17:47:31,828 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 100.00, Test_acc 56.57
2024-08-30 17:47:33,565 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.16, Train_acc 100.00, Test_acc 56.54
2024-08-30 17:47:35,030 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.34, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.50
2024-08-30 17:47:36,946 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 100.00, Test_acc 56.89
2024-08-30 17:47:39,128 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.36, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.48
2024-08-30 17:47:41,286 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.87
2024-08-30 17:47:43,449 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.16, Train_acc 99.98, Test_acc 57.37
2024-08-30 17:47:45,521 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 99.93, Test_acc 55.44
2024-08-30 17:47:46,903 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.09, Spatial_loss 0.52, Flat_loss 0.22, Train_acc 98.71, Test_acc 60.31
2024-08-30 17:47:48,240 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.05, Spatial_loss 0.43, Flat_loss 0.19, Train_acc 99.82, Test_acc 55.44
2024-08-30 17:47:49,809 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.17, Train_acc 100.00, Test_acc 60.20
2024-08-30 17:47:52,095 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 100.00, Test_acc 61.22
2024-08-30 17:47:53,804 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 99.98, Test_acc 58.17
2024-08-30 17:47:55,715 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.35
2024-08-30 17:47:57,546 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.05, Spatial_loss 0.34, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.74
2024-08-30 17:47:59,692 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.05, Spatial_loss 0.33, Flat_loss 0.16, Train_acc 100.00, Test_acc 63.22
2024-08-30 17:48:01,636 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 100.00, Test_acc 62.04
2024-08-30 17:48:03,679 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 99.98, Test_acc 60.87
2024-08-30 17:48:05,927 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.05, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 100.00, Test_acc 61.87
2024-08-30 17:48:07,920 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.52
2024-08-30 17:48:09,749 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.43
2024-08-30 17:48:11,630 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.19
2024-08-30 17:48:13,847 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.37
2024-08-30 17:48:15,952 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.80
2024-08-30 17:48:18,239 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.31
2024-08-30 17:48:20,501 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.13
2024-08-30 17:48:22,690 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.15, Train_acc 99.98, Test_acc 59.20
2024-08-30 17:48:24,855 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.15
2024-08-30 17:48:26,549 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.19
2024-08-30 17:48:28,302 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.98, Test_acc 60.83
2024-08-30 17:48:30,239 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.06, Spatial_loss 0.35, Flat_loss 0.17, Train_acc 99.76, Test_acc 61.48
2024-08-30 17:48:32,592 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.26
2024-08-30 17:48:34,208 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.32, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.72
2024-08-30 17:48:35,668 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.93
2024-08-30 17:48:36,969 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 99.98, Test_acc 58.69
2024-08-30 17:48:38,389 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.05, Spatial_loss 0.36, Flat_loss 0.17, Train_acc 99.84, Test_acc 60.15
2024-08-30 17:48:39,883 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.16, Train_acc 100.00, Test_acc 60.11
2024-08-30 17:48:41,988 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.16, Train_acc 100.00, Test_acc 59.20
2024-08-30 17:48:44,233 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.05, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 99.98, Test_acc 58.91
2024-08-30 17:48:46,330 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.65
2024-08-30 17:48:48,484 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 99.98, Test_acc 60.43
2024-08-30 17:48:50,469 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.26
2024-08-30 17:48:52,231 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.24
2024-08-30 17:48:54,153 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.11
2024-08-30 17:48:56,570 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.91
2024-08-30 17:48:58,845 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.67
2024-08-30 17:49:00,483 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.57
2024-08-30 17:49:02,230 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.17
2024-08-30 17:49:04,255 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.30, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.00
2024-08-30 17:49:06,225 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.44
2024-08-30 17:49:08,478 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.31, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.96
2024-08-30 17:49:10,296 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.61
2024-08-30 17:49:12,621 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.91
2024-08-30 17:49:14,522 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.09
2024-08-30 17:49:16,493 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.72
2024-08-30 17:49:17,826 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.59
2024-08-30 17:49:20,082 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.76
2024-08-30 17:49:21,443 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 99.98, Test_acc 60.80
2024-08-30 17:49:23,658 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.76
2024-08-30 17:49:25,602 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.28, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.50
2024-08-30 17:49:27,417 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.30
2024-08-30 17:49:28,811 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.29, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.54
2024-08-30 17:49:30,381 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.41
2024-08-30 17:49:32,261 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.15, Train_acc 100.00, Test_acc 60.70
2024-08-30 17:49:34,523 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.11
2024-08-30 17:49:36,682 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.33
2024-08-30 17:49:38,358 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.27, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.74
2024-08-30 17:49:40,083 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.44
2024-08-30 17:49:42,150 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.02
2024-08-30 17:49:44,257 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.50
2024-08-30 17:49:45,954 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.28
2024-08-30 17:49:48,281 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.28
2024-08-30 17:49:50,480 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.35
2024-08-30 17:49:52,676 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 59.91
2024-08-30 17:49:54,623 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.44
2024-08-30 17:49:56,162 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.00
2024-08-30 17:49:58,021 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.54
2024-08-30 17:50:00,065 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.46
2024-08-30 17:50:02,423 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.43
2024-08-30 17:50:04,381 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.09
2024-08-30 17:50:06,031 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.26, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.19
2024-08-30 17:50:08,260 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.19
2024-08-30 17:50:09,658 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.30
2024-08-30 17:50:11,603 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.93
2024-08-30 17:50:13,180 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.63
2024-08-30 17:50:15,438 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.11
2024-08-30 17:50:17,378 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.05, Spatial_loss 0.25, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.00
2024-08-30 17:50:19,675 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.25, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.91
2024-08-30 17:50:21,295 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.39
2024-08-30 17:50:22,767 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.37
2024-08-30 17:50:24,969 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.13
2024-08-30 17:50:27,111 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 99.98, Test_acc 62.70
2024-08-30 17:50:29,025 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.19
2024-08-30 17:50:31,165 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.57
2024-08-30 17:50:32,850 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.28
2024-08-30 17:50:35,211 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.98
2024-08-30 17:50:37,072 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.05, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.09
2024-08-30 17:50:38,930 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.15, Train_acc 100.00, Test_acc 61.41
2024-08-30 17:50:40,930 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 100.00, Test_acc 62.83
2024-08-30 17:50:42,610 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.59
2024-08-30 17:50:44,866 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.93
2024-08-30 17:50:46,677 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.44
2024-08-30 17:50:48,526 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.43
2024-08-30 17:50:50,886 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.63
2024-08-30 17:50:53,019 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.24, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.24
2024-08-30 17:50:55,127 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.23, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.80
2024-08-30 17:50:56,605 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.54
2024-08-30 17:50:58,407 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.33
2024-08-30 17:50:59,992 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.48
2024-08-30 17:51:01,604 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.87
2024-08-30 17:51:03,160 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.52
2024-08-30 17:51:04,971 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.98
2024-08-30 17:51:06,970 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.48
2024-08-30 17:51:09,320 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.74
2024-08-30 17:51:11,145 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 61.57
2024-08-30 17:51:12,903 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.46
2024-08-30 17:51:15,080 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.33
2024-08-30 17:51:16,908 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.46
2024-08-30 17:51:19,269 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.37
2024-08-30 17:51:21,630 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.41
2024-08-30 17:51:23,730 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.50
2024-08-30 17:51:25,855 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.59
2024-08-30 17:51:27,894 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.57
2024-08-30 17:51:29,684 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.22, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.04
2024-08-30 17:51:31,497 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.56
2024-08-30 17:51:33,700 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.63
2024-08-30 17:51:35,870 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.81
2024-08-30 17:51:38,105 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.56
2024-08-30 17:51:40,044 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.19
2024-08-30 17:51:41,690 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.35
2024-08-30 17:51:43,150 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.52
2024-08-30 17:51:44,978 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.87
2024-08-30 17:51:46,887 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.15
2024-08-30 17:51:48,701 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.91
2024-08-30 17:51:50,388 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.52
2024-08-30 17:51:52,169 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.69
2024-08-30 17:51:53,634 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.63
2024-08-30 17:51:55,367 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.65
2024-08-30 17:51:57,638 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.21, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.19
2024-08-30 17:51:59,829 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.11
2024-08-30 17:52:01,346 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.31
2024-08-30 17:52:02,821 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.74
2024-08-30 17:52:04,655 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.33
2024-08-30 17:52:06,761 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.37
2024-08-30 17:52:09,019 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 63.04
2024-08-30 17:52:10,578 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.70
2024-08-30 17:52:10,985 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-30 17:52:10,985 [base.py] => Reducing exemplars...(71 per classes)
2024-08-30 17:52:12,481 [base.py] => Constructing exemplars...(71 per classes)
2024-08-30 17:52:14,224 [base.py] => Reducing exemplars...(55 per classes)
2024-08-30 17:52:15,654 [base.py] => Constructing exemplars...(55 per classes)
2024-08-30 17:52:18,003 [podnet.py] => Exemplar size: 495
2024-08-30 17:52:18,003 [trainer.py] => CNN: {'total': 62.7, '00-04': 53.7, '05-06': 49.25, '07-08': 98.67, 'old': 52.43, 'new': 98.67}
2024-08-30 17:52:18,003 [trainer.py] => NME: {'total': 68.2, '00-04': 70.27, '05-06': 43.33, '07-08': 87.92, 'old': 62.57, 'new': 87.92}
2024-08-30 17:52:18,003 [trainer.py] => CNN top1 curve: [88.9, 71.0, 62.7]
2024-08-30 17:52:18,003 [trainer.py] => CNN top5 curve: [100.0, 98.0, 93.56]
2024-08-30 17:52:18,004 [trainer.py] => NME top1 curve: [88.9, 77.64, 68.2]
2024-08-30 17:52:18,004 [trainer.py] => NME top5 curve: [100.0, 98.07, 95.56]

2024-08-30 17:52:18,004 [trainer.py] => Average Accuracy (CNN): 74.2
2024-08-30 17:52:18,004 [trainer.py] => Average Accuracy (NME): 78.24666666666667
2024-08-30 17:52:18,004 [trainer.py] => Forgetting (CNN): 40.935
