2024-09-25 17:00:47,586 [trainer.py] => config: ./exps/podnet.json
2024-09-25 17:00:47,586 [trainer.py] => prefix: cil
2024-09-25 17:00:47,586 [trainer.py] => dataset: hrrp9
2024-09-25 17:00:47,586 [trainer.py] => memory_size: 500
2024-09-25 17:00:47,586 [trainer.py] => memory_per_class: 20
2024-09-25 17:00:47,586 [trainer.py] => fixed_memory: False
2024-09-25 17:00:47,586 [trainer.py] => shuffle: True
2024-09-25 17:00:47,587 [trainer.py] => init_cls: 5
2024-09-25 17:00:47,587 [trainer.py] => increment: 2
2024-09-25 17:00:47,587 [trainer.py] => model_name: podnet
2024-09-25 17:00:47,587 [trainer.py] => convnet_type: resnet18
2024-09-25 17:00:47,587 [trainer.py] => init_train: True
2024-09-25 17:00:47,587 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-09-25 17:00:47,587 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-09-25 17:00:47,587 [trainer.py] => device: [device(type='cuda', index=0)]
2024-09-25 17:00:47,587 [trainer.py] => seed: 1993
2024-09-25 17:00:47,587 [trainer.py] => pod: w
2024-09-25 17:00:47,587 [trainer.py] => init_epochs: 300
2024-09-25 17:00:47,587 [trainer.py] => epochs: 300
2024-09-25 17:00:47,587 [trainer.py] => lrate: 0.1
2024-09-25 17:00:47,587 [trainer.py] => ft_epochs: 20
2024-09-25 17:00:47,587 [trainer.py] => ft_lrate: 0.005
2024-09-25 17:00:47,587 [trainer.py] => momentum: 0
2024-09-25 17:00:47,587 [trainer.py] => batch_size: 128
2024-09-25 17:00:47,587 [trainer.py] => lambda_c_base: 0.8
2024-09-25 17:00:47,587 [trainer.py] => lambda_f_base: 1
2024-09-25 17:00:47,587 [trainer.py] => nb_proxy: 10
2024-09-25 17:00:47,587 [trainer.py] => weight_decay: 0.0005
2024-09-25 17:00:47,587 [trainer.py] => num_workers: 4
2024-09-25 17:00:48,017 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-09-25 17:00:48,149 [trainer.py] => All params: 3843904
2024-09-25 17:00:48,150 [trainer.py] => Trainable params: 3843904
2024-09-25 17:00:48,150 [podnet.py] => Learning on 0-5
2024-09-25 17:00:48,186 [podnet.py] => Adaptive factor: 0
2024-09-25 17:00:51,310 [podnet.py] => Task 0, Epoch 1/300 (LR 0.10000) => LSC_loss 1.55, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.42, Test_acc 41.87
2024-09-25 17:00:53,302 [podnet.py] => Task 0, Epoch 2/300 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.23, Test_acc 37.30
2024-09-25 17:00:55,466 [podnet.py] => Task 0, Epoch 3/300 (LR 0.09998) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.62, Test_acc 47.87
2024-09-25 17:00:57,643 [podnet.py] => Task 0, Epoch 4/300 (LR 0.09996) => LSC_loss 0.52, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.08, Test_acc 38.17
2024-09-25 17:00:59,924 [podnet.py] => Task 0, Epoch 5/300 (LR 0.09993) => LSC_loss 0.36, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.39, Test_acc 69.80
2024-09-25 17:01:02,149 [podnet.py] => Task 0, Epoch 6/300 (LR 0.09990) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.70, Test_acc 70.43
2024-09-25 17:01:04,435 [podnet.py] => Task 0, Epoch 7/300 (LR 0.09987) => LSC_loss 0.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.34, Test_acc 75.27
2024-09-25 17:01:06,602 [podnet.py] => Task 0, Epoch 8/300 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.59, Test_acc 73.77
2024-09-25 17:01:08,778 [podnet.py] => Task 0, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.85, Test_acc 65.53
2024-09-25 17:01:10,997 [podnet.py] => Task 0, Epoch 10/300 (LR 0.09973) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.17, Test_acc 78.07
2024-09-25 17:01:13,277 [podnet.py] => Task 0, Epoch 11/300 (LR 0.09967) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.06, Test_acc 80.93
2024-09-25 17:01:15,515 [podnet.py] => Task 0, Epoch 12/300 (LR 0.09961) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 81.23
2024-09-25 17:01:17,851 [podnet.py] => Task 0, Epoch 13/300 (LR 0.09954) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 82.17
2024-09-25 17:01:20,117 [podnet.py] => Task 0, Epoch 14/300 (LR 0.09946) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 81.47
2024-09-25 17:01:22,255 [podnet.py] => Task 0, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.12, Test_acc 75.97
2024-09-25 17:01:24,436 [podnet.py] => Task 0, Epoch 16/300 (LR 0.09930) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.59, Test_acc 82.17
2024-09-25 17:01:26,560 [podnet.py] => Task 0, Epoch 17/300 (LR 0.09921) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.96, Test_acc 67.23
2024-09-25 17:01:28,811 [podnet.py] => Task 0, Epoch 18/300 (LR 0.09911) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.50, Test_acc 81.17
2024-09-25 17:01:31,054 [podnet.py] => Task 0, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.33, Test_acc 81.63
2024-09-25 17:01:33,248 [podnet.py] => Task 0, Epoch 20/300 (LR 0.09891) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.26, Test_acc 73.83
2024-09-25 17:01:35,633 [podnet.py] => Task 0, Epoch 21/300 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 79.47
2024-09-25 17:01:37,939 [podnet.py] => Task 0, Epoch 22/300 (LR 0.09868) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.03, Test_acc 81.03
2024-09-25 17:01:40,243 [podnet.py] => Task 0, Epoch 23/300 (LR 0.09856) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 78.73
2024-09-25 17:01:42,419 [podnet.py] => Task 0, Epoch 24/300 (LR 0.09843) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.22, Test_acc 86.43
2024-09-25 17:01:44,653 [podnet.py] => Task 0, Epoch 25/300 (LR 0.09830) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 84.20
2024-09-25 17:01:46,939 [podnet.py] => Task 0, Epoch 26/300 (LR 0.09816) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 84.57
2024-09-25 17:01:49,133 [podnet.py] => Task 0, Epoch 27/300 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.62, Test_acc 67.90
2024-09-25 17:01:51,324 [podnet.py] => Task 0, Epoch 28/300 (LR 0.09787) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.75, Test_acc 64.13
2024-09-25 17:01:53,579 [podnet.py] => Task 0, Epoch 29/300 (LR 0.09771) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.15, Test_acc 87.27
2024-09-25 17:01:55,882 [podnet.py] => Task 0, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 79.23
2024-09-25 17:01:58,171 [podnet.py] => Task 0, Epoch 31/300 (LR 0.09739) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.52, Test_acc 78.57
2024-09-25 17:02:00,592 [podnet.py] => Task 0, Epoch 32/300 (LR 0.09722) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 82.93
2024-09-25 17:02:02,846 [podnet.py] => Task 0, Epoch 33/300 (LR 0.09704) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 72.73
2024-09-25 17:02:05,093 [podnet.py] => Task 0, Epoch 34/300 (LR 0.09686) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 75.20
2024-09-25 17:02:07,289 [podnet.py] => Task 0, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 84.80
2024-09-25 17:02:09,376 [podnet.py] => Task 0, Epoch 36/300 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 78.53
2024-09-25 17:02:11,616 [podnet.py] => Task 0, Epoch 37/300 (LR 0.09629) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.57, Test_acc 83.87
2024-09-25 17:02:13,582 [podnet.py] => Task 0, Epoch 38/300 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 83.90
2024-09-25 17:02:15,773 [podnet.py] => Task 0, Epoch 39/300 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.30, Test_acc 76.87
2024-09-25 17:02:17,825 [podnet.py] => Task 0, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 74.63
2024-09-25 17:02:19,896 [podnet.py] => Task 0, Epoch 41/300 (LR 0.09546) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.63, Test_acc 83.53
2024-09-25 17:02:22,232 [podnet.py] => Task 0, Epoch 42/300 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 84.43
2024-09-25 17:02:24,342 [podnet.py] => Task 0, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.79, Test_acc 79.93
2024-09-25 17:02:26,606 [podnet.py] => Task 0, Epoch 44/300 (LR 0.09479) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 84.30
2024-09-25 17:02:28,899 [podnet.py] => Task 0, Epoch 45/300 (LR 0.09455) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 77.60
2024-09-25 17:02:31,136 [podnet.py] => Task 0, Epoch 46/300 (LR 0.09431) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 84.73
2024-09-25 17:02:33,443 [podnet.py] => Task 0, Epoch 47/300 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 81.40
2024-09-25 17:02:35,731 [podnet.py] => Task 0, Epoch 48/300 (LR 0.09382) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.04, Test_acc 83.87
2024-09-25 17:02:38,129 [podnet.py] => Task 0, Epoch 49/300 (LR 0.09356) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.22, Test_acc 81.23
2024-09-25 17:02:40,424 [podnet.py] => Task 0, Epoch 50/300 (LR 0.09330) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.05, Test_acc 82.07
2024-09-25 17:02:42,629 [podnet.py] => Task 0, Epoch 51/300 (LR 0.09304) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.98, Test_acc 87.03
2024-09-25 17:02:44,887 [podnet.py] => Task 0, Epoch 52/300 (LR 0.09277) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 69.33
2024-09-25 17:02:47,152 [podnet.py] => Task 0, Epoch 53/300 (LR 0.09249) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 81.00
2024-09-25 17:02:49,396 [podnet.py] => Task 0, Epoch 54/300 (LR 0.09222) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 81.97
2024-09-25 17:02:51,431 [podnet.py] => Task 0, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 80.57
2024-09-25 17:02:53,707 [podnet.py] => Task 0, Epoch 56/300 (LR 0.09165) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.52, Test_acc 82.73
2024-09-25 17:02:56,009 [podnet.py] => Task 0, Epoch 57/300 (LR 0.09135) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 85.30
2024-09-25 17:02:58,192 [podnet.py] => Task 0, Epoch 58/300 (LR 0.09106) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.80, Test_acc 84.37
2024-09-25 17:03:00,500 [podnet.py] => Task 0, Epoch 59/300 (LR 0.09076) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 79.07
2024-09-25 17:03:02,814 [podnet.py] => Task 0, Epoch 60/300 (LR 0.09045) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.38, Test_acc 68.50
2024-09-25 17:03:05,159 [podnet.py] => Task 0, Epoch 61/300 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.77, Test_acc 85.00
2024-09-25 17:03:07,398 [podnet.py] => Task 0, Epoch 62/300 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.17, Test_acc 86.03
2024-09-25 17:03:09,636 [podnet.py] => Task 0, Epoch 63/300 (LR 0.08951) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.94, Test_acc 82.17
2024-09-25 17:03:11,951 [podnet.py] => Task 0, Epoch 64/300 (LR 0.08918) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 83.77
2024-09-25 17:03:14,215 [podnet.py] => Task 0, Epoch 65/300 (LR 0.08886) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 85.13
2024-09-25 17:03:16,452 [podnet.py] => Task 0, Epoch 66/300 (LR 0.08853) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.43
2024-09-25 17:03:18,622 [podnet.py] => Task 0, Epoch 67/300 (LR 0.08819) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 83.23
2024-09-25 17:03:20,921 [podnet.py] => Task 0, Epoch 68/300 (LR 0.08785) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.10, Test_acc 81.23
2024-09-25 17:03:23,263 [podnet.py] => Task 0, Epoch 69/300 (LR 0.08751) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.66, Test_acc 84.57
2024-09-25 17:03:25,486 [podnet.py] => Task 0, Epoch 70/300 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 86.57
2024-09-25 17:03:27,754 [podnet.py] => Task 0, Epoch 71/300 (LR 0.08680) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.43, Test_acc 81.97
2024-09-25 17:03:29,989 [podnet.py] => Task 0, Epoch 72/300 (LR 0.08645) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.05, Test_acc 81.00
2024-09-25 17:03:32,313 [podnet.py] => Task 0, Epoch 73/300 (LR 0.08609) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 83.13
2024-09-25 17:03:34,584 [podnet.py] => Task 0, Epoch 74/300 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 86.60
2024-09-25 17:03:36,859 [podnet.py] => Task 0, Epoch 75/300 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 85.20
2024-09-25 17:03:39,123 [podnet.py] => Task 0, Epoch 76/300 (LR 0.08498) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.51, Test_acc 78.03
2024-09-25 17:03:41,268 [podnet.py] => Task 0, Epoch 77/300 (LR 0.08461) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 75.20
2024-09-25 17:03:43,465 [podnet.py] => Task 0, Epoch 78/300 (LR 0.08423) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.16, Test_acc 84.77
2024-09-25 17:03:45,656 [podnet.py] => Task 0, Epoch 79/300 (LR 0.08384) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.87, Test_acc 79.53
2024-09-25 17:03:47,989 [podnet.py] => Task 0, Epoch 80/300 (LR 0.08346) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 85.03
2024-09-25 17:03:50,303 [podnet.py] => Task 0, Epoch 81/300 (LR 0.08307) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 84.77
2024-09-25 17:03:52,612 [podnet.py] => Task 0, Epoch 82/300 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.33
2024-09-25 17:03:54,860 [podnet.py] => Task 0, Epoch 83/300 (LR 0.08227) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 77.43
2024-09-25 17:03:57,093 [podnet.py] => Task 0, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 81.97
2024-09-25 17:03:59,367 [podnet.py] => Task 0, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 85.43
2024-09-25 17:04:01,664 [podnet.py] => Task 0, Epoch 86/300 (LR 0.08106) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.59, Test_acc 84.40
2024-09-25 17:04:03,945 [podnet.py] => Task 0, Epoch 87/300 (LR 0.08065) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 83.03
2024-09-25 17:04:06,239 [podnet.py] => Task 0, Epoch 88/300 (LR 0.08023) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.67, Test_acc 82.57
2024-09-25 17:04:08,467 [podnet.py] => Task 0, Epoch 89/300 (LR 0.07981) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.62, Test_acc 83.57
2024-09-25 17:04:10,748 [podnet.py] => Task 0, Epoch 90/300 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 85.13
2024-09-25 17:04:13,031 [podnet.py] => Task 0, Epoch 91/300 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 76.03
2024-09-25 17:04:15,296 [podnet.py] => Task 0, Epoch 92/300 (LR 0.07854) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.98, Test_acc 85.27
2024-09-25 17:04:17,508 [podnet.py] => Task 0, Epoch 93/300 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 81.17
2024-09-25 17:04:19,779 [podnet.py] => Task 0, Epoch 94/300 (LR 0.07767) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 84.73
2024-09-25 17:04:22,073 [podnet.py] => Task 0, Epoch 95/300 (LR 0.07723) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 83.43
2024-09-25 17:04:24,278 [podnet.py] => Task 0, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 84.30
2024-09-25 17:04:26,620 [podnet.py] => Task 0, Epoch 97/300 (LR 0.07635) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 86.70
2024-09-25 17:04:28,907 [podnet.py] => Task 0, Epoch 98/300 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 78.37
2024-09-25 17:04:31,154 [podnet.py] => Task 0, Epoch 99/300 (LR 0.07545) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.93, Test_acc 85.03
2024-09-25 17:04:33,342 [podnet.py] => Task 0, Epoch 100/300 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 78.57
2024-09-25 17:04:35,445 [podnet.py] => Task 0, Epoch 101/300 (LR 0.07455) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 85.93
2024-09-25 17:04:37,655 [podnet.py] => Task 0, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 85.50
2024-09-25 17:04:39,965 [podnet.py] => Task 0, Epoch 103/300 (LR 0.07363) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.18, Test_acc 85.47
2024-09-25 17:04:42,221 [podnet.py] => Task 0, Epoch 104/300 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.46, Test_acc 85.33
2024-09-25 17:04:44,459 [podnet.py] => Task 0, Epoch 105/300 (LR 0.07270) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.27, Test_acc 85.53
2024-09-25 17:04:46,821 [podnet.py] => Task 0, Epoch 106/300 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 85.97
2024-09-25 17:04:49,090 [podnet.py] => Task 0, Epoch 107/300 (LR 0.07176) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 84.03
2024-09-25 17:04:51,310 [podnet.py] => Task 0, Epoch 108/300 (LR 0.07129) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 82.87
2024-09-25 17:04:53,506 [podnet.py] => Task 0, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.70
2024-09-25 17:04:55,732 [podnet.py] => Task 0, Epoch 110/300 (LR 0.07034) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 83.33
2024-09-25 17:04:57,975 [podnet.py] => Task 0, Epoch 111/300 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 82.57
2024-09-25 17:05:00,276 [podnet.py] => Task 0, Epoch 112/300 (LR 0.06938) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 84.00
2024-09-25 17:05:02,492 [podnet.py] => Task 0, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.05, Test_acc 85.83
2024-09-25 17:05:04,656 [podnet.py] => Task 0, Epoch 114/300 (LR 0.06841) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 88.30
2024-09-25 17:05:06,850 [podnet.py] => Task 0, Epoch 115/300 (LR 0.06792) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.80
2024-09-25 17:05:09,123 [podnet.py] => Task 0, Epoch 116/300 (LR 0.06743) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 84.63
2024-09-25 17:05:11,334 [podnet.py] => Task 0, Epoch 117/300 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 86.07
2024-09-25 17:05:13,528 [podnet.py] => Task 0, Epoch 118/300 (LR 0.06644) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 80.60
2024-09-25 17:05:15,768 [podnet.py] => Task 0, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.98, Test_acc 81.13
2024-09-25 17:05:18,152 [podnet.py] => Task 0, Epoch 120/300 (LR 0.06545) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.03, Test_acc 84.43
2024-09-25 17:05:20,396 [podnet.py] => Task 0, Epoch 121/300 (LR 0.06495) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 81.63
2024-09-25 17:05:22,666 [podnet.py] => Task 0, Epoch 122/300 (LR 0.06445) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.86, Test_acc 82.20
2024-09-25 17:05:24,791 [podnet.py] => Task 0, Epoch 123/300 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 81.10
2024-09-25 17:05:27,088 [podnet.py] => Task 0, Epoch 124/300 (LR 0.06345) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.14, Test_acc 81.73
2024-09-25 17:05:29,332 [podnet.py] => Task 0, Epoch 125/300 (LR 0.06294) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.27
2024-09-25 17:05:31,460 [podnet.py] => Task 0, Epoch 126/300 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 84.63
2024-09-25 17:05:33,615 [podnet.py] => Task 0, Epoch 127/300 (LR 0.06193) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 87.23
2024-09-25 17:05:35,935 [podnet.py] => Task 0, Epoch 128/300 (LR 0.06142) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 86.90
2024-09-25 17:05:38,214 [podnet.py] => Task 0, Epoch 129/300 (LR 0.06091) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.00
2024-09-25 17:05:40,474 [podnet.py] => Task 0, Epoch 130/300 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 86.97
2024-09-25 17:05:42,717 [podnet.py] => Task 0, Epoch 131/300 (LR 0.05988) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 88.40
2024-09-25 17:05:44,878 [podnet.py] => Task 0, Epoch 132/300 (LR 0.05937) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 85.53
2024-09-25 17:05:47,121 [podnet.py] => Task 0, Epoch 133/300 (LR 0.05885) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 85.43
2024-09-25 17:05:49,362 [podnet.py] => Task 0, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 82.57
2024-09-25 17:05:51,694 [podnet.py] => Task 0, Epoch 135/300 (LR 0.05782) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 84.40
2024-09-25 17:05:53,930 [podnet.py] => Task 0, Epoch 136/300 (LR 0.05730) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 85.17
2024-09-25 17:05:56,202 [podnet.py] => Task 0, Epoch 137/300 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 82.23
2024-09-25 17:05:58,456 [podnet.py] => Task 0, Epoch 138/300 (LR 0.05627) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 83.10
2024-09-25 17:06:00,633 [podnet.py] => Task 0, Epoch 139/300 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.59, Test_acc 86.60
2024-09-25 17:06:02,797 [podnet.py] => Task 0, Epoch 140/300 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 84.23
2024-09-25 17:06:05,114 [podnet.py] => Task 0, Epoch 141/300 (LR 0.05471) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.78, Test_acc 86.33
2024-09-25 17:06:07,350 [podnet.py] => Task 0, Epoch 142/300 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 84.87
2024-09-25 17:06:09,621 [podnet.py] => Task 0, Epoch 143/300 (LR 0.05366) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 75.87
2024-09-25 17:06:11,796 [podnet.py] => Task 0, Epoch 144/300 (LR 0.05314) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 83.63
2024-09-25 17:06:14,070 [podnet.py] => Task 0, Epoch 145/300 (LR 0.05262) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 87.57
2024-09-25 17:06:16,339 [podnet.py] => Task 0, Epoch 146/300 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 84.13
2024-09-25 17:06:18,562 [podnet.py] => Task 0, Epoch 147/300 (LR 0.05157) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.54, Test_acc 87.20
2024-09-25 17:06:20,834 [podnet.py] => Task 0, Epoch 148/300 (LR 0.05105) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 84.43
2024-09-25 17:06:23,098 [podnet.py] => Task 0, Epoch 149/300 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 81.70
2024-09-25 17:06:25,451 [podnet.py] => Task 0, Epoch 150/300 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.37
2024-09-25 17:06:27,739 [podnet.py] => Task 0, Epoch 151/300 (LR 0.04948) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 86.90
2024-09-25 17:06:30,035 [podnet.py] => Task 0, Epoch 152/300 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 89.60
2024-09-25 17:06:32,267 [podnet.py] => Task 0, Epoch 153/300 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 87.30
2024-09-25 17:06:34,471 [podnet.py] => Task 0, Epoch 154/300 (LR 0.04791) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 84.20
2024-09-25 17:06:36,765 [podnet.py] => Task 0, Epoch 155/300 (LR 0.04738) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.34, Test_acc 79.87
2024-09-25 17:06:38,988 [podnet.py] => Task 0, Epoch 156/300 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 84.63
2024-09-25 17:06:41,253 [podnet.py] => Task 0, Epoch 157/300 (LR 0.04634) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 87.77
2024-09-25 17:06:43,501 [podnet.py] => Task 0, Epoch 158/300 (LR 0.04582) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.77
2024-09-25 17:06:45,748 [podnet.py] => Task 0, Epoch 159/300 (LR 0.04529) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 85.43
2024-09-25 17:06:48,024 [podnet.py] => Task 0, Epoch 160/300 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 81.00
2024-09-25 17:06:50,236 [podnet.py] => Task 0, Epoch 161/300 (LR 0.04425) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 86.57
2024-09-25 17:06:52,424 [podnet.py] => Task 0, Epoch 162/300 (LR 0.04373) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.77
2024-09-25 17:06:54,647 [podnet.py] => Task 0, Epoch 163/300 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 90.13
2024-09-25 17:06:56,918 [podnet.py] => Task 0, Epoch 164/300 (LR 0.04270) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-09-25 17:06:59,023 [podnet.py] => Task 0, Epoch 165/300 (LR 0.04218) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.17
2024-09-25 17:07:01,340 [podnet.py] => Task 0, Epoch 166/300 (LR 0.04166) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.25, Test_acc 78.60
2024-09-25 17:07:03,720 [podnet.py] => Task 0, Epoch 167/300 (LR 0.04115) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 87.93
2024-09-25 17:07:06,006 [podnet.py] => Task 0, Epoch 168/300 (LR 0.04063) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 86.00
2024-09-25 17:07:08,196 [podnet.py] => Task 0, Epoch 169/300 (LR 0.04012) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.43
2024-09-25 17:07:10,399 [podnet.py] => Task 0, Epoch 170/300 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 85.47
2024-09-25 17:07:12,583 [podnet.py] => Task 0, Epoch 171/300 (LR 0.03909) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 84.80
2024-09-25 17:07:14,850 [podnet.py] => Task 0, Epoch 172/300 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 89.10
2024-09-25 17:07:17,128 [podnet.py] => Task 0, Epoch 173/300 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.20
2024-09-25 17:07:19,378 [podnet.py] => Task 0, Epoch 174/300 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 85.90
2024-09-25 17:07:21,505 [podnet.py] => Task 0, Epoch 175/300 (LR 0.03706) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 81.13
2024-09-25 17:07:23,737 [podnet.py] => Task 0, Epoch 176/300 (LR 0.03655) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 84.47
2024-09-25 17:07:26,035 [podnet.py] => Task 0, Epoch 177/300 (LR 0.03605) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.88, Test_acc 83.20
2024-09-25 17:07:28,242 [podnet.py] => Task 0, Epoch 178/300 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.10
2024-09-25 17:07:30,483 [podnet.py] => Task 0, Epoch 179/300 (LR 0.03505) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 89.07
2024-09-25 17:07:32,772 [podnet.py] => Task 0, Epoch 180/300 (LR 0.03455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.67
2024-09-25 17:07:34,905 [podnet.py] => Task 0, Epoch 181/300 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 88.87
2024-09-25 17:07:37,103 [podnet.py] => Task 0, Epoch 182/300 (LR 0.03356) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 87.67
2024-09-25 17:07:39,336 [podnet.py] => Task 0, Epoch 183/300 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.87
2024-09-25 17:07:41,482 [podnet.py] => Task 0, Epoch 184/300 (LR 0.03257) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-09-25 17:07:43,751 [podnet.py] => Task 0, Epoch 185/300 (LR 0.03208) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.83
2024-09-25 17:07:46,030 [podnet.py] => Task 0, Epoch 186/300 (LR 0.03159) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 84.30
2024-09-25 17:07:48,365 [podnet.py] => Task 0, Epoch 187/300 (LR 0.03111) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 83.33
2024-09-25 17:07:50,680 [podnet.py] => Task 0, Epoch 188/300 (LR 0.03062) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.16, Test_acc 87.17
2024-09-25 17:07:52,881 [podnet.py] => Task 0, Epoch 189/300 (LR 0.03014) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 86.53
2024-09-25 17:07:55,006 [podnet.py] => Task 0, Epoch 190/300 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 88.07
2024-09-25 17:07:57,336 [podnet.py] => Task 0, Epoch 191/300 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.23
2024-09-25 17:07:59,565 [podnet.py] => Task 0, Epoch 192/300 (LR 0.02871) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.17
2024-09-25 17:08:01,850 [podnet.py] => Task 0, Epoch 193/300 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 84.20
2024-09-25 17:08:04,139 [podnet.py] => Task 0, Epoch 194/300 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 85.20
2024-09-25 17:08:06,490 [podnet.py] => Task 0, Epoch 195/300 (LR 0.02730) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 89.23
2024-09-25 17:08:08,640 [podnet.py] => Task 0, Epoch 196/300 (LR 0.02684) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 79.03
2024-09-25 17:08:10,846 [podnet.py] => Task 0, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 86.13
2024-09-25 17:08:12,956 [podnet.py] => Task 0, Epoch 198/300 (LR 0.02591) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 88.73
2024-09-25 17:08:15,255 [podnet.py] => Task 0, Epoch 199/300 (LR 0.02545) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.67
2024-09-25 17:08:17,409 [podnet.py] => Task 0, Epoch 200/300 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.63
2024-09-25 17:08:19,638 [podnet.py] => Task 0, Epoch 201/300 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.07
2024-09-25 17:08:21,820 [podnet.py] => Task 0, Epoch 202/300 (LR 0.02410) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.39, Test_acc 83.60
2024-09-25 17:08:24,143 [podnet.py] => Task 0, Epoch 203/300 (LR 0.02365) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 86.70
2024-09-25 17:08:26,386 [podnet.py] => Task 0, Epoch 204/300 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 87.73
2024-09-25 17:08:28,632 [podnet.py] => Task 0, Epoch 205/300 (LR 0.02277) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-09-25 17:08:30,934 [podnet.py] => Task 0, Epoch 206/300 (LR 0.02233) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-09-25 17:08:33,259 [podnet.py] => Task 0, Epoch 207/300 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 87.43
2024-09-25 17:08:35,555 [podnet.py] => Task 0, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.99, Test_acc 77.57
2024-09-25 17:08:37,796 [podnet.py] => Task 0, Epoch 209/300 (LR 0.02104) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 84.73
2024-09-25 17:08:40,100 [podnet.py] => Task 0, Epoch 210/300 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 88.00
2024-09-25 17:08:42,375 [podnet.py] => Task 0, Epoch 211/300 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.40
2024-09-25 17:08:44,523 [podnet.py] => Task 0, Epoch 212/300 (LR 0.01977) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-09-25 17:08:46,750 [podnet.py] => Task 0, Epoch 213/300 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.37
2024-09-25 17:08:48,957 [podnet.py] => Task 0, Epoch 214/300 (LR 0.01894) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 87.13
2024-09-25 17:08:51,175 [podnet.py] => Task 0, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 87.57
2024-09-25 17:08:53,407 [podnet.py] => Task 0, Epoch 216/300 (LR 0.01813) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 88.33
2024-09-25 17:08:55,688 [podnet.py] => Task 0, Epoch 217/300 (LR 0.01773) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-09-25 17:08:57,986 [podnet.py] => Task 0, Epoch 218/300 (LR 0.01733) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.60
2024-09-25 17:09:00,236 [podnet.py] => Task 0, Epoch 219/300 (LR 0.01693) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-09-25 17:09:02,493 [podnet.py] => Task 0, Epoch 220/300 (LR 0.01654) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-09-25 17:09:04,750 [podnet.py] => Task 0, Epoch 221/300 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.07
2024-09-25 17:09:06,938 [podnet.py] => Task 0, Epoch 222/300 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-09-25 17:09:09,262 [podnet.py] => Task 0, Epoch 223/300 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-09-25 17:09:11,555 [podnet.py] => Task 0, Epoch 224/300 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-09-25 17:09:13,802 [podnet.py] => Task 0, Epoch 225/300 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 88.80
2024-09-25 17:09:16,077 [podnet.py] => Task 0, Epoch 226/300 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-09-25 17:09:18,137 [podnet.py] => Task 0, Epoch 227/300 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.20
2024-09-25 17:09:20,397 [podnet.py] => Task 0, Epoch 228/300 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-09-25 17:09:22,742 [podnet.py] => Task 0, Epoch 229/300 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-09-25 17:09:24,985 [podnet.py] => Task 0, Epoch 230/300 (LR 0.01284) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-09-25 17:09:27,222 [podnet.py] => Task 0, Epoch 231/300 (LR 0.01249) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.43
2024-09-25 17:09:29,410 [podnet.py] => Task 0, Epoch 232/300 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 84.13
2024-09-25 17:09:31,619 [podnet.py] => Task 0, Epoch 233/300 (LR 0.01181) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.13
2024-09-25 17:09:33,862 [podnet.py] => Task 0, Epoch 234/300 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.47
2024-09-25 17:09:36,041 [podnet.py] => Task 0, Epoch 235/300 (LR 0.01114) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-09-25 17:09:38,180 [podnet.py] => Task 0, Epoch 236/300 (LR 0.01082) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-09-25 17:09:40,405 [podnet.py] => Task 0, Epoch 237/300 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:09:42,654 [podnet.py] => Task 0, Epoch 238/300 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-09-25 17:09:44,685 [podnet.py] => Task 0, Epoch 239/300 (LR 0.00986) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-09-25 17:09:46,777 [podnet.py] => Task 0, Epoch 240/300 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-09-25 17:09:48,956 [podnet.py] => Task 0, Epoch 241/300 (LR 0.00924) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-09-25 17:09:51,216 [podnet.py] => Task 0, Epoch 242/300 (LR 0.00894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-09-25 17:09:53,440 [podnet.py] => Task 0, Epoch 243/300 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-09-25 17:09:55,626 [podnet.py] => Task 0, Epoch 244/300 (LR 0.00835) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-09-25 17:09:57,871 [podnet.py] => Task 0, Epoch 245/300 (LR 0.00807) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-09-25 17:10:00,037 [podnet.py] => Task 0, Epoch 246/300 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-09-25 17:10:02,307 [podnet.py] => Task 0, Epoch 247/300 (LR 0.00751) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-09-25 17:10:04,572 [podnet.py] => Task 0, Epoch 248/300 (LR 0.00723) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:10:06,668 [podnet.py] => Task 0, Epoch 249/300 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-09-25 17:10:08,680 [podnet.py] => Task 0, Epoch 250/300 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-09-25 17:10:10,960 [podnet.py] => Task 0, Epoch 251/300 (LR 0.00644) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-09-25 17:10:13,265 [podnet.py] => Task 0, Epoch 252/300 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:10:15,567 [podnet.py] => Task 0, Epoch 253/300 (LR 0.00593) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-09-25 17:10:17,789 [podnet.py] => Task 0, Epoch 254/300 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-09-25 17:10:19,977 [podnet.py] => Task 0, Epoch 255/300 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:10:22,082 [podnet.py] => Task 0, Epoch 256/300 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-09-25 17:10:24,300 [podnet.py] => Task 0, Epoch 257/300 (LR 0.00498) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.63
2024-09-25 17:10:26,591 [podnet.py] => Task 0, Epoch 258/300 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.53
2024-09-25 17:10:28,814 [podnet.py] => Task 0, Epoch 259/300 (LR 0.00454) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-09-25 17:10:31,078 [podnet.py] => Task 0, Epoch 260/300 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:10:33,293 [podnet.py] => Task 0, Epoch 261/300 (LR 0.00411) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:10:35,542 [podnet.py] => Task 0, Epoch 262/300 (LR 0.00391) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.70
2024-09-25 17:10:37,805 [podnet.py] => Task 0, Epoch 263/300 (LR 0.00371) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.50
2024-09-25 17:10:40,075 [podnet.py] => Task 0, Epoch 264/300 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.10
2024-09-25 17:10:42,309 [podnet.py] => Task 0, Epoch 265/300 (LR 0.00332) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.03
2024-09-25 17:10:44,524 [podnet.py] => Task 0, Epoch 266/300 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-09-25 17:10:46,720 [podnet.py] => Task 0, Epoch 267/300 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-09-25 17:10:48,982 [podnet.py] => Task 0, Epoch 268/300 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.07
2024-09-25 17:10:51,265 [podnet.py] => Task 0, Epoch 269/300 (LR 0.00261) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-09-25 17:10:53,543 [podnet.py] => Task 0, Epoch 270/300 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-09-25 17:10:55,772 [podnet.py] => Task 0, Epoch 271/300 (LR 0.00229) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.10
2024-09-25 17:10:57,893 [podnet.py] => Task 0, Epoch 272/300 (LR 0.00213) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-09-25 17:11:00,177 [podnet.py] => Task 0, Epoch 273/300 (LR 0.00199) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.97
2024-09-25 17:11:02,382 [podnet.py] => Task 0, Epoch 274/300 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.53
2024-09-25 17:11:04,730 [podnet.py] => Task 0, Epoch 275/300 (LR 0.00170) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-09-25 17:11:06,938 [podnet.py] => Task 0, Epoch 276/300 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-09-25 17:11:09,215 [podnet.py] => Task 0, Epoch 277/300 (LR 0.00144) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.37
2024-09-25 17:11:11,547 [podnet.py] => Task 0, Epoch 278/300 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-09-25 17:11:13,698 [podnet.py] => Task 0, Epoch 279/300 (LR 0.00120) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-09-25 17:11:15,975 [podnet.py] => Task 0, Epoch 280/300 (LR 0.00109) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.67
2024-09-25 17:11:18,195 [podnet.py] => Task 0, Epoch 281/300 (LR 0.00099) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.87
2024-09-25 17:11:20,385 [podnet.py] => Task 0, Epoch 282/300 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.13
2024-09-25 17:11:22,632 [podnet.py] => Task 0, Epoch 283/300 (LR 0.00079) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.20
2024-09-25 17:11:24,896 [podnet.py] => Task 0, Epoch 284/300 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-09-25 17:11:26,929 [podnet.py] => Task 0, Epoch 285/300 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.27
2024-09-25 17:11:29,103 [podnet.py] => Task 0, Epoch 286/300 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.20
2024-09-25 17:11:31,276 [podnet.py] => Task 0, Epoch 287/300 (LR 0.00046) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.23
2024-09-25 17:11:33,207 [podnet.py] => Task 0, Epoch 288/300 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.30
2024-09-25 17:11:35,418 [podnet.py] => Task 0, Epoch 289/300 (LR 0.00033) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.40
2024-09-25 17:11:37,697 [podnet.py] => Task 0, Epoch 290/300 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-09-25 17:11:39,869 [podnet.py] => Task 0, Epoch 291/300 (LR 0.00022) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 90.30
2024-09-25 17:11:42,206 [podnet.py] => Task 0, Epoch 292/300 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-09-25 17:11:44,508 [podnet.py] => Task 0, Epoch 293/300 (LR 0.00013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-09-25 17:11:46,702 [podnet.py] => Task 0, Epoch 294/300 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-09-25 17:11:48,997 [podnet.py] => Task 0, Epoch 295/300 (LR 0.00007) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.37
2024-09-25 17:11:51,232 [podnet.py] => Task 0, Epoch 296/300 (LR 0.00004) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.10
2024-09-25 17:11:53,450 [podnet.py] => Task 0, Epoch 297/300 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-09-25 17:11:55,586 [podnet.py] => Task 0, Epoch 298/300 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.27
2024-09-25 17:11:57,725 [podnet.py] => Task 0, Epoch 299/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-09-25 17:12:00,032 [podnet.py] => Task 0, Epoch 300/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-09-25 17:12:00,034 [base.py] => Reducing exemplars...(100 per classes)
2024-09-25 17:12:00,034 [base.py] => Constructing exemplars...(100 per classes)
2024-09-25 17:12:05,934 [podnet.py] => Exemplar size: 500
2024-09-25 17:12:05,935 [trainer.py] => CNN: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-09-25 17:12:05,935 [trainer.py] => NME: {'total': 90.03, '00-04': 90.03, 'old': 0, 'new': 90.03}
2024-09-25 17:12:05,935 [trainer.py] => CNN top1 curve: [90.0]
2024-09-25 17:12:05,935 [trainer.py] => CNN top5 curve: [100.0]
2024-09-25 17:12:05,935 [trainer.py] => NME top1 curve: [90.03]
2024-09-25 17:12:05,935 [trainer.py] => NME top5 curve: [100.0]

2024-09-25 17:12:05,935 [trainer.py] => Average Accuracy (CNN): 90.0
2024-09-25 17:12:05,935 [trainer.py] => Average Accuracy (NME): 90.03
2024-09-25 17:12:05,935 [trainer.py] => All params: 3869505
2024-09-25 17:12:05,936 [trainer.py] => Trainable params: 3869505
2024-09-25 17:12:05,936 [podnet.py] => Learning on 5-7
2024-09-25 17:12:05,950 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-09-25 17:12:07,867 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 0.97, Spatial_loss 0.38, Flat_loss 0.40, Train_acc 74.13, Test_acc 45.21
2024-09-25 17:12:09,720 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.35, Spatial_loss 0.31, Flat_loss 0.30, Train_acc 93.62, Test_acc 56.71
2024-09-25 17:12:11,406 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.27, Spatial_loss 0.28, Flat_loss 0.25, Train_acc 95.69, Test_acc 62.10
2024-09-25 17:12:13,149 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.22, Spatial_loss 0.26, Flat_loss 0.22, Train_acc 97.36, Test_acc 56.95
2024-09-25 17:12:14,856 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.20, Spatial_loss 0.24, Flat_loss 0.21, Train_acc 97.38, Test_acc 63.07
2024-09-25 17:12:16,625 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.18, Spatial_loss 0.23, Flat_loss 0.20, Train_acc 98.29, Test_acc 64.74
2024-09-25 17:12:18,386 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.17, Spatial_loss 0.23, Flat_loss 0.19, Train_acc 98.49, Test_acc 60.67
2024-09-25 17:12:20,083 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.16, Spatial_loss 0.22, Flat_loss 0.19, Train_acc 98.36, Test_acc 65.64
2024-09-25 17:12:21,857 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.15, Spatial_loss 0.21, Flat_loss 0.19, Train_acc 98.93, Test_acc 65.71
2024-09-25 17:12:23,611 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.15, Spatial_loss 0.21, Flat_loss 0.19, Train_acc 98.96, Test_acc 64.02
2024-09-25 17:12:25,344 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.15, Spatial_loss 0.21, Flat_loss 0.18, Train_acc 98.96, Test_acc 69.19
2024-09-25 17:12:27,043 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.14, Spatial_loss 0.20, Flat_loss 0.18, Train_acc 99.29, Test_acc 67.60
2024-09-25 17:12:28,745 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.13, Spatial_loss 0.20, Flat_loss 0.18, Train_acc 99.42, Test_acc 70.48
2024-09-25 17:12:30,591 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.13, Spatial_loss 0.19, Flat_loss 0.17, Train_acc 99.56, Test_acc 62.07
2024-09-25 17:12:32,370 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.13, Spatial_loss 0.19, Flat_loss 0.18, Train_acc 99.53, Test_acc 67.86
2024-09-25 17:12:34,077 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.12, Spatial_loss 0.18, Flat_loss 0.17, Train_acc 99.67, Test_acc 66.52
2024-09-25 17:12:35,762 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.13, Spatial_loss 0.18, Flat_loss 0.17, Train_acc 99.60, Test_acc 57.69
2024-09-25 17:12:37,456 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.13, Spatial_loss 0.20, Flat_loss 0.18, Train_acc 99.31, Test_acc 65.38
2024-09-25 17:12:39,116 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.11, Spatial_loss 0.18, Flat_loss 0.17, Train_acc 99.82, Test_acc 66.38
2024-09-25 17:12:40,851 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.12, Spatial_loss 0.18, Flat_loss 0.17, Train_acc 99.62, Test_acc 67.48
2024-09-25 17:12:42,531 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.11, Spatial_loss 0.18, Flat_loss 0.16, Train_acc 99.93, Test_acc 72.86
2024-09-25 17:12:44,194 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.11, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.89, Test_acc 68.90
2024-09-25 17:12:45,959 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.98, Test_acc 68.64
2024-09-25 17:12:47,716 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.91, Test_acc 66.07
2024-09-25 17:12:49,480 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.87, Test_acc 53.88
2024-09-25 17:12:51,235 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.84, Test_acc 66.40
2024-09-25 17:12:52,981 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.10, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 100.00, Test_acc 70.88
2024-09-25 17:12:54,838 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.89, Test_acc 69.60
2024-09-25 17:12:56,507 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.96, Test_acc 75.76
2024-09-25 17:12:58,248 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.98, Test_acc 72.60
2024-09-25 17:13:00,086 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.96, Test_acc 69.69
2024-09-25 17:13:01,858 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.10, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.89, Test_acc 28.55
2024-09-25 17:13:03,571 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.20, Spatial_loss 0.24, Flat_loss 0.21, Train_acc 96.18, Test_acc 71.02
2024-09-25 17:13:05,260 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.78, Test_acc 67.67
2024-09-25 17:13:07,028 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.87, Test_acc 67.95
2024-09-25 17:13:08,715 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.91, Test_acc 69.71
2024-09-25 17:13:10,465 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.93, Test_acc 73.29
2024-09-25 17:13:12,146 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.91, Test_acc 75.98
2024-09-25 17:13:13,877 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.15, Train_acc 99.93, Test_acc 66.60
2024-09-25 17:13:15,553 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.09, Spatial_loss 0.15, Flat_loss 0.15, Train_acc 99.82, Test_acc 67.19
2024-09-25 17:13:17,262 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.84, Test_acc 63.83
2024-09-25 17:13:19,025 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.89, Test_acc 69.31
2024-09-25 17:13:20,854 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.15, Train_acc 99.98, Test_acc 55.38
2024-09-25 17:13:22,513 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.15, Train_acc 99.96, Test_acc 67.69
2024-09-25 17:13:24,215 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.96, Test_acc 65.76
2024-09-25 17:13:25,952 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.87, Test_acc 67.90
2024-09-25 17:13:27,757 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.98, Test_acc 69.55
2024-09-25 17:13:29,489 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 100.00, Test_acc 69.10
2024-09-25 17:13:31,203 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.93, Test_acc 73.26
2024-09-25 17:13:32,890 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.96, Test_acc 67.05
2024-09-25 17:13:34,626 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 100.00, Test_acc 70.45
2024-09-25 17:13:36,293 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 100.00, Test_acc 69.60
2024-09-25 17:13:37,975 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.93, Test_acc 72.40
2024-09-25 17:13:39,707 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.98, Test_acc 72.29
2024-09-25 17:13:41,431 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 100.00, Test_acc 62.81
2024-09-25 17:13:43,193 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.91, Test_acc 72.55
2024-09-25 17:13:45,102 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 71.57
2024-09-25 17:13:46,881 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 100.00, Test_acc 73.50
2024-09-25 17:13:48,603 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.96, Test_acc 72.29
2024-09-25 17:13:50,336 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.98, Test_acc 68.43
2024-09-25 17:13:52,069 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 73.81
2024-09-25 17:13:53,779 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.17
2024-09-25 17:13:55,558 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.96, Test_acc 72.98
2024-09-25 17:13:57,259 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.93, Test_acc 69.55
2024-09-25 17:13:58,925 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 65.43
2024-09-25 17:14:00,716 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 73.07
2024-09-25 17:14:02,439 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 69.48
2024-09-25 17:14:04,238 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 71.50
2024-09-25 17:14:06,003 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 71.98
2024-09-25 17:14:07,795 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 69.88
2024-09-25 17:14:09,576 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.64
2024-09-25 17:14:11,305 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 73.69
2024-09-25 17:14:13,055 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.64
2024-09-25 17:14:14,825 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 71.81
2024-09-25 17:14:16,515 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.31
2024-09-25 17:14:18,275 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 69.48
2024-09-25 17:14:19,988 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.88
2024-09-25 17:14:21,825 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.93, Test_acc 71.05
2024-09-25 17:14:23,547 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.12
2024-09-25 17:14:25,313 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.96, Test_acc 75.33
2024-09-25 17:14:27,048 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.96, Test_acc 69.40
2024-09-25 17:14:28,745 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.31
2024-09-25 17:14:30,424 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.93, Test_acc 71.10
2024-09-25 17:14:32,197 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.05
2024-09-25 17:14:33,924 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.81
2024-09-25 17:14:35,652 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 74.90
2024-09-25 17:14:37,325 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.93, Test_acc 68.33
2024-09-25 17:14:39,063 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 73.45
2024-09-25 17:14:40,800 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 72.48
2024-09-25 17:14:42,643 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 69.71
2024-09-25 17:14:44,349 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.96, Test_acc 77.10
2024-09-25 17:14:46,029 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.69
2024-09-25 17:14:47,810 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.50
2024-09-25 17:14:49,694 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.95
2024-09-25 17:14:51,410 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.10
2024-09-25 17:14:53,129 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 70.50
2024-09-25 17:14:54,906 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.76
2024-09-25 17:14:56,727 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 70.67
2024-09-25 17:14:58,571 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.38
2024-09-25 17:15:00,357 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.19
2024-09-25 17:15:02,185 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.10
2024-09-25 17:15:04,007 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.98, Test_acc 77.26
2024-09-25 17:15:05,829 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.29
2024-09-25 17:15:07,586 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 100.00, Test_acc 73.02
2024-09-25 17:15:09,470 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 100.00, Test_acc 73.07
2024-09-25 17:15:11,316 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 100.00, Test_acc 73.69
2024-09-25 17:15:13,110 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.90
2024-09-25 17:15:14,906 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 100.00, Test_acc 72.81
2024-09-25 17:15:16,822 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.98
2024-09-25 17:15:18,513 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.98, Test_acc 73.45
2024-09-25 17:15:20,403 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.98, Test_acc 75.21
2024-09-25 17:15:22,208 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.76
2024-09-25 17:15:24,040 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.96, Test_acc 71.50
2024-09-25 17:15:25,832 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.00
2024-09-25 17:15:27,600 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.98, Test_acc 72.71
2024-09-25 17:15:29,387 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.98
2024-09-25 17:15:31,196 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.19
2024-09-25 17:15:32,978 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 73.38
2024-09-25 17:15:34,707 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 69.88
2024-09-25 17:15:36,498 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.96, Test_acc 71.86
2024-09-25 17:15:38,283 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.62
2024-09-25 17:15:40,110 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.60
2024-09-25 17:15:41,973 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.26
2024-09-25 17:15:43,945 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.26
2024-09-25 17:15:45,756 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.07
2024-09-25 17:15:47,479 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.74
2024-09-25 17:15:49,313 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.79
2024-09-25 17:15:51,215 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 76.10
2024-09-25 17:15:53,000 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.95
2024-09-25 17:15:54,885 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.98
2024-09-25 17:15:56,702 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.38
2024-09-25 17:15:58,468 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.93, Test_acc 72.29
2024-09-25 17:16:00,226 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.96, Test_acc 70.29
2024-09-25 17:16:02,010 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.96, Test_acc 73.02
2024-09-25 17:16:03,845 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.96, Test_acc 76.79
2024-09-25 17:16:05,645 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.88
2024-09-25 17:16:07,563 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.43
2024-09-25 17:16:09,358 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.38
2024-09-25 17:16:11,170 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 73.93
2024-09-25 17:16:12,987 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.64
2024-09-25 17:16:14,779 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.40
2024-09-25 17:16:16,570 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.69
2024-09-25 17:16:18,385 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.90
2024-09-25 17:16:20,200 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.14
2024-09-25 17:16:22,056 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.60
2024-09-25 17:16:23,808 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.36
2024-09-25 17:16:25,567 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.74
2024-09-25 17:16:27,423 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.50
2024-09-25 17:16:29,241 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.95
2024-09-25 17:16:31,097 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.43
2024-09-25 17:16:32,870 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.81
2024-09-25 17:16:34,682 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.14
2024-09-25 17:16:36,541 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.52
2024-09-25 17:16:38,434 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.24
2024-09-25 17:16:40,243 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.96, Test_acc 71.52
2024-09-25 17:16:42,204 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.21
2024-09-25 17:16:44,001 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.55
2024-09-25 17:16:45,770 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.55
2024-09-25 17:16:47,657 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.79
2024-09-25 17:16:49,427 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.74
2024-09-25 17:16:51,209 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.05
2024-09-25 17:16:53,019 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.40
2024-09-25 17:16:54,866 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.31
2024-09-25 17:16:56,606 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.93, Test_acc 48.48
2024-09-25 17:16:58,507 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.06, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.62, Test_acc 71.67
2024-09-25 17:17:00,331 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.79
2024-09-25 17:17:02,211 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.89, Test_acc 30.62
2024-09-25 17:17:04,100 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.11, Spatial_loss 0.14, Flat_loss 0.15, Train_acc 98.42, Test_acc 71.67
2024-09-25 17:17:05,943 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.12, Train_acc 100.00, Test_acc 73.19
2024-09-25 17:17:07,733 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.96, Test_acc 72.52
2024-09-25 17:17:09,503 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.24
2024-09-25 17:17:11,290 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.17
2024-09-25 17:17:13,106 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.40
2024-09-25 17:17:14,957 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.00
2024-09-25 17:17:16,826 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.60
2024-09-25 17:17:18,694 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.60
2024-09-25 17:17:20,561 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.55
2024-09-25 17:17:22,358 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.88
2024-09-25 17:17:24,264 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.83
2024-09-25 17:17:26,197 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.93
2024-09-25 17:17:28,114 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.26
2024-09-25 17:17:30,049 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.24
2024-09-25 17:17:31,960 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:17:33,891 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 76.33
2024-09-25 17:17:35,748 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.81
2024-09-25 17:17:37,693 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 71.81
2024-09-25 17:17:39,582 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.43
2024-09-25 17:17:41,506 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.81
2024-09-25 17:17:43,389 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.83
2024-09-25 17:17:45,324 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.67
2024-09-25 17:17:47,314 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.26
2024-09-25 17:17:49,210 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.76
2024-09-25 17:17:51,110 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.19
2024-09-25 17:17:53,027 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.71
2024-09-25 17:17:54,870 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.88
2024-09-25 17:17:56,723 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.81
2024-09-25 17:17:58,679 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 73.98
2024-09-25 17:18:00,626 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.17
2024-09-25 17:18:02,550 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.40
2024-09-25 17:18:04,471 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.17
2024-09-25 17:18:06,375 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:18:08,280 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.90
2024-09-25 17:18:10,184 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.00
2024-09-25 17:18:12,142 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.95
2024-09-25 17:18:14,123 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.64
2024-09-25 17:18:16,167 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.10
2024-09-25 17:18:18,085 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.79
2024-09-25 17:18:20,009 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.10
2024-09-25 17:18:21,961 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.79
2024-09-25 17:18:23,921 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.90
2024-09-25 17:18:25,831 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.62
2024-09-25 17:18:27,747 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:18:29,683 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.79
2024-09-25 17:18:31,663 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:18:33,589 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 99.98, Test_acc 67.76
2024-09-25 17:18:35,496 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.60
2024-09-25 17:18:37,427 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.60
2024-09-25 17:18:39,400 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.00
2024-09-25 17:18:41,281 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.95
2024-09-25 17:18:43,238 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.93
2024-09-25 17:18:45,172 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.81
2024-09-25 17:18:47,136 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.95
2024-09-25 17:18:49,049 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.52
2024-09-25 17:18:51,020 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.55
2024-09-25 17:18:52,896 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.71
2024-09-25 17:18:54,759 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.07
2024-09-25 17:18:56,670 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.48
2024-09-25 17:18:58,598 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.95
2024-09-25 17:19:00,486 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:19:02,410 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.02
2024-09-25 17:19:04,357 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.64
2024-09-25 17:19:06,313 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 99.98, Test_acc 74.76
2024-09-25 17:19:08,254 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.71
2024-09-25 17:19:10,175 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.43
2024-09-25 17:19:12,048 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.24
2024-09-25 17:19:13,966 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.38
2024-09-25 17:19:15,820 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.64
2024-09-25 17:19:17,728 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.26
2024-09-25 17:19:19,640 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:19:21,636 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:19:23,621 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.83
2024-09-25 17:19:25,522 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.52
2024-09-25 17:19:27,415 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.40
2024-09-25 17:19:29,320 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.55
2024-09-25 17:19:31,138 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.95
2024-09-25 17:19:32,997 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.19
2024-09-25 17:19:35,020 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.17
2024-09-25 17:19:37,008 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.17
2024-09-25 17:19:38,948 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:19:40,878 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.79
2024-09-25 17:19:42,754 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:19:44,689 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.88
2024-09-25 17:19:46,676 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.17
2024-09-25 17:19:48,598 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.33
2024-09-25 17:19:50,551 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.76
2024-09-25 17:19:52,476 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.10
2024-09-25 17:19:54,353 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.93
2024-09-25 17:19:56,276 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.02
2024-09-25 17:19:58,145 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:20:00,061 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.07
2024-09-25 17:20:02,022 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.93
2024-09-25 17:20:03,885 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:20:05,843 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.19
2024-09-25 17:20:07,716 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.05, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.17
2024-09-25 17:20:09,641 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.21
2024-09-25 17:20:11,606 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.76
2024-09-25 17:20:13,541 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.21
2024-09-25 17:20:15,384 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:20:17,417 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.36
2024-09-25 17:20:19,298 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.31
2024-09-25 17:20:21,186 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.26
2024-09-25 17:20:23,036 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.26
2024-09-25 17:20:24,925 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 99.96, Test_acc 74.17
2024-09-25 17:20:26,791 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.05
2024-09-25 17:20:28,702 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:20:30,627 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.14
2024-09-25 17:20:32,474 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.40
2024-09-25 17:20:34,393 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.26
2024-09-25 17:20:36,254 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.36
2024-09-25 17:20:38,192 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.24
2024-09-25 17:20:40,052 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:20:41,930 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.86
2024-09-25 17:20:43,897 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.05
2024-09-25 17:20:45,758 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.19
2024-09-25 17:20:47,603 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.21
2024-09-25 17:20:49,421 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.19
2024-09-25 17:20:51,308 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.17
2024-09-25 17:20:53,235 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:20:55,114 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.24
2024-09-25 17:20:57,043 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.05
2024-09-25 17:20:58,980 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.98
2024-09-25 17:21:01,025 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.05, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:21:02,922 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.19
2024-09-25 17:21:04,792 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.17
2024-09-25 17:21:06,798 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:21:08,744 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:21:10,636 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.10
2024-09-25 17:21:12,694 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.24
2024-09-25 17:21:14,658 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.31
2024-09-25 17:21:16,549 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.07
2024-09-25 17:21:16,551 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-09-25 17:21:16,551 [base.py] => Reducing exemplars...(100 per classes)
2024-09-25 17:21:17,707 [base.py] => Constructing exemplars...(100 per classes)
2024-09-25 17:21:19,968 [base.py] => Reducing exemplars...(71 per classes)
2024-09-25 17:21:21,168 [base.py] => Constructing exemplars...(71 per classes)
2024-09-25 17:21:24,229 [podnet.py] => Exemplar size: 497
2024-09-25 17:21:24,229 [trainer.py] => CNN: {'total': 74.07, '00-04': 66.03, '05-06': 94.17, 'old': 66.03, 'new': 94.17}
2024-09-25 17:21:24,229 [trainer.py] => NME: {'total': 77.64, '00-04': 80.27, '05-06': 71.08, 'old': 80.27, 'new': 71.08}
2024-09-25 17:21:24,230 [trainer.py] => CNN top1 curve: [90.0, 74.07]
2024-09-25 17:21:24,230 [trainer.py] => CNN top5 curve: [100.0, 98.71]
2024-09-25 17:21:24,230 [trainer.py] => NME top1 curve: [90.03, 77.64]
2024-09-25 17:21:24,230 [trainer.py] => NME top5 curve: [100.0, 98.74]

2024-09-25 17:21:24,230 [trainer.py] => Average Accuracy (CNN): 82.035
2024-09-25 17:21:24,230 [trainer.py] => Average Accuracy (NME): 83.83500000000001
2024-09-25 17:21:24,230 [trainer.py] => All params: 3879745
2024-09-25 17:21:24,230 [trainer.py] => Trainable params: 3879745
2024-09-25 17:21:24,232 [podnet.py] => Learning on 7-9
2024-09-25 17:21:24,291 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-09-25 17:21:26,477 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 1.01, Spatial_loss 0.43, Flat_loss 0.62, Train_acc 81.10, Test_acc 29.26
2024-09-25 17:21:28,544 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.26, Spatial_loss 0.30, Flat_loss 0.32, Train_acc 95.44, Test_acc 23.83
2024-09-25 17:21:30,521 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.23, Spatial_loss 0.29, Flat_loss 0.25, Train_acc 96.55, Test_acc 40.17
2024-09-25 17:21:32,547 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.14, Spatial_loss 0.24, Flat_loss 0.21, Train_acc 99.00, Test_acc 62.33
2024-09-25 17:21:34,504 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.13, Spatial_loss 0.22, Flat_loss 0.18, Train_acc 99.51, Test_acc 31.20
2024-09-25 17:21:36,465 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.12, Spatial_loss 0.23, Flat_loss 0.18, Train_acc 99.42, Test_acc 55.52
2024-09-25 17:21:38,480 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.12, Spatial_loss 0.21, Flat_loss 0.17, Train_acc 99.84, Test_acc 60.89
2024-09-25 17:21:40,516 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.11, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 99.89, Test_acc 62.30
2024-09-25 17:21:42,436 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.11, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 99.84, Test_acc 55.43
2024-09-25 17:21:44,393 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.13, Spatial_loss 0.20, Flat_loss 0.16, Train_acc 99.20, Test_acc 66.13
2024-09-25 17:21:46,312 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.11, Spatial_loss 0.19, Flat_loss 0.15, Train_acc 99.80, Test_acc 59.39
2024-09-25 17:21:48,458 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.11, Spatial_loss 0.18, Flat_loss 0.14, Train_acc 99.76, Test_acc 63.02
2024-09-25 17:21:50,416 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.10, Spatial_loss 0.19, Flat_loss 0.14, Train_acc 99.91, Test_acc 60.52
2024-09-25 17:21:52,417 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.96, Test_acc 48.11
2024-09-25 17:21:54,448 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.11, Spatial_loss 0.19, Flat_loss 0.14, Train_acc 99.71, Test_acc 53.98
2024-09-25 17:21:56,536 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.93, Test_acc 60.24
2024-09-25 17:21:58,617 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.13, Train_acc 100.00, Test_acc 57.63
2024-09-25 17:22:00,705 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.13, Train_acc 100.00, Test_acc 58.20
2024-09-25 17:22:02,688 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 99.98, Test_acc 59.31
2024-09-25 17:22:04,745 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.91, Test_acc 55.44
2024-09-25 17:22:06,821 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 63.37
2024-09-25 17:22:08,864 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.78, Test_acc 62.44
2024-09-25 17:22:11,023 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 99.93, Test_acc 63.87
2024-09-25 17:22:13,053 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 99.96, Test_acc 64.30
2024-09-25 17:22:15,075 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 99.96, Test_acc 45.56
2024-09-25 17:22:17,065 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 99.82, Test_acc 27.72
2024-09-25 17:22:19,044 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.11, Spatial_loss 0.18, Flat_loss 0.14, Train_acc 99.53, Test_acc 68.28
2024-09-25 17:22:20,955 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 100.00, Test_acc 56.91
2024-09-25 17:22:22,962 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 99.96, Test_acc 61.72
2024-09-25 17:22:25,038 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 62.87
2024-09-25 17:22:27,050 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 100.00, Test_acc 62.35
2024-09-25 17:22:29,148 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.83
2024-09-25 17:22:31,189 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 63.37
2024-09-25 17:22:33,198 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 56.00
2024-09-25 17:22:35,213 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 62.31
2024-09-25 17:22:37,191 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.98, Test_acc 65.81
2024-09-25 17:22:39,166 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.96, Test_acc 64.93
2024-09-25 17:22:41,138 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.93, Test_acc 67.78
2024-09-25 17:22:43,133 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 51.63
2024-09-25 17:22:45,138 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.12, Train_acc 99.71, Test_acc 56.30
2024-09-25 17:22:47,204 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 64.56
2024-09-25 17:22:49,131 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 55.93
2024-09-25 17:22:51,179 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 62.33
2024-09-25 17:22:53,182 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 99.80, Test_acc 61.91
2024-09-25 17:22:55,256 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 63.50
2024-09-25 17:22:57,334 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.98, Test_acc 60.83
2024-09-25 17:22:59,309 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 61.26
2024-09-25 17:23:01,278 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.11, Train_acc 99.87, Test_acc 64.43
2024-09-25 17:23:03,227 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.11, Train_acc 100.00, Test_acc 64.50
2024-09-25 17:23:05,285 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.98, Test_acc 58.76
2024-09-25 17:23:07,332 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.93, Test_acc 60.13
2024-09-25 17:23:09,327 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.31
2024-09-25 17:23:11,362 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.98, Test_acc 58.07
2024-09-25 17:23:13,354 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.04
2024-09-25 17:23:15,395 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.96, Test_acc 60.50
2024-09-25 17:23:17,396 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 63.22
2024-09-25 17:23:19,409 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.39
2024-09-25 17:23:21,429 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.00
2024-09-25 17:23:23,467 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.33
2024-09-25 17:23:25,519 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.11, Train_acc 99.91, Test_acc 63.33
2024-09-25 17:23:27,608 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 63.30
2024-09-25 17:23:29,576 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.96, Test_acc 63.17
2024-09-25 17:23:31,601 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.07
2024-09-25 17:23:33,649 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.59
2024-09-25 17:23:35,693 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.98, Test_acc 64.22
2024-09-25 17:23:37,772 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 99.98, Test_acc 66.91
2024-09-25 17:23:39,782 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.96, Test_acc 51.65
2024-09-25 17:23:41,792 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 99.91, Test_acc 34.46
2024-09-25 17:23:43,796 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.14, Train_acc 99.00, Test_acc 63.94
2024-09-25 17:23:45,876 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.82, Test_acc 63.98
2024-09-25 17:23:47,956 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.98, Test_acc 61.39
2024-09-25 17:23:49,981 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.80
2024-09-25 17:23:52,037 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.81
2024-09-25 17:23:54,169 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.20
2024-09-25 17:23:56,131 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.83
2024-09-25 17:23:58,069 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.76
2024-09-25 17:24:00,117 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 60.65
2024-09-25 17:24:02,110 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 99.98, Test_acc 66.96
2024-09-25 17:24:04,210 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.52
2024-09-25 17:24:06,225 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.74
2024-09-25 17:24:08,210 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.44
2024-09-25 17:24:10,290 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.07
2024-09-25 17:24:12,415 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 65.50
2024-09-25 17:24:14,481 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.69
2024-09-25 17:24:16,564 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.57
2024-09-25 17:24:18,586 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.54
2024-09-25 17:24:20,712 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.80
2024-09-25 17:24:22,689 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.72
2024-09-25 17:24:24,717 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.74
2024-09-25 17:24:26,709 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.46
2024-09-25 17:24:28,831 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.02
2024-09-25 17:24:30,879 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.41
2024-09-25 17:24:32,929 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 60.63
2024-09-25 17:24:34,943 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.98
2024-09-25 17:24:36,957 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.41
2024-09-25 17:24:38,909 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.24
2024-09-25 17:24:40,950 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.54
2024-09-25 17:24:42,953 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.72
2024-09-25 17:24:44,881 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 55.44
2024-09-25 17:24:46,876 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.84, Test_acc 65.09
2024-09-25 17:24:48,819 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.98
2024-09-25 17:24:50,851 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.15
2024-09-25 17:24:52,820 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.31
2024-09-25 17:24:54,822 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.83
2024-09-25 17:24:56,885 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 59.43
2024-09-25 17:24:58,889 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.96, Test_acc 57.94
2024-09-25 17:25:00,918 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.98, Test_acc 61.50
2024-09-25 17:25:02,927 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.46
2024-09-25 17:25:04,962 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.70
2024-09-25 17:25:06,969 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.93, Test_acc 11.39
2024-09-25 17:25:08,987 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.27, Spatial_loss 0.29, Flat_loss 0.20, Train_acc 95.51, Test_acc 32.76
2024-09-25 17:25:10,930 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.12, Spatial_loss 0.20, Flat_loss 0.14, Train_acc 98.51, Test_acc 60.15
2024-09-25 17:25:12,945 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.91, Test_acc 63.41
2024-09-25 17:25:14,950 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.65
2024-09-25 17:25:16,958 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.65
2024-09-25 17:25:18,990 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.10, Train_acc 100.00, Test_acc 65.96
2024-09-25 17:25:21,100 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.28
2024-09-25 17:25:23,114 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.69
2024-09-25 17:25:25,137 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.33
2024-09-25 17:25:27,128 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.98, Test_acc 64.11
2024-09-25 17:25:29,142 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.31
2024-09-25 17:25:31,323 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.72
2024-09-25 17:25:33,249 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.93
2024-09-25 17:25:35,374 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.39
2024-09-25 17:25:37,556 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.06
2024-09-25 17:25:39,671 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.02
2024-09-25 17:25:41,684 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.04
2024-09-25 17:25:43,826 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.98
2024-09-25 17:25:45,867 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.94
2024-09-25 17:25:47,975 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.35
2024-09-25 17:25:50,024 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.74
2024-09-25 17:25:52,101 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.61
2024-09-25 17:25:54,127 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.17
2024-09-25 17:25:56,329 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.30
2024-09-25 17:25:58,415 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 60.94
2024-09-25 17:26:00,584 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.61
2024-09-25 17:26:02,769 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 60.70
2024-09-25 17:26:04,917 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.54
2024-09-25 17:26:07,010 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.65
2024-09-25 17:26:09,129 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.22
2024-09-25 17:26:11,211 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.39
2024-09-25 17:26:13,378 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.24
2024-09-25 17:26:15,426 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.00
2024-09-25 17:26:17,742 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.31
2024-09-25 17:26:19,853 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.89
2024-09-25 17:26:21,892 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 99.98, Test_acc 64.91
2024-09-25 17:26:23,971 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.67
2024-09-25 17:26:26,193 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.50
2024-09-25 17:26:28,270 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.65
2024-09-25 17:26:30,301 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.22
2024-09-25 17:26:32,395 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.50
2024-09-25 17:26:34,599 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.17
2024-09-25 17:26:36,585 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.00
2024-09-25 17:26:38,722 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.48
2024-09-25 17:26:40,848 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.87
2024-09-25 17:26:42,964 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.83
2024-09-25 17:26:44,989 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.91
2024-09-25 17:26:47,186 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.46
2024-09-25 17:26:49,447 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.63
2024-09-25 17:26:51,544 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.02
2024-09-25 17:26:53,609 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.94
2024-09-25 17:26:55,657 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.19
2024-09-25 17:26:57,811 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.17
2024-09-25 17:26:59,970 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.07
2024-09-25 17:27:02,174 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.20
2024-09-25 17:27:04,362 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.76
2024-09-25 17:27:06,497 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.85
2024-09-25 17:27:08,618 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.78
2024-09-25 17:27:10,817 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.54
2024-09-25 17:27:12,864 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.06
2024-09-25 17:27:15,003 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.52
2024-09-25 17:27:17,264 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.04
2024-09-25 17:27:19,460 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 99.98, Test_acc 66.33
2024-09-25 17:27:21,513 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.59
2024-09-25 17:27:23,508 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.46
2024-09-25 17:27:25,512 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.37
2024-09-25 17:27:27,497 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.72
2024-09-25 17:27:29,443 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.65
2024-09-25 17:27:31,482 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.15
2024-09-25 17:27:33,529 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.20
2024-09-25 17:27:35,648 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.48
2024-09-25 17:27:37,639 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.44
2024-09-25 17:27:39,624 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.24
2024-09-25 17:27:41,712 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.43
2024-09-25 17:27:43,758 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.74
2024-09-25 17:27:45,913 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 62.69
2024-09-25 17:27:48,002 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.63
2024-09-25 17:27:50,234 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 38.44
2024-09-25 17:27:52,256 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.09
2024-09-25 17:27:54,479 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.46
2024-09-25 17:27:56,517 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.24
2024-09-25 17:27:58,676 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.50
2024-09-25 17:28:00,772 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.15
2024-09-25 17:28:02,942 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.48
2024-09-25 17:28:05,030 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.83
2024-09-25 17:28:07,188 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.31
2024-09-25 17:28:09,398 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.07
2024-09-25 17:28:11,543 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.15
2024-09-25 17:28:13,712 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.54
2024-09-25 17:28:15,759 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.46
2024-09-25 17:28:17,920 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.87
2024-09-25 17:28:20,023 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.57
2024-09-25 17:28:22,133 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.30
2024-09-25 17:28:24,131 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.78
2024-09-25 17:28:26,300 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.98
2024-09-25 17:28:28,269 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.04
2024-09-25 17:28:30,321 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.91
2024-09-25 17:28:32,446 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.35
2024-09-25 17:28:34,479 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.93
2024-09-25 17:28:36,679 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.07
2024-09-25 17:28:38,817 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.76
2024-09-25 17:28:41,056 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.50
2024-09-25 17:28:43,179 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.98
2024-09-25 17:28:45,305 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.80
2024-09-25 17:28:47,292 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 62.70
2024-09-25 17:28:49,463 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.04
2024-09-25 17:28:51,623 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.65
2024-09-25 17:28:53,732 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.13
2024-09-25 17:28:55,911 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.46
2024-09-25 17:28:57,964 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.70
2024-09-25 17:29:00,029 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.31
2024-09-25 17:29:02,165 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.98
2024-09-25 17:29:04,343 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.94
2024-09-25 17:29:06,397 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.59
2024-09-25 17:29:08,507 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.46
2024-09-25 17:29:10,557 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.06
2024-09-25 17:29:12,606 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.28
2024-09-25 17:29:14,648 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.52
2024-09-25 17:29:16,661 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.91
2024-09-25 17:29:18,696 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 62.72
2024-09-25 17:29:20,808 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.72
2024-09-25 17:29:23,091 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.50
2024-09-25 17:29:25,221 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.74
2024-09-25 17:29:27,387 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.85
2024-09-25 17:29:29,574 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.74
2024-09-25 17:29:31,716 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.22
2024-09-25 17:29:33,809 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.85
2024-09-25 17:29:35,887 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.06
2024-09-25 17:29:37,907 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.76
2024-09-25 17:29:39,969 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.37
2024-09-25 17:29:41,944 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.61
2024-09-25 17:29:43,926 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.85
2024-09-25 17:29:45,942 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.06
2024-09-25 17:29:48,024 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.30
2024-09-25 17:29:50,058 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.78
2024-09-25 17:29:52,139 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.94
2024-09-25 17:29:54,129 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.96
2024-09-25 17:29:56,176 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.30
2024-09-25 17:29:58,167 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.78
2024-09-25 17:30:00,182 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.59
2024-09-25 17:30:02,208 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.94
2024-09-25 17:30:04,211 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.83
2024-09-25 17:30:06,226 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.80
2024-09-25 17:30:08,236 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.74
2024-09-25 17:30:10,223 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.78
2024-09-25 17:30:12,358 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.57
2024-09-25 17:30:14,351 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.70
2024-09-25 17:30:16,364 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.15
2024-09-25 17:30:18,324 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.61
2024-09-25 17:30:20,353 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.43
2024-09-25 17:30:22,336 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.37
2024-09-25 17:30:24,382 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.56
2024-09-25 17:30:26,375 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.39
2024-09-25 17:30:28,396 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.76
2024-09-25 17:30:30,385 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.87
2024-09-25 17:30:32,391 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.61
2024-09-25 17:30:34,320 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.93
2024-09-25 17:30:36,337 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.04
2024-09-25 17:30:38,351 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.04
2024-09-25 17:30:40,457 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.74
2024-09-25 17:30:42,501 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.89
2024-09-25 17:30:44,551 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.54
2024-09-25 17:30:46,616 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.48
2024-09-25 17:30:48,612 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.59
2024-09-25 17:30:50,708 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.94
2024-09-25 17:30:52,832 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.50
2024-09-25 17:30:54,767 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.67
2024-09-25 17:30:56,899 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.69
2024-09-25 17:30:58,933 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.17
2024-09-25 17:31:01,013 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.52
2024-09-25 17:31:02,951 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.44
2024-09-25 17:31:05,056 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.46
2024-09-25 17:31:07,144 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.61
2024-09-25 17:31:09,181 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.04
2024-09-25 17:31:11,209 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.91
2024-09-25 17:31:13,215 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.93
2024-09-25 17:31:15,168 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.91
2024-09-25 17:31:17,182 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.85
2024-09-25 17:31:19,193 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.94
2024-09-25 17:31:21,264 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.61
2024-09-25 17:31:23,321 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.87
2024-09-25 17:31:25,293 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.89
2024-09-25 17:31:27,278 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.57
2024-09-25 17:31:29,325 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.67
2024-09-25 17:31:31,324 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.67
2024-09-25 17:31:33,251 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.87
2024-09-25 17:31:35,312 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.81
2024-09-25 17:31:37,219 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.02
2024-09-25 17:31:39,263 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.19
2024-09-25 17:31:41,292 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.72
2024-09-25 17:31:41,293 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-09-25 17:31:41,293 [base.py] => Reducing exemplars...(71 per classes)
2024-09-25 17:31:42,923 [base.py] => Constructing exemplars...(71 per classes)
2024-09-25 17:31:45,058 [base.py] => Reducing exemplars...(55 per classes)
2024-09-25 17:31:46,629 [base.py] => Constructing exemplars...(55 per classes)
2024-09-25 17:31:49,749 [podnet.py] => Exemplar size: 495
2024-09-25 17:31:49,749 [trainer.py] => CNN: {'total': 65.72, '00-04': 54.6, '05-06': 61.75, '07-08': 97.5, 'old': 56.64, 'new': 97.5}
2024-09-25 17:31:49,749 [trainer.py] => NME: {'total': 73.06, '00-04': 74.93, '05-06': 53.08, '07-08': 88.33, 'old': 68.69, 'new': 88.33}
2024-09-25 17:31:49,749 [trainer.py] => CNN top1 curve: [90.0, 74.07, 65.72]
2024-09-25 17:31:49,749 [trainer.py] => CNN top5 curve: [100.0, 98.71, 94.69]
2024-09-25 17:31:49,749 [trainer.py] => NME top1 curve: [90.03, 77.64, 73.06]
2024-09-25 17:31:49,749 [trainer.py] => NME top5 curve: [100.0, 98.74, 95.91]

2024-09-25 17:31:49,750 [trainer.py] => Average Accuracy (CNN): 76.59666666666666
2024-09-25 17:31:49,750 [trainer.py] => Average Accuracy (NME): 80.24333333333334
2024-09-25 17:31:49,750 [trainer.py] => Forgetting (CNN): 33.91
