2024-08-31 16:27:49,474 [trainer.py] => config: ./exps/podnet.json
2024-08-31 16:27:49,474 [trainer.py] => prefix: reproduce
2024-08-31 16:27:49,474 [trainer.py] => dataset: hrrp9
2024-08-31 16:27:49,474 [trainer.py] => memory_size: 500
2024-08-31 16:27:49,474 [trainer.py] => memory_per_class: 20
2024-08-31 16:27:49,474 [trainer.py] => fixed_memory: False
2024-08-31 16:27:49,474 [trainer.py] => shuffle: True
2024-08-31 16:27:49,474 [trainer.py] => init_cls: 5
2024-08-31 16:27:49,474 [trainer.py] => increment: 2
2024-08-31 16:27:49,474 [trainer.py] => model_name: podnet
2024-08-31 16:27:49,474 [trainer.py] => convnet_type: resnet18
2024-08-31 16:27:49,474 [trainer.py] => init_train: True
2024-08-31 16:27:49,474 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-08-31 16:27:49,474 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-08-31 16:27:49,474 [trainer.py] => device: [device(type='cuda', index=6)]
2024-08-31 16:27:49,474 [trainer.py] => seed: 1993
2024-08-31 16:27:49,943 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-08-31 16:27:50,043 [trainer.py] => All params: 3843904
2024-08-31 16:27:50,043 [trainer.py] => Trainable params: 3843904
2024-08-31 16:27:50,044 [podnet.py] => Learning on 0-5
2024-08-31 16:27:50,080 [podnet.py] => Adaptive factor: 0
2024-08-31 16:27:53,471 [podnet.py] => Task 0, Epoch 1/300 (LR 0.10000) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.02, Test_acc 43.47
2024-08-31 16:27:55,573 [podnet.py] => Task 0, Epoch 2/300 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.86, Test_acc 32.63
2024-08-31 16:27:57,265 [podnet.py] => Task 0, Epoch 3/300 (LR 0.09998) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.47, Test_acc 36.30
2024-08-31 16:27:59,055 [podnet.py] => Task 0, Epoch 4/300 (LR 0.09996) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.33, Test_acc 59.63
2024-08-31 16:28:01,455 [podnet.py] => Task 0, Epoch 5/300 (LR 0.09993) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.73, Test_acc 55.37
2024-08-31 16:28:03,320 [podnet.py] => Task 0, Epoch 6/300 (LR 0.09990) => LSC_loss 0.25, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.02, Test_acc 72.20
2024-08-31 16:28:06,085 [podnet.py] => Task 0, Epoch 7/300 (LR 0.09987) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.42, Test_acc 67.40
2024-08-31 16:28:07,995 [podnet.py] => Task 0, Epoch 8/300 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.66, Test_acc 74.30
2024-08-31 16:28:09,486 [podnet.py] => Task 0, Epoch 9/300 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.07, Test_acc 74.60
2024-08-31 16:28:11,245 [podnet.py] => Task 0, Epoch 10/300 (LR 0.09973) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.47, Test_acc 78.70
2024-08-31 16:28:13,820 [podnet.py] => Task 0, Epoch 11/300 (LR 0.09967) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 70.47
2024-08-31 16:28:16,487 [podnet.py] => Task 0, Epoch 12/300 (LR 0.09961) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.31, Test_acc 82.27
2024-08-31 16:28:18,530 [podnet.py] => Task 0, Epoch 13/300 (LR 0.09954) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.29, Test_acc 82.53
2024-08-31 16:28:20,464 [podnet.py] => Task 0, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 67.27
2024-08-31 16:28:22,373 [podnet.py] => Task 0, Epoch 15/300 (LR 0.09938) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.94, Test_acc 79.93
2024-08-31 16:28:24,223 [podnet.py] => Task 0, Epoch 16/300 (LR 0.09930) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.08, Test_acc 83.07
2024-08-31 16:28:26,856 [podnet.py] => Task 0, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 72.60
2024-08-31 16:28:29,725 [podnet.py] => Task 0, Epoch 18/300 (LR 0.09911) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.56, Test_acc 80.30
2024-08-31 16:28:32,286 [podnet.py] => Task 0, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 81.57
2024-08-31 16:28:34,685 [podnet.py] => Task 0, Epoch 20/300 (LR 0.09891) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 79.03
2024-08-31 16:28:36,504 [podnet.py] => Task 0, Epoch 21/300 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 83.07
2024-08-31 16:28:38,178 [podnet.py] => Task 0, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.83, Test_acc 74.17
2024-08-31 16:28:39,989 [podnet.py] => Task 0, Epoch 23/300 (LR 0.09856) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 82.63
2024-08-31 16:28:41,985 [podnet.py] => Task 0, Epoch 24/300 (LR 0.09843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 82.83
2024-08-31 16:28:44,153 [podnet.py] => Task 0, Epoch 25/300 (LR 0.09830) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.27, Test_acc 84.33
2024-08-31 16:28:46,456 [podnet.py] => Task 0, Epoch 26/300 (LR 0.09816) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.83, Test_acc 85.20
2024-08-31 16:28:49,071 [podnet.py] => Task 0, Epoch 27/300 (LR 0.09801) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 85.90
2024-08-31 16:28:51,710 [podnet.py] => Task 0, Epoch 28/300 (LR 0.09787) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 74.20
2024-08-31 16:28:53,537 [podnet.py] => Task 0, Epoch 29/300 (LR 0.09771) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 82.03
2024-08-31 16:28:55,711 [podnet.py] => Task 0, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 76.93
2024-08-31 16:28:57,543 [podnet.py] => Task 0, Epoch 31/300 (LR 0.09739) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 79.40
2024-08-31 16:28:59,525 [podnet.py] => Task 0, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.66, Test_acc 77.10
2024-08-31 16:29:01,698 [podnet.py] => Task 0, Epoch 33/300 (LR 0.09704) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.02, Test_acc 79.23
2024-08-31 16:29:03,394 [podnet.py] => Task 0, Epoch 34/300 (LR 0.09686) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.07, Test_acc 86.00
2024-08-31 16:29:05,157 [podnet.py] => Task 0, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 74.73
2024-08-31 16:29:07,234 [podnet.py] => Task 0, Epoch 36/300 (LR 0.09649) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.14, Test_acc 79.27
2024-08-31 16:29:09,179 [podnet.py] => Task 0, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 84.97
2024-08-31 16:29:10,931 [podnet.py] => Task 0, Epoch 38/300 (LR 0.09609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 84.70
2024-08-31 16:29:13,572 [podnet.py] => Task 0, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 82.83
2024-08-31 16:29:15,978 [podnet.py] => Task 0, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 75.70
2024-08-31 16:29:18,047 [podnet.py] => Task 0, Epoch 41/300 (LR 0.09546) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.97, Test_acc 83.57
2024-08-31 16:29:20,103 [podnet.py] => Task 0, Epoch 42/300 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 82.43
2024-08-31 16:29:22,031 [podnet.py] => Task 0, Epoch 43/300 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 86.70
2024-08-31 16:29:24,286 [podnet.py] => Task 0, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 71.70
2024-08-31 16:29:26,038 [podnet.py] => Task 0, Epoch 45/300 (LR 0.09455) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 84.30
2024-08-31 16:29:27,975 [podnet.py] => Task 0, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 80.70
2024-08-31 16:29:29,827 [podnet.py] => Task 0, Epoch 47/300 (LR 0.09407) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.17, Test_acc 84.67
2024-08-31 16:29:31,492 [podnet.py] => Task 0, Epoch 48/300 (LR 0.09382) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 83.63
2024-08-31 16:29:33,465 [podnet.py] => Task 0, Epoch 49/300 (LR 0.09356) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 85.03
2024-08-31 16:29:35,734 [podnet.py] => Task 0, Epoch 50/300 (LR 0.09330) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 82.57
2024-08-31 16:29:37,292 [podnet.py] => Task 0, Epoch 51/300 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 88.17
2024-08-31 16:29:39,575 [podnet.py] => Task 0, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 80.73
2024-08-31 16:29:41,591 [podnet.py] => Task 0, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 78.07
2024-08-31 16:29:43,385 [podnet.py] => Task 0, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.67
2024-08-31 16:29:45,795 [podnet.py] => Task 0, Epoch 55/300 (LR 0.09193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 82.83
2024-08-31 16:29:47,746 [podnet.py] => Task 0, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 82.33
2024-08-31 16:29:50,169 [podnet.py] => Task 0, Epoch 57/300 (LR 0.09135) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.60, Test_acc 82.83
2024-08-31 16:29:51,758 [podnet.py] => Task 0, Epoch 58/300 (LR 0.09106) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.50, Test_acc 84.40
2024-08-31 16:29:53,995 [podnet.py] => Task 0, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.01, Test_acc 70.30
2024-08-31 16:29:55,717 [podnet.py] => Task 0, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.83
2024-08-31 16:29:57,521 [podnet.py] => Task 0, Epoch 61/300 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.69, Test_acc 83.20
2024-08-31 16:29:59,801 [podnet.py] => Task 0, Epoch 62/300 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 86.53
2024-08-31 16:30:02,043 [podnet.py] => Task 0, Epoch 63/300 (LR 0.08951) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 84.83
2024-08-31 16:30:04,200 [podnet.py] => Task 0, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 84.63
2024-08-31 16:30:06,000 [podnet.py] => Task 0, Epoch 65/300 (LR 0.08886) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 81.80
2024-08-31 16:30:07,792 [podnet.py] => Task 0, Epoch 66/300 (LR 0.08853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 81.17
2024-08-31 16:30:10,001 [podnet.py] => Task 0, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 82.20
2024-08-31 16:30:12,629 [podnet.py] => Task 0, Epoch 68/300 (LR 0.08785) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.91, Test_acc 83.43
2024-08-31 16:30:14,773 [podnet.py] => Task 0, Epoch 69/300 (LR 0.08751) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 84.27
2024-08-31 16:30:17,474 [podnet.py] => Task 0, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.47, Test_acc 84.80
2024-08-31 16:30:19,347 [podnet.py] => Task 0, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 88.13
2024-08-31 16:30:21,717 [podnet.py] => Task 0, Epoch 72/300 (LR 0.08645) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 87.83
2024-08-31 16:30:23,561 [podnet.py] => Task 0, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.82, Test_acc 87.00
2024-08-31 16:30:25,617 [podnet.py] => Task 0, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 87.03
2024-08-31 16:30:27,783 [podnet.py] => Task 0, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.63
2024-08-31 16:30:29,727 [podnet.py] => Task 0, Epoch 76/300 (LR 0.08498) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.71, Test_acc 78.20
2024-08-31 16:30:31,925 [podnet.py] => Task 0, Epoch 77/300 (LR 0.08461) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 84.30
2024-08-31 16:30:33,864 [podnet.py] => Task 0, Epoch 78/300 (LR 0.08423) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 86.53
2024-08-31 16:30:35,530 [podnet.py] => Task 0, Epoch 79/300 (LR 0.08384) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 80.93
2024-08-31 16:30:38,067 [podnet.py] => Task 0, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.19, Test_acc 83.10
2024-08-31 16:30:40,711 [podnet.py] => Task 0, Epoch 81/300 (LR 0.08307) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.57
2024-08-31 16:30:42,350 [podnet.py] => Task 0, Epoch 82/300 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 86.83
2024-08-31 16:30:44,103 [podnet.py] => Task 0, Epoch 83/300 (LR 0.08227) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 84.97
2024-08-31 16:30:46,072 [podnet.py] => Task 0, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.52, Test_acc 80.90
2024-08-31 16:30:48,161 [podnet.py] => Task 0, Epoch 85/300 (LR 0.08147) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.01, Test_acc 86.27
2024-08-31 16:30:50,356 [podnet.py] => Task 0, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.85, Test_acc 80.87
2024-08-31 16:30:52,226 [podnet.py] => Task 0, Epoch 87/300 (LR 0.08065) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.98, Test_acc 78.57
2024-08-31 16:30:54,133 [podnet.py] => Task 0, Epoch 88/300 (LR 0.08023) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.55, Test_acc 85.57
2024-08-31 16:30:56,180 [podnet.py] => Task 0, Epoch 89/300 (LR 0.07981) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.87
2024-08-31 16:30:57,939 [podnet.py] => Task 0, Epoch 90/300 (LR 0.07939) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.83
2024-08-31 16:30:59,760 [podnet.py] => Task 0, Epoch 91/300 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 82.47
2024-08-31 16:31:01,530 [podnet.py] => Task 0, Epoch 92/300 (LR 0.07854) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 86.23
2024-08-31 16:31:03,416 [podnet.py] => Task 0, Epoch 93/300 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 87.07
2024-08-31 16:31:04,882 [podnet.py] => Task 0, Epoch 94/300 (LR 0.07767) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.24, Test_acc 84.03
2024-08-31 16:31:07,594 [podnet.py] => Task 0, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 80.70
2024-08-31 16:31:09,663 [podnet.py] => Task 0, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 85.07
2024-08-31 16:31:12,042 [podnet.py] => Task 0, Epoch 97/300 (LR 0.07635) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.31, Test_acc 79.37
2024-08-31 16:31:13,802 [podnet.py] => Task 0, Epoch 98/300 (LR 0.07590) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 87.33
2024-08-31 16:31:16,120 [podnet.py] => Task 0, Epoch 99/300 (LR 0.07545) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.21, Test_acc 84.17
2024-08-31 16:31:18,395 [podnet.py] => Task 0, Epoch 100/300 (LR 0.07500) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 85.00
2024-08-31 16:31:20,080 [podnet.py] => Task 0, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 82.50
2024-08-31 16:31:21,775 [podnet.py] => Task 0, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 83.40
2024-08-31 16:31:24,050 [podnet.py] => Task 0, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.89, Test_acc 83.40
2024-08-31 16:31:25,970 [podnet.py] => Task 0, Epoch 104/300 (LR 0.07316) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 80.57
2024-08-31 16:31:27,901 [podnet.py] => Task 0, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 88.03
2024-08-31 16:31:29,632 [podnet.py] => Task 0, Epoch 106/300 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 85.80
2024-08-31 16:31:31,317 [podnet.py] => Task 0, Epoch 107/300 (LR 0.07176) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 80.70
2024-08-31 16:31:33,090 [podnet.py] => Task 0, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 85.73
2024-08-31 16:31:34,487 [podnet.py] => Task 0, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.50
2024-08-31 16:31:36,898 [podnet.py] => Task 0, Epoch 110/300 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 87.13
2024-08-31 16:31:38,665 [podnet.py] => Task 0, Epoch 111/300 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 87.13
2024-08-31 16:31:40,935 [podnet.py] => Task 0, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.84, Test_acc 84.77
2024-08-31 16:31:42,690 [podnet.py] => Task 0, Epoch 113/300 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 85.40
2024-08-31 16:31:44,540 [podnet.py] => Task 0, Epoch 114/300 (LR 0.06841) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 83.33
2024-08-31 16:31:46,848 [podnet.py] => Task 0, Epoch 115/300 (LR 0.06792) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 84.27
2024-08-31 16:31:48,679 [podnet.py] => Task 0, Epoch 116/300 (LR 0.06743) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.03, Test_acc 84.73
2024-08-31 16:31:51,274 [podnet.py] => Task 0, Epoch 117/300 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 79.07
2024-08-31 16:31:52,945 [podnet.py] => Task 0, Epoch 118/300 (LR 0.06644) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 84.63
2024-08-31 16:31:54,905 [podnet.py] => Task 0, Epoch 119/300 (LR 0.06595) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.53
2024-08-31 16:31:57,076 [podnet.py] => Task 0, Epoch 120/300 (LR 0.06545) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.43, Test_acc 83.80
2024-08-31 16:31:58,754 [podnet.py] => Task 0, Epoch 121/300 (LR 0.06495) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 87.63
2024-08-31 16:32:01,007 [podnet.py] => Task 0, Epoch 122/300 (LR 0.06445) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 83.27
2024-08-31 16:32:02,539 [podnet.py] => Task 0, Epoch 123/300 (LR 0.06395) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.34, Test_acc 82.97
2024-08-31 16:32:04,768 [podnet.py] => Task 0, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 83.40
2024-08-31 16:32:07,014 [podnet.py] => Task 0, Epoch 125/300 (LR 0.06294) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.77
2024-08-31 16:32:09,147 [podnet.py] => Task 0, Epoch 126/300 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 86.57
2024-08-31 16:32:11,094 [podnet.py] => Task 0, Epoch 127/300 (LR 0.06193) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.36, Test_acc 86.00
2024-08-31 16:32:13,020 [podnet.py] => Task 0, Epoch 128/300 (LR 0.06142) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 84.60
2024-08-31 16:32:14,741 [podnet.py] => Task 0, Epoch 129/300 (LR 0.06091) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 87.97
2024-08-31 16:32:16,353 [podnet.py] => Task 0, Epoch 130/300 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 82.23
2024-08-31 16:32:18,265 [podnet.py] => Task 0, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.50, Test_acc 86.90
2024-08-31 16:32:20,085 [podnet.py] => Task 0, Epoch 132/300 (LR 0.05937) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.73
2024-08-31 16:32:21,945 [podnet.py] => Task 0, Epoch 133/300 (LR 0.05885) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.57
2024-08-31 16:32:23,837 [podnet.py] => Task 0, Epoch 134/300 (LR 0.05834) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.87
2024-08-31 16:32:25,993 [podnet.py] => Task 0, Epoch 135/300 (LR 0.05782) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 85.53
2024-08-31 16:32:27,843 [podnet.py] => Task 0, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.63, Test_acc 79.13
2024-08-31 16:32:29,989 [podnet.py] => Task 0, Epoch 137/300 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 88.00
2024-08-31 16:32:31,754 [podnet.py] => Task 0, Epoch 138/300 (LR 0.05627) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.33
2024-08-31 16:32:34,052 [podnet.py] => Task 0, Epoch 139/300 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 88.17
2024-08-31 16:32:35,560 [podnet.py] => Task 0, Epoch 140/300 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 82.47
2024-08-31 16:32:37,775 [podnet.py] => Task 0, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 84.93
2024-08-31 16:32:39,755 [podnet.py] => Task 0, Epoch 142/300 (LR 0.05418) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.70, Test_acc 84.13
2024-08-31 16:32:42,143 [podnet.py] => Task 0, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 84.60
2024-08-31 16:32:43,627 [podnet.py] => Task 0, Epoch 144/300 (LR 0.05314) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 87.83
2024-08-31 16:32:46,263 [podnet.py] => Task 0, Epoch 145/300 (LR 0.05262) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.60
2024-08-31 16:32:48,140 [podnet.py] => Task 0, Epoch 146/300 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 81.50
2024-08-31 16:32:50,310 [podnet.py] => Task 0, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 86.77
2024-08-31 16:32:52,079 [podnet.py] => Task 0, Epoch 148/300 (LR 0.05105) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 84.77
2024-08-31 16:32:54,118 [podnet.py] => Task 0, Epoch 149/300 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 85.70
2024-08-31 16:32:55,853 [podnet.py] => Task 0, Epoch 150/300 (LR 0.05000) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 86.57
2024-08-31 16:32:57,927 [podnet.py] => Task 0, Epoch 151/300 (LR 0.04948) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 84.43
2024-08-31 16:32:59,793 [podnet.py] => Task 0, Epoch 152/300 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 88.60
2024-08-31 16:33:01,977 [podnet.py] => Task 0, Epoch 153/300 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 87.07
2024-08-31 16:33:03,819 [podnet.py] => Task 0, Epoch 154/300 (LR 0.04791) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 87.57
2024-08-31 16:33:05,795 [podnet.py] => Task 0, Epoch 155/300 (LR 0.04738) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 81.10
2024-08-31 16:33:07,657 [podnet.py] => Task 0, Epoch 156/300 (LR 0.04686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.67, Test_acc 88.10
2024-08-31 16:33:09,293 [podnet.py] => Task 0, Epoch 157/300 (LR 0.04634) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 86.87
2024-08-31 16:33:10,719 [podnet.py] => Task 0, Epoch 158/300 (LR 0.04582) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.53
2024-08-31 16:33:12,795 [podnet.py] => Task 0, Epoch 159/300 (LR 0.04529) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.43
2024-08-31 16:33:14,686 [podnet.py] => Task 0, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 86.20
2024-08-31 16:33:17,189 [podnet.py] => Task 0, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 84.87
2024-08-31 16:33:18,855 [podnet.py] => Task 0, Epoch 162/300 (LR 0.04373) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 88.27
2024-08-31 16:33:21,089 [podnet.py] => Task 0, Epoch 163/300 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 85.63
2024-08-31 16:33:23,063 [podnet.py] => Task 0, Epoch 164/300 (LR 0.04270) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 83.27
2024-08-31 16:33:24,549 [podnet.py] => Task 0, Epoch 165/300 (LR 0.04218) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 85.37
2024-08-31 16:33:26,515 [podnet.py] => Task 0, Epoch 166/300 (LR 0.04166) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.33, Test_acc 89.13
2024-08-31 16:33:28,285 [podnet.py] => Task 0, Epoch 167/300 (LR 0.04115) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 86.80
2024-08-31 16:33:29,992 [podnet.py] => Task 0, Epoch 168/300 (LR 0.04063) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 90.17
2024-08-31 16:33:31,809 [podnet.py] => Task 0, Epoch 169/300 (LR 0.04012) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.03
2024-08-31 16:33:34,231 [podnet.py] => Task 0, Epoch 170/300 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 89.47
2024-08-31 16:33:36,077 [podnet.py] => Task 0, Epoch 171/300 (LR 0.03909) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.13
2024-08-31 16:33:38,442 [podnet.py] => Task 0, Epoch 172/300 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 89.47
2024-08-31 16:33:40,073 [podnet.py] => Task 0, Epoch 173/300 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.13
2024-08-31 16:33:41,462 [podnet.py] => Task 0, Epoch 174/300 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 84.43
2024-08-31 16:33:43,921 [podnet.py] => Task 0, Epoch 175/300 (LR 0.03706) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.30
2024-08-31 16:33:45,769 [podnet.py] => Task 0, Epoch 176/300 (LR 0.03655) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 81.37
2024-08-31 16:33:47,586 [podnet.py] => Task 0, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.73, Test_acc 85.23
2024-08-31 16:33:49,564 [podnet.py] => Task 0, Epoch 178/300 (LR 0.03555) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 85.83
2024-08-31 16:33:51,891 [podnet.py] => Task 0, Epoch 179/300 (LR 0.03505) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 83.43
2024-08-31 16:33:53,818 [podnet.py] => Task 0, Epoch 180/300 (LR 0.03455) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 88.90
2024-08-31 16:33:55,666 [podnet.py] => Task 0, Epoch 181/300 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 88.60
2024-08-31 16:33:57,861 [podnet.py] => Task 0, Epoch 182/300 (LR 0.03356) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-08-31 16:34:00,479 [podnet.py] => Task 0, Epoch 183/300 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.73
2024-08-31 16:34:02,761 [podnet.py] => Task 0, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.11, Test_acc 88.17
2024-08-31 16:34:04,457 [podnet.py] => Task 0, Epoch 185/300 (LR 0.03208) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 87.47
2024-08-31 16:34:06,728 [podnet.py] => Task 0, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 79.03
2024-08-31 16:34:08,879 [podnet.py] => Task 0, Epoch 187/300 (LR 0.03111) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.53, Test_acc 84.80
2024-08-31 16:34:11,386 [podnet.py] => Task 0, Epoch 188/300 (LR 0.03062) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 85.53
2024-08-31 16:34:13,125 [podnet.py] => Task 0, Epoch 189/300 (LR 0.03014) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.35, Test_acc 88.90
2024-08-31 16:34:14,942 [podnet.py] => Task 0, Epoch 190/300 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 86.80
2024-08-31 16:34:16,957 [podnet.py] => Task 0, Epoch 191/300 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 88.20
2024-08-31 16:34:18,768 [podnet.py] => Task 0, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 86.90
2024-08-31 16:34:20,129 [podnet.py] => Task 0, Epoch 193/300 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 89.30
2024-08-31 16:34:22,243 [podnet.py] => Task 0, Epoch 194/300 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 90.27
2024-08-31 16:34:24,691 [podnet.py] => Task 0, Epoch 195/300 (LR 0.02730) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.47
2024-08-31 16:34:26,973 [podnet.py] => Task 0, Epoch 196/300 (LR 0.02684) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.73
2024-08-31 16:34:28,847 [podnet.py] => Task 0, Epoch 197/300 (LR 0.02637) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.63
2024-08-31 16:34:30,856 [podnet.py] => Task 0, Epoch 198/300 (LR 0.02591) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.23
2024-08-31 16:34:32,875 [podnet.py] => Task 0, Epoch 199/300 (LR 0.02545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.60
2024-08-31 16:34:34,969 [podnet.py] => Task 0, Epoch 200/300 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.67
2024-08-31 16:34:36,730 [podnet.py] => Task 0, Epoch 201/300 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.67
2024-08-31 16:34:38,544 [podnet.py] => Task 0, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 88.40
2024-08-31 16:34:40,809 [podnet.py] => Task 0, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 88.30
2024-08-31 16:34:43,254 [podnet.py] => Task 0, Epoch 204/300 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 87.87
2024-08-31 16:34:45,282 [podnet.py] => Task 0, Epoch 205/300 (LR 0.02277) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 88.73
2024-08-31 16:34:48,049 [podnet.py] => Task 0, Epoch 206/300 (LR 0.02233) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 90.10
2024-08-31 16:34:50,192 [podnet.py] => Task 0, Epoch 207/300 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.13
2024-08-31 16:34:52,673 [podnet.py] => Task 0, Epoch 208/300 (LR 0.02146) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 88.40
2024-08-31 16:34:54,630 [podnet.py] => Task 0, Epoch 209/300 (LR 0.02104) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 89.43
2024-08-31 16:34:57,030 [podnet.py] => Task 0, Epoch 210/300 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 86.40
2024-08-31 16:34:59,043 [podnet.py] => Task 0, Epoch 211/300 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 86.90
2024-08-31 16:35:01,038 [podnet.py] => Task 0, Epoch 212/300 (LR 0.01977) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 86.67
2024-08-31 16:35:02,870 [podnet.py] => Task 0, Epoch 213/300 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-08-31 16:35:04,491 [podnet.py] => Task 0, Epoch 214/300 (LR 0.01894) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 89.67
2024-08-31 16:35:06,917 [podnet.py] => Task 0, Epoch 215/300 (LR 0.01853) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 89.77
2024-08-31 16:35:08,869 [podnet.py] => Task 0, Epoch 216/300 (LR 0.01813) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.20
2024-08-31 16:35:10,928 [podnet.py] => Task 0, Epoch 217/300 (LR 0.01773) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 87.83
2024-08-31 16:35:13,196 [podnet.py] => Task 0, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 89.30
2024-08-31 16:35:15,338 [podnet.py] => Task 0, Epoch 219/300 (LR 0.01693) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.13
2024-08-31 16:35:17,286 [podnet.py] => Task 0, Epoch 220/300 (LR 0.01654) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.87
2024-08-31 16:35:18,964 [podnet.py] => Task 0, Epoch 221/300 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.27
2024-08-31 16:35:21,076 [podnet.py] => Task 0, Epoch 222/300 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-31 16:35:23,614 [podnet.py] => Task 0, Epoch 223/300 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-31 16:35:26,143 [podnet.py] => Task 0, Epoch 224/300 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-31 16:35:27,727 [podnet.py] => Task 0, Epoch 225/300 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-31 16:35:29,437 [podnet.py] => Task 0, Epoch 226/300 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-31 16:35:31,260 [podnet.py] => Task 0, Epoch 227/300 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.43
2024-08-31 16:35:33,029 [podnet.py] => Task 0, Epoch 228/300 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-08-31 16:35:34,830 [podnet.py] => Task 0, Epoch 229/300 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-31 16:35:36,646 [podnet.py] => Task 0, Epoch 230/300 (LR 0.01284) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-08-31 16:35:38,246 [podnet.py] => Task 0, Epoch 231/300 (LR 0.01249) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.00
2024-08-31 16:35:40,326 [podnet.py] => Task 0, Epoch 232/300 (LR 0.01215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 88.90
2024-08-31 16:35:42,345 [podnet.py] => Task 0, Epoch 233/300 (LR 0.01181) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.07
2024-08-31 16:35:44,661 [podnet.py] => Task 0, Epoch 234/300 (LR 0.01147) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-08-31 16:35:46,667 [podnet.py] => Task 0, Epoch 235/300 (LR 0.01114) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.03
2024-08-31 16:35:48,767 [podnet.py] => Task 0, Epoch 236/300 (LR 0.01082) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 89.43
2024-08-31 16:35:51,133 [podnet.py] => Task 0, Epoch 237/300 (LR 0.01049) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-31 16:35:53,536 [podnet.py] => Task 0, Epoch 238/300 (LR 0.01017) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-31 16:35:55,740 [podnet.py] => Task 0, Epoch 239/300 (LR 0.00986) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.97
2024-08-31 16:35:57,630 [podnet.py] => Task 0, Epoch 240/300 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:35:59,780 [podnet.py] => Task 0, Epoch 241/300 (LR 0.00924) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-31 16:36:01,321 [podnet.py] => Task 0, Epoch 242/300 (LR 0.00894) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.33
2024-08-31 16:36:02,847 [podnet.py] => Task 0, Epoch 243/300 (LR 0.00865) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-31 16:36:04,543 [podnet.py] => Task 0, Epoch 244/300 (LR 0.00835) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-31 16:36:06,259 [podnet.py] => Task 0, Epoch 245/300 (LR 0.00807) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.73
2024-08-31 16:36:08,245 [podnet.py] => Task 0, Epoch 246/300 (LR 0.00778) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-31 16:36:10,007 [podnet.py] => Task 0, Epoch 247/300 (LR 0.00751) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-31 16:36:11,454 [podnet.py] => Task 0, Epoch 248/300 (LR 0.00723) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.80
2024-08-31 16:36:13,491 [podnet.py] => Task 0, Epoch 249/300 (LR 0.00696) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.87
2024-08-31 16:36:15,658 [podnet.py] => Task 0, Epoch 250/300 (LR 0.00670) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:36:18,002 [podnet.py] => Task 0, Epoch 251/300 (LR 0.00644) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.70
2024-08-31 16:36:19,807 [podnet.py] => Task 0, Epoch 252/300 (LR 0.00618) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-31 16:36:21,720 [podnet.py] => Task 0, Epoch 253/300 (LR 0.00593) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.60
2024-08-31 16:36:23,658 [podnet.py] => Task 0, Epoch 254/300 (LR 0.00569) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:36:25,881 [podnet.py] => Task 0, Epoch 255/300 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-31 16:36:27,752 [podnet.py] => Task 0, Epoch 256/300 (LR 0.00521) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-31 16:36:29,570 [podnet.py] => Task 0, Epoch 257/300 (LR 0.00498) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.57
2024-08-31 16:36:31,616 [podnet.py] => Task 0, Epoch 258/300 (LR 0.00476) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.87
2024-08-31 16:36:33,857 [podnet.py] => Task 0, Epoch 259/300 (LR 0.00454) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.73
2024-08-31 16:36:35,608 [podnet.py] => Task 0, Epoch 260/300 (LR 0.00432) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.77
2024-08-31 16:36:37,887 [podnet.py] => Task 0, Epoch 261/300 (LR 0.00411) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.83
2024-08-31 16:36:40,348 [podnet.py] => Task 0, Epoch 262/300 (LR 0.00391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-31 16:36:42,119 [podnet.py] => Task 0, Epoch 263/300 (LR 0.00371) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-08-31 16:36:43,952 [podnet.py] => Task 0, Epoch 264/300 (LR 0.00351) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-31 16:36:45,738 [podnet.py] => Task 0, Epoch 265/300 (LR 0.00332) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-31 16:36:47,732 [podnet.py] => Task 0, Epoch 266/300 (LR 0.00314) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-08-31 16:36:50,383 [podnet.py] => Task 0, Epoch 267/300 (LR 0.00296) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-31 16:36:52,363 [podnet.py] => Task 0, Epoch 268/300 (LR 0.00278) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-31 16:36:54,470 [podnet.py] => Task 0, Epoch 269/300 (LR 0.00261) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-31 16:36:56,460 [podnet.py] => Task 0, Epoch 270/300 (LR 0.00245) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.63
2024-08-31 16:36:58,462 [podnet.py] => Task 0, Epoch 271/300 (LR 0.00229) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-31 16:37:00,254 [podnet.py] => Task 0, Epoch 272/300 (LR 0.00213) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.90
2024-08-31 16:37:02,466 [podnet.py] => Task 0, Epoch 273/300 (LR 0.00199) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.67
2024-08-31 16:37:04,370 [podnet.py] => Task 0, Epoch 274/300 (LR 0.00184) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-08-31 16:37:06,675 [podnet.py] => Task 0, Epoch 275/300 (LR 0.00170) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-31 16:37:08,625 [podnet.py] => Task 0, Epoch 276/300 (LR 0.00157) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.40
2024-08-31 16:37:10,717 [podnet.py] => Task 0, Epoch 277/300 (LR 0.00144) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-31 16:37:13,110 [podnet.py] => Task 0, Epoch 278/300 (LR 0.00132) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.70
2024-08-31 16:37:15,055 [podnet.py] => Task 0, Epoch 279/300 (LR 0.00120) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 88.97
2024-08-31 16:37:17,537 [podnet.py] => Task 0, Epoch 280/300 (LR 0.00109) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:37:19,315 [podnet.py] => Task 0, Epoch 281/300 (LR 0.00099) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-31 16:37:21,436 [podnet.py] => Task 0, Epoch 282/300 (LR 0.00089) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-31 16:37:23,835 [podnet.py] => Task 0, Epoch 283/300 (LR 0.00079) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.20
2024-08-31 16:37:25,954 [podnet.py] => Task 0, Epoch 284/300 (LR 0.00070) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.67
2024-08-31 16:37:27,811 [podnet.py] => Task 0, Epoch 285/300 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.53
2024-08-31 16:37:29,507 [podnet.py] => Task 0, Epoch 286/300 (LR 0.00054) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.57
2024-08-31 16:37:31,253 [podnet.py] => Task 0, Epoch 287/300 (LR 0.00046) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.17
2024-08-31 16:37:33,402 [podnet.py] => Task 0, Epoch 288/300 (LR 0.00039) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:37:36,088 [podnet.py] => Task 0, Epoch 289/300 (LR 0.00033) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.23
2024-08-31 16:37:38,115 [podnet.py] => Task 0, Epoch 290/300 (LR 0.00027) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-31 16:37:40,177 [podnet.py] => Task 0, Epoch 291/300 (LR 0.00022) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.43
2024-08-31 16:37:42,053 [podnet.py] => Task 0, Epoch 292/300 (LR 0.00018) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:37:43,864 [podnet.py] => Task 0, Epoch 293/300 (LR 0.00013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-08-31 16:37:45,579 [podnet.py] => Task 0, Epoch 294/300 (LR 0.00010) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.30
2024-08-31 16:37:47,714 [podnet.py] => Task 0, Epoch 295/300 (LR 0.00007) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-31 16:37:49,481 [podnet.py] => Task 0, Epoch 296/300 (LR 0.00004) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-08-31 16:37:51,382 [podnet.py] => Task 0, Epoch 297/300 (LR 0.00002) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.87
2024-08-31 16:37:53,176 [podnet.py] => Task 0, Epoch 298/300 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.13
2024-08-31 16:37:54,942 [podnet.py] => Task 0, Epoch 299/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-08-31 16:37:56,947 [podnet.py] => Task 0, Epoch 300/300 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.90
2024-08-31 16:37:57,182 [base.py] => Reducing exemplars...(100 per classes)
2024-08-31 16:37:57,182 [base.py] => Constructing exemplars...(100 per classes)
2024-08-31 16:38:01,937 [podnet.py] => Exemplar size: 500
2024-08-31 16:38:01,937 [trainer.py] => CNN: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-31 16:38:01,937 [trainer.py] => NME: {'total': 88.9, '00-04': 88.9, 'old': 0, 'new': 88.9}
2024-08-31 16:38:01,937 [trainer.py] => CNN top1 curve: [88.9]
2024-08-31 16:38:01,937 [trainer.py] => CNN top5 curve: [100.0]
2024-08-31 16:38:01,937 [trainer.py] => NME top1 curve: [88.9]
2024-08-31 16:38:01,937 [trainer.py] => NME top5 curve: [100.0]

2024-08-31 16:38:01,937 [trainer.py] => Average Accuracy (CNN): 88.9
2024-08-31 16:38:01,937 [trainer.py] => Average Accuracy (NME): 88.9
2024-08-31 16:38:01,937 [trainer.py] => All params: 3869505
2024-08-31 16:38:01,938 [trainer.py] => Trainable params: 3869505
2024-08-31 16:38:01,938 [podnet.py] => Learning on 5-7
2024-08-31 16:38:01,968 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-08-31 16:38:03,705 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 1.13, Spatial_loss 0.48, Flat_loss 0.77, Train_acc 71.62, Test_acc 25.69
2024-08-31 16:38:05,492 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.57, Spatial_loss 0.41, Flat_loss 0.57, Train_acc 86.47, Test_acc 17.43
2024-08-31 16:38:06,946 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.45, Spatial_loss 0.42, Flat_loss 0.51, Train_acc 89.13, Test_acc 42.62
2024-08-31 16:38:08,398 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.36, Spatial_loss 0.39, Flat_loss 0.45, Train_acc 90.98, Test_acc 30.93
2024-08-31 16:38:09,846 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.27, Spatial_loss 0.36, Flat_loss 0.42, Train_acc 93.29, Test_acc 51.74
2024-08-31 16:38:11,181 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.21, Spatial_loss 0.32, Flat_loss 0.37, Train_acc 95.36, Test_acc 48.79
2024-08-31 16:38:12,812 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.18, Spatial_loss 0.32, Flat_loss 0.36, Train_acc 95.58, Test_acc 55.26
2024-08-31 16:38:14,124 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.17, Spatial_loss 0.30, Flat_loss 0.34, Train_acc 95.91, Test_acc 47.81
2024-08-31 16:38:15,413 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.15, Spatial_loss 0.31, Flat_loss 0.34, Train_acc 96.53, Test_acc 58.05
2024-08-31 16:38:16,865 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.11, Spatial_loss 0.28, Flat_loss 0.31, Train_acc 98.04, Test_acc 63.48
2024-08-31 16:38:18,513 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.09, Spatial_loss 0.25, Flat_loss 0.28, Train_acc 98.76, Test_acc 59.14
2024-08-31 16:38:20,303 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.09, Spatial_loss 0.25, Flat_loss 0.28, Train_acc 98.78, Test_acc 62.36
2024-08-31 16:38:21,973 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.08, Spatial_loss 0.23, Flat_loss 0.26, Train_acc 99.47, Test_acc 58.79
2024-08-31 16:38:23,641 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.07, Spatial_loss 0.22, Flat_loss 0.25, Train_acc 99.42, Test_acc 66.74
2024-08-31 16:38:25,042 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.22, Flat_loss 0.25, Train_acc 99.64, Test_acc 62.95
2024-08-31 16:38:26,510 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.07, Spatial_loss 0.22, Flat_loss 0.25, Train_acc 99.64, Test_acc 65.43
2024-08-31 16:38:28,435 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.06, Spatial_loss 0.21, Flat_loss 0.23, Train_acc 99.69, Test_acc 66.52
2024-08-31 16:38:30,234 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.06, Spatial_loss 0.20, Flat_loss 0.22, Train_acc 99.80, Test_acc 66.64
2024-08-31 16:38:31,953 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.20, Flat_loss 0.22, Train_acc 99.67, Test_acc 61.93
2024-08-31 16:38:33,448 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.07, Spatial_loss 0.22, Flat_loss 0.24, Train_acc 99.31, Test_acc 57.05
2024-08-31 16:38:35,085 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.13, Spatial_loss 0.29, Flat_loss 0.30, Train_acc 96.80, Test_acc 43.74
2024-08-31 16:38:36,944 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.23, Spatial_loss 0.36, Flat_loss 0.38, Train_acc 93.71, Test_acc 41.64
2024-08-31 16:38:38,843 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.18, Spatial_loss 0.35, Flat_loss 0.37, Train_acc 95.33, Test_acc 56.21
2024-08-31 16:38:40,246 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.09, Spatial_loss 0.29, Flat_loss 0.30, Train_acc 98.49, Test_acc 58.48
2024-08-31 16:38:41,880 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.06, Spatial_loss 0.24, Flat_loss 0.26, Train_acc 99.49, Test_acc 65.43
2024-08-31 16:38:43,605 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.05, Spatial_loss 0.21, Flat_loss 0.23, Train_acc 99.89, Test_acc 64.50
2024-08-31 16:38:45,011 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.21, Train_acc 99.91, Test_acc 65.14
2024-08-31 16:38:46,358 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.21, Train_acc 99.84, Test_acc 62.71
2024-08-31 16:38:48,067 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.20, Train_acc 99.80, Test_acc 64.90
2024-08-31 16:38:49,466 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.19, Train_acc 99.91, Test_acc 70.38
2024-08-31 16:38:51,461 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.07, Spatial_loss 0.19, Flat_loss 0.20, Train_acc 99.11, Test_acc 67.83
2024-08-31 16:38:52,639 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.19, Train_acc 99.80, Test_acc 62.93
2024-08-31 16:38:54,660 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.19, Train_acc 99.91, Test_acc 66.21
2024-08-31 16:38:56,104 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.19, Train_acc 99.84, Test_acc 66.83
2024-08-31 16:38:57,910 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.11, Spatial_loss 0.26, Flat_loss 0.26, Train_acc 97.93, Test_acc 52.62
2024-08-31 16:38:59,281 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.13, Spatial_loss 0.29, Flat_loss 0.30, Train_acc 96.84, Test_acc 47.52
2024-08-31 16:39:00,711 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.16, Spatial_loss 0.33, Flat_loss 0.34, Train_acc 95.93, Test_acc 43.38
2024-08-31 16:39:02,597 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.10, Spatial_loss 0.28, Flat_loss 0.29, Train_acc 98.00, Test_acc 56.95
2024-08-31 16:39:04,040 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.07, Spatial_loss 0.25, Flat_loss 0.26, Train_acc 98.96, Test_acc 64.64
2024-08-31 16:39:05,722 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.23, Train_acc 99.73, Test_acc 68.88
2024-08-31 16:39:07,153 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.20, Train_acc 99.93, Test_acc 65.76
2024-08-31 16:39:08,709 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.19, Train_acc 99.87, Test_acc 63.83
2024-08-31 16:39:10,746 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.06, Spatial_loss 0.21, Flat_loss 0.22, Train_acc 99.58, Test_acc 69.55
2024-08-31 16:39:12,531 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.19, Train_acc 99.91, Test_acc 68.69
2024-08-31 16:39:14,316 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.18, Train_acc 99.93, Test_acc 68.64
2024-08-31 16:39:15,775 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.17, Train_acc 99.91, Test_acc 68.38
2024-08-31 16:39:17,613 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.96, Test_acc 72.31
2024-08-31 16:39:19,163 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.98, Test_acc 70.40
2024-08-31 16:39:20,565 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.87, Test_acc 62.36
2024-08-31 16:39:22,010 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.96, Test_acc 67.17
2024-08-31 16:39:23,441 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.16, Train_acc 99.93, Test_acc 65.67
2024-08-31 16:39:25,442 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.96, Test_acc 65.19
2024-08-31 16:39:26,905 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.98, Test_acc 67.90
2024-08-31 16:39:28,587 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.93, Test_acc 69.50
2024-08-31 16:39:30,124 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.17, Train_acc 99.87, Test_acc 66.83
2024-08-31 16:39:31,922 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.19, Train_acc 99.51, Test_acc 58.83
2024-08-31 16:39:33,463 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.18, Train_acc 99.89, Test_acc 70.57
2024-08-31 16:39:34,921 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.84, Test_acc 68.26
2024-08-31 16:39:36,328 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.82, Test_acc 66.98
2024-08-31 16:39:38,133 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.98, Test_acc 67.67
2024-08-31 16:39:39,516 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.93, Test_acc 65.19
2024-08-31 16:39:41,289 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.93, Test_acc 63.69
2024-08-31 16:39:42,979 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.13, Spatial_loss 0.25, Flat_loss 0.26, Train_acc 96.91, Test_acc 35.10
2024-08-31 16:39:44,460 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.20, Spatial_loss 0.34, Flat_loss 0.34, Train_acc 94.71, Test_acc 35.29
2024-08-31 16:39:45,987 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.25, Spatial_loss 0.38, Flat_loss 0.39, Train_acc 93.51, Test_acc 57.50
2024-08-31 16:39:47,882 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.15, Spatial_loss 0.34, Flat_loss 0.34, Train_acc 96.18, Test_acc 60.00
2024-08-31 16:39:49,402 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.07, Spatial_loss 0.27, Flat_loss 0.27, Train_acc 98.91, Test_acc 65.10
2024-08-31 16:39:51,104 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.22, Train_acc 99.78, Test_acc 64.14
2024-08-31 16:39:53,081 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.20, Train_acc 99.91, Test_acc 72.55
2024-08-31 16:39:54,511 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.04, Spatial_loss 0.18, Flat_loss 0.19, Train_acc 99.93, Test_acc 65.74
2024-08-31 16:39:56,032 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.18, Train_acc 99.89, Test_acc 69.45
2024-08-31 16:39:57,459 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.17, Train_acc 99.96, Test_acc 70.48
2024-08-31 16:39:58,987 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.98, Test_acc 70.52
2024-08-31 16:40:00,382 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.16, Train_acc 99.93, Test_acc 70.74
2024-08-31 16:40:02,169 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.93, Test_acc 69.95
2024-08-31 16:40:03,446 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.82, Test_acc 65.62
2024-08-31 16:40:04,789 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.96, Test_acc 67.62
2024-08-31 16:40:06,102 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.15, Train_acc 99.98, Test_acc 68.88
2024-08-31 16:40:07,334 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.93, Test_acc 63.81
2024-08-31 16:40:09,074 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.20, Train_acc 99.58, Test_acc 65.31
2024-08-31 16:40:10,336 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.96, Test_acc 66.38
2024-08-31 16:40:12,188 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 100.00, Test_acc 68.00
2024-08-31 16:40:13,713 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.98, Test_acc 69.26
2024-08-31 16:40:15,082 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.98, Test_acc 70.81
2024-08-31 16:40:16,550 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.14, Train_acc 100.00, Test_acc 68.29
2024-08-31 16:40:17,869 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.93, Test_acc 69.76
2024-08-31 16:40:19,775 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.14, Train_acc 99.91, Test_acc 67.05
2024-08-31 16:40:21,518 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.19, Train_acc 99.47, Test_acc 66.71
2024-08-31 16:40:23,141 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.78, Test_acc 69.55
2024-08-31 16:40:24,614 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.15, Train_acc 99.96, Test_acc 71.76
2024-08-31 16:40:25,906 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.93, Test_acc 69.21
2024-08-31 16:40:27,043 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.87, Test_acc 67.57
2024-08-31 16:40:28,602 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.96, Test_acc 68.48
2024-08-31 16:40:30,430 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 100.00, Test_acc 68.52
2024-08-31 16:40:32,051 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.15, Train_acc 99.91, Test_acc 69.90
2024-08-31 16:40:33,893 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 99.93, Test_acc 65.48
2024-08-31 16:40:35,355 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.14, Train_acc 99.98, Test_acc 64.36
2024-08-31 16:40:36,762 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 99.98, Test_acc 69.02
2024-08-31 16:40:38,216 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 100.00, Test_acc 67.52
2024-08-31 16:40:39,812 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.52
2024-08-31 16:40:41,770 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.31
2024-08-31 16:40:43,584 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 99.98, Test_acc 67.10
2024-08-31 16:40:45,327 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.88
2024-08-31 16:40:46,921 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.88
2024-08-31 16:40:48,380 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.40
2024-08-31 16:40:49,779 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.21
2024-08-31 16:40:51,374 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.98, Test_acc 66.81
2024-08-31 16:40:53,220 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 100.00, Test_acc 67.21
2024-08-31 16:40:55,010 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.31
2024-08-31 16:40:56,417 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 100.00, Test_acc 69.95
2024-08-31 16:40:58,224 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.98, Test_acc 53.48
2024-08-31 16:40:59,745 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.22, Spatial_loss 0.29, Flat_loss 0.31, Train_acc 94.22, Test_acc 48.69
2024-08-31 16:41:00,995 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.34, Spatial_loss 0.39, Flat_loss 0.42, Train_acc 91.40, Test_acc 37.50
2024-08-31 16:41:02,834 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.16, Spatial_loss 0.34, Flat_loss 0.35, Train_acc 95.82, Test_acc 51.86
2024-08-31 16:41:04,521 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.08, Spatial_loss 0.26, Flat_loss 0.27, Train_acc 98.69, Test_acc 67.62
2024-08-31 16:41:06,400 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.21, Flat_loss 0.22, Train_acc 99.73, Test_acc 72.62
2024-08-31 16:41:07,924 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.19, Train_acc 99.78, Test_acc 63.55
2024-08-31 16:41:09,590 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.18, Train_acc 99.96, Test_acc 70.33
2024-08-31 16:41:11,363 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.18, Train_acc 99.84, Test_acc 63.36
2024-08-31 16:41:13,168 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.98, Test_acc 71.07
2024-08-31 16:41:15,173 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.16, Train_acc 99.91, Test_acc 71.14
2024-08-31 16:41:16,946 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.15, Train_acc 99.91, Test_acc 66.17
2024-08-31 16:41:18,268 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.07, Spatial_loss 0.22, Flat_loss 0.21, Train_acc 98.93, Test_acc 62.64
2024-08-31 16:41:19,857 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.20, Train_acc 99.56, Test_acc 66.05
2024-08-31 16:41:21,628 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.91, Test_acc 68.24
2024-08-31 16:41:22,922 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.91, Test_acc 69.83
2024-08-31 16:41:24,581 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.91, Test_acc 63.38
2024-08-31 16:41:26,330 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.16, Train_acc 99.93, Test_acc 71.00
2024-08-31 16:41:27,830 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.14, Train_acc 99.93, Test_acc 69.71
2024-08-31 16:41:29,173 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.17, Train_acc 99.87, Test_acc 66.74
2024-08-31 16:41:30,631 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.15, Train_acc 99.96, Test_acc 69.67
2024-08-31 16:41:32,587 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.89, Test_acc 66.81
2024-08-31 16:41:34,114 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.07, Spatial_loss 0.21, Flat_loss 0.21, Train_acc 98.98, Test_acc 67.62
2024-08-31 16:41:35,693 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.17, Train_acc 99.84, Test_acc 65.33
2024-08-31 16:41:37,112 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.15, Train_acc 99.91, Test_acc 66.17
2024-08-31 16:41:38,508 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.16, Train_acc 99.89, Test_acc 68.86
2024-08-31 16:41:39,933 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.14, Train_acc 99.96, Test_acc 68.07
2024-08-31 16:41:41,507 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 99.98, Test_acc 68.95
2024-08-31 16:41:43,194 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 100.00, Test_acc 67.50
2024-08-31 16:41:45,241 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 100.00, Test_acc 67.48
2024-08-31 16:41:46,902 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.17
2024-08-31 16:41:48,218 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.74
2024-08-31 16:41:49,581 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 100.00, Test_acc 68.29
2024-08-31 16:41:51,044 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 99.91, Test_acc 62.90
2024-08-31 16:41:52,666 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.04, Spatial_loss 0.17, Flat_loss 0.17, Train_acc 99.80, Test_acc 64.19
2024-08-31 16:41:54,364 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.15, Train_acc 99.62, Test_acc 66.36
2024-08-31 16:41:55,818 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.14, Train_acc 99.93, Test_acc 68.93
2024-08-31 16:41:57,782 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.13, Train_acc 99.96, Test_acc 70.69
2024-08-31 16:41:59,204 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.96, Test_acc 69.83
2024-08-31 16:42:00,743 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 100.00, Test_acc 70.17
2024-08-31 16:42:02,246 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.96, Test_acc 68.76
2024-08-31 16:42:04,043 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.64
2024-08-31 16:42:05,427 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.13, Train_acc 100.00, Test_acc 69.69
2024-08-31 16:42:06,835 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.13, Train_acc 99.98, Test_acc 67.95
2024-08-31 16:42:08,199 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 99.96, Test_acc 67.69
2024-08-31 16:42:09,750 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.14
2024-08-31 16:42:11,205 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.64
2024-08-31 16:42:12,967 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.05
2024-08-31 16:42:14,419 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.43
2024-08-31 16:42:15,824 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 66.21
2024-08-31 16:42:17,229 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.00
2024-08-31 16:42:18,857 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.36
2024-08-31 16:42:20,285 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.83
2024-08-31 16:42:21,678 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.93
2024-08-31 16:42:23,366 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.31
2024-08-31 16:42:25,188 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 100.00, Test_acc 71.21
2024-08-31 16:42:27,068 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.83
2024-08-31 16:42:28,746 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.81
2024-08-31 16:42:30,338 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 99.98, Test_acc 70.12
2024-08-31 16:42:32,094 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 99.98, Test_acc 69.62
2024-08-31 16:42:33,616 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.86
2024-08-31 16:42:34,885 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.62
2024-08-31 16:42:36,301 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.13, Train_acc 99.93, Test_acc 69.29
2024-08-31 16:42:38,009 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.05
2024-08-31 16:42:39,390 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.83
2024-08-31 16:42:40,843 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.64
2024-08-31 16:42:42,320 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.12
2024-08-31 16:42:44,215 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.33
2024-08-31 16:42:45,385 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.12, Train_acc 99.98, Test_acc 70.21
2024-08-31 16:42:46,574 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.40
2024-08-31 16:42:47,977 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.21
2024-08-31 16:42:49,474 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.62
2024-08-31 16:42:50,824 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.38
2024-08-31 16:42:52,157 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 100.00, Test_acc 69.10
2024-08-31 16:42:53,828 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.96, Test_acc 70.12
2024-08-31 16:42:55,422 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.12
2024-08-31 16:42:56,771 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.96, Test_acc 71.36
2024-08-31 16:42:57,987 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.12, Train_acc 99.98, Test_acc 68.10
2024-08-31 16:42:59,151 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.05
2024-08-31 16:43:00,572 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.24
2024-08-31 16:43:02,289 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.07
2024-08-31 16:43:03,627 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.88
2024-08-31 16:43:04,901 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.21
2024-08-31 16:43:06,571 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 67.69
2024-08-31 16:43:08,197 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.02
2024-08-31 16:43:09,820 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 71.02
2024-08-31 16:43:11,270 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 99.96, Test_acc 70.69
2024-08-31 16:43:12,970 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.26
2024-08-31 16:43:14,353 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.67
2024-08-31 16:43:15,811 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.05
2024-08-31 16:43:17,747 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.03, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.81
2024-08-31 16:43:19,189 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.98
2024-08-31 16:43:20,532 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.00
2024-08-31 16:43:22,395 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.64
2024-08-31 16:43:23,766 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 99.96, Test_acc 70.12
2024-08-31 16:43:25,203 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.67
2024-08-31 16:43:26,995 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.05
2024-08-31 16:43:28,648 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.64
2024-08-31 16:43:30,496 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.50
2024-08-31 16:43:31,916 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 99.98, Test_acc 68.90
2024-08-31 16:43:33,558 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.12
2024-08-31 16:43:35,090 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.05
2024-08-31 16:43:36,454 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.17
2024-08-31 16:43:37,887 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.67
2024-08-31 16:43:39,694 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.36
2024-08-31 16:43:41,423 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.71
2024-08-31 16:43:43,140 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.74
2024-08-31 16:43:44,556 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.19
2024-08-31 16:43:46,224 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.98
2024-08-31 16:43:47,950 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.74
2024-08-31 16:43:49,755 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.40
2024-08-31 16:43:51,105 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.03, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 69.36
2024-08-31 16:43:52,743 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 99.98, Test_acc 70.10
2024-08-31 16:43:54,150 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.55
2024-08-31 16:43:55,561 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.40
2024-08-31 16:43:57,578 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.12
2024-08-31 16:43:59,098 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-08-31 16:44:00,798 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.14
2024-08-31 16:44:02,206 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.57
2024-08-31 16:44:03,951 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.00
2024-08-31 16:44:05,488 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.38
2024-08-31 16:44:07,298 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.76
2024-08-31 16:44:08,941 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.48
2024-08-31 16:44:10,587 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.74
2024-08-31 16:44:11,895 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.98
2024-08-31 16:44:13,408 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.40
2024-08-31 16:44:14,967 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.17
2024-08-31 16:44:16,419 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.83
2024-08-31 16:44:17,798 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.81
2024-08-31 16:44:19,424 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.21
2024-08-31 16:44:21,023 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.17
2024-08-31 16:44:22,492 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 99.96, Test_acc 73.38
2024-08-31 16:44:24,241 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.11, Train_acc 99.84, Test_acc 70.33
2024-08-31 16:44:25,764 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.38
2024-08-31 16:44:27,203 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.21
2024-08-31 16:44:28,601 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.83
2024-08-31 16:44:30,351 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.88
2024-08-31 16:44:31,592 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.90
2024-08-31 16:44:32,943 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-31 16:44:34,397 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.14
2024-08-31 16:44:35,920 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.14
2024-08-31 16:44:38,022 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.86
2024-08-31 16:44:40,035 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.29
2024-08-31 16:44:41,575 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.67
2024-08-31 16:44:43,293 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.86
2024-08-31 16:44:44,760 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.98
2024-08-31 16:44:46,264 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-31 16:44:47,716 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.81
2024-08-31 16:44:49,430 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.60
2024-08-31 16:44:51,031 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.14
2024-08-31 16:44:53,004 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.02
2024-08-31 16:44:54,490 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.33
2024-08-31 16:44:55,946 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.03, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.29
2024-08-31 16:44:57,477 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.17
2024-08-31 16:44:58,645 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.10
2024-08-31 16:45:00,574 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.74
2024-08-31 16:45:01,971 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.10
2024-08-31 16:45:03,454 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.52
2024-08-31 16:45:04,716 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.12
2024-08-31 16:45:06,128 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.52
2024-08-31 16:45:07,820 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.33
2024-08-31 16:45:09,406 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.90
2024-08-31 16:45:10,862 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.74
2024-08-31 16:45:12,355 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.38
2024-08-31 16:45:13,851 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.79
2024-08-31 16:45:15,295 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.50
2024-08-31 16:45:16,842 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.29
2024-08-31 16:45:18,490 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.43
2024-08-31 16:45:20,043 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.26
2024-08-31 16:45:21,432 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.31
2024-08-31 16:45:23,009 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.76
2024-08-31 16:45:24,511 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.45
2024-08-31 16:45:25,874 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.98
2024-08-31 16:45:27,411 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.43
2024-08-31 16:45:28,821 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.50
2024-08-31 16:45:30,499 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.17
2024-08-31 16:45:32,319 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.55
2024-08-31 16:45:33,758 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.57
2024-08-31 16:45:35,732 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.67
2024-08-31 16:45:37,314 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.60
2024-08-31 16:45:38,989 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.83
2024-08-31 16:45:40,902 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.31
2024-08-31 16:45:42,656 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.69
2024-08-31 16:45:44,601 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.50
2024-08-31 16:45:46,335 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.88
2024-08-31 16:45:47,724 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.60
2024-08-31 16:45:49,474 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.69
2024-08-31 16:45:51,244 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.69
2024-08-31 16:45:52,737 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.69
2024-08-31 16:45:54,408 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.06, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.93
2024-08-31 16:45:54,699 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-31 16:45:54,700 [base.py] => Reducing exemplars...(100 per classes)
2024-08-31 16:45:55,391 [base.py] => Constructing exemplars...(100 per classes)
2024-08-31 16:45:57,034 [base.py] => Reducing exemplars...(71 per classes)
2024-08-31 16:45:57,820 [base.py] => Constructing exemplars...(71 per classes)
2024-08-31 16:45:59,913 [podnet.py] => Exemplar size: 497
2024-08-31 16:45:59,913 [trainer.py] => CNN: {'total': 71.93, '00-04': 62.27, '05-06': 96.08, 'old': 62.27, 'new': 96.08}
2024-08-31 16:45:59,913 [trainer.py] => NME: {'total': 77.24, '00-04': 78.03, '05-06': 75.25, 'old': 78.03, 'new': 75.25}
2024-08-31 16:45:59,913 [trainer.py] => CNN top1 curve: [88.9, 71.93]
2024-08-31 16:45:59,913 [trainer.py] => CNN top5 curve: [100.0, 98.17]
2024-08-31 16:45:59,913 [trainer.py] => NME top1 curve: [88.9, 77.24]
2024-08-31 16:45:59,913 [trainer.py] => NME top5 curve: [100.0, 98.31]

2024-08-31 16:45:59,913 [trainer.py] => Average Accuracy (CNN): 80.415
2024-08-31 16:45:59,913 [trainer.py] => Average Accuracy (NME): 83.07
2024-08-31 16:45:59,914 [trainer.py] => All params: 3879745
2024-08-31 16:45:59,914 [trainer.py] => Trainable params: 3879745
2024-08-31 16:45:59,915 [podnet.py] => Learning on 7-9
2024-08-31 16:45:59,956 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-08-31 16:46:01,670 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 1.59, Spatial_loss 0.60, Flat_loss 1.29, Train_acc 75.38, Test_acc 20.80
2024-08-31 16:46:03,118 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.81, Spatial_loss 0.47, Flat_loss 1.09, Train_acc 87.41, Test_acc 18.89
2024-08-31 16:46:04,715 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.68, Spatial_loss 0.42, Flat_loss 0.93, Train_acc 87.99, Test_acc 25.48
2024-08-31 16:46:06,322 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.66, Spatial_loss 0.45, Flat_loss 0.80, Train_acc 87.37, Test_acc 21.54
2024-08-31 16:46:07,939 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.58, Spatial_loss 0.46, Flat_loss 0.73, Train_acc 87.53, Test_acc 23.43
2024-08-31 16:46:09,501 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.48, Spatial_loss 0.42, Flat_loss 0.65, Train_acc 89.95, Test_acc 27.22
2024-08-31 16:46:11,150 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.45, Spatial_loss 0.41, Flat_loss 0.62, Train_acc 90.79, Test_acc 32.85
2024-08-31 16:46:12,922 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.39, Spatial_loss 0.40, Flat_loss 0.58, Train_acc 91.53, Test_acc 35.93
2024-08-31 16:46:14,789 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.32, Spatial_loss 0.38, Flat_loss 0.54, Train_acc 92.97, Test_acc 35.37
2024-08-31 16:46:16,076 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.30, Spatial_loss 0.38, Flat_loss 0.52, Train_acc 93.37, Test_acc 42.35
2024-08-31 16:46:18,040 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.25, Spatial_loss 0.36, Flat_loss 0.49, Train_acc 94.89, Test_acc 46.57
2024-08-31 16:46:19,496 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.24, Spatial_loss 0.37, Flat_loss 0.48, Train_acc 94.95, Test_acc 40.78
2024-08-31 16:46:21,467 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.20, Spatial_loss 0.34, Flat_loss 0.46, Train_acc 96.04, Test_acc 50.09
2024-08-31 16:46:23,000 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.26, Spatial_loss 0.39, Flat_loss 0.48, Train_acc 94.04, Test_acc 42.04
2024-08-31 16:46:24,327 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.28, Spatial_loss 0.39, Flat_loss 0.48, Train_acc 93.95, Test_acc 38.57
2024-08-31 16:46:26,118 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.17, Spatial_loss 0.36, Flat_loss 0.44, Train_acc 96.78, Test_acc 25.41
2024-08-31 16:46:27,903 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.20, Spatial_loss 0.37, Flat_loss 0.45, Train_acc 95.86, Test_acc 46.41
2024-08-31 16:46:29,351 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.15, Spatial_loss 0.34, Flat_loss 0.42, Train_acc 97.64, Test_acc 39.70
2024-08-31 16:46:31,141 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.39, Spatial_loss 0.45, Flat_loss 0.51, Train_acc 91.37, Test_acc 46.78
2024-08-31 16:46:32,763 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.19, Spatial_loss 0.37, Flat_loss 0.44, Train_acc 95.98, Test_acc 49.54
2024-08-31 16:46:34,243 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.12, Spatial_loss 0.33, Flat_loss 0.41, Train_acc 98.31, Test_acc 53.17
2024-08-31 16:46:35,689 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.12, Spatial_loss 0.32, Flat_loss 0.40, Train_acc 97.89, Test_acc 49.22
2024-08-31 16:46:37,680 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.09, Spatial_loss 0.30, Flat_loss 0.38, Train_acc 99.00, Test_acc 58.78
2024-08-31 16:46:39,337 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.07, Spatial_loss 0.28, Flat_loss 0.36, Train_acc 99.64, Test_acc 59.44
2024-08-31 16:46:41,200 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.07, Spatial_loss 0.27, Flat_loss 0.35, Train_acc 99.84, Test_acc 55.54
2024-08-31 16:46:42,800 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.07, Spatial_loss 0.27, Flat_loss 0.36, Train_acc 99.62, Test_acc 57.44
2024-08-31 16:46:44,267 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.06, Spatial_loss 0.26, Flat_loss 0.34, Train_acc 99.82, Test_acc 58.94
2024-08-31 16:46:45,923 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.06, Spatial_loss 0.25, Flat_loss 0.33, Train_acc 99.82, Test_acc 58.70
2024-08-31 16:46:47,764 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.06, Spatial_loss 0.24, Flat_loss 0.32, Train_acc 99.91, Test_acc 58.24
2024-08-31 16:46:49,212 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.06, Spatial_loss 0.24, Flat_loss 0.32, Train_acc 99.87, Test_acc 59.94
2024-08-31 16:46:51,143 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.05, Spatial_loss 0.23, Flat_loss 0.31, Train_acc 99.96, Test_acc 59.41
2024-08-31 16:46:52,777 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.06, Spatial_loss 0.22, Flat_loss 0.31, Train_acc 99.89, Test_acc 61.46
2024-08-31 16:46:54,082 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.06, Spatial_loss 0.22, Flat_loss 0.31, Train_acc 99.82, Test_acc 58.57
2024-08-31 16:46:55,613 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.07, Spatial_loss 0.26, Flat_loss 0.33, Train_acc 99.24, Test_acc 55.80
2024-08-31 16:46:57,529 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.24, Flat_loss 0.32, Train_acc 99.80, Test_acc 60.13
2024-08-31 16:46:59,192 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.05, Spatial_loss 0.21, Flat_loss 0.30, Train_acc 99.93, Test_acc 60.28
2024-08-31 16:47:01,319 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.06, Spatial_loss 0.22, Flat_loss 0.30, Train_acc 99.78, Test_acc 58.50
2024-08-31 16:47:03,236 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.21, Flat_loss 0.29, Train_acc 99.93, Test_acc 59.76
2024-08-31 16:47:04,875 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.28, Train_acc 99.98, Test_acc 61.65
2024-08-31 16:47:06,785 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.28, Train_acc 99.96, Test_acc 62.33
2024-08-31 16:47:08,401 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.27, Train_acc 99.98, Test_acc 60.72
2024-08-31 16:47:09,941 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.27, Train_acc 99.96, Test_acc 61.43
2024-08-31 16:47:11,728 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.27, Train_acc 99.96, Test_acc 60.54
2024-08-31 16:47:13,270 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.27, Train_acc 99.98, Test_acc 53.04
2024-08-31 16:47:14,874 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.16, Spatial_loss 0.29, Flat_loss 0.35, Train_acc 96.69, Test_acc 21.83
2024-08-31 16:47:16,148 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.37, Spatial_loss 0.41, Flat_loss 0.46, Train_acc 91.97, Test_acc 30.65
2024-08-31 16:47:18,079 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.46, Spatial_loss 0.45, Flat_loss 0.52, Train_acc 90.44, Test_acc 29.41
2024-08-31 16:47:19,965 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.19, Spatial_loss 0.37, Flat_loss 0.42, Train_acc 95.86, Test_acc 49.28
2024-08-31 16:47:21,752 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.11, Spatial_loss 0.32, Flat_loss 0.37, Train_acc 97.87, Test_acc 55.13
2024-08-31 16:47:23,698 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.07, Spatial_loss 0.29, Flat_loss 0.34, Train_acc 99.27, Test_acc 59.24
2024-08-31 16:47:25,327 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.06, Spatial_loss 0.25, Flat_loss 0.31, Train_acc 99.80, Test_acc 62.61
2024-08-31 16:47:27,084 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.05, Spatial_loss 0.23, Flat_loss 0.30, Train_acc 99.93, Test_acc 62.78
2024-08-31 16:47:28,513 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.28, Train_acc 100.00, Test_acc 63.28
2024-08-31 16:47:30,158 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.05, Spatial_loss 0.21, Flat_loss 0.28, Train_acc 99.98, Test_acc 62.39
2024-08-31 16:47:31,879 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.27, Train_acc 99.93, Test_acc 62.04
2024-08-31 16:47:33,731 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.27, Train_acc 99.93, Test_acc 62.07
2024-08-31 16:47:35,238 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.26, Train_acc 99.93, Test_acc 61.30
2024-08-31 16:47:36,651 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.15, Spatial_loss 0.28, Flat_loss 0.34, Train_acc 96.35, Test_acc 55.57
2024-08-31 16:47:38,180 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.07, Spatial_loss 0.26, Flat_loss 0.32, Train_acc 99.18, Test_acc 61.59
2024-08-31 16:47:39,843 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 0.25, Flat_loss 0.31, Train_acc 99.13, Test_acc 61.72
2024-08-31 16:47:41,749 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.28, Train_acc 99.87, Test_acc 61.61
2024-08-31 16:47:43,375 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.27, Train_acc 99.91, Test_acc 60.33
2024-08-31 16:47:45,361 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.26, Train_acc 100.00, Test_acc 63.61
2024-08-31 16:47:46,623 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.26, Train_acc 100.00, Test_acc 64.87
2024-08-31 16:47:48,110 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.25, Train_acc 99.96, Test_acc 64.35
2024-08-31 16:47:50,169 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.25, Train_acc 99.91, Test_acc 64.41
2024-08-31 16:47:51,614 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.25, Train_acc 99.98, Test_acc 62.76
2024-08-31 16:47:53,628 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.24, Train_acc 99.96, Test_acc 62.43
2024-08-31 16:47:55,089 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 100.00, Test_acc 61.94
2024-08-31 16:47:56,995 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 100.00, Test_acc 63.39
2024-08-31 16:47:58,451 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 100.00, Test_acc 64.02
2024-08-31 16:48:00,209 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 100.00, Test_acc 63.87
2024-08-31 16:48:02,129 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 99.98, Test_acc 62.89
2024-08-31 16:48:03,767 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 99.96, Test_acc 60.80
2024-08-31 16:48:05,555 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 99.98, Test_acc 62.20
2024-08-31 16:48:07,162 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 99.98, Test_acc 63.69
2024-08-31 16:48:08,912 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 100.00, Test_acc 61.81
2024-08-31 16:48:10,485 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 100.00, Test_acc 64.00
2024-08-31 16:48:12,020 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 100.00, Test_acc 64.13
2024-08-31 16:48:13,770 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 99.98, Test_acc 62.78
2024-08-31 16:48:15,668 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 99.98, Test_acc 62.78
2024-08-31 16:48:17,629 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 99.96, Test_acc 63.74
2024-08-31 16:48:18,861 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.04, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 99.98, Test_acc 61.59
2024-08-31 16:48:20,512 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 100.00, Test_acc 63.67
2024-08-31 16:48:22,292 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 99.96, Test_acc 63.70
2024-08-31 16:48:24,053 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.04, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 100.00, Test_acc 59.48
2024-08-31 16:48:25,954 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 99.93, Test_acc 62.24
2024-08-31 16:48:27,715 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.24, Train_acc 99.84, Test_acc 60.96
2024-08-31 16:48:29,242 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.33, Spatial_loss 0.35, Flat_loss 0.40, Train_acc 93.48, Test_acc 18.39
2024-08-31 16:48:31,128 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.38, Spatial_loss 0.43, Flat_loss 0.46, Train_acc 91.53, Test_acc 36.96
2024-08-31 16:48:32,612 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.20, Spatial_loss 0.37, Flat_loss 0.41, Train_acc 95.35, Test_acc 47.57
2024-08-31 16:48:34,164 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.15, Spatial_loss 0.33, Flat_loss 0.37, Train_acc 97.20, Test_acc 55.41
2024-08-31 16:48:35,628 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.11, Spatial_loss 0.33, Flat_loss 0.35, Train_acc 98.07, Test_acc 44.93
2024-08-31 16:48:37,212 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.11, Spatial_loss 0.29, Flat_loss 0.33, Train_acc 98.18, Test_acc 53.30
2024-08-31 16:48:38,884 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.10, Spatial_loss 0.30, Flat_loss 0.33, Train_acc 98.47, Test_acc 58.02
2024-08-31 16:48:40,605 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.24, Flat_loss 0.29, Train_acc 99.67, Test_acc 63.00
2024-08-31 16:48:42,569 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.05, Spatial_loss 0.21, Flat_loss 0.27, Train_acc 99.98, Test_acc 62.13
2024-08-31 16:48:44,317 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.26, Train_acc 99.98, Test_acc 62.43
2024-08-31 16:48:46,230 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.26, Train_acc 99.91, Test_acc 62.74
2024-08-31 16:48:47,815 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.24, Train_acc 100.00, Test_acc 63.54
2024-08-31 16:48:49,709 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.24, Train_acc 99.96, Test_acc 62.19
2024-08-31 16:48:51,440 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.23, Train_acc 99.98, Test_acc 63.00
2024-08-31 16:48:52,893 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.23, Train_acc 100.00, Test_acc 63.33
2024-08-31 16:48:54,879 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.23, Train_acc 100.00, Test_acc 64.56
2024-08-31 16:48:56,522 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.23, Train_acc 100.00, Test_acc 64.89
2024-08-31 16:48:58,189 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 100.00, Test_acc 64.63
2024-08-31 16:48:59,936 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 99.98, Test_acc 64.02
2024-08-31 16:49:01,595 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 100.00, Test_acc 62.50
2024-08-31 16:49:03,419 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.23, Train_acc 100.00, Test_acc 62.31
2024-08-31 16:49:04,876 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 100.00, Test_acc 63.07
2024-08-31 16:49:06,090 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.22, Train_acc 100.00, Test_acc 63.59
2024-08-31 16:49:07,892 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 99.96, Test_acc 62.98
2024-08-31 16:49:10,127 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 62.31
2024-08-31 16:49:12,421 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 62.43
2024-08-31 16:49:14,661 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 99.98, Test_acc 63.76
2024-08-31 16:49:16,891 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 99.98, Test_acc 64.65
2024-08-31 16:49:18,884 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 99.98, Test_acc 64.80
2024-08-31 16:49:20,968 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 100.00, Test_acc 63.28
2024-08-31 16:49:23,265 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 100.00, Test_acc 64.78
2024-08-31 16:49:25,396 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.21, Train_acc 100.00, Test_acc 63.09
2024-08-31 16:49:27,272 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 99.98, Test_acc 65.04
2024-08-31 16:49:29,508 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.08, Spatial_loss 0.20, Flat_loss 0.26, Train_acc 99.13, Test_acc 56.30
2024-08-31 16:49:31,773 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.15, Spatial_loss 0.29, Flat_loss 0.33, Train_acc 96.84, Test_acc 41.83
2024-08-31 16:49:33,858 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.22, Spatial_loss 0.33, Flat_loss 0.36, Train_acc 95.69, Test_acc 38.41
2024-08-31 16:49:36,153 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.12, Spatial_loss 0.32, Flat_loss 0.36, Train_acc 97.60, Test_acc 50.61
2024-08-31 16:49:38,475 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.10, Spatial_loss 0.28, Flat_loss 0.32, Train_acc 98.22, Test_acc 58.31
2024-08-31 16:49:40,674 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.06, Spatial_loss 0.23, Flat_loss 0.28, Train_acc 99.67, Test_acc 63.31
2024-08-31 16:49:42,941 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.19, Flat_loss 0.25, Train_acc 99.96, Test_acc 63.76
2024-08-31 16:49:45,111 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.06, Spatial_loss 0.18, Flat_loss 0.24, Train_acc 99.93, Test_acc 53.30
2024-08-31 16:49:47,443 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.19, Spatial_loss 0.33, Flat_loss 0.35, Train_acc 96.04, Test_acc 40.56
2024-08-31 16:49:49,695 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.11, Spatial_loss 0.29, Flat_loss 0.33, Train_acc 98.18, Test_acc 60.43
2024-08-31 16:49:51,850 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.22, Flat_loss 0.27, Train_acc 99.80, Test_acc 59.48
2024-08-31 16:49:54,066 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.20, Flat_loss 0.25, Train_acc 100.00, Test_acc 62.96
2024-08-31 16:49:56,354 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.18, Flat_loss 0.24, Train_acc 100.00, Test_acc 61.65
2024-08-31 16:49:58,646 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.24, Train_acc 99.96, Test_acc 62.96
2024-08-31 16:50:00,662 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.17, Flat_loss 0.22, Train_acc 99.98, Test_acc 64.54
2024-08-31 16:50:02,891 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.22, Train_acc 100.00, Test_acc 63.65
2024-08-31 16:50:04,848 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.22, Train_acc 99.98, Test_acc 64.28
2024-08-31 16:50:07,099 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.21, Train_acc 100.00, Test_acc 64.15
2024-08-31 16:50:09,411 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.21, Train_acc 100.00, Test_acc 64.00
2024-08-31 16:50:11,629 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.15, Flat_loss 0.21, Train_acc 100.00, Test_acc 64.80
2024-08-31 16:50:13,818 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 63.11
2024-08-31 16:50:16,049 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 65.76
2024-08-31 16:50:18,289 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 64.78
2024-08-31 16:50:20,488 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.20, Train_acc 100.00, Test_acc 65.65
2024-08-31 16:50:22,492 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 63.94
2024-08-31 16:50:24,520 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 65.37
2024-08-31 16:50:26,559 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 64.96
2024-08-31 16:50:28,730 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 65.69
2024-08-31 16:50:30,743 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 99.98, Test_acc 63.98
2024-08-31 16:50:32,804 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 64.57
2024-08-31 16:50:34,885 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 64.85
2024-08-31 16:50:36,945 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 63.80
2024-08-31 16:50:39,210 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 65.07
2024-08-31 16:50:41,353 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 65.22
2024-08-31 16:50:43,575 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.07
2024-08-31 16:50:45,911 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.65
2024-08-31 16:50:48,054 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 99.98, Test_acc 65.04
2024-08-31 16:50:50,324 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.70
2024-08-31 16:50:52,382 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.67
2024-08-31 16:50:54,470 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.98, Test_acc 61.83
2024-08-31 16:50:56,676 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.18, Flat_loss 0.24, Train_acc 99.78, Test_acc 62.28
2024-08-31 16:50:58,796 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.05, Spatial_loss 0.16, Flat_loss 0.22, Train_acc 99.91, Test_acc 63.91
2024-08-31 16:51:00,914 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 63.20
2024-08-31 16:51:03,229 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 63.85
2024-08-31 16:51:05,433 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 99.98, Test_acc 65.13
2024-08-31 16:51:07,605 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 99.98, Test_acc 66.28
2024-08-31 16:51:09,759 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.06, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 99.67, Test_acc 63.19
2024-08-31 16:51:11,841 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.04, Spatial_loss 0.13, Flat_loss 0.20, Train_acc 100.00, Test_acc 64.15
2024-08-31 16:51:14,072 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.69
2024-08-31 16:51:15,895 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 65.50
2024-08-31 16:51:18,124 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.96, Test_acc 64.44
2024-08-31 16:51:20,278 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 100.00, Test_acc 65.33
2024-08-31 16:51:22,547 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.83
2024-08-31 16:51:24,753 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 65.04
2024-08-31 16:51:27,071 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.61
2024-08-31 16:51:29,355 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.07
2024-08-31 16:51:31,461 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 65.24
2024-08-31 16:51:33,524 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 65.09
2024-08-31 16:51:35,385 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.98, Test_acc 64.80
2024-08-31 16:51:37,489 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.65
2024-08-31 16:51:39,638 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.74
2024-08-31 16:51:41,888 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.30
2024-08-31 16:51:44,258 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.52
2024-08-31 16:51:46,595 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.98, Test_acc 63.37
2024-08-31 16:51:48,870 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.52
2024-08-31 16:51:50,988 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.78
2024-08-31 16:51:53,091 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.02
2024-08-31 16:51:55,460 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.96, Test_acc 64.17
2024-08-31 16:51:57,714 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.07, Spatial_loss 0.18, Flat_loss 0.25, Train_acc 99.29, Test_acc 62.20
2024-08-31 16:51:59,831 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.14, Flat_loss 0.21, Train_acc 100.00, Test_acc 64.04
2024-08-31 16:52:02,177 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.20, Train_acc 100.00, Test_acc 63.76
2024-08-31 16:52:04,507 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 99.98, Test_acc 65.31
2024-08-31 16:52:06,751 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.04, Spatial_loss 0.12, Flat_loss 0.19, Train_acc 100.00, Test_acc 65.17
2024-08-31 16:52:08,921 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.98, Test_acc 64.59
2024-08-31 16:52:11,113 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.41
2024-08-31 16:52:12,416 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 63.19
2024-08-31 16:52:14,559 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 66.09
2024-08-31 16:52:16,467 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.76
2024-08-31 16:52:18,751 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.56
2024-08-31 16:52:20,871 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.43
2024-08-31 16:52:23,125 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 99.98, Test_acc 64.13
2024-08-31 16:52:25,133 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 99.98, Test_acc 66.31
2024-08-31 16:52:27,231 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.19, Train_acc 100.00, Test_acc 64.94
2024-08-31 16:52:29,463 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.17
2024-08-31 16:52:31,619 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.11, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.54
2024-08-31 16:52:33,935 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.98
2024-08-31 16:52:36,216 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.13
2024-08-31 16:52:38,375 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.69
2024-08-31 16:52:40,575 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.15
2024-08-31 16:52:42,880 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 99.98, Test_acc 64.81
2024-08-31 16:52:45,209 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 64.33
2024-08-31 16:52:47,516 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.57
2024-08-31 16:52:49,776 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.39
2024-08-31 16:52:51,995 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.20
2024-08-31 16:52:54,313 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.37
2024-08-31 16:52:56,439 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.13
2024-08-31 16:52:58,611 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 99.98, Test_acc 65.89
2024-08-31 16:53:00,629 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.85
2024-08-31 16:53:02,706 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.37
2024-08-31 16:53:04,953 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 100.00, Test_acc 65.67
2024-08-31 16:53:07,092 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.94
2024-08-31 16:53:08,281 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.70
2024-08-31 16:53:10,518 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.19
2024-08-31 16:53:12,571 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.67
2024-08-31 16:53:14,831 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.46
2024-08-31 16:53:17,092 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.50
2024-08-31 16:53:19,427 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 64.78
2024-08-31 16:53:21,684 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.00
2024-08-31 16:53:23,934 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 64.67
2024-08-31 16:53:26,065 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.26
2024-08-31 16:53:27,333 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 64.41
2024-08-31 16:53:29,354 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.07
2024-08-31 16:53:31,768 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.94
2024-08-31 16:53:33,938 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.70
2024-08-31 16:53:35,862 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.70
2024-08-31 16:53:38,046 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.44
2024-08-31 16:53:40,114 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.56
2024-08-31 16:53:42,226 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.00
2024-08-31 16:53:44,278 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.93
2024-08-31 16:53:46,399 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.59
2024-08-31 16:53:48,263 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.93
2024-08-31 16:53:50,429 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.02
2024-08-31 16:53:52,781 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 64.76
2024-08-31 16:53:55,112 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.04
2024-08-31 16:53:56,952 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.24
2024-08-31 16:53:59,155 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.72
2024-08-31 16:54:01,322 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.13
2024-08-31 16:54:03,577 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.19
2024-08-31 16:54:05,753 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.19
2024-08-31 16:54:07,696 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 99.98, Test_acc 65.22
2024-08-31 16:54:09,793 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.18, Train_acc 99.80, Test_acc 65.20
2024-08-31 16:54:11,056 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.18, Train_acc 100.00, Test_acc 66.06
2024-08-31 16:54:13,245 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.15
2024-08-31 16:54:14,456 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.96
2024-08-31 16:54:16,648 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.91
2024-08-31 16:54:18,865 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.33
2024-08-31 16:54:21,111 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.28
2024-08-31 16:54:23,288 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.15
2024-08-31 16:54:25,365 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.00
2024-08-31 16:54:27,676 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.52
2024-08-31 16:54:29,556 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.19
2024-08-31 16:54:31,238 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.02
2024-08-31 16:54:33,383 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.35
2024-08-31 16:54:34,642 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.98
2024-08-31 16:54:36,640 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.31
2024-08-31 16:54:38,754 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.04
2024-08-31 16:54:40,836 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.91
2024-08-31 16:54:43,001 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.43
2024-08-31 16:54:45,263 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.04
2024-08-31 16:54:47,608 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 99.98, Test_acc 66.00
2024-08-31 16:54:49,893 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.94
2024-08-31 16:54:51,992 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.69
2024-08-31 16:54:54,119 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.93
2024-08-31 16:54:56,306 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.57
2024-08-31 16:54:58,179 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.93
2024-08-31 16:55:00,415 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.87
2024-08-31 16:55:02,372 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.61
2024-08-31 16:55:04,499 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.50
2024-08-31 16:55:06,651 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.74
2024-08-31 16:55:08,713 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.17
2024-08-31 16:55:10,888 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.43
2024-08-31 16:55:13,129 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.22
2024-08-31 16:55:15,376 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.46
2024-08-31 16:55:17,568 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.00
2024-08-31 16:55:19,823 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.65
2024-08-31 16:55:21,815 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.30
2024-08-31 16:55:23,993 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.67
2024-08-31 16:55:26,283 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.80
2024-08-31 16:55:28,532 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.31
2024-08-31 16:55:30,562 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.59
2024-08-31 16:55:32,783 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.93
2024-08-31 16:55:34,817 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.31
2024-08-31 16:55:37,151 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.15
2024-08-31 16:55:39,292 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.26
2024-08-31 16:55:41,360 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.00
2024-08-31 16:55:43,519 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.74
2024-08-31 16:55:45,880 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 66.67
2024-08-31 16:55:47,907 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.85
2024-08-31 16:55:50,094 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.17, Train_acc 100.00, Test_acc 65.81
2024-08-31 16:55:50,452 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-08-31 16:55:50,452 [base.py] => Reducing exemplars...(71 per classes)
2024-08-31 16:55:51,382 [base.py] => Constructing exemplars...(71 per classes)
2024-08-31 16:55:52,781 [base.py] => Reducing exemplars...(55 per classes)
2024-08-31 16:55:53,719 [base.py] => Constructing exemplars...(55 per classes)
2024-08-31 16:55:55,669 [podnet.py] => Exemplar size: 495
2024-08-31 16:55:55,669 [trainer.py] => CNN: {'total': 65.81, '00-04': 57.6, '05-06': 53.75, '07-08': 98.42, 'old': 56.5, 'new': 98.42}
2024-08-31 16:55:55,669 [trainer.py] => NME: {'total': 68.8, '00-04': 71.43, '05-06': 46.83, '07-08': 84.17, 'old': 64.4, 'new': 84.17}
2024-08-31 16:55:55,669 [trainer.py] => CNN top1 curve: [88.9, 71.93, 65.81]
2024-08-31 16:55:55,669 [trainer.py] => CNN top5 curve: [100.0, 98.17, 92.24]
2024-08-31 16:55:55,669 [trainer.py] => NME top1 curve: [88.9, 77.24, 68.8]
2024-08-31 16:55:55,669 [trainer.py] => NME top5 curve: [100.0, 98.31, 94.2]

2024-08-31 16:55:55,669 [trainer.py] => Average Accuracy (CNN): 75.54666666666667
2024-08-31 16:55:55,669 [trainer.py] => Average Accuracy (NME): 78.31333333333333
2024-08-31 16:55:55,670 [trainer.py] => Forgetting (CNN): 36.815
