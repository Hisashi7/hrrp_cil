2024-09-25 17:14:51,338 [trainer.py] => config: ./exps/podnet.json
2024-09-25 17:14:51,338 [trainer.py] => prefix: cil
2024-09-25 17:14:51,338 [trainer.py] => dataset: hrrp9
2024-09-25 17:14:51,338 [trainer.py] => memory_size: 500
2024-09-25 17:14:51,339 [trainer.py] => memory_per_class: 20
2024-09-25 17:14:51,339 [trainer.py] => fixed_memory: False
2024-09-25 17:14:51,339 [trainer.py] => shuffle: True
2024-09-25 17:14:51,339 [trainer.py] => init_cls: 5
2024-09-25 17:14:51,339 [trainer.py] => increment: 2
2024-09-25 17:14:51,339 [trainer.py] => model_name: podnet
2024-09-25 17:14:51,339 [trainer.py] => convnet_type: resnet18
2024-09-25 17:14:51,339 [trainer.py] => init_train: True
2024-09-25 17:14:51,339 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-09-25 17:14:51,339 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-09-25 17:14:51,339 [trainer.py] => device: [device(type='cuda', index=1)]
2024-09-25 17:14:51,339 [trainer.py] => seed: 1993
2024-09-25 17:14:51,339 [trainer.py] => pod: w
2024-09-25 17:14:51,339 [trainer.py] => init_epochs: 229
2024-09-25 17:14:51,339 [trainer.py] => epochs: 300
2024-09-25 17:14:51,339 [trainer.py] => lrate: 0.1
2024-09-25 17:14:51,339 [trainer.py] => ft_epochs: 20
2024-09-25 17:14:51,339 [trainer.py] => ft_lrate: 0.005
2024-09-25 17:14:51,339 [trainer.py] => momentum: 0.1
2024-09-25 17:14:51,339 [trainer.py] => batch_size: 128
2024-09-25 17:14:51,339 [trainer.py] => lambda_c_base: 0.8
2024-09-25 17:14:51,339 [trainer.py] => lambda_f_base: 1
2024-09-25 17:14:51,339 [trainer.py] => nb_proxy: 10
2024-09-25 17:14:51,339 [trainer.py] => weight_decay: 0.0005
2024-09-25 17:14:51,340 [trainer.py] => num_workers: 4
2024-09-25 17:14:52,011 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-09-25 17:14:52,145 [trainer.py] => All params: 3843904
2024-09-25 17:14:52,145 [trainer.py] => Trainable params: 3843904
2024-09-25 17:14:52,146 [podnet.py] => Learning on 0-5
2024-09-25 17:14:52,191 [podnet.py] => Adaptive factor: 0
2024-09-25 17:14:55,674 [podnet.py] => Task 0, Epoch 1/229 (LR 0.10000) => LSC_loss 1.55, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.42, Test_acc 41.87
2024-09-25 17:14:57,969 [podnet.py] => Task 0, Epoch 2/229 (LR 0.09999) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.23, Test_acc 37.30
2024-09-25 17:15:00,347 [podnet.py] => Task 0, Epoch 3/229 (LR 0.09998) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.62, Test_acc 47.87
2024-09-25 17:15:02,699 [podnet.py] => Task 0, Epoch 4/229 (LR 0.09996) => LSC_loss 0.52, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.08, Test_acc 38.17
2024-09-25 17:15:05,018 [podnet.py] => Task 0, Epoch 5/229 (LR 0.09993) => LSC_loss 0.36, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.39, Test_acc 69.80
2024-09-25 17:15:07,333 [podnet.py] => Task 0, Epoch 6/229 (LR 0.09990) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.70, Test_acc 70.43
2024-09-25 17:15:09,681 [podnet.py] => Task 0, Epoch 7/229 (LR 0.09987) => LSC_loss 0.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.34, Test_acc 75.27
2024-09-25 17:15:11,952 [podnet.py] => Task 0, Epoch 8/229 (LR 0.09982) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.59, Test_acc 73.77
2024-09-25 17:15:14,279 [podnet.py] => Task 0, Epoch 9/229 (LR 0.09978) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.85, Test_acc 65.53
2024-09-25 17:15:16,473 [podnet.py] => Task 0, Epoch 10/229 (LR 0.09973) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.17, Test_acc 78.07
2024-09-25 17:15:18,953 [podnet.py] => Task 0, Epoch 11/229 (LR 0.09967) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.06, Test_acc 80.93
2024-09-25 17:15:21,206 [podnet.py] => Task 0, Epoch 12/229 (LR 0.09961) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 81.23
2024-09-25 17:15:23,552 [podnet.py] => Task 0, Epoch 13/229 (LR 0.09954) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 82.17
2024-09-25 17:15:25,836 [podnet.py] => Task 0, Epoch 14/229 (LR 0.09946) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.61, Test_acc 81.47
2024-09-25 17:15:28,128 [podnet.py] => Task 0, Epoch 15/229 (LR 0.09938) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.12, Test_acc 75.97
2024-09-25 17:15:30,556 [podnet.py] => Task 0, Epoch 16/229 (LR 0.09930) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.59, Test_acc 82.17
2024-09-25 17:15:32,737 [podnet.py] => Task 0, Epoch 17/229 (LR 0.09921) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.96, Test_acc 67.23
2024-09-25 17:15:35,134 [podnet.py] => Task 0, Epoch 18/229 (LR 0.09911) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.50, Test_acc 81.17
2024-09-25 17:15:37,456 [podnet.py] => Task 0, Epoch 19/229 (LR 0.09901) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.33, Test_acc 81.63
2024-09-25 17:15:39,748 [podnet.py] => Task 0, Epoch 20/229 (LR 0.09891) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.26, Test_acc 73.83
2024-09-25 17:15:42,088 [podnet.py] => Task 0, Epoch 21/229 (LR 0.09880) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.13, Test_acc 79.47
2024-09-25 17:15:44,452 [podnet.py] => Task 0, Epoch 22/229 (LR 0.09868) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.03, Test_acc 81.03
2024-09-25 17:15:46,742 [podnet.py] => Task 0, Epoch 23/229 (LR 0.09856) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.57, Test_acc 78.73
2024-09-25 17:15:49,247 [podnet.py] => Task 0, Epoch 24/229 (LR 0.09843) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.22, Test_acc 86.43
2024-09-25 17:15:51,634 [podnet.py] => Task 0, Epoch 25/229 (LR 0.09830) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 84.20
2024-09-25 17:15:53,895 [podnet.py] => Task 0, Epoch 26/229 (LR 0.09816) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 84.57
2024-09-25 17:15:56,189 [podnet.py] => Task 0, Epoch 27/229 (LR 0.09801) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.62, Test_acc 67.90
2024-09-25 17:15:58,584 [podnet.py] => Task 0, Epoch 28/229 (LR 0.09787) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.75, Test_acc 64.13
2024-09-25 17:16:00,867 [podnet.py] => Task 0, Epoch 29/229 (LR 0.09771) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.15, Test_acc 87.27
2024-09-25 17:16:03,159 [podnet.py] => Task 0, Epoch 30/229 (LR 0.09755) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 79.23
2024-09-25 17:16:05,461 [podnet.py] => Task 0, Epoch 31/229 (LR 0.09739) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.52, Test_acc 78.57
2024-09-25 17:16:07,787 [podnet.py] => Task 0, Epoch 32/229 (LR 0.09722) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 82.93
2024-09-25 17:16:10,061 [podnet.py] => Task 0, Epoch 33/229 (LR 0.09704) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 72.73
2024-09-25 17:16:12,445 [podnet.py] => Task 0, Epoch 34/229 (LR 0.09686) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 75.20
2024-09-25 17:16:14,769 [podnet.py] => Task 0, Epoch 35/229 (LR 0.09668) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 84.80
2024-09-25 17:16:17,149 [podnet.py] => Task 0, Epoch 36/229 (LR 0.09649) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 78.53
2024-09-25 17:16:19,433 [podnet.py] => Task 0, Epoch 37/229 (LR 0.09629) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.57, Test_acc 83.87
2024-09-25 17:16:21,646 [podnet.py] => Task 0, Epoch 38/229 (LR 0.09609) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 83.90
2024-09-25 17:16:23,973 [podnet.py] => Task 0, Epoch 39/229 (LR 0.09589) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.30, Test_acc 76.87
2024-09-25 17:16:26,302 [podnet.py] => Task 0, Epoch 40/229 (LR 0.09568) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.53, Test_acc 74.63
2024-09-25 17:16:28,593 [podnet.py] => Task 0, Epoch 41/229 (LR 0.09546) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.63, Test_acc 83.53
2024-09-25 17:16:30,904 [podnet.py] => Task 0, Epoch 42/229 (LR 0.09524) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.80, Test_acc 84.43
2024-09-25 17:16:33,257 [podnet.py] => Task 0, Epoch 43/229 (LR 0.09502) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.79, Test_acc 79.93
2024-09-25 17:16:35,632 [podnet.py] => Task 0, Epoch 44/229 (LR 0.09479) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.17, Test_acc 84.30
2024-09-25 17:16:38,162 [podnet.py] => Task 0, Epoch 45/229 (LR 0.09455) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.95, Test_acc 77.60
2024-09-25 17:16:40,620 [podnet.py] => Task 0, Epoch 46/229 (LR 0.09431) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 84.73
2024-09-25 17:16:42,846 [podnet.py] => Task 0, Epoch 47/229 (LR 0.09407) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 81.40
2024-09-25 17:16:45,165 [podnet.py] => Task 0, Epoch 48/229 (LR 0.09382) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.04, Test_acc 83.87
2024-09-25 17:16:47,500 [podnet.py] => Task 0, Epoch 49/229 (LR 0.09356) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.22, Test_acc 81.23
2024-09-25 17:16:49,910 [podnet.py] => Task 0, Epoch 50/229 (LR 0.09330) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.05, Test_acc 82.07
2024-09-25 17:16:52,236 [podnet.py] => Task 0, Epoch 51/229 (LR 0.09304) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.98, Test_acc 87.03
2024-09-25 17:16:54,618 [podnet.py] => Task 0, Epoch 52/229 (LR 0.09277) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 69.33
2024-09-25 17:16:57,066 [podnet.py] => Task 0, Epoch 53/229 (LR 0.09249) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.15, Test_acc 81.00
2024-09-25 17:16:59,351 [podnet.py] => Task 0, Epoch 54/229 (LR 0.09222) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 81.97
2024-09-25 17:17:01,691 [podnet.py] => Task 0, Epoch 55/229 (LR 0.09193) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.65, Test_acc 80.57
2024-09-25 17:17:04,038 [podnet.py] => Task 0, Epoch 56/229 (LR 0.09165) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.52, Test_acc 82.73
2024-09-25 17:17:06,421 [podnet.py] => Task 0, Epoch 57/229 (LR 0.09135) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.20, Test_acc 85.30
2024-09-25 17:17:08,761 [podnet.py] => Task 0, Epoch 58/229 (LR 0.09106) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.80, Test_acc 84.37
2024-09-25 17:17:11,087 [podnet.py] => Task 0, Epoch 59/229 (LR 0.09076) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 79.07
2024-09-25 17:17:13,386 [podnet.py] => Task 0, Epoch 60/229 (LR 0.09045) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.38, Test_acc 68.50
2024-09-25 17:17:15,743 [podnet.py] => Task 0, Epoch 61/229 (LR 0.09014) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.77, Test_acc 85.00
2024-09-25 17:17:18,034 [podnet.py] => Task 0, Epoch 62/229 (LR 0.08983) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.17, Test_acc 86.03
2024-09-25 17:17:20,303 [podnet.py] => Task 0, Epoch 63/229 (LR 0.08951) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.94, Test_acc 82.17
2024-09-25 17:17:22,627 [podnet.py] => Task 0, Epoch 64/229 (LR 0.08918) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 83.77
2024-09-25 17:17:24,905 [podnet.py] => Task 0, Epoch 65/229 (LR 0.08886) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 85.13
2024-09-25 17:17:27,184 [podnet.py] => Task 0, Epoch 66/229 (LR 0.08853) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.09, Test_acc 86.43
2024-09-25 17:17:29,502 [podnet.py] => Task 0, Epoch 67/229 (LR 0.08819) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 83.23
2024-09-25 17:17:31,775 [podnet.py] => Task 0, Epoch 68/229 (LR 0.08785) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.10, Test_acc 81.23
2024-09-25 17:17:34,080 [podnet.py] => Task 0, Epoch 69/229 (LR 0.08751) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.66, Test_acc 84.57
2024-09-25 17:17:36,400 [podnet.py] => Task 0, Epoch 70/229 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.49, Test_acc 86.57
2024-09-25 17:17:38,813 [podnet.py] => Task 0, Epoch 71/229 (LR 0.08680) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.43, Test_acc 81.97
2024-09-25 17:17:41,057 [podnet.py] => Task 0, Epoch 72/229 (LR 0.08645) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.05, Test_acc 81.00
2024-09-25 17:17:43,520 [podnet.py] => Task 0, Epoch 73/229 (LR 0.08609) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 83.13
2024-09-25 17:17:45,841 [podnet.py] => Task 0, Epoch 74/229 (LR 0.08572) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 86.60
2024-09-25 17:17:48,158 [podnet.py] => Task 0, Epoch 75/229 (LR 0.08536) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 85.20
2024-09-25 17:17:50,496 [podnet.py] => Task 0, Epoch 76/229 (LR 0.08498) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.51, Test_acc 78.03
2024-09-25 17:17:52,720 [podnet.py] => Task 0, Epoch 77/229 (LR 0.08461) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.13, Test_acc 75.20
2024-09-25 17:17:55,070 [podnet.py] => Task 0, Epoch 78/229 (LR 0.08423) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.16, Test_acc 84.77
2024-09-25 17:17:57,320 [podnet.py] => Task 0, Epoch 79/229 (LR 0.08384) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.87, Test_acc 79.53
2024-09-25 17:17:59,655 [podnet.py] => Task 0, Epoch 80/229 (LR 0.08346) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 85.03
2024-09-25 17:18:02,026 [podnet.py] => Task 0, Epoch 81/229 (LR 0.08307) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.02, Test_acc 84.77
2024-09-25 17:18:04,371 [podnet.py] => Task 0, Epoch 82/229 (LR 0.08267) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.33
2024-09-25 17:18:06,649 [podnet.py] => Task 0, Epoch 83/229 (LR 0.08227) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 77.43
2024-09-25 17:18:08,967 [podnet.py] => Task 0, Epoch 84/229 (LR 0.08187) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 81.97
2024-09-25 17:18:11,100 [podnet.py] => Task 0, Epoch 85/229 (LR 0.08147) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 85.43
2024-09-25 17:18:13,329 [podnet.py] => Task 0, Epoch 86/229 (LR 0.08106) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.59, Test_acc 84.40
2024-09-25 17:18:15,630 [podnet.py] => Task 0, Epoch 87/229 (LR 0.08065) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 83.03
2024-09-25 17:18:17,989 [podnet.py] => Task 0, Epoch 88/229 (LR 0.08023) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.67, Test_acc 82.57
2024-09-25 17:18:20,304 [podnet.py] => Task 0, Epoch 89/229 (LR 0.07981) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.62, Test_acc 83.57
2024-09-25 17:18:22,584 [podnet.py] => Task 0, Epoch 90/229 (LR 0.07939) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 85.13
2024-09-25 17:18:24,894 [podnet.py] => Task 0, Epoch 91/229 (LR 0.07896) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 76.03
2024-09-25 17:18:27,142 [podnet.py] => Task 0, Epoch 92/229 (LR 0.07854) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.98, Test_acc 85.27
2024-09-25 17:18:29,358 [podnet.py] => Task 0, Epoch 93/229 (LR 0.07810) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 81.17
2024-09-25 17:18:31,650 [podnet.py] => Task 0, Epoch 94/229 (LR 0.07767) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 84.73
2024-09-25 17:18:34,061 [podnet.py] => Task 0, Epoch 95/229 (LR 0.07723) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 83.43
2024-09-25 17:18:36,269 [podnet.py] => Task 0, Epoch 96/229 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.51, Test_acc 84.30
2024-09-25 17:18:38,596 [podnet.py] => Task 0, Epoch 97/229 (LR 0.07635) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.25, Test_acc 86.70
2024-09-25 17:18:40,859 [podnet.py] => Task 0, Epoch 98/229 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.56, Test_acc 78.37
2024-09-25 17:18:43,294 [podnet.py] => Task 0, Epoch 99/229 (LR 0.07545) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.93, Test_acc 85.03
2024-09-25 17:18:45,634 [podnet.py] => Task 0, Epoch 100/229 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 78.57
2024-09-25 17:18:47,877 [podnet.py] => Task 0, Epoch 101/229 (LR 0.07455) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 85.93
2024-09-25 17:18:50,221 [podnet.py] => Task 0, Epoch 102/229 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.45, Test_acc 85.50
2024-09-25 17:18:52,634 [podnet.py] => Task 0, Epoch 103/229 (LR 0.07363) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.18, Test_acc 85.47
2024-09-25 17:18:55,036 [podnet.py] => Task 0, Epoch 104/229 (LR 0.07316) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.46, Test_acc 85.33
2024-09-25 17:18:57,337 [podnet.py] => Task 0, Epoch 105/229 (LR 0.07270) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.27, Test_acc 85.53
2024-09-25 17:18:59,768 [podnet.py] => Task 0, Epoch 106/229 (LR 0.07223) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 85.97
2024-09-25 17:19:02,137 [podnet.py] => Task 0, Epoch 107/229 (LR 0.07176) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 84.03
2024-09-25 17:19:04,496 [podnet.py] => Task 0, Epoch 108/229 (LR 0.07129) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 82.87
2024-09-25 17:19:06,847 [podnet.py] => Task 0, Epoch 109/229 (LR 0.07081) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 82.70
2024-09-25 17:19:09,040 [podnet.py] => Task 0, Epoch 110/229 (LR 0.07034) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 83.33
2024-09-25 17:19:11,408 [podnet.py] => Task 0, Epoch 111/229 (LR 0.06986) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 82.57
2024-09-25 17:19:13,749 [podnet.py] => Task 0, Epoch 112/229 (LR 0.06938) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 84.00
2024-09-25 17:19:16,107 [podnet.py] => Task 0, Epoch 113/229 (LR 0.06889) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.05, Test_acc 85.83
2024-09-25 17:19:18,291 [podnet.py] => Task 0, Epoch 114/229 (LR 0.06841) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 88.30
2024-09-25 17:19:20,541 [podnet.py] => Task 0, Epoch 115/229 (LR 0.06792) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 86.80
2024-09-25 17:19:22,859 [podnet.py] => Task 0, Epoch 116/229 (LR 0.06743) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 84.63
2024-09-25 17:19:25,197 [podnet.py] => Task 0, Epoch 117/229 (LR 0.06694) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 86.07
2024-09-25 17:19:27,674 [podnet.py] => Task 0, Epoch 118/229 (LR 0.06644) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.41, Test_acc 80.60
2024-09-25 17:19:30,006 [podnet.py] => Task 0, Epoch 119/229 (LR 0.06595) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.98, Test_acc 81.13
2024-09-25 17:19:32,331 [podnet.py] => Task 0, Epoch 120/229 (LR 0.06545) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.03, Test_acc 84.43
2024-09-25 17:19:34,616 [podnet.py] => Task 0, Epoch 121/229 (LR 0.06495) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 81.63
2024-09-25 17:19:37,032 [podnet.py] => Task 0, Epoch 122/229 (LR 0.06445) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.86, Test_acc 82.20
2024-09-25 17:19:39,440 [podnet.py] => Task 0, Epoch 123/229 (LR 0.06395) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 81.10
2024-09-25 17:19:41,718 [podnet.py] => Task 0, Epoch 124/229 (LR 0.06345) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.14, Test_acc 81.73
2024-09-25 17:19:43,969 [podnet.py] => Task 0, Epoch 125/229 (LR 0.06294) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.22, Test_acc 83.27
2024-09-25 17:19:46,330 [podnet.py] => Task 0, Epoch 126/229 (LR 0.06243) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 84.63
2024-09-25 17:19:48,694 [podnet.py] => Task 0, Epoch 127/229 (LR 0.06193) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 87.23
2024-09-25 17:19:51,273 [podnet.py] => Task 0, Epoch 128/229 (LR 0.06142) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 86.90
2024-09-25 17:19:53,617 [podnet.py] => Task 0, Epoch 129/229 (LR 0.06091) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.10, Test_acc 87.00
2024-09-25 17:19:56,046 [podnet.py] => Task 0, Epoch 130/229 (LR 0.06040) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 86.97
2024-09-25 17:19:58,308 [podnet.py] => Task 0, Epoch 131/229 (LR 0.05988) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 88.40
2024-09-25 17:20:00,744 [podnet.py] => Task 0, Epoch 132/229 (LR 0.05937) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.86, Test_acc 85.53
2024-09-25 17:20:03,094 [podnet.py] => Task 0, Epoch 133/229 (LR 0.05885) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 85.43
2024-09-25 17:20:05,421 [podnet.py] => Task 0, Epoch 134/229 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.54, Test_acc 82.57
2024-09-25 17:20:07,822 [podnet.py] => Task 0, Epoch 135/229 (LR 0.05782) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 84.40
2024-09-25 17:20:10,233 [podnet.py] => Task 0, Epoch 136/229 (LR 0.05730) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.39, Test_acc 85.17
2024-09-25 17:20:12,512 [podnet.py] => Task 0, Epoch 137/229 (LR 0.05679) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 82.23
2024-09-25 17:20:14,781 [podnet.py] => Task 0, Epoch 138/229 (LR 0.05627) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 83.10
2024-09-25 17:20:17,053 [podnet.py] => Task 0, Epoch 139/229 (LR 0.05575) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.59, Test_acc 86.60
2024-09-25 17:20:19,295 [podnet.py] => Task 0, Epoch 140/229 (LR 0.05523) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 84.23
2024-09-25 17:20:21,686 [podnet.py] => Task 0, Epoch 141/229 (LR 0.05471) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.78, Test_acc 86.33
2024-09-25 17:20:24,024 [podnet.py] => Task 0, Epoch 142/229 (LR 0.05418) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.15, Test_acc 84.87
2024-09-25 17:20:26,324 [podnet.py] => Task 0, Epoch 143/229 (LR 0.05366) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 75.87
2024-09-25 17:20:28,675 [podnet.py] => Task 0, Epoch 144/229 (LR 0.05314) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 83.63
2024-09-25 17:20:31,008 [podnet.py] => Task 0, Epoch 145/229 (LR 0.05262) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 87.57
2024-09-25 17:20:33,287 [podnet.py] => Task 0, Epoch 146/229 (LR 0.05209) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 84.13
2024-09-25 17:20:35,597 [podnet.py] => Task 0, Epoch 147/229 (LR 0.05157) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.54, Test_acc 87.20
2024-09-25 17:20:37,808 [podnet.py] => Task 0, Epoch 148/229 (LR 0.05105) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 84.43
2024-09-25 17:20:40,153 [podnet.py] => Task 0, Epoch 149/229 (LR 0.05052) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 81.70
2024-09-25 17:20:42,512 [podnet.py] => Task 0, Epoch 150/229 (LR 0.05000) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 89.37
2024-09-25 17:20:44,745 [podnet.py] => Task 0, Epoch 151/229 (LR 0.04948) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 86.90
2024-09-25 17:20:47,008 [podnet.py] => Task 0, Epoch 152/229 (LR 0.04895) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 89.60
2024-09-25 17:20:49,239 [podnet.py] => Task 0, Epoch 153/229 (LR 0.04843) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 87.30
2024-09-25 17:20:51,509 [podnet.py] => Task 0, Epoch 154/229 (LR 0.04791) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 84.20
2024-09-25 17:20:53,738 [podnet.py] => Task 0, Epoch 155/229 (LR 0.04738) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.34, Test_acc 79.87
2024-09-25 17:20:56,057 [podnet.py] => Task 0, Epoch 156/229 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 84.63
2024-09-25 17:20:58,316 [podnet.py] => Task 0, Epoch 157/229 (LR 0.04634) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 87.77
2024-09-25 17:21:00,692 [podnet.py] => Task 0, Epoch 158/229 (LR 0.04582) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.77
2024-09-25 17:21:02,963 [podnet.py] => Task 0, Epoch 159/229 (LR 0.04529) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 85.43
2024-09-25 17:21:05,186 [podnet.py] => Task 0, Epoch 160/229 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.79, Test_acc 81.00
2024-09-25 17:21:07,482 [podnet.py] => Task 0, Epoch 161/229 (LR 0.04425) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 86.57
2024-09-25 17:21:09,827 [podnet.py] => Task 0, Epoch 162/229 (LR 0.04373) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 84.77
2024-09-25 17:21:12,130 [podnet.py] => Task 0, Epoch 163/229 (LR 0.04321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.82, Test_acc 90.13
2024-09-25 17:21:14,503 [podnet.py] => Task 0, Epoch 164/229 (LR 0.04270) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 89.37
2024-09-25 17:21:16,626 [podnet.py] => Task 0, Epoch 165/229 (LR 0.04218) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.17
2024-09-25 17:21:18,853 [podnet.py] => Task 0, Epoch 166/229 (LR 0.04166) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.25, Test_acc 78.60
2024-09-25 17:21:21,178 [podnet.py] => Task 0, Epoch 167/229 (LR 0.04115) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 87.93
2024-09-25 17:21:23,625 [podnet.py] => Task 0, Epoch 168/229 (LR 0.04063) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.51, Test_acc 86.00
2024-09-25 17:21:25,934 [podnet.py] => Task 0, Epoch 169/229 (LR 0.04012) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 86.43
2024-09-25 17:21:28,277 [podnet.py] => Task 0, Epoch 170/229 (LR 0.03960) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.75, Test_acc 85.47
2024-09-25 17:21:30,533 [podnet.py] => Task 0, Epoch 171/229 (LR 0.03909) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 84.80
2024-09-25 17:21:33,046 [podnet.py] => Task 0, Epoch 172/229 (LR 0.03858) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 89.10
2024-09-25 17:21:35,422 [podnet.py] => Task 0, Epoch 173/229 (LR 0.03807) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.87, Test_acc 89.20
2024-09-25 17:21:37,825 [podnet.py] => Task 0, Epoch 174/229 (LR 0.03757) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 85.90
2024-09-25 17:21:40,124 [podnet.py] => Task 0, Epoch 175/229 (LR 0.03706) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 81.13
2024-09-25 17:21:42,566 [podnet.py] => Task 0, Epoch 176/229 (LR 0.03655) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.38, Test_acc 84.47
2024-09-25 17:21:44,858 [podnet.py] => Task 0, Epoch 177/229 (LR 0.03605) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.88, Test_acc 83.20
2024-09-25 17:21:47,233 [podnet.py] => Task 0, Epoch 178/229 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.78, Test_acc 86.10
2024-09-25 17:21:49,523 [podnet.py] => Task 0, Epoch 179/229 (LR 0.03505) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.79, Test_acc 89.07
2024-09-25 17:21:51,797 [podnet.py] => Task 0, Epoch 180/229 (LR 0.03455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.67
2024-09-25 17:21:54,199 [podnet.py] => Task 0, Epoch 181/229 (LR 0.03405) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 88.87
2024-09-25 17:21:56,533 [podnet.py] => Task 0, Epoch 182/229 (LR 0.03356) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 87.67
2024-09-25 17:21:58,858 [podnet.py] => Task 0, Epoch 183/229 (LR 0.03306) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.87
2024-09-25 17:22:01,232 [podnet.py] => Task 0, Epoch 184/229 (LR 0.03257) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 90.10
2024-09-25 17:22:03,513 [podnet.py] => Task 0, Epoch 185/229 (LR 0.03208) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 88.83
2024-09-25 17:22:05,794 [podnet.py] => Task 0, Epoch 186/229 (LR 0.03159) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.71, Test_acc 84.30
2024-09-25 17:22:08,016 [podnet.py] => Task 0, Epoch 187/229 (LR 0.03111) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 83.33
2024-09-25 17:22:10,361 [podnet.py] => Task 0, Epoch 188/229 (LR 0.03062) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.16, Test_acc 87.17
2024-09-25 17:22:12,864 [podnet.py] => Task 0, Epoch 189/229 (LR 0.03014) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 86.53
2024-09-25 17:22:15,365 [podnet.py] => Task 0, Epoch 190/229 (LR 0.02966) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 88.07
2024-09-25 17:22:17,618 [podnet.py] => Task 0, Epoch 191/229 (LR 0.02919) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.23
2024-09-25 17:22:19,930 [podnet.py] => Task 0, Epoch 192/229 (LR 0.02871) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.89, Test_acc 88.17
2024-09-25 17:22:22,139 [podnet.py] => Task 0, Epoch 193/229 (LR 0.02824) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 84.20
2024-09-25 17:22:24,345 [podnet.py] => Task 0, Epoch 194/229 (LR 0.02777) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 85.20
2024-09-25 17:22:26,610 [podnet.py] => Task 0, Epoch 195/229 (LR 0.02730) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.42, Test_acc 89.23
2024-09-25 17:22:28,882 [podnet.py] => Task 0, Epoch 196/229 (LR 0.02684) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.55, Test_acc 79.03
2024-09-25 17:22:31,256 [podnet.py] => Task 0, Epoch 197/229 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 86.13
2024-09-25 17:22:33,657 [podnet.py] => Task 0, Epoch 198/229 (LR 0.02591) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 88.73
2024-09-25 17:22:35,951 [podnet.py] => Task 0, Epoch 199/229 (LR 0.02545) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 89.67
2024-09-25 17:22:38,109 [podnet.py] => Task 0, Epoch 200/229 (LR 0.02500) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.63
2024-09-25 17:22:40,393 [podnet.py] => Task 0, Epoch 201/229 (LR 0.02455) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.07
2024-09-25 17:22:42,914 [podnet.py] => Task 0, Epoch 202/229 (LR 0.02410) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.39, Test_acc 83.60
2024-09-25 17:22:45,118 [podnet.py] => Task 0, Epoch 203/229 (LR 0.02365) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.58, Test_acc 86.70
2024-09-25 17:22:47,389 [podnet.py] => Task 0, Epoch 204/229 (LR 0.02321) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 87.73
2024-09-25 17:22:49,677 [podnet.py] => Task 0, Epoch 205/229 (LR 0.02277) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.17
2024-09-25 17:22:51,920 [podnet.py] => Task 0, Epoch 206/229 (LR 0.02233) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.47
2024-09-25 17:22:54,304 [podnet.py] => Task 0, Epoch 207/229 (LR 0.02190) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 87.43
2024-09-25 17:22:56,677 [podnet.py] => Task 0, Epoch 208/229 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.99, Test_acc 77.57
2024-09-25 17:22:59,018 [podnet.py] => Task 0, Epoch 209/229 (LR 0.02104) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 84.73
2024-09-25 17:23:01,383 [podnet.py] => Task 0, Epoch 210/229 (LR 0.02061) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.47, Test_acc 88.00
2024-09-25 17:23:03,740 [podnet.py] => Task 0, Epoch 211/229 (LR 0.02019) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 88.40
2024-09-25 17:23:06,001 [podnet.py] => Task 0, Epoch 212/229 (LR 0.01977) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 88.97
2024-09-25 17:23:08,246 [podnet.py] => Task 0, Epoch 213/229 (LR 0.01935) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 89.37
2024-09-25 17:23:10,481 [podnet.py] => Task 0, Epoch 214/229 (LR 0.01894) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.37, Test_acc 87.13
2024-09-25 17:23:12,782 [podnet.py] => Task 0, Epoch 215/229 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 87.57
2024-09-25 17:23:15,080 [podnet.py] => Task 0, Epoch 216/229 (LR 0.01813) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 88.33
2024-09-25 17:23:17,381 [podnet.py] => Task 0, Epoch 217/229 (LR 0.01773) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 88.80
2024-09-25 17:23:19,911 [podnet.py] => Task 0, Epoch 218/229 (LR 0.01733) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.98, Test_acc 89.60
2024-09-25 17:23:22,310 [podnet.py] => Task 0, Epoch 219/229 (LR 0.01693) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.50
2024-09-25 17:23:24,618 [podnet.py] => Task 0, Epoch 220/229 (LR 0.01654) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.00
2024-09-25 17:23:26,900 [podnet.py] => Task 0, Epoch 221/229 (LR 0.01616) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.07
2024-09-25 17:23:29,095 [podnet.py] => Task 0, Epoch 222/229 (LR 0.01577) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-09-25 17:23:31,382 [podnet.py] => Task 0, Epoch 223/229 (LR 0.01539) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-09-25 17:23:33,710 [podnet.py] => Task 0, Epoch 224/229 (LR 0.01502) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 89.73
2024-09-25 17:23:36,052 [podnet.py] => Task 0, Epoch 225/229 (LR 0.01464) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.99, Test_acc 88.80
2024-09-25 17:23:38,263 [podnet.py] => Task 0, Epoch 226/229 (LR 0.01428) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.13
2024-09-25 17:23:40,647 [podnet.py] => Task 0, Epoch 227/229 (LR 0.01391) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.20
2024-09-25 17:23:42,918 [podnet.py] => Task 0, Epoch 228/229 (LR 0.01355) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 90.33
2024-09-25 17:23:45,247 [podnet.py] => Task 0, Epoch 229/229 (LR 0.01320) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 89.93
2024-09-25 17:23:45,248 [base.py] => Reducing exemplars...(100 per classes)
2024-09-25 17:23:45,249 [base.py] => Constructing exemplars...(100 per classes)
2024-09-25 17:23:51,439 [podnet.py] => Exemplar size: 500
2024-09-25 17:23:51,439 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-09-25 17:23:51,439 [trainer.py] => NME: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-09-25 17:23:51,439 [trainer.py] => CNN top1 curve: [89.93]
2024-09-25 17:23:51,439 [trainer.py] => CNN top5 curve: [100.0]
2024-09-25 17:23:51,439 [trainer.py] => NME top1 curve: [89.93]
2024-09-25 17:23:51,440 [trainer.py] => NME top5 curve: [100.0]

2024-09-25 17:23:51,440 [trainer.py] => Average Accuracy (CNN): 89.93
2024-09-25 17:23:51,440 [trainer.py] => Average Accuracy (NME): 89.93
2024-09-25 17:23:51,440 [trainer.py] => All params: 3869505
2024-09-25 17:23:51,440 [trainer.py] => Trainable params: 3869505
2024-09-25 17:23:51,441 [podnet.py] => Learning on 5-7
2024-09-25 17:23:51,456 [podnet.py] => Adaptive factor: 1.8708286933869707
2024-09-25 17:23:53,464 [podnet.py] => Task 1, Epoch 1/300 (LR 0.10000) => LSC_loss 0.95, Spatial_loss 0.35, Flat_loss 0.37, Train_acc 75.04, Test_acc 68.69
2024-09-25 17:23:55,327 [podnet.py] => Task 1, Epoch 2/300 (LR 0.09999) => LSC_loss 0.33, Spatial_loss 0.27, Flat_loss 0.26, Train_acc 94.91, Test_acc 31.21
2024-09-25 17:23:57,176 [podnet.py] => Task 1, Epoch 3/300 (LR 0.09998) => LSC_loss 0.26, Spatial_loss 0.26, Flat_loss 0.23, Train_acc 96.53, Test_acc 74.48
2024-09-25 17:23:59,038 [podnet.py] => Task 1, Epoch 4/300 (LR 0.09996) => LSC_loss 0.21, Spatial_loss 0.23, Flat_loss 0.20, Train_acc 98.02, Test_acc 68.17
2024-09-25 17:24:00,910 [podnet.py] => Task 1, Epoch 5/300 (LR 0.09993) => LSC_loss 0.19, Spatial_loss 0.22, Flat_loss 0.20, Train_acc 98.24, Test_acc 63.52
2024-09-25 17:24:02,741 [podnet.py] => Task 1, Epoch 6/300 (LR 0.09990) => LSC_loss 0.17, Spatial_loss 0.21, Flat_loss 0.19, Train_acc 98.93, Test_acc 71.60
2024-09-25 17:24:04,653 [podnet.py] => Task 1, Epoch 7/300 (LR 0.09987) => LSC_loss 0.17, Spatial_loss 0.21, Flat_loss 0.18, Train_acc 98.84, Test_acc 48.95
2024-09-25 17:24:06,528 [podnet.py] => Task 1, Epoch 8/300 (LR 0.09982) => LSC_loss 0.16, Spatial_loss 0.21, Flat_loss 0.19, Train_acc 99.13, Test_acc 67.76
2024-09-25 17:24:08,344 [podnet.py] => Task 1, Epoch 9/300 (LR 0.09978) => LSC_loss 0.15, Spatial_loss 0.20, Flat_loss 0.17, Train_acc 99.51, Test_acc 60.17
2024-09-25 17:24:10,303 [podnet.py] => Task 1, Epoch 10/300 (LR 0.09973) => LSC_loss 0.14, Spatial_loss 0.20, Flat_loss 0.17, Train_acc 99.47, Test_acc 70.71
2024-09-25 17:24:12,178 [podnet.py] => Task 1, Epoch 11/300 (LR 0.09967) => LSC_loss 0.14, Spatial_loss 0.19, Flat_loss 0.17, Train_acc 99.62, Test_acc 71.05
2024-09-25 17:24:14,123 [podnet.py] => Task 1, Epoch 12/300 (LR 0.09961) => LSC_loss 0.14, Spatial_loss 0.19, Flat_loss 0.17, Train_acc 99.22, Test_acc 68.29
2024-09-25 17:24:15,934 [podnet.py] => Task 1, Epoch 13/300 (LR 0.09954) => LSC_loss 0.13, Spatial_loss 0.18, Flat_loss 0.16, Train_acc 99.80, Test_acc 59.24
2024-09-25 17:24:17,812 [podnet.py] => Task 1, Epoch 14/300 (LR 0.09946) => LSC_loss 0.12, Spatial_loss 0.18, Flat_loss 0.16, Train_acc 99.69, Test_acc 71.10
2024-09-25 17:24:19,659 [podnet.py] => Task 1, Epoch 15/300 (LR 0.09938) => LSC_loss 0.12, Spatial_loss 0.18, Flat_loss 0.16, Train_acc 99.87, Test_acc 67.17
2024-09-25 17:24:21,496 [podnet.py] => Task 1, Epoch 16/300 (LR 0.09930) => LSC_loss 0.12, Spatial_loss 0.18, Flat_loss 0.17, Train_acc 99.78, Test_acc 67.26
2024-09-25 17:24:23,281 [podnet.py] => Task 1, Epoch 17/300 (LR 0.09921) => LSC_loss 0.11, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.67, Test_acc 60.31
2024-09-25 17:24:25,183 [podnet.py] => Task 1, Epoch 18/300 (LR 0.09911) => LSC_loss 0.12, Spatial_loss 0.19, Flat_loss 0.17, Train_acc 99.47, Test_acc 67.29
2024-09-25 17:24:27,067 [podnet.py] => Task 1, Epoch 19/300 (LR 0.09901) => LSC_loss 0.11, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.89, Test_acc 63.02
2024-09-25 17:24:28,835 [podnet.py] => Task 1, Epoch 20/300 (LR 0.09891) => LSC_loss 0.11, Spatial_loss 0.18, Flat_loss 0.16, Train_acc 99.56, Test_acc 73.69
2024-09-25 17:24:30,734 [podnet.py] => Task 1, Epoch 21/300 (LR 0.09880) => LSC_loss 0.11, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 99.69, Test_acc 69.40
2024-09-25 17:24:32,577 [podnet.py] => Task 1, Epoch 22/300 (LR 0.09868) => LSC_loss 0.10, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.91, Test_acc 67.76
2024-09-25 17:24:34,423 [podnet.py] => Task 1, Epoch 23/300 (LR 0.09856) => LSC_loss 0.10, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.89, Test_acc 66.07
2024-09-25 17:24:36,325 [podnet.py] => Task 1, Epoch 24/300 (LR 0.09843) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.15, Train_acc 99.91, Test_acc 69.79
2024-09-25 17:24:38,145 [podnet.py] => Task 1, Epoch 25/300 (LR 0.09830) => LSC_loss 0.10, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.84, Test_acc 68.07
2024-09-25 17:24:40,090 [podnet.py] => Task 1, Epoch 26/300 (LR 0.09816) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.87, Test_acc 68.14
2024-09-25 17:24:41,939 [podnet.py] => Task 1, Epoch 27/300 (LR 0.09801) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.91, Test_acc 64.55
2024-09-25 17:24:43,749 [podnet.py] => Task 1, Epoch 28/300 (LR 0.09787) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.91, Test_acc 72.12
2024-09-25 17:24:45,599 [podnet.py] => Task 1, Epoch 29/300 (LR 0.09771) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.93, Test_acc 64.02
2024-09-25 17:24:47,477 [podnet.py] => Task 1, Epoch 30/300 (LR 0.09755) => LSC_loss 0.09, Spatial_loss 0.15, Flat_loss 0.15, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:24:49,354 [podnet.py] => Task 1, Epoch 31/300 (LR 0.09739) => LSC_loss 0.09, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.96, Test_acc 69.21
2024-09-25 17:24:51,186 [podnet.py] => Task 1, Epoch 32/300 (LR 0.09722) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.93, Test_acc 70.36
2024-09-25 17:24:53,027 [podnet.py] => Task 1, Epoch 33/300 (LR 0.09704) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.98, Test_acc 70.43
2024-09-25 17:24:54,905 [podnet.py] => Task 1, Epoch 34/300 (LR 0.09686) => LSC_loss 0.09, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.96, Test_acc 49.55
2024-09-25 17:24:56,776 [podnet.py] => Task 1, Epoch 35/300 (LR 0.09668) => LSC_loss 0.10, Spatial_loss 0.17, Flat_loss 0.16, Train_acc 99.33, Test_acc 72.79
2024-09-25 17:24:58,697 [podnet.py] => Task 1, Epoch 36/300 (LR 0.09649) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.93, Test_acc 74.33
2024-09-25 17:25:00,646 [podnet.py] => Task 1, Epoch 37/300 (LR 0.09629) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.89, Test_acc 69.81
2024-09-25 17:25:02,583 [podnet.py] => Task 1, Epoch 38/300 (LR 0.09609) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.98, Test_acc 74.57
2024-09-25 17:25:04,454 [podnet.py] => Task 1, Epoch 39/300 (LR 0.09589) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.93, Test_acc 71.31
2024-09-25 17:25:06,411 [podnet.py] => Task 1, Epoch 40/300 (LR 0.09568) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.98, Test_acc 69.14
2024-09-25 17:25:08,399 [podnet.py] => Task 1, Epoch 41/300 (LR 0.09546) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.14, Train_acc 99.96, Test_acc 72.14
2024-09-25 17:25:10,372 [podnet.py] => Task 1, Epoch 42/300 (LR 0.09524) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.96, Test_acc 69.95
2024-09-25 17:25:12,313 [podnet.py] => Task 1, Epoch 43/300 (LR 0.09502) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.96, Test_acc 72.43
2024-09-25 17:25:14,215 [podnet.py] => Task 1, Epoch 44/300 (LR 0.09479) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.98, Test_acc 72.71
2024-09-25 17:25:16,062 [podnet.py] => Task 1, Epoch 45/300 (LR 0.09455) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.86
2024-09-25 17:25:17,891 [podnet.py] => Task 1, Epoch 46/300 (LR 0.09431) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.96, Test_acc 77.81
2024-09-25 17:25:19,743 [podnet.py] => Task 1, Epoch 47/300 (LR 0.09407) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 72.69
2024-09-25 17:25:21,608 [podnet.py] => Task 1, Epoch 48/300 (LR 0.09382) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.96, Test_acc 71.67
2024-09-25 17:25:23,494 [podnet.py] => Task 1, Epoch 49/300 (LR 0.09356) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 100.00, Test_acc 73.43
2024-09-25 17:25:25,279 [podnet.py] => Task 1, Epoch 50/300 (LR 0.09330) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.96, Test_acc 69.19
2024-09-25 17:25:27,185 [podnet.py] => Task 1, Epoch 51/300 (LR 0.09304) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.98, Test_acc 68.31
2024-09-25 17:25:29,131 [podnet.py] => Task 1, Epoch 52/300 (LR 0.09277) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 99.96, Test_acc 69.19
2024-09-25 17:25:30,991 [podnet.py] => Task 1, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 71.07
2024-09-25 17:25:32,920 [podnet.py] => Task 1, Epoch 54/300 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 69.90
2024-09-25 17:25:34,856 [podnet.py] => Task 1, Epoch 55/300 (LR 0.09193) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.13, Train_acc 100.00, Test_acc 70.86
2024-09-25 17:25:36,931 [podnet.py] => Task 1, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.96, Test_acc 66.64
2024-09-25 17:25:38,859 [podnet.py] => Task 1, Epoch 57/300 (LR 0.09135) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.14, Train_acc 99.51, Test_acc 76.07
2024-09-25 17:25:40,857 [podnet.py] => Task 1, Epoch 58/300 (LR 0.09106) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 70.48
2024-09-25 17:25:42,905 [podnet.py] => Task 1, Epoch 59/300 (LR 0.09076) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.91, Test_acc 71.19
2024-09-25 17:25:44,873 [podnet.py] => Task 1, Epoch 60/300 (LR 0.09045) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 73.29
2024-09-25 17:25:46,828 [podnet.py] => Task 1, Epoch 61/300 (LR 0.09014) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.98, Test_acc 73.90
2024-09-25 17:25:48,917 [podnet.py] => Task 1, Epoch 62/300 (LR 0.08983) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.98, Test_acc 72.64
2024-09-25 17:25:50,792 [podnet.py] => Task 1, Epoch 63/300 (LR 0.08951) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.98, Test_acc 76.69
2024-09-25 17:25:52,844 [podnet.py] => Task 1, Epoch 64/300 (LR 0.08918) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 100.00, Test_acc 70.62
2024-09-25 17:25:54,828 [podnet.py] => Task 1, Epoch 65/300 (LR 0.08886) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.91, Test_acc 74.86
2024-09-25 17:25:56,786 [podnet.py] => Task 1, Epoch 66/300 (LR 0.08853) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 68.81
2024-09-25 17:25:58,798 [podnet.py] => Task 1, Epoch 67/300 (LR 0.08819) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.98, Test_acc 72.71
2024-09-25 17:26:00,783 [podnet.py] => Task 1, Epoch 68/300 (LR 0.08785) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.89, Test_acc 68.24
2024-09-25 17:26:02,778 [podnet.py] => Task 1, Epoch 69/300 (LR 0.08751) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 100.00, Test_acc 71.86
2024-09-25 17:26:04,822 [podnet.py] => Task 1, Epoch 70/300 (LR 0.08716) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.98, Test_acc 76.76
2024-09-25 17:26:06,753 [podnet.py] => Task 1, Epoch 71/300 (LR 0.08680) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 73.07
2024-09-25 17:26:08,692 [podnet.py] => Task 1, Epoch 72/300 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.91, Test_acc 40.93
2024-09-25 17:26:10,750 [podnet.py] => Task 1, Epoch 73/300 (LR 0.08609) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.15, Train_acc 99.18, Test_acc 71.40
2024-09-25 17:26:12,832 [podnet.py] => Task 1, Epoch 74/300 (LR 0.08572) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.10
2024-09-25 17:26:14,890 [podnet.py] => Task 1, Epoch 75/300 (LR 0.08536) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.12
2024-09-25 17:26:16,940 [podnet.py] => Task 1, Epoch 76/300 (LR 0.08498) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 74.31
2024-09-25 17:26:18,823 [podnet.py] => Task 1, Epoch 77/300 (LR 0.08461) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 76.67
2024-09-25 17:26:20,887 [podnet.py] => Task 1, Epoch 78/300 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.83
2024-09-25 17:26:22,836 [podnet.py] => Task 1, Epoch 79/300 (LR 0.08384) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.96, Test_acc 77.48
2024-09-25 17:26:24,922 [podnet.py] => Task 1, Epoch 80/300 (LR 0.08346) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 73.81
2024-09-25 17:26:26,873 [podnet.py] => Task 1, Epoch 81/300 (LR 0.08307) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 73.69
2024-09-25 17:26:28,843 [podnet.py] => Task 1, Epoch 82/300 (LR 0.08267) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 76.43
2024-09-25 17:26:30,891 [podnet.py] => Task 1, Epoch 83/300 (LR 0.08227) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.96, Test_acc 72.45
2024-09-25 17:26:32,857 [podnet.py] => Task 1, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 74.40
2024-09-25 17:26:34,834 [podnet.py] => Task 1, Epoch 85/300 (LR 0.08147) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 67.83
2024-09-25 17:26:36,851 [podnet.py] => Task 1, Epoch 86/300 (LR 0.08106) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.17
2024-09-25 17:26:38,781 [podnet.py] => Task 1, Epoch 87/300 (LR 0.08065) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 71.05
2024-09-25 17:26:40,737 [podnet.py] => Task 1, Epoch 88/300 (LR 0.08023) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 64.00
2024-09-25 17:26:42,842 [podnet.py] => Task 1, Epoch 89/300 (LR 0.07981) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.96, Test_acc 67.83
2024-09-25 17:26:44,793 [podnet.py] => Task 1, Epoch 90/300 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 100.00, Test_acc 70.26
2024-09-25 17:26:46,752 [podnet.py] => Task 1, Epoch 91/300 (LR 0.07896) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 72.19
2024-09-25 17:26:48,645 [podnet.py] => Task 1, Epoch 92/300 (LR 0.07854) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.12, Train_acc 99.98, Test_acc 75.98
2024-09-25 17:26:50,637 [podnet.py] => Task 1, Epoch 93/300 (LR 0.07810) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.60
2024-09-25 17:26:52,636 [podnet.py] => Task 1, Epoch 94/300 (LR 0.07767) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.67
2024-09-25 17:26:54,633 [podnet.py] => Task 1, Epoch 95/300 (LR 0.07723) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.76
2024-09-25 17:26:56,726 [podnet.py] => Task 1, Epoch 96/300 (LR 0.07679) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.26
2024-09-25 17:26:58,743 [podnet.py] => Task 1, Epoch 97/300 (LR 0.07635) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.83
2024-09-25 17:27:00,621 [podnet.py] => Task 1, Epoch 98/300 (LR 0.07590) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.93
2024-09-25 17:27:02,499 [podnet.py] => Task 1, Epoch 99/300 (LR 0.07545) => LSC_loss 0.05, Spatial_loss 0.12, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.71
2024-09-25 17:27:04,464 [podnet.py] => Task 1, Epoch 100/300 (LR 0.07500) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.07
2024-09-25 17:27:06,669 [podnet.py] => Task 1, Epoch 101/300 (LR 0.07455) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.45
2024-09-25 17:27:08,669 [podnet.py] => Task 1, Epoch 102/300 (LR 0.07409) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.98
2024-09-25 17:27:10,632 [podnet.py] => Task 1, Epoch 103/300 (LR 0.07363) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.11, Train_acc 99.96, Test_acc 64.95
2024-09-25 17:27:12,686 [podnet.py] => Task 1, Epoch 104/300 (LR 0.07316) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.64, Test_acc 76.29
2024-09-25 17:27:14,650 [podnet.py] => Task 1, Epoch 105/300 (LR 0.07270) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.29
2024-09-25 17:27:16,621 [podnet.py] => Task 1, Epoch 106/300 (LR 0.07223) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.19
2024-09-25 17:27:18,660 [podnet.py] => Task 1, Epoch 107/300 (LR 0.07176) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.21
2024-09-25 17:27:20,619 [podnet.py] => Task 1, Epoch 108/300 (LR 0.07129) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.96, Test_acc 71.02
2024-09-25 17:27:22,480 [podnet.py] => Task 1, Epoch 109/300 (LR 0.07081) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.40
2024-09-25 17:27:24,349 [podnet.py] => Task 1, Epoch 110/300 (LR 0.07034) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.83
2024-09-25 17:27:26,240 [podnet.py] => Task 1, Epoch 111/300 (LR 0.06986) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 63.02
2024-09-25 17:27:28,084 [podnet.py] => Task 1, Epoch 112/300 (LR 0.06938) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.13, Train_acc 99.53, Test_acc 59.29
2024-09-25 17:27:29,930 [podnet.py] => Task 1, Epoch 113/300 (LR 0.06889) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.12, Train_acc 99.87, Test_acc 71.21
2024-09-25 17:27:31,815 [podnet.py] => Task 1, Epoch 114/300 (LR 0.06841) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.50
2024-09-25 17:27:33,708 [podnet.py] => Task 1, Epoch 115/300 (LR 0.06792) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.86
2024-09-25 17:27:35,512 [podnet.py] => Task 1, Epoch 116/300 (LR 0.06743) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.76
2024-09-25 17:27:37,427 [podnet.py] => Task 1, Epoch 117/300 (LR 0.06694) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.02
2024-09-25 17:27:39,242 [podnet.py] => Task 1, Epoch 118/300 (LR 0.06644) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.90
2024-09-25 17:27:41,158 [podnet.py] => Task 1, Epoch 119/300 (LR 0.06595) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.62
2024-09-25 17:27:42,979 [podnet.py] => Task 1, Epoch 120/300 (LR 0.06545) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.74
2024-09-25 17:27:44,864 [podnet.py] => Task 1, Epoch 121/300 (LR 0.06495) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.96, Test_acc 72.14
2024-09-25 17:27:46,967 [podnet.py] => Task 1, Epoch 122/300 (LR 0.06445) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.36
2024-09-25 17:27:49,086 [podnet.py] => Task 1, Epoch 123/300 (LR 0.06395) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.93
2024-09-25 17:27:51,081 [podnet.py] => Task 1, Epoch 124/300 (LR 0.06345) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 70.81
2024-09-25 17:27:52,997 [podnet.py] => Task 1, Epoch 125/300 (LR 0.06294) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.45
2024-09-25 17:27:55,040 [podnet.py] => Task 1, Epoch 126/300 (LR 0.06243) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.64
2024-09-25 17:27:57,142 [podnet.py] => Task 1, Epoch 127/300 (LR 0.06193) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 73.62
2024-09-25 17:27:59,176 [podnet.py] => Task 1, Epoch 128/300 (LR 0.06142) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:28:01,243 [podnet.py] => Task 1, Epoch 129/300 (LR 0.06091) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.95
2024-09-25 17:28:03,255 [podnet.py] => Task 1, Epoch 130/300 (LR 0.06040) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 69.55
2024-09-25 17:28:05,163 [podnet.py] => Task 1, Epoch 131/300 (LR 0.05988) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 99.98, Test_acc 71.00
2024-09-25 17:28:07,200 [podnet.py] => Task 1, Epoch 132/300 (LR 0.05937) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 74.52
2024-09-25 17:28:09,145 [podnet.py] => Task 1, Epoch 133/300 (LR 0.05885) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 75.26
2024-09-25 17:28:11,121 [podnet.py] => Task 1, Epoch 134/300 (LR 0.05834) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 75.74
2024-09-25 17:28:13,080 [podnet.py] => Task 1, Epoch 135/300 (LR 0.05782) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 72.10
2024-09-25 17:28:15,243 [podnet.py] => Task 1, Epoch 136/300 (LR 0.05730) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.55
2024-09-25 17:28:17,174 [podnet.py] => Task 1, Epoch 137/300 (LR 0.05679) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.05
2024-09-25 17:28:19,087 [podnet.py] => Task 1, Epoch 138/300 (LR 0.05627) => LSC_loss 0.05, Spatial_loss 0.11, Flat_loss 0.11, Train_acc 100.00, Test_acc 76.45
2024-09-25 17:28:21,141 [podnet.py] => Task 1, Epoch 139/300 (LR 0.05575) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.64
2024-09-25 17:28:23,050 [podnet.py] => Task 1, Epoch 140/300 (LR 0.05523) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.12
2024-09-25 17:28:25,170 [podnet.py] => Task 1, Epoch 141/300 (LR 0.05471) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 71.05
2024-09-25 17:28:27,212 [podnet.py] => Task 1, Epoch 142/300 (LR 0.05418) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.64
2024-09-25 17:28:29,074 [podnet.py] => Task 1, Epoch 143/300 (LR 0.05366) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:28:31,053 [podnet.py] => Task 1, Epoch 144/300 (LR 0.05314) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.19
2024-09-25 17:28:33,074 [podnet.py] => Task 1, Epoch 145/300 (LR 0.05262) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.57
2024-09-25 17:28:35,105 [podnet.py] => Task 1, Epoch 146/300 (LR 0.05209) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.71
2024-09-25 17:28:37,119 [podnet.py] => Task 1, Epoch 147/300 (LR 0.05157) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 99.96, Test_acc 70.86
2024-09-25 17:28:39,156 [podnet.py] => Task 1, Epoch 148/300 (LR 0.05105) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 100.00, Test_acc 73.00
2024-09-25 17:28:41,199 [podnet.py] => Task 1, Epoch 149/300 (LR 0.05052) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 72.83
2024-09-25 17:28:43,191 [podnet.py] => Task 1, Epoch 150/300 (LR 0.05000) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.48
2024-09-25 17:28:45,144 [podnet.py] => Task 1, Epoch 151/300 (LR 0.04948) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.55
2024-09-25 17:28:47,123 [podnet.py] => Task 1, Epoch 152/300 (LR 0.04895) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.17
2024-09-25 17:28:49,181 [podnet.py] => Task 1, Epoch 153/300 (LR 0.04843) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.55
2024-09-25 17:28:51,181 [podnet.py] => Task 1, Epoch 154/300 (LR 0.04791) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 99.98, Test_acc 69.69
2024-09-25 17:28:53,174 [podnet.py] => Task 1, Epoch 155/300 (LR 0.04738) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.38
2024-09-25 17:28:55,234 [podnet.py] => Task 1, Epoch 156/300 (LR 0.04686) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.57
2024-09-25 17:28:57,314 [podnet.py] => Task 1, Epoch 157/300 (LR 0.04634) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.86
2024-09-25 17:28:59,312 [podnet.py] => Task 1, Epoch 158/300 (LR 0.04582) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.52
2024-09-25 17:29:01,227 [podnet.py] => Task 1, Epoch 159/300 (LR 0.04529) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.07
2024-09-25 17:29:03,240 [podnet.py] => Task 1, Epoch 160/300 (LR 0.04477) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.00
2024-09-25 17:29:05,190 [podnet.py] => Task 1, Epoch 161/300 (LR 0.04425) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.40
2024-09-25 17:29:07,188 [podnet.py] => Task 1, Epoch 162/300 (LR 0.04373) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.48
2024-09-25 17:29:09,136 [podnet.py] => Task 1, Epoch 163/300 (LR 0.04321) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.76
2024-09-25 17:29:11,256 [podnet.py] => Task 1, Epoch 164/300 (LR 0.04270) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 71.05
2024-09-25 17:29:13,278 [podnet.py] => Task 1, Epoch 165/300 (LR 0.04218) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 99.98, Test_acc 72.83
2024-09-25 17:29:15,237 [podnet.py] => Task 1, Epoch 166/300 (LR 0.04166) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 99.98, Test_acc 71.36
2024-09-25 17:29:17,183 [podnet.py] => Task 1, Epoch 167/300 (LR 0.04115) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 99.96, Test_acc 75.00
2024-09-25 17:29:19,282 [podnet.py] => Task 1, Epoch 168/300 (LR 0.04063) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.95
2024-09-25 17:29:21,389 [podnet.py] => Task 1, Epoch 169/300 (LR 0.04012) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.76
2024-09-25 17:29:23,347 [podnet.py] => Task 1, Epoch 170/300 (LR 0.03960) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.00
2024-09-25 17:29:25,409 [podnet.py] => Task 1, Epoch 171/300 (LR 0.03909) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 70.43
2024-09-25 17:29:27,421 [podnet.py] => Task 1, Epoch 172/300 (LR 0.03858) => LSC_loss 0.04, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.19
2024-09-25 17:29:29,434 [podnet.py] => Task 1, Epoch 173/300 (LR 0.03807) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 76.10
2024-09-25 17:29:31,368 [podnet.py] => Task 1, Epoch 174/300 (LR 0.03757) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.93
2024-09-25 17:29:33,465 [podnet.py] => Task 1, Epoch 175/300 (LR 0.03706) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:29:35,365 [podnet.py] => Task 1, Epoch 176/300 (LR 0.03655) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.12
2024-09-25 17:29:37,133 [podnet.py] => Task 1, Epoch 177/300 (LR 0.03605) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.79
2024-09-25 17:29:38,959 [podnet.py] => Task 1, Epoch 178/300 (LR 0.03555) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.55
2024-09-25 17:29:40,826 [podnet.py] => Task 1, Epoch 179/300 (LR 0.03505) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.45
2024-09-25 17:29:42,663 [podnet.py] => Task 1, Epoch 180/300 (LR 0.03455) => LSC_loss 0.05, Spatial_loss 0.10, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.19
2024-09-25 17:29:44,483 [podnet.py] => Task 1, Epoch 181/300 (LR 0.03405) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.36
2024-09-25 17:29:46,400 [podnet.py] => Task 1, Epoch 182/300 (LR 0.03356) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.29
2024-09-25 17:29:48,213 [podnet.py] => Task 1, Epoch 183/300 (LR 0.03306) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.19
2024-09-25 17:29:50,062 [podnet.py] => Task 1, Epoch 184/300 (LR 0.03257) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 76.60
2024-09-25 17:29:51,962 [podnet.py] => Task 1, Epoch 185/300 (LR 0.03208) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.50
2024-09-25 17:29:53,813 [podnet.py] => Task 1, Epoch 186/300 (LR 0.03159) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.67
2024-09-25 17:29:55,635 [podnet.py] => Task 1, Epoch 187/300 (LR 0.03111) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.79
2024-09-25 17:29:57,525 [podnet.py] => Task 1, Epoch 188/300 (LR 0.03062) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.71
2024-09-25 17:29:59,413 [podnet.py] => Task 1, Epoch 189/300 (LR 0.03014) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:30:01,221 [podnet.py] => Task 1, Epoch 190/300 (LR 0.02966) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.07
2024-09-25 17:30:03,054 [podnet.py] => Task 1, Epoch 191/300 (LR 0.02919) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.45
2024-09-25 17:30:04,919 [podnet.py] => Task 1, Epoch 192/300 (LR 0.02871) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.67
2024-09-25 17:30:06,852 [podnet.py] => Task 1, Epoch 193/300 (LR 0.02824) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.67
2024-09-25 17:30:08,793 [podnet.py] => Task 1, Epoch 194/300 (LR 0.02777) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.40
2024-09-25 17:30:10,714 [podnet.py] => Task 1, Epoch 195/300 (LR 0.02730) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.26
2024-09-25 17:30:12,558 [podnet.py] => Task 1, Epoch 196/300 (LR 0.02684) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.86
2024-09-25 17:30:14,385 [podnet.py] => Task 1, Epoch 197/300 (LR 0.02637) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:30:16,212 [podnet.py] => Task 1, Epoch 198/300 (LR 0.02591) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.31
2024-09-25 17:30:17,996 [podnet.py] => Task 1, Epoch 199/300 (LR 0.02545) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 76.62
2024-09-25 17:30:19,817 [podnet.py] => Task 1, Epoch 200/300 (LR 0.02500) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.14
2024-09-25 17:30:21,613 [podnet.py] => Task 1, Epoch 201/300 (LR 0.02455) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:30:23,441 [podnet.py] => Task 1, Epoch 202/300 (LR 0.02410) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.50
2024-09-25 17:30:25,357 [podnet.py] => Task 1, Epoch 203/300 (LR 0.02365) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.38
2024-09-25 17:30:27,259 [podnet.py] => Task 1, Epoch 204/300 (LR 0.02321) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 72.95
2024-09-25 17:30:29,141 [podnet.py] => Task 1, Epoch 205/300 (LR 0.02277) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.12
2024-09-25 17:30:31,002 [podnet.py] => Task 1, Epoch 206/300 (LR 0.02233) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.88
2024-09-25 17:30:32,863 [podnet.py] => Task 1, Epoch 207/300 (LR 0.02190) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.33
2024-09-25 17:30:34,667 [podnet.py] => Task 1, Epoch 208/300 (LR 0.02146) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.43
2024-09-25 17:30:36,518 [podnet.py] => Task 1, Epoch 209/300 (LR 0.02104) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.86
2024-09-25 17:30:38,352 [podnet.py] => Task 1, Epoch 210/300 (LR 0.02061) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.21
2024-09-25 17:30:40,327 [podnet.py] => Task 1, Epoch 211/300 (LR 0.02019) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.24
2024-09-25 17:30:42,213 [podnet.py] => Task 1, Epoch 212/300 (LR 0.01977) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.76
2024-09-25 17:30:44,168 [podnet.py] => Task 1, Epoch 213/300 (LR 0.01935) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.36
2024-09-25 17:30:46,000 [podnet.py] => Task 1, Epoch 214/300 (LR 0.01894) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.43
2024-09-25 17:30:47,825 [podnet.py] => Task 1, Epoch 215/300 (LR 0.01853) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.76
2024-09-25 17:30:49,722 [podnet.py] => Task 1, Epoch 216/300 (LR 0.01813) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.62
2024-09-25 17:30:51,562 [podnet.py] => Task 1, Epoch 217/300 (LR 0.01773) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:30:53,402 [podnet.py] => Task 1, Epoch 218/300 (LR 0.01733) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.76
2024-09-25 17:30:55,295 [podnet.py] => Task 1, Epoch 219/300 (LR 0.01693) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.55
2024-09-25 17:30:57,093 [podnet.py] => Task 1, Epoch 220/300 (LR 0.01654) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.17
2024-09-25 17:30:58,931 [podnet.py] => Task 1, Epoch 221/300 (LR 0.01616) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:31:00,816 [podnet.py] => Task 1, Epoch 222/300 (LR 0.01577) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.00
2024-09-25 17:31:02,725 [podnet.py] => Task 1, Epoch 223/300 (LR 0.01539) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.79
2024-09-25 17:31:04,540 [podnet.py] => Task 1, Epoch 224/300 (LR 0.01502) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.76
2024-09-25 17:31:06,390 [podnet.py] => Task 1, Epoch 225/300 (LR 0.01464) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.43
2024-09-25 17:31:08,261 [podnet.py] => Task 1, Epoch 226/300 (LR 0.01428) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:31:10,161 [podnet.py] => Task 1, Epoch 227/300 (LR 0.01391) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.93
2024-09-25 17:31:11,976 [podnet.py] => Task 1, Epoch 228/300 (LR 0.01355) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.40
2024-09-25 17:31:13,799 [podnet.py] => Task 1, Epoch 229/300 (LR 0.01320) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.10
2024-09-25 17:31:15,610 [podnet.py] => Task 1, Epoch 230/300 (LR 0.01284) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.79
2024-09-25 17:31:17,362 [podnet.py] => Task 1, Epoch 231/300 (LR 0.01249) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.90
2024-09-25 17:31:19,249 [podnet.py] => Task 1, Epoch 232/300 (LR 0.01215) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.31
2024-09-25 17:31:21,103 [podnet.py] => Task 1, Epoch 233/300 (LR 0.01181) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.38
2024-09-25 17:31:22,995 [podnet.py] => Task 1, Epoch 234/300 (LR 0.01147) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:31:24,883 [podnet.py] => Task 1, Epoch 235/300 (LR 0.01114) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 99.98, Test_acc 73.95
2024-09-25 17:31:26,780 [podnet.py] => Task 1, Epoch 236/300 (LR 0.01082) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.64
2024-09-25 17:31:28,561 [podnet.py] => Task 1, Epoch 237/300 (LR 0.01049) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.67
2024-09-25 17:31:30,431 [podnet.py] => Task 1, Epoch 238/300 (LR 0.01017) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 99.91, Test_acc 68.02
2024-09-25 17:31:32,281 [podnet.py] => Task 1, Epoch 239/300 (LR 0.00986) => LSC_loss 0.04, Spatial_loss 0.09, Flat_loss 0.11, Train_acc 99.98, Test_acc 74.29
2024-09-25 17:31:34,167 [podnet.py] => Task 1, Epoch 240/300 (LR 0.00955) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 73.50
2024-09-25 17:31:36,000 [podnet.py] => Task 1, Epoch 241/300 (LR 0.00924) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.52
2024-09-25 17:31:37,899 [podnet.py] => Task 1, Epoch 242/300 (LR 0.00894) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.64
2024-09-25 17:31:39,701 [podnet.py] => Task 1, Epoch 243/300 (LR 0.00865) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:31:41,593 [podnet.py] => Task 1, Epoch 244/300 (LR 0.00835) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.14
2024-09-25 17:31:43,526 [podnet.py] => Task 1, Epoch 245/300 (LR 0.00807) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.98
2024-09-25 17:31:45,430 [podnet.py] => Task 1, Epoch 246/300 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.50
2024-09-25 17:31:47,486 [podnet.py] => Task 1, Epoch 247/300 (LR 0.00751) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:31:49,387 [podnet.py] => Task 1, Epoch 248/300 (LR 0.00723) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.55
2024-09-25 17:31:51,315 [podnet.py] => Task 1, Epoch 249/300 (LR 0.00696) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.67
2024-09-25 17:31:53,106 [podnet.py] => Task 1, Epoch 250/300 (LR 0.00670) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.93
2024-09-25 17:31:54,877 [podnet.py] => Task 1, Epoch 251/300 (LR 0.00644) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.67
2024-09-25 17:31:56,670 [podnet.py] => Task 1, Epoch 252/300 (LR 0.00618) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:31:58,455 [podnet.py] => Task 1, Epoch 253/300 (LR 0.00593) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.29
2024-09-25 17:32:00,227 [podnet.py] => Task 1, Epoch 254/300 (LR 0.00569) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.81
2024-09-25 17:32:02,052 [podnet.py] => Task 1, Epoch 255/300 (LR 0.00545) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.95
2024-09-25 17:32:03,987 [podnet.py] => Task 1, Epoch 256/300 (LR 0.00521) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.55
2024-09-25 17:32:05,788 [podnet.py] => Task 1, Epoch 257/300 (LR 0.00498) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:32:07,600 [podnet.py] => Task 1, Epoch 258/300 (LR 0.00476) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.93
2024-09-25 17:32:09,460 [podnet.py] => Task 1, Epoch 259/300 (LR 0.00454) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:32:11,298 [podnet.py] => Task 1, Epoch 260/300 (LR 0.00432) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.07
2024-09-25 17:32:13,129 [podnet.py] => Task 1, Epoch 261/300 (LR 0.00411) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:32:15,049 [podnet.py] => Task 1, Epoch 262/300 (LR 0.00391) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:32:16,832 [podnet.py] => Task 1, Epoch 263/300 (LR 0.00371) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.98
2024-09-25 17:32:18,705 [podnet.py] => Task 1, Epoch 264/300 (LR 0.00351) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.83
2024-09-25 17:32:20,526 [podnet.py] => Task 1, Epoch 265/300 (LR 0.00332) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:32:22,277 [podnet.py] => Task 1, Epoch 266/300 (LR 0.00314) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:32:24,087 [podnet.py] => Task 1, Epoch 267/300 (LR 0.00296) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:32:25,892 [podnet.py] => Task 1, Epoch 268/300 (LR 0.00278) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:32:27,638 [podnet.py] => Task 1, Epoch 269/300 (LR 0.00261) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:32:29,425 [podnet.py] => Task 1, Epoch 270/300 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.95
2024-09-25 17:32:31,178 [podnet.py] => Task 1, Epoch 271/300 (LR 0.00229) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.93
2024-09-25 17:32:32,929 [podnet.py] => Task 1, Epoch 272/300 (LR 0.00213) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.12
2024-09-25 17:32:34,702 [podnet.py] => Task 1, Epoch 273/300 (LR 0.00199) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.00
2024-09-25 17:32:36,561 [podnet.py] => Task 1, Epoch 274/300 (LR 0.00184) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.00
2024-09-25 17:32:38,377 [podnet.py] => Task 1, Epoch 275/300 (LR 0.00170) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.12
2024-09-25 17:32:40,209 [podnet.py] => Task 1, Epoch 276/300 (LR 0.00157) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.81
2024-09-25 17:32:41,932 [podnet.py] => Task 1, Epoch 277/300 (LR 0.00144) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.81
2024-09-25 17:32:43,711 [podnet.py] => Task 1, Epoch 278/300 (LR 0.00132) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.98
2024-09-25 17:32:45,477 [podnet.py] => Task 1, Epoch 279/300 (LR 0.00120) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.93
2024-09-25 17:32:47,241 [podnet.py] => Task 1, Epoch 280/300 (LR 0.00109) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.02
2024-09-25 17:32:49,023 [podnet.py] => Task 1, Epoch 281/300 (LR 0.00099) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:32:50,837 [podnet.py] => Task 1, Epoch 282/300 (LR 0.00089) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.95
2024-09-25 17:32:52,787 [podnet.py] => Task 1, Epoch 283/300 (LR 0.00079) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.14
2024-09-25 17:32:54,546 [podnet.py] => Task 1, Epoch 284/300 (LR 0.00070) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:32:56,279 [podnet.py] => Task 1, Epoch 285/300 (LR 0.00062) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:32:58,007 [podnet.py] => Task 1, Epoch 286/300 (LR 0.00054) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.09, Train_acc 99.98, Test_acc 74.64
2024-09-25 17:32:59,845 [podnet.py] => Task 1, Epoch 287/300 (LR 0.00046) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:33:01,709 [podnet.py] => Task 1, Epoch 288/300 (LR 0.00039) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.00
2024-09-25 17:33:03,392 [podnet.py] => Task 1, Epoch 289/300 (LR 0.00033) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:33:05,218 [podnet.py] => Task 1, Epoch 290/300 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.90
2024-09-25 17:33:07,129 [podnet.py] => Task 1, Epoch 291/300 (LR 0.00022) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.10
2024-09-25 17:33:09,020 [podnet.py] => Task 1, Epoch 292/300 (LR 0.00018) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.74
2024-09-25 17:33:10,843 [podnet.py] => Task 1, Epoch 293/300 (LR 0.00013) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:33:12,701 [podnet.py] => Task 1, Epoch 294/300 (LR 0.00010) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.05
2024-09-25 17:33:14,555 [podnet.py] => Task 1, Epoch 295/300 (LR 0.00007) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.98
2024-09-25 17:33:16,302 [podnet.py] => Task 1, Epoch 296/300 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 75.00
2024-09-25 17:33:18,196 [podnet.py] => Task 1, Epoch 297/300 (LR 0.00002) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.71
2024-09-25 17:33:20,038 [podnet.py] => Task 1, Epoch 298/300 (LR 0.00001) => LSC_loss 0.04, Spatial_loss 0.08, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.86
2024-09-25 17:33:21,791 [podnet.py] => Task 1, Epoch 299/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.10, Train_acc 100.00, Test_acc 74.69
2024-09-25 17:33:23,521 [podnet.py] => Task 1, Epoch 300/300 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 0.07, Flat_loss 0.09, Train_acc 100.00, Test_acc 75.02
2024-09-25 17:33:23,523 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-09-25 17:33:23,523 [base.py] => Reducing exemplars...(100 per classes)
2024-09-25 17:33:24,681 [base.py] => Constructing exemplars...(100 per classes)
2024-09-25 17:33:26,869 [base.py] => Reducing exemplars...(71 per classes)
2024-09-25 17:33:28,025 [base.py] => Constructing exemplars...(71 per classes)
2024-09-25 17:33:30,926 [podnet.py] => Exemplar size: 497
2024-09-25 17:33:30,926 [trainer.py] => CNN: {'total': 75.02, '00-04': 67.37, '05-06': 94.17, 'old': 67.37, 'new': 94.17}
2024-09-25 17:33:30,926 [trainer.py] => NME: {'total': 77.81, '00-04': 82.43, '05-06': 66.25, 'old': 82.43, 'new': 66.25}
2024-09-25 17:33:30,926 [trainer.py] => CNN top1 curve: [89.93, 75.02]
2024-09-25 17:33:30,926 [trainer.py] => CNN top5 curve: [100.0, 98.79]
2024-09-25 17:33:30,926 [trainer.py] => NME top1 curve: [89.93, 77.81]
2024-09-25 17:33:30,926 [trainer.py] => NME top5 curve: [100.0, 98.81]

2024-09-25 17:33:30,926 [trainer.py] => Average Accuracy (CNN): 82.475
2024-09-25 17:33:30,927 [trainer.py] => Average Accuracy (NME): 83.87
2024-09-25 17:33:30,927 [trainer.py] => All params: 3879745
2024-09-25 17:33:30,927 [trainer.py] => Trainable params: 3879745
2024-09-25 17:33:30,928 [podnet.py] => Learning on 7-9
2024-09-25 17:33:30,945 [podnet.py] => Adaptive factor: 2.1213203435596424
2024-09-25 17:33:32,826 [podnet.py] => Task 2, Epoch 1/300 (LR 0.10000) => LSC_loss 0.94, Spatial_loss 0.38, Flat_loss 0.51, Train_acc 82.81, Test_acc 37.02
2024-09-25 17:33:34,722 [podnet.py] => Task 2, Epoch 2/300 (LR 0.09999) => LSC_loss 0.22, Spatial_loss 0.29, Flat_loss 0.27, Train_acc 96.89, Test_acc 50.20
2024-09-25 17:33:36,611 [podnet.py] => Task 2, Epoch 3/300 (LR 0.09998) => LSC_loss 0.15, Spatial_loss 0.24, Flat_loss 0.20, Train_acc 98.58, Test_acc 45.78
2024-09-25 17:33:38,505 [podnet.py] => Task 2, Epoch 4/300 (LR 0.09996) => LSC_loss 0.13, Spatial_loss 0.22, Flat_loss 0.18, Train_acc 99.29, Test_acc 62.24
2024-09-25 17:33:40,495 [podnet.py] => Task 2, Epoch 5/300 (LR 0.09993) => LSC_loss 0.12, Spatial_loss 0.20, Flat_loss 0.15, Train_acc 99.78, Test_acc 60.63
2024-09-25 17:33:42,344 [podnet.py] => Task 2, Epoch 6/300 (LR 0.09990) => LSC_loss 0.11, Spatial_loss 0.19, Flat_loss 0.14, Train_acc 99.76, Test_acc 65.65
2024-09-25 17:33:44,218 [podnet.py] => Task 2, Epoch 7/300 (LR 0.09987) => LSC_loss 0.10, Spatial_loss 0.19, Flat_loss 0.13, Train_acc 99.93, Test_acc 66.74
2024-09-25 17:33:46,097 [podnet.py] => Task 2, Epoch 8/300 (LR 0.09982) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.93, Test_acc 57.11
2024-09-25 17:33:47,957 [podnet.py] => Task 2, Epoch 9/300 (LR 0.09978) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.87, Test_acc 57.52
2024-09-25 17:33:49,824 [podnet.py] => Task 2, Epoch 10/300 (LR 0.09973) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.76, Test_acc 64.30
2024-09-25 17:33:51,730 [podnet.py] => Task 2, Epoch 11/300 (LR 0.09967) => LSC_loss 0.10, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 99.96, Test_acc 65.96
2024-09-25 17:33:53,668 [podnet.py] => Task 2, Epoch 12/300 (LR 0.09961) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 99.96, Test_acc 60.46
2024-09-25 17:33:55,578 [podnet.py] => Task 2, Epoch 13/300 (LR 0.09954) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 99.98, Test_acc 65.89
2024-09-25 17:33:57,489 [podnet.py] => Task 2, Epoch 14/300 (LR 0.09946) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 100.00, Test_acc 63.57
2024-09-25 17:33:59,473 [podnet.py] => Task 2, Epoch 15/300 (LR 0.09938) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.11, Train_acc 100.00, Test_acc 61.24
2024-09-25 17:34:01,285 [podnet.py] => Task 2, Epoch 16/300 (LR 0.09930) => LSC_loss 0.09, Spatial_loss 0.17, Flat_loss 0.12, Train_acc 99.87, Test_acc 44.13
2024-09-25 17:34:03,123 [podnet.py] => Task 2, Epoch 17/300 (LR 0.09921) => LSC_loss 0.09, Spatial_loss 0.18, Flat_loss 0.12, Train_acc 99.78, Test_acc 67.89
2024-09-25 17:34:05,011 [podnet.py] => Task 2, Epoch 18/300 (LR 0.09911) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 99.96, Test_acc 59.24
2024-09-25 17:34:06,940 [podnet.py] => Task 2, Epoch 19/300 (LR 0.09901) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 99.98, Test_acc 60.04
2024-09-25 17:34:08,826 [podnet.py] => Task 2, Epoch 20/300 (LR 0.09891) => LSC_loss 0.09, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 99.91, Test_acc 67.74
2024-09-25 17:34:10,752 [podnet.py] => Task 2, Epoch 21/300 (LR 0.09880) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 65.67
2024-09-25 17:34:12,583 [podnet.py] => Task 2, Epoch 22/300 (LR 0.09868) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 66.28
2024-09-25 17:34:14,447 [podnet.py] => Task 2, Epoch 23/300 (LR 0.09856) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.89
2024-09-25 17:34:16,360 [podnet.py] => Task 2, Epoch 24/300 (LR 0.09843) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 64.85
2024-09-25 17:34:18,290 [podnet.py] => Task 2, Epoch 25/300 (LR 0.09830) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 52.39
2024-09-25 17:34:20,185 [podnet.py] => Task 2, Epoch 26/300 (LR 0.09816) => LSC_loss 0.11, Spatial_loss 0.18, Flat_loss 0.13, Train_acc 99.02, Test_acc 68.54
2024-09-25 17:34:22,106 [podnet.py] => Task 2, Epoch 27/300 (LR 0.09801) => LSC_loss 0.08, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 99.93, Test_acc 56.00
2024-09-25 17:34:23,944 [podnet.py] => Task 2, Epoch 28/300 (LR 0.09787) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 100.00, Test_acc 68.56
2024-09-25 17:34:25,825 [podnet.py] => Task 2, Epoch 29/300 (LR 0.09771) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.94
2024-09-25 17:34:27,733 [podnet.py] => Task 2, Epoch 30/300 (LR 0.09755) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.19
2024-09-25 17:34:29,611 [podnet.py] => Task 2, Epoch 31/300 (LR 0.09739) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.19
2024-09-25 17:34:31,541 [podnet.py] => Task 2, Epoch 32/300 (LR 0.09722) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 68.48
2024-09-25 17:34:33,370 [podnet.py] => Task 2, Epoch 33/300 (LR 0.09704) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 66.81
2024-09-25 17:34:35,349 [podnet.py] => Task 2, Epoch 34/300 (LR 0.09686) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 99.96, Test_acc 67.48
2024-09-25 17:34:37,345 [podnet.py] => Task 2, Epoch 35/300 (LR 0.09668) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.98, Test_acc 63.63
2024-09-25 17:34:39,183 [podnet.py] => Task 2, Epoch 36/300 (LR 0.09649) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 100.00, Test_acc 64.63
2024-09-25 17:34:41,021 [podnet.py] => Task 2, Epoch 37/300 (LR 0.09629) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 65.70
2024-09-25 17:34:42,861 [podnet.py] => Task 2, Epoch 38/300 (LR 0.09609) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 62.93
2024-09-25 17:34:44,685 [podnet.py] => Task 2, Epoch 39/300 (LR 0.09589) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.20
2024-09-25 17:34:46,566 [podnet.py] => Task 2, Epoch 40/300 (LR 0.09568) => LSC_loss 0.08, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.98, Test_acc 33.56
2024-09-25 17:34:48,447 [podnet.py] => Task 2, Epoch 41/300 (LR 0.09546) => LSC_loss 0.15, Spatial_loss 0.23, Flat_loss 0.15, Train_acc 97.89, Test_acc 29.81
2024-09-25 17:34:50,288 [podnet.py] => Task 2, Epoch 42/300 (LR 0.09524) => LSC_loss 0.12, Spatial_loss 0.22, Flat_loss 0.15, Train_acc 99.04, Test_acc 65.72
2024-09-25 17:34:52,083 [podnet.py] => Task 2, Epoch 43/300 (LR 0.09502) => LSC_loss 0.07, Spatial_loss 0.16, Flat_loss 0.11, Train_acc 100.00, Test_acc 63.15
2024-09-25 17:34:53,992 [podnet.py] => Task 2, Epoch 44/300 (LR 0.09479) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 99.98, Test_acc 56.89
2024-09-25 17:34:55,847 [podnet.py] => Task 2, Epoch 45/300 (LR 0.09455) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.98, Test_acc 64.30
2024-09-25 17:34:57,780 [podnet.py] => Task 2, Epoch 46/300 (LR 0.09431) => LSC_loss 0.07, Spatial_loss 0.15, Flat_loss 0.10, Train_acc 99.98, Test_acc 54.39
2024-09-25 17:34:59,625 [podnet.py] => Task 2, Epoch 47/300 (LR 0.09407) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.96, Test_acc 63.20
2024-09-25 17:35:01,488 [podnet.py] => Task 2, Epoch 48/300 (LR 0.09382) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.69, Test_acc 50.24
2024-09-25 17:35:03,409 [podnet.py] => Task 2, Epoch 49/300 (LR 0.09356) => LSC_loss 0.08, Spatial_loss 0.15, Flat_loss 0.11, Train_acc 99.80, Test_acc 65.06
2024-09-25 17:35:05,259 [podnet.py] => Task 2, Epoch 50/300 (LR 0.09330) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.02
2024-09-25 17:35:07,108 [podnet.py] => Task 2, Epoch 51/300 (LR 0.09304) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 61.76
2024-09-25 17:35:09,008 [podnet.py] => Task 2, Epoch 52/300 (LR 0.09277) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 100.00, Test_acc 67.94
2024-09-25 17:35:10,875 [podnet.py] => Task 2, Epoch 53/300 (LR 0.09249) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.56
2024-09-25 17:35:12,691 [podnet.py] => Task 2, Epoch 54/300 (LR 0.09222) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.93
2024-09-25 17:35:14,626 [podnet.py] => Task 2, Epoch 55/300 (LR 0.09193) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.98
2024-09-25 17:35:16,583 [podnet.py] => Task 2, Epoch 56/300 (LR 0.09165) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.63
2024-09-25 17:35:18,560 [podnet.py] => Task 2, Epoch 57/300 (LR 0.09135) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.07
2024-09-25 17:35:20,496 [podnet.py] => Task 2, Epoch 58/300 (LR 0.09106) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.31
2024-09-25 17:35:22,435 [podnet.py] => Task 2, Epoch 59/300 (LR 0.09076) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.85
2024-09-25 17:35:24,465 [podnet.py] => Task 2, Epoch 60/300 (LR 0.09045) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.09, Train_acc 100.00, Test_acc 39.00
2024-09-25 17:35:26,289 [podnet.py] => Task 2, Epoch 61/300 (LR 0.09014) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.89, Test_acc 66.87
2024-09-25 17:35:28,209 [podnet.py] => Task 2, Epoch 62/300 (LR 0.08983) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.65
2024-09-25 17:35:30,107 [podnet.py] => Task 2, Epoch 63/300 (LR 0.08951) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.96
2024-09-25 17:35:31,998 [podnet.py] => Task 2, Epoch 64/300 (LR 0.08918) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.93
2024-09-25 17:35:33,924 [podnet.py] => Task 2, Epoch 65/300 (LR 0.08886) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.98, Test_acc 67.39
2024-09-25 17:35:35,799 [podnet.py] => Task 2, Epoch 66/300 (LR 0.08853) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 56.89
2024-09-25 17:35:37,878 [podnet.py] => Task 2, Epoch 67/300 (LR 0.08819) => LSC_loss 0.07, Spatial_loss 0.14, Flat_loss 0.10, Train_acc 99.82, Test_acc 62.48
2024-09-25 17:35:39,801 [podnet.py] => Task 2, Epoch 68/300 (LR 0.08785) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.57
2024-09-25 17:35:41,756 [podnet.py] => Task 2, Epoch 69/300 (LR 0.08751) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.50
2024-09-25 17:35:43,656 [podnet.py] => Task 2, Epoch 70/300 (LR 0.08716) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.30
2024-09-25 17:35:45,588 [podnet.py] => Task 2, Epoch 71/300 (LR 0.08680) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.76
2024-09-25 17:35:47,557 [podnet.py] => Task 2, Epoch 72/300 (LR 0.08645) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.59
2024-09-25 17:35:49,428 [podnet.py] => Task 2, Epoch 73/300 (LR 0.08609) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 63.52
2024-09-25 17:35:51,332 [podnet.py] => Task 2, Epoch 74/300 (LR 0.08572) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.98
2024-09-25 17:35:53,272 [podnet.py] => Task 2, Epoch 75/300 (LR 0.08536) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.59
2024-09-25 17:35:55,272 [podnet.py] => Task 2, Epoch 76/300 (LR 0.08498) => LSC_loss 0.07, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 61.33
2024-09-25 17:35:57,294 [podnet.py] => Task 2, Epoch 77/300 (LR 0.08461) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.98, Test_acc 65.50
2024-09-25 17:35:59,146 [podnet.py] => Task 2, Epoch 78/300 (LR 0.08423) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.76
2024-09-25 17:36:01,055 [podnet.py] => Task 2, Epoch 79/300 (LR 0.08384) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.24
2024-09-25 17:36:02,942 [podnet.py] => Task 2, Epoch 80/300 (LR 0.08346) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.96, Test_acc 64.17
2024-09-25 17:36:04,791 [podnet.py] => Task 2, Epoch 81/300 (LR 0.08307) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 65.91
2024-09-25 17:36:06,777 [podnet.py] => Task 2, Epoch 82/300 (LR 0.08267) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 60.19
2024-09-25 17:36:08,714 [podnet.py] => Task 2, Epoch 83/300 (LR 0.08227) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.96, Test_acc 68.31
2024-09-25 17:36:10,602 [podnet.py] => Task 2, Epoch 84/300 (LR 0.08187) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.96
2024-09-25 17:36:12,509 [podnet.py] => Task 2, Epoch 85/300 (LR 0.08147) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 66.89
2024-09-25 17:36:14,467 [podnet.py] => Task 2, Epoch 86/300 (LR 0.08106) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.93, Test_acc 67.13
2024-09-25 17:36:16,337 [podnet.py] => Task 2, Epoch 87/300 (LR 0.08065) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 68.30
2024-09-25 17:36:18,162 [podnet.py] => Task 2, Epoch 88/300 (LR 0.08023) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.89
2024-09-25 17:36:20,038 [podnet.py] => Task 2, Epoch 89/300 (LR 0.07981) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 62.24
2024-09-25 17:36:21,839 [podnet.py] => Task 2, Epoch 90/300 (LR 0.07939) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 64.80
2024-09-25 17:36:23,659 [podnet.py] => Task 2, Epoch 91/300 (LR 0.07896) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 100.00, Test_acc 67.19
2024-09-25 17:36:25,602 [podnet.py] => Task 2, Epoch 92/300 (LR 0.07854) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.67
2024-09-25 17:36:27,498 [podnet.py] => Task 2, Epoch 93/300 (LR 0.07810) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 100.00, Test_acc 60.07
2024-09-25 17:36:29,392 [podnet.py] => Task 2, Epoch 94/300 (LR 0.07767) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.96, Test_acc 66.91
2024-09-25 17:36:31,211 [podnet.py] => Task 2, Epoch 95/300 (LR 0.07723) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.26
2024-09-25 17:36:33,070 [podnet.py] => Task 2, Epoch 96/300 (LR 0.07679) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.26
2024-09-25 17:36:34,955 [podnet.py] => Task 2, Epoch 97/300 (LR 0.07635) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.37
2024-09-25 17:36:36,897 [podnet.py] => Task 2, Epoch 98/300 (LR 0.07590) => LSC_loss 0.07, Spatial_loss 0.13, Flat_loss 0.09, Train_acc 99.80, Test_acc 67.80
2024-09-25 17:36:38,912 [podnet.py] => Task 2, Epoch 99/300 (LR 0.07545) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.30
2024-09-25 17:36:40,954 [podnet.py] => Task 2, Epoch 100/300 (LR 0.07500) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.63
2024-09-25 17:36:42,894 [podnet.py] => Task 2, Epoch 101/300 (LR 0.07455) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.89
2024-09-25 17:36:44,791 [podnet.py] => Task 2, Epoch 102/300 (LR 0.07409) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.93
2024-09-25 17:36:46,699 [podnet.py] => Task 2, Epoch 103/300 (LR 0.07363) => LSC_loss 0.06, Spatial_loss 0.13, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.22
2024-09-25 17:36:48,637 [podnet.py] => Task 2, Epoch 104/300 (LR 0.07316) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.11
2024-09-25 17:36:50,546 [podnet.py] => Task 2, Epoch 105/300 (LR 0.07270) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 60.02
2024-09-25 17:36:52,401 [podnet.py] => Task 2, Epoch 106/300 (LR 0.07223) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.70
2024-09-25 17:36:54,345 [podnet.py] => Task 2, Epoch 107/300 (LR 0.07176) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 99.98, Test_acc 67.56
2024-09-25 17:36:56,204 [podnet.py] => Task 2, Epoch 108/300 (LR 0.07129) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.87
2024-09-25 17:36:58,023 [podnet.py] => Task 2, Epoch 109/300 (LR 0.07081) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 99.98, Test_acc 67.00
2024-09-25 17:36:59,879 [podnet.py] => Task 2, Epoch 110/300 (LR 0.07034) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.19
2024-09-25 17:37:01,751 [podnet.py] => Task 2, Epoch 111/300 (LR 0.06986) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.94
2024-09-25 17:37:03,625 [podnet.py] => Task 2, Epoch 112/300 (LR 0.06938) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.26
2024-09-25 17:37:05,527 [podnet.py] => Task 2, Epoch 113/300 (LR 0.06889) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.93
2024-09-25 17:37:07,452 [podnet.py] => Task 2, Epoch 114/300 (LR 0.06841) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 62.09
2024-09-25 17:37:09,300 [podnet.py] => Task 2, Epoch 115/300 (LR 0.06792) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 99.98, Test_acc 65.93
2024-09-25 17:37:11,219 [podnet.py] => Task 2, Epoch 116/300 (LR 0.06743) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.80
2024-09-25 17:37:13,093 [podnet.py] => Task 2, Epoch 117/300 (LR 0.06694) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.28
2024-09-25 17:37:15,084 [podnet.py] => Task 2, Epoch 118/300 (LR 0.06644) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.50
2024-09-25 17:37:16,955 [podnet.py] => Task 2, Epoch 119/300 (LR 0.06595) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.74
2024-09-25 17:37:18,786 [podnet.py] => Task 2, Epoch 120/300 (LR 0.06545) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.89
2024-09-25 17:37:20,682 [podnet.py] => Task 2, Epoch 121/300 (LR 0.06495) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.70
2024-09-25 17:37:22,523 [podnet.py] => Task 2, Epoch 122/300 (LR 0.06445) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 99.98, Test_acc 64.83
2024-09-25 17:37:24,494 [podnet.py] => Task 2, Epoch 123/300 (LR 0.06395) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.83
2024-09-25 17:37:26,313 [podnet.py] => Task 2, Epoch 124/300 (LR 0.06345) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.35
2024-09-25 17:37:28,164 [podnet.py] => Task 2, Epoch 125/300 (LR 0.06294) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.04
2024-09-25 17:37:30,030 [podnet.py] => Task 2, Epoch 126/300 (LR 0.06243) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.30
2024-09-25 17:37:31,936 [podnet.py] => Task 2, Epoch 127/300 (LR 0.06193) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.52
2024-09-25 17:37:33,792 [podnet.py] => Task 2, Epoch 128/300 (LR 0.06142) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.80
2024-09-25 17:37:35,661 [podnet.py] => Task 2, Epoch 129/300 (LR 0.06091) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 61.80
2024-09-25 17:37:37,511 [podnet.py] => Task 2, Epoch 130/300 (LR 0.06040) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 99.98, Test_acc 68.35
2024-09-25 17:37:39,420 [podnet.py] => Task 2, Epoch 131/300 (LR 0.05988) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.22
2024-09-25 17:37:41,373 [podnet.py] => Task 2, Epoch 132/300 (LR 0.05937) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.28
2024-09-25 17:37:43,245 [podnet.py] => Task 2, Epoch 133/300 (LR 0.05885) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.61
2024-09-25 17:37:45,170 [podnet.py] => Task 2, Epoch 134/300 (LR 0.05834) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.81
2024-09-25 17:37:47,138 [podnet.py] => Task 2, Epoch 135/300 (LR 0.05782) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.35
2024-09-25 17:37:48,997 [podnet.py] => Task 2, Epoch 136/300 (LR 0.05730) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.31
2024-09-25 17:37:50,871 [podnet.py] => Task 2, Epoch 137/300 (LR 0.05679) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.69
2024-09-25 17:37:52,725 [podnet.py] => Task 2, Epoch 138/300 (LR 0.05627) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.37
2024-09-25 17:37:54,606 [podnet.py] => Task 2, Epoch 139/300 (LR 0.05575) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 47.87
2024-09-25 17:37:56,535 [podnet.py] => Task 2, Epoch 140/300 (LR 0.05523) => LSC_loss 0.06, Spatial_loss 0.12, Flat_loss 0.09, Train_acc 99.98, Test_acc 67.35
2024-09-25 17:37:58,562 [podnet.py] => Task 2, Epoch 141/300 (LR 0.05471) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 99.98, Test_acc 67.81
2024-09-25 17:38:00,614 [podnet.py] => Task 2, Epoch 142/300 (LR 0.05418) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.48
2024-09-25 17:38:02,585 [podnet.py] => Task 2, Epoch 143/300 (LR 0.05366) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.48
2024-09-25 17:38:04,493 [podnet.py] => Task 2, Epoch 144/300 (LR 0.05314) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.54
2024-09-25 17:38:06,376 [podnet.py] => Task 2, Epoch 145/300 (LR 0.05262) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.04
2024-09-25 17:38:08,337 [podnet.py] => Task 2, Epoch 146/300 (LR 0.05209) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.09
2024-09-25 17:38:10,221 [podnet.py] => Task 2, Epoch 147/300 (LR 0.05157) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.59
2024-09-25 17:38:12,200 [podnet.py] => Task 2, Epoch 148/300 (LR 0.05105) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 99.98, Test_acc 64.56
2024-09-25 17:38:14,144 [podnet.py] => Task 2, Epoch 149/300 (LR 0.05052) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.63
2024-09-25 17:38:16,098 [podnet.py] => Task 2, Epoch 150/300 (LR 0.05000) => LSC_loss 0.06, Spatial_loss 0.11, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.22
2024-09-25 17:38:18,117 [podnet.py] => Task 2, Epoch 151/300 (LR 0.04948) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.96
2024-09-25 17:38:20,069 [podnet.py] => Task 2, Epoch 152/300 (LR 0.04895) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.44
2024-09-25 17:38:22,082 [podnet.py] => Task 2, Epoch 153/300 (LR 0.04843) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.94
2024-09-25 17:38:24,028 [podnet.py] => Task 2, Epoch 154/300 (LR 0.04791) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.46
2024-09-25 17:38:25,990 [podnet.py] => Task 2, Epoch 155/300 (LR 0.04738) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.56
2024-09-25 17:38:28,041 [podnet.py] => Task 2, Epoch 156/300 (LR 0.04686) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.50
2024-09-25 17:38:29,995 [podnet.py] => Task 2, Epoch 157/300 (LR 0.04634) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.91
2024-09-25 17:38:31,975 [podnet.py] => Task 2, Epoch 158/300 (LR 0.04582) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 99.98, Test_acc 65.63
2024-09-25 17:38:33,909 [podnet.py] => Task 2, Epoch 159/300 (LR 0.04529) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.69
2024-09-25 17:38:35,873 [podnet.py] => Task 2, Epoch 160/300 (LR 0.04477) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.59
2024-09-25 17:38:37,884 [podnet.py] => Task 2, Epoch 161/300 (LR 0.04425) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 99.98, Test_acc 63.24
2024-09-25 17:38:39,821 [podnet.py] => Task 2, Epoch 162/300 (LR 0.04373) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.44
2024-09-25 17:38:41,810 [podnet.py] => Task 2, Epoch 163/300 (LR 0.04321) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.22
2024-09-25 17:38:43,925 [podnet.py] => Task 2, Epoch 164/300 (LR 0.04270) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 63.41
2024-09-25 17:38:45,854 [podnet.py] => Task 2, Epoch 165/300 (LR 0.04218) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.65
2024-09-25 17:38:47,772 [podnet.py] => Task 2, Epoch 166/300 (LR 0.04166) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.74
2024-09-25 17:38:49,664 [podnet.py] => Task 2, Epoch 167/300 (LR 0.04115) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.96
2024-09-25 17:38:51,608 [podnet.py] => Task 2, Epoch 168/300 (LR 0.04063) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.20
2024-09-25 17:38:53,545 [podnet.py] => Task 2, Epoch 169/300 (LR 0.04012) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.00
2024-09-25 17:38:55,634 [podnet.py] => Task 2, Epoch 170/300 (LR 0.03960) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.13
2024-09-25 17:38:57,627 [podnet.py] => Task 2, Epoch 171/300 (LR 0.03909) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.78
2024-09-25 17:38:59,640 [podnet.py] => Task 2, Epoch 172/300 (LR 0.03858) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.39
2024-09-25 17:39:01,606 [podnet.py] => Task 2, Epoch 173/300 (LR 0.03807) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 68.20
2024-09-25 17:39:03,566 [podnet.py] => Task 2, Epoch 174/300 (LR 0.03757) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.41
2024-09-25 17:39:05,539 [podnet.py] => Task 2, Epoch 175/300 (LR 0.03706) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.48
2024-09-25 17:39:07,418 [podnet.py] => Task 2, Epoch 176/300 (LR 0.03655) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.70
2024-09-25 17:39:09,441 [podnet.py] => Task 2, Epoch 177/300 (LR 0.03605) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 64.78
2024-09-25 17:39:11,468 [podnet.py] => Task 2, Epoch 178/300 (LR 0.03555) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.89
2024-09-25 17:39:13,364 [podnet.py] => Task 2, Epoch 179/300 (LR 0.03505) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.09
2024-09-25 17:39:15,366 [podnet.py] => Task 2, Epoch 180/300 (LR 0.03455) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.48
2024-09-25 17:39:17,383 [podnet.py] => Task 2, Epoch 181/300 (LR 0.03405) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:39:19,476 [podnet.py] => Task 2, Epoch 182/300 (LR 0.03356) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.08, Train_acc 100.00, Test_acc 66.96
2024-09-25 17:39:21,475 [podnet.py] => Task 2, Epoch 183/300 (LR 0.03306) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.08, Train_acc 100.00, Test_acc 65.28
2024-09-25 17:39:23,461 [podnet.py] => Task 2, Epoch 184/300 (LR 0.03257) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.65
2024-09-25 17:39:25,560 [podnet.py] => Task 2, Epoch 185/300 (LR 0.03208) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.06
2024-09-25 17:39:27,510 [podnet.py] => Task 2, Epoch 186/300 (LR 0.03159) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 69.59
2024-09-25 17:39:29,458 [podnet.py] => Task 2, Epoch 187/300 (LR 0.03111) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.61
2024-09-25 17:39:31,430 [podnet.py] => Task 2, Epoch 188/300 (LR 0.03062) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.28
2024-09-25 17:39:33,319 [podnet.py] => Task 2, Epoch 189/300 (LR 0.03014) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.44
2024-09-25 17:39:35,186 [podnet.py] => Task 2, Epoch 190/300 (LR 0.02966) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.52
2024-09-25 17:39:37,050 [podnet.py] => Task 2, Epoch 191/300 (LR 0.02919) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.63
2024-09-25 17:39:39,059 [podnet.py] => Task 2, Epoch 192/300 (LR 0.02871) => LSC_loss 0.06, Spatial_loss 0.10, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.83
2024-09-25 17:39:41,036 [podnet.py] => Task 2, Epoch 193/300 (LR 0.02824) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.07
2024-09-25 17:39:43,024 [podnet.py] => Task 2, Epoch 194/300 (LR 0.02777) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.06
2024-09-25 17:39:45,044 [podnet.py] => Task 2, Epoch 195/300 (LR 0.02730) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.06
2024-09-25 17:39:46,972 [podnet.py] => Task 2, Epoch 196/300 (LR 0.02684) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.61
2024-09-25 17:39:48,910 [podnet.py] => Task 2, Epoch 197/300 (LR 0.02637) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.35
2024-09-25 17:39:50,865 [podnet.py] => Task 2, Epoch 198/300 (LR 0.02591) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.94
2024-09-25 17:39:52,706 [podnet.py] => Task 2, Epoch 199/300 (LR 0.02545) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.26
2024-09-25 17:39:54,623 [podnet.py] => Task 2, Epoch 200/300 (LR 0.02500) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.54
2024-09-25 17:39:56,617 [podnet.py] => Task 2, Epoch 201/300 (LR 0.02455) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.89
2024-09-25 17:39:58,498 [podnet.py] => Task 2, Epoch 202/300 (LR 0.02410) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.37
2024-09-25 17:40:00,450 [podnet.py] => Task 2, Epoch 203/300 (LR 0.02365) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.50
2024-09-25 17:40:02,374 [podnet.py] => Task 2, Epoch 204/300 (LR 0.02321) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.80
2024-09-25 17:40:04,293 [podnet.py] => Task 2, Epoch 205/300 (LR 0.02277) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 65.44
2024-09-25 17:40:06,289 [podnet.py] => Task 2, Epoch 206/300 (LR 0.02233) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.26
2024-09-25 17:40:08,262 [podnet.py] => Task 2, Epoch 207/300 (LR 0.02190) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.33
2024-09-25 17:40:10,306 [podnet.py] => Task 2, Epoch 208/300 (LR 0.02146) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 65.87
2024-09-25 17:40:12,227 [podnet.py] => Task 2, Epoch 209/300 (LR 0.02104) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.96
2024-09-25 17:40:14,091 [podnet.py] => Task 2, Epoch 210/300 (LR 0.02061) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.44
2024-09-25 17:40:15,969 [podnet.py] => Task 2, Epoch 211/300 (LR 0.02019) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.39
2024-09-25 17:40:17,788 [podnet.py] => Task 2, Epoch 212/300 (LR 0.01977) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.19
2024-09-25 17:40:19,676 [podnet.py] => Task 2, Epoch 213/300 (LR 0.01935) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:40:21,600 [podnet.py] => Task 2, Epoch 214/300 (LR 0.01894) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.85
2024-09-25 17:40:23,511 [podnet.py] => Task 2, Epoch 215/300 (LR 0.01853) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 65.69
2024-09-25 17:40:25,346 [podnet.py] => Task 2, Epoch 216/300 (LR 0.01813) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.65
2024-09-25 17:40:27,269 [podnet.py] => Task 2, Epoch 217/300 (LR 0.01773) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.20
2024-09-25 17:40:29,116 [podnet.py] => Task 2, Epoch 218/300 (LR 0.01733) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.57
2024-09-25 17:40:31,081 [podnet.py] => Task 2, Epoch 219/300 (LR 0.01693) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.65
2024-09-25 17:40:32,940 [podnet.py] => Task 2, Epoch 220/300 (LR 0.01654) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.37
2024-09-25 17:40:34,803 [podnet.py] => Task 2, Epoch 221/300 (LR 0.01616) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.39
2024-09-25 17:40:36,679 [podnet.py] => Task 2, Epoch 222/300 (LR 0.01577) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.70
2024-09-25 17:40:38,540 [podnet.py] => Task 2, Epoch 223/300 (LR 0.01539) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.81
2024-09-25 17:40:40,374 [podnet.py] => Task 2, Epoch 224/300 (LR 0.01502) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.87
2024-09-25 17:40:42,212 [podnet.py] => Task 2, Epoch 225/300 (LR 0.01464) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.61
2024-09-25 17:40:44,075 [podnet.py] => Task 2, Epoch 226/300 (LR 0.01428) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:40:45,879 [podnet.py] => Task 2, Epoch 227/300 (LR 0.01391) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:40:47,778 [podnet.py] => Task 2, Epoch 228/300 (LR 0.01355) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.56
2024-09-25 17:40:49,771 [podnet.py] => Task 2, Epoch 229/300 (LR 0.01320) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.30
2024-09-25 17:40:51,632 [podnet.py] => Task 2, Epoch 230/300 (LR 0.01284) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.78
2024-09-25 17:40:53,576 [podnet.py] => Task 2, Epoch 231/300 (LR 0.01249) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:40:55,467 [podnet.py] => Task 2, Epoch 232/300 (LR 0.01215) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.50
2024-09-25 17:40:57,411 [podnet.py] => Task 2, Epoch 233/300 (LR 0.01181) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.09
2024-09-25 17:40:59,194 [podnet.py] => Task 2, Epoch 234/300 (LR 0.01147) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.89
2024-09-25 17:41:01,063 [podnet.py] => Task 2, Epoch 235/300 (LR 0.01114) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.52
2024-09-25 17:41:02,920 [podnet.py] => Task 2, Epoch 236/300 (LR 0.01082) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.35
2024-09-25 17:41:04,755 [podnet.py] => Task 2, Epoch 237/300 (LR 0.01049) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:41:06,647 [podnet.py] => Task 2, Epoch 238/300 (LR 0.01017) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.11
2024-09-25 17:41:08,609 [podnet.py] => Task 2, Epoch 239/300 (LR 0.00986) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 68.00
2024-09-25 17:41:10,528 [podnet.py] => Task 2, Epoch 240/300 (LR 0.00955) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.91
2024-09-25 17:41:12,424 [podnet.py] => Task 2, Epoch 241/300 (LR 0.00924) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.93
2024-09-25 17:41:14,372 [podnet.py] => Task 2, Epoch 242/300 (LR 0.00894) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.28
2024-09-25 17:41:16,325 [podnet.py] => Task 2, Epoch 243/300 (LR 0.00865) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.37
2024-09-25 17:41:18,281 [podnet.py] => Task 2, Epoch 244/300 (LR 0.00835) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.83
2024-09-25 17:41:20,267 [podnet.py] => Task 2, Epoch 245/300 (LR 0.00807) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.02
2024-09-25 17:41:22,102 [podnet.py] => Task 2, Epoch 246/300 (LR 0.00778) => LSC_loss 0.05, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.04
2024-09-25 17:41:23,990 [podnet.py] => Task 2, Epoch 247/300 (LR 0.00751) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.20
2024-09-25 17:41:25,843 [podnet.py] => Task 2, Epoch 248/300 (LR 0.00723) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.76
2024-09-25 17:41:27,626 [podnet.py] => Task 2, Epoch 249/300 (LR 0.00696) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.30
2024-09-25 17:41:29,422 [podnet.py] => Task 2, Epoch 250/300 (LR 0.00670) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.26
2024-09-25 17:41:31,323 [podnet.py] => Task 2, Epoch 251/300 (LR 0.00644) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.11
2024-09-25 17:41:33,324 [podnet.py] => Task 2, Epoch 252/300 (LR 0.00618) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.06
2024-09-25 17:41:35,213 [podnet.py] => Task 2, Epoch 253/300 (LR 0.00593) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.17
2024-09-25 17:41:37,139 [podnet.py] => Task 2, Epoch 254/300 (LR 0.00569) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.07
2024-09-25 17:41:39,029 [podnet.py] => Task 2, Epoch 255/300 (LR 0.00545) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.33
2024-09-25 17:41:40,902 [podnet.py] => Task 2, Epoch 256/300 (LR 0.00521) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:41:42,798 [podnet.py] => Task 2, Epoch 257/300 (LR 0.00498) => LSC_loss 0.06, Spatial_loss 0.09, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.81
2024-09-25 17:41:44,634 [podnet.py] => Task 2, Epoch 258/300 (LR 0.00476) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.87
2024-09-25 17:41:46,509 [podnet.py] => Task 2, Epoch 259/300 (LR 0.00454) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 65.80
2024-09-25 17:41:48,508 [podnet.py] => Task 2, Epoch 260/300 (LR 0.00432) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.19
2024-09-25 17:41:50,446 [podnet.py] => Task 2, Epoch 261/300 (LR 0.00411) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.28
2024-09-25 17:41:52,309 [podnet.py] => Task 2, Epoch 262/300 (LR 0.00391) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.17
2024-09-25 17:41:54,148 [podnet.py] => Task 2, Epoch 263/300 (LR 0.00371) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.31
2024-09-25 17:41:56,018 [podnet.py] => Task 2, Epoch 264/300 (LR 0.00351) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:41:58,002 [podnet.py] => Task 2, Epoch 265/300 (LR 0.00332) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.07
2024-09-25 17:41:59,857 [podnet.py] => Task 2, Epoch 266/300 (LR 0.00314) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.37
2024-09-25 17:42:01,753 [podnet.py] => Task 2, Epoch 267/300 (LR 0.00296) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.56
2024-09-25 17:42:03,613 [podnet.py] => Task 2, Epoch 268/300 (LR 0.00278) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.52
2024-09-25 17:42:05,485 [podnet.py] => Task 2, Epoch 269/300 (LR 0.00261) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.15
2024-09-25 17:42:07,402 [podnet.py] => Task 2, Epoch 270/300 (LR 0.00245) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.09
2024-09-25 17:42:09,302 [podnet.py] => Task 2, Epoch 271/300 (LR 0.00229) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.04
2024-09-25 17:42:11,268 [podnet.py] => Task 2, Epoch 272/300 (LR 0.00213) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.72
2024-09-25 17:42:13,110 [podnet.py] => Task 2, Epoch 273/300 (LR 0.00199) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.33
2024-09-25 17:42:15,018 [podnet.py] => Task 2, Epoch 274/300 (LR 0.00184) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.26
2024-09-25 17:42:16,986 [podnet.py] => Task 2, Epoch 275/300 (LR 0.00170) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.33
2024-09-25 17:42:18,808 [podnet.py] => Task 2, Epoch 276/300 (LR 0.00157) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.56
2024-09-25 17:42:20,638 [podnet.py] => Task 2, Epoch 277/300 (LR 0.00144) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.19
2024-09-25 17:42:22,499 [podnet.py] => Task 2, Epoch 278/300 (LR 0.00132) => LSC_loss 0.06, Spatial_loss 0.07, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.50
2024-09-25 17:42:24,424 [podnet.py] => Task 2, Epoch 279/300 (LR 0.00120) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.09
2024-09-25 17:42:26,251 [podnet.py] => Task 2, Epoch 280/300 (LR 0.00109) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.06
2024-09-25 17:42:28,052 [podnet.py] => Task 2, Epoch 281/300 (LR 0.00099) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.98
2024-09-25 17:42:29,903 [podnet.py] => Task 2, Epoch 282/300 (LR 0.00089) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.17
2024-09-25 17:42:31,812 [podnet.py] => Task 2, Epoch 283/300 (LR 0.00079) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.37
2024-09-25 17:42:33,634 [podnet.py] => Task 2, Epoch 284/300 (LR 0.00070) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.43
2024-09-25 17:42:35,507 [podnet.py] => Task 2, Epoch 285/300 (LR 0.00062) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.57
2024-09-25 17:42:37,439 [podnet.py] => Task 2, Epoch 286/300 (LR 0.00054) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.20
2024-09-25 17:42:39,318 [podnet.py] => Task 2, Epoch 287/300 (LR 0.00046) => LSC_loss 0.06, Spatial_loss 0.07, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.91
2024-09-25 17:42:41,250 [podnet.py] => Task 2, Epoch 288/300 (LR 0.00039) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.65
2024-09-25 17:42:43,061 [podnet.py] => Task 2, Epoch 289/300 (LR 0.00033) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.00
2024-09-25 17:42:44,929 [podnet.py] => Task 2, Epoch 290/300 (LR 0.00027) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 66.96
2024-09-25 17:42:46,776 [podnet.py] => Task 2, Epoch 291/300 (LR 0.00022) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.24
2024-09-25 17:42:48,604 [podnet.py] => Task 2, Epoch 292/300 (LR 0.00018) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.24
2024-09-25 17:42:50,500 [podnet.py] => Task 2, Epoch 293/300 (LR 0.00013) => LSC_loss 0.05, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.31
2024-09-25 17:42:52,309 [podnet.py] => Task 2, Epoch 294/300 (LR 0.00010) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.24
2024-09-25 17:42:54,143 [podnet.py] => Task 2, Epoch 295/300 (LR 0.00007) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.44
2024-09-25 17:42:56,112 [podnet.py] => Task 2, Epoch 296/300 (LR 0.00004) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.30
2024-09-25 17:42:58,093 [podnet.py] => Task 2, Epoch 297/300 (LR 0.00002) => LSC_loss 0.05, Spatial_loss 0.07, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.52
2024-09-25 17:43:00,030 [podnet.py] => Task 2, Epoch 298/300 (LR 0.00001) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.94
2024-09-25 17:43:02,099 [podnet.py] => Task 2, Epoch 299/300 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 0.08, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.67
2024-09-25 17:43:03,896 [podnet.py] => Task 2, Epoch 300/300 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 0.07, Flat_loss 0.07, Train_acc 100.00, Test_acc 67.67
2024-09-25 17:43:03,898 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2024-09-25 17:43:03,898 [base.py] => Reducing exemplars...(71 per classes)
2024-09-25 17:43:05,585 [base.py] => Constructing exemplars...(71 per classes)
2024-09-25 17:43:07,671 [base.py] => Reducing exemplars...(55 per classes)
2024-09-25 17:43:09,550 [base.py] => Constructing exemplars...(55 per classes)
2024-09-25 17:43:12,542 [podnet.py] => Exemplar size: 495
2024-09-25 17:43:12,543 [trainer.py] => CNN: {'total': 67.67, '00-04': 57.13, '05-06': 63.08, '07-08': 98.58, 'old': 58.83, 'new': 98.58}
2024-09-25 17:43:12,543 [trainer.py] => NME: {'total': 73.54, '00-04': 77.03, '05-06': 51.0, '07-08': 87.33, 'old': 69.6, 'new': 87.33}
2024-09-25 17:43:12,543 [trainer.py] => CNN top1 curve: [89.93, 75.02, 67.67]
2024-09-25 17:43:12,543 [trainer.py] => CNN top5 curve: [100.0, 98.79, 94.85]
2024-09-25 17:43:12,543 [trainer.py] => NME top1 curve: [89.93, 77.81, 73.54]
2024-09-25 17:43:12,543 [trainer.py] => NME top5 curve: [100.0, 98.81, 96.11]

2024-09-25 17:43:12,543 [trainer.py] => Average Accuracy (CNN): 77.54
2024-09-25 17:43:12,543 [trainer.py] => Average Accuracy (NME): 80.42666666666668
2024-09-25 17:43:12,544 [trainer.py] => Forgetting (CNN): 31.945000000000004
