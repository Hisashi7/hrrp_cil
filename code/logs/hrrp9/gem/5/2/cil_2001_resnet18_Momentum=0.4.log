2024-10-28 09:20:22,023 [trainer.py] => config: ./exps/gem.json
2024-10-28 09:20:22,023 [trainer.py] => prefix: cil
2024-10-28 09:20:22,023 [trainer.py] => dataset: hrrp9
2024-10-28 09:20:22,023 [trainer.py] => memory_size: 500
2024-10-28 09:20:22,023 [trainer.py] => memory_per_class: 20
2024-10-28 09:20:22,023 [trainer.py] => fixed_memory: False
2024-10-28 09:20:22,024 [trainer.py] => shuffle: True
2024-10-28 09:20:22,024 [trainer.py] => init_cls: 5
2024-10-28 09:20:22,024 [trainer.py] => increment: 2
2024-10-28 09:20:22,024 [trainer.py] => model_name: gem
2024-10-28 09:20:22,024 [trainer.py] => convnet_type: resnet18
2024-10-28 09:20:22,024 [trainer.py] => init_train: False
2024-10-28 09:20:22,024 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42503.pth
2024-10-28 09:20:22,024 [trainer.py] => fc_path1: checkpoints/init_train/fc_42503.pth
2024-10-28 09:20:22,024 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_35172.pth
2024-10-28 09:20:22,024 [trainer.py] => fc_path: checkpoints/init_train/fc_35172.pth
2024-10-28 09:20:22,032 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-28 09:20:22,033 [trainer.py] => seed: 2001
2024-10-28 09:20:22,033 [trainer.py] => epochs: 150
2024-10-28 09:20:22,033 [trainer.py] => lrate: 0.1
2024-10-28 09:20:22,033 [trainer.py] => milestones: [50, 80, 120]
2024-10-28 09:20:22,033 [trainer.py] => lrate_decay: 0.1
2024-10-28 09:20:22,033 [trainer.py] => momentum: 0.4
2024-10-28 09:20:22,033 [trainer.py] => batch_size: 128
2024-10-28 09:20:22,033 [trainer.py] => weight_decay: 0.0002
2024-10-28 09:20:22,034 [trainer.py] => num_workers: 4
2024-10-28 09:20:22,833 [data_manager.py] => [3, 5, 1, 7, 2, 8, 6, 4, 0]
2024-10-28 09:20:24,272 [trainer.py] => All params: 3843904
2024-10-28 09:20:24,273 [trainer.py] => Trainable params: 3843904
2024-10-28 09:20:24,276 [gem.py] => Learning on 0-5
2024-10-28 09:20:24,400 [gem.py] => init_train?---False
2024-10-28 09:20:25,363 [base.py] => Reducing exemplars...(100 per classes)
2024-10-28 09:20:25,364 [base.py] => Constructing exemplars...(100 per classes)
2024-10-28 09:20:35,071 [trainer.py] => All params: 3846469
2024-10-28 09:20:36,389 [gem.py] => Exemplar size: 500
2024-10-28 09:20:36,389 [trainer.py] => CNN: {'total': 90.13, '00-04': 90.13, 'old': 0, 'new': 90.13}
2024-10-28 09:20:36,389 [trainer.py] => NME: {'total': 89.53, '00-04': 89.53, 'old': 0, 'new': 89.53}
2024-10-28 09:20:36,389 [trainer.py] => CNN top1 curve: [90.13]
2024-10-28 09:20:36,389 [trainer.py] => CNN top5 curve: [100.0]
2024-10-28 09:20:36,389 [trainer.py] => NME top1 curve: [89.53]
2024-10-28 09:20:36,389 [trainer.py] => NME top5 curve: [100.0]

2024-10-28 09:20:36,390 [trainer.py] => Average Accuracy (CNN): 90.13
2024-10-28 09:20:36,390 [trainer.py] => Average Accuracy (NME): 89.53
2024-10-28 09:20:36,390 [trainer.py] => All params: 3846469
2024-10-28 09:20:36,391 [trainer.py] => Trainable params: 3846469
2024-10-28 09:20:36,393 [gem.py] => Learning on 5-7
2024-10-28 09:20:47,018 [gem.py] => Task 1, Epoch 1/150 => Loss 0.226, Train_accy 92.18, Test_accy 48.83
2024-10-28 09:21:31,384 [gem.py] => Task 1, Epoch 6/150 => Loss 0.003, Train_accy 100.00, Test_accy 59.86
2024-10-28 09:22:18,034 [gem.py] => Task 1, Epoch 11/150 => Loss 0.001, Train_accy 100.00, Test_accy 60.19
2024-10-28 09:23:06,174 [gem.py] => Task 1, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 61.57
2024-10-28 09:23:56,918 [gem.py] => Task 1, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.17
2024-10-28 09:24:47,556 [gem.py] => Task 1, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.21
2024-10-28 09:25:41,379 [gem.py] => Task 1, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 61.29
2024-10-28 09:26:20,966 [gem.py] => Task 1, Epoch 36/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.88
2024-10-28 09:27:08,183 [gem.py] => Task 1, Epoch 41/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.21
2024-10-28 09:27:52,683 [gem.py] => Task 1, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.64
2024-10-28 09:28:39,198 [gem.py] => Task 1, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.48
2024-10-28 09:29:27,343 [gem.py] => Task 1, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.21
2024-10-28 09:30:15,947 [gem.py] => Task 1, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.38
2024-10-28 09:31:10,369 [gem.py] => Task 1, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.33
2024-10-28 09:31:59,161 [gem.py] => Task 1, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.50
2024-10-28 09:32:50,833 [gem.py] => Task 1, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.36
2024-10-28 09:33:42,262 [gem.py] => Task 1, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.31
2024-10-28 09:34:35,867 [gem.py] => Task 1, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.95
2024-10-28 09:35:28,357 [gem.py] => Task 1, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.50
2024-10-28 09:36:18,086 [gem.py] => Task 1, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.52
2024-10-28 09:37:06,203 [gem.py] => Task 1, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.31
2024-10-28 09:37:58,885 [gem.py] => Task 1, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.14
2024-10-28 09:38:48,434 [gem.py] => Task 1, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.90
2024-10-28 09:39:36,933 [gem.py] => Task 1, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.31
2024-10-28 09:40:29,787 [gem.py] => Task 1, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.21
2024-10-28 09:41:22,636 [gem.py] => Task 1, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.00
2024-10-28 09:42:18,921 [gem.py] => Task 1, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.45
2024-10-28 09:43:11,292 [gem.py] => Task 1, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.79
2024-10-28 09:44:01,429 [gem.py] => Task 1, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.60
2024-10-28 09:44:51,794 [gem.py] => Task 1, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.38
2024-10-28 09:45:34,616 [gem.py] => Task 1, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-10-28 09:45:34,627 [base.py] => Reducing exemplars...(71 per classes)
2024-10-28 09:45:37,075 [base.py] => Constructing exemplars...(71 per classes)
2024-10-28 09:45:40,643 [trainer.py] => All params: 3847495
2024-10-28 09:45:42,293 [gem.py] => Exemplar size: 497
2024-10-28 09:45:42,294 [trainer.py] => CNN: {'total': 62.79, '00-04': 67.03, '05-06': 52.17, 'old': 67.03, 'new': 52.17}
2024-10-28 09:45:42,294 [trainer.py] => NME: {'total': 63.6, '00-04': 60.13, '05-06': 72.25, 'old': 60.13, 'new': 72.25}
2024-10-28 09:45:42,294 [trainer.py] => CNN top1 curve: [90.13, 62.79]
2024-10-28 09:45:42,294 [trainer.py] => CNN top5 curve: [100.0, 97.48]
2024-10-28 09:45:42,294 [trainer.py] => NME top1 curve: [89.53, 63.6]
2024-10-28 09:45:42,295 [trainer.py] => NME top5 curve: [100.0, 96.93]

2024-10-28 09:45:42,295 [trainer.py] => Average Accuracy (CNN): 76.46
2024-10-28 09:45:42,303 [trainer.py] => Average Accuracy (NME): 76.565
2024-10-28 09:45:42,304 [trainer.py] => All params: 3847495
2024-10-28 09:45:42,305 [trainer.py] => Trainable params: 3847495
2024-10-28 09:45:42,306 [gem.py] => Learning on 7-9
2024-10-28 09:45:57,102 [gem.py] => Task 2, Epoch 1/150 => Loss 0.243, Train_accy 91.40, Test_accy 42.59
2024-10-28 09:47:06,146 [gem.py] => Task 2, Epoch 6/150 => Loss 0.004, Train_accy 100.00, Test_accy 47.69
2024-10-28 09:48:27,379 [gem.py] => Task 2, Epoch 11/150 => Loss 0.001, Train_accy 100.00, Test_accy 45.46
2024-10-28 09:49:48,268 [gem.py] => Task 2, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 44.07
2024-10-28 09:51:10,058 [gem.py] => Task 2, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 43.96
2024-10-28 09:52:27,583 [gem.py] => Task 2, Epoch 26/150 => Loss 0.000, Train_accy 100.00, Test_accy 43.72
2024-10-28 09:53:45,755 [gem.py] => Task 2, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 45.46
2024-10-28 09:54:57,148 [gem.py] => Task 2, Epoch 36/150 => Loss 0.000, Train_accy 100.00, Test_accy 42.48
2024-10-28 09:56:07,623 [gem.py] => Task 2, Epoch 41/150 => Loss 0.000, Train_accy 100.00, Test_accy 42.43
2024-10-28 09:57:25,667 [gem.py] => Task 2, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.89
2024-10-28 09:58:42,918 [gem.py] => Task 2, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.87
2024-10-28 10:00:03,464 [gem.py] => Task 2, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.87
2024-10-28 10:01:28,722 [gem.py] => Task 2, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.63
2024-10-28 10:02:55,163 [gem.py] => Task 2, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.70
2024-10-28 10:04:19,132 [gem.py] => Task 2, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.96
2024-10-28 10:05:40,783 [gem.py] => Task 2, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.93
2024-10-28 10:06:58,630 [gem.py] => Task 2, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.61
2024-10-28 10:08:17,604 [gem.py] => Task 2, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.35
2024-10-28 10:09:33,908 [gem.py] => Task 2, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.63
2024-10-28 10:10:54,009 [gem.py] => Task 2, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.83
2024-10-28 10:12:18,483 [gem.py] => Task 2, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.65
2024-10-28 10:13:37,636 [gem.py] => Task 2, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.81
2024-10-28 10:14:58,933 [gem.py] => Task 2, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.76
2024-10-28 10:16:20,023 [gem.py] => Task 2, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.61
2024-10-28 10:17:42,580 [gem.py] => Task 2, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.72
2024-10-28 10:19:03,507 [gem.py] => Task 2, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.87
2024-10-28 10:20:24,465 [gem.py] => Task 2, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.85
2024-10-28 10:21:41,373 [gem.py] => Task 2, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.72
2024-10-28 10:22:28,367 [gem.py] => Task 2, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.78
2024-10-28 10:23:15,844 [gem.py] => Task 2, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 41.81
2024-10-28 10:23:43,767 [gem.py] => Task 2, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-10-28 10:23:43,768 [base.py] => Reducing exemplars...(55 per classes)
2024-10-28 10:23:46,291 [base.py] => Constructing exemplars...(55 per classes)
2024-10-28 10:23:48,645 [trainer.py] => All params: 3848521
2024-10-28 10:23:49,857 [gem.py] => Exemplar size: 495
2024-10-28 10:23:49,858 [trainer.py] => CNN: {'total': 41.61, '00-04': 42.87, '05-06': 9.17, '07-08': 70.92, 'old': 33.24, 'new': 70.92}
2024-10-28 10:23:49,858 [trainer.py] => NME: {'total': 48.8, '00-04': 53.13, '05-06': 22.5, '07-08': 64.25, 'old': 44.38, 'new': 64.25}
2024-10-28 10:23:49,858 [trainer.py] => CNN top1 curve: [90.13, 62.79, 41.61]
2024-10-28 10:23:49,858 [trainer.py] => CNN top5 curve: [100.0, 97.48, 90.78]
2024-10-28 10:23:49,858 [trainer.py] => NME top1 curve: [89.53, 63.6, 48.8]
2024-10-28 10:23:49,858 [trainer.py] => NME top5 curve: [100.0, 96.93, 92.02]

2024-10-28 10:23:49,858 [trainer.py] => Average Accuracy (CNN): 64.84333333333332
2024-10-28 10:23:49,858 [trainer.py] => Average Accuracy (NME): 67.31
2024-10-28 10:23:49,859 [trainer.py] => Forgetting (CNN): 45.129999999999995
