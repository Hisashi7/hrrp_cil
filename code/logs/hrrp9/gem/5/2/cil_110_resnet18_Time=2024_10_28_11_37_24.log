2024-10-28 11:37:24,957 [trainer.py] => config: ./exps/gem.json
2024-10-28 11:37:24,957 [trainer.py] => prefix: cil
2024-10-28 11:37:24,957 [trainer.py] => dataset: hrrp9
2024-10-28 11:37:24,958 [trainer.py] => memory_size: 500
2024-10-28 11:37:24,961 [trainer.py] => memory_per_class: 20
2024-10-28 11:37:24,965 [trainer.py] => fixed_memory: False
2024-10-28 11:37:24,965 [trainer.py] => shuffle: True
2024-10-28 11:37:24,965 [trainer.py] => init_cls: 5
2024-10-28 11:37:24,966 [trainer.py] => increment: 2
2024-10-28 11:37:24,966 [trainer.py] => model_name: gem
2024-10-28 11:37:24,966 [trainer.py] => convnet_type: resnet18
2024-10-28 11:37:24,966 [trainer.py] => init_train: False
2024-10-28 11:37:24,967 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42503.pth
2024-10-28 11:37:24,967 [trainer.py] => fc_path1: checkpoints/init_train/fc_42503.pth
2024-10-28 11:37:24,967 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_35172.pth
2024-10-28 11:37:24,967 [trainer.py] => fc_path2: checkpoints/init_train/fc_35172.pth
2024-10-28 11:37:24,967 [trainer.py] => seed: 110
2024-10-28 11:37:24,968 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42871.pth
2024-10-28 11:37:24,968 [trainer.py] => fc_path: checkpoints/init_train/fc_42871.pth
2024-10-28 11:37:24,968 [trainer.py] => device: [device(type='cuda', index=4)]
2024-10-28 11:37:24,968 [trainer.py] => seed2: [2001]
2024-10-28 11:37:24,969 [trainer.py] => epochs: 150
2024-10-28 11:37:24,969 [trainer.py] => lrate: 0.1
2024-10-28 11:37:24,969 [trainer.py] => milestones: [50, 80, 120]
2024-10-28 11:37:24,969 [trainer.py] => lrate_decay: 0.1
2024-10-28 11:37:24,970 [trainer.py] => momentum: 0
2024-10-28 11:37:24,970 [trainer.py] => batch_size: 128
2024-10-28 11:37:24,971 [trainer.py] => weight_decay: 0.0002
2024-10-28 11:37:24,971 [trainer.py] => num_workers: 4
2024-10-28 11:37:25,718 [data_manager.py] => [4, 2, 8, 7, 1, 6, 5, 3, 0]
2024-10-28 11:37:27,185 [trainer.py] => All params: 3843904
2024-10-28 11:37:27,195 [trainer.py] => Trainable params: 3843904
2024-10-28 11:37:27,198 [gem.py] => Learning on 0-5
2024-10-28 11:37:27,572 [gem.py] => init_train?---False
2024-10-28 11:37:28,516 [base.py] => Reducing exemplars...(100 per classes)
2024-10-28 11:37:28,516 [base.py] => Constructing exemplars...(100 per classes)
2024-10-28 11:37:36,361 [trainer.py] => All params: 3846469
2024-10-28 11:37:37,283 [gem.py] => Exemplar size: 500
2024-10-28 11:37:37,284 [trainer.py] => CNN: {'total': 96.3, '00-04': 96.3, 'old': 0, 'new': 96.3}
2024-10-28 11:37:37,284 [trainer.py] => NME: {'total': 96.27, '00-04': 96.27, 'old': 0, 'new': 96.27}
2024-10-28 11:37:37,284 [trainer.py] => CNN top1 curve: [96.3]
2024-10-28 11:37:37,284 [trainer.py] => CNN top5 curve: [100.0]
2024-10-28 11:37:37,284 [trainer.py] => NME top1 curve: [96.27]
2024-10-28 11:37:37,285 [trainer.py] => NME top5 curve: [100.0]

2024-10-28 11:37:37,285 [trainer.py] => Average Accuracy (CNN): 96.3
2024-10-28 11:37:37,285 [trainer.py] => Average Accuracy (NME): 96.27
2024-10-28 11:37:37,286 [trainer.py] => All params: 3846469
2024-10-28 11:37:37,286 [trainer.py] => Trainable params: 3846469
2024-10-28 11:37:37,288 [gem.py] => Learning on 5-7
2024-10-28 11:37:47,363 [gem.py] => Task 1, Epoch 1/150 => Loss 0.254, Train_accy 93.00, Test_accy 65.88
2024-10-28 11:38:29,433 [gem.py] => Task 1, Epoch 6/150 => Loss 0.007, Train_accy 100.00, Test_accy 69.67
2024-10-28 11:39:17,286 [gem.py] => Task 1, Epoch 11/150 => Loss 0.003, Train_accy 100.00, Test_accy 71.02
2024-10-28 11:40:02,475 [gem.py] => Task 1, Epoch 16/150 => Loss 0.002, Train_accy 100.00, Test_accy 70.67
2024-10-28 11:40:44,011 [gem.py] => Task 1, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 70.67
2024-10-28 11:41:27,185 [gem.py] => Task 1, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 70.21
2024-10-28 11:42:07,344 [gem.py] => Task 1, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 68.43
2024-10-28 11:42:44,745 [gem.py] => Task 1, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 68.98
2024-10-28 11:43:22,039 [gem.py] => Task 1, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 69.29
2024-10-28 11:43:57,073 [gem.py] => Task 1, Epoch 46/150 => Loss 0.001, Train_accy 100.00, Test_accy 69.45
2024-10-28 11:44:44,775 [gem.py] => Task 1, Epoch 51/150 => Loss 0.001, Train_accy 100.00, Test_accy 69.79
2024-10-28 11:45:36,386 [gem.py] => Task 1, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.74
2024-10-28 11:46:25,910 [gem.py] => Task 1, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.24
2024-10-28 11:47:14,311 [gem.py] => Task 1, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.81
2024-10-28 11:48:05,319 [gem.py] => Task 1, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.98
2024-10-28 11:48:53,851 [gem.py] => Task 1, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.69
2024-10-28 11:49:42,160 [gem.py] => Task 1, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.07
2024-10-28 11:50:28,103 [gem.py] => Task 1, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.21
2024-10-28 11:51:15,739 [gem.py] => Task 1, Epoch 91/150 => Loss 0.001, Train_accy 100.00, Test_accy 69.88
2024-10-28 11:52:03,447 [gem.py] => Task 1, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.38
2024-10-28 11:52:43,608 [gem.py] => Task 1, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.00
2024-10-28 11:53:20,529 [gem.py] => Task 1, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.67
2024-10-28 11:53:49,300 [gem.py] => Task 1, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.40
2024-10-28 11:54:17,207 [gem.py] => Task 1, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.00
2024-10-28 11:54:45,996 [gem.py] => Task 1, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 68.52
2024-10-28 11:55:14,167 [gem.py] => Task 1, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.05
2024-10-28 11:55:42,234 [gem.py] => Task 1, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.02
2024-10-28 11:56:11,806 [gem.py] => Task 1, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.29
2024-10-28 11:56:40,838 [gem.py] => Task 1, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.45
2024-10-28 11:57:09,755 [gem.py] => Task 1, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 69.45
2024-10-28 11:57:32,325 [gem.py] => Task 1, Epoch 150/150 => Loss 0.001, Train_accy 100.00
2024-10-28 11:57:32,335 [base.py] => Reducing exemplars...(71 per classes)
2024-10-28 11:57:33,782 [base.py] => Constructing exemplars...(71 per classes)
2024-10-28 11:57:35,974 [trainer.py] => All params: 3847495
2024-10-28 11:57:36,915 [gem.py] => Exemplar size: 497
2024-10-28 11:57:36,915 [trainer.py] => CNN: {'total': 68.19, '00-04': 80.1, '05-06': 38.42, 'old': 80.1, 'new': 38.42}
2024-10-28 11:57:36,915 [trainer.py] => NME: {'total': 73.95, '00-04': 70.37, '05-06': 82.92, 'old': 70.37, 'new': 82.92}
2024-10-28 11:57:36,916 [trainer.py] => CNN top1 curve: [96.3, 68.19]
2024-10-28 11:57:36,916 [trainer.py] => CNN top5 curve: [100.0, 98.52]
2024-10-28 11:57:36,916 [trainer.py] => NME top1 curve: [96.27, 73.95]
2024-10-28 11:57:36,916 [trainer.py] => NME top5 curve: [100.0, 98.43]

2024-10-28 11:57:36,916 [trainer.py] => Average Accuracy (CNN): 82.245
2024-10-28 11:57:36,916 [trainer.py] => Average Accuracy (NME): 85.11
2024-10-28 11:57:36,916 [trainer.py] => All params: 3847495
2024-10-28 11:57:36,917 [trainer.py] => Trainable params: 3847495
2024-10-28 11:57:36,918 [gem.py] => Learning on 7-9
2024-10-28 11:57:43,380 [gem.py] => Task 2, Epoch 1/150 => Loss 0.230, Train_accy 92.68, Test_accy 54.81
2024-10-28 11:58:09,384 [gem.py] => Task 2, Epoch 6/150 => Loss 0.009, Train_accy 99.95, Test_accy 51.37
2024-10-28 11:58:38,271 [gem.py] => Task 2, Epoch 11/150 => Loss 0.003, Train_accy 99.98, Test_accy 49.30
2024-10-28 11:59:07,826 [gem.py] => Task 2, Epoch 16/150 => Loss 0.002, Train_accy 100.00, Test_accy 47.87
2024-10-28 11:59:37,658 [gem.py] => Task 2, Epoch 21/150 => Loss 0.002, Train_accy 100.00, Test_accy 48.96
2024-10-28 12:00:07,367 [gem.py] => Task 2, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 48.69
2024-10-28 12:00:36,989 [gem.py] => Task 2, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 44.13
2024-10-28 12:01:01,630 [gem.py] => Task 2, Epoch 36/150 => Loss 0.008, Train_accy 99.88, Test_accy 43.87
2024-10-28 12:01:31,783 [gem.py] => Task 2, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 43.39
2024-10-28 12:02:02,296 [gem.py] => Task 2, Epoch 46/150 => Loss 0.001, Train_accy 100.00, Test_accy 41.91
2024-10-28 12:02:32,623 [gem.py] => Task 2, Epoch 51/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.96
2024-10-28 12:03:01,470 [gem.py] => Task 2, Epoch 56/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.50
2024-10-28 12:03:31,450 [gem.py] => Task 2, Epoch 61/150 => Loss 0.001, Train_accy 100.00, Test_accy 41.07
2024-10-28 12:04:01,183 [gem.py] => Task 2, Epoch 66/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.70
2024-10-28 12:04:32,250 [gem.py] => Task 2, Epoch 71/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.83
2024-10-28 12:05:03,963 [gem.py] => Task 2, Epoch 76/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.56
2024-10-28 12:05:34,519 [gem.py] => Task 2, Epoch 81/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.59
2024-10-28 12:06:06,214 [gem.py] => Task 2, Epoch 86/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.19
2024-10-28 12:06:38,835 [gem.py] => Task 2, Epoch 91/150 => Loss 0.001, Train_accy 100.00, Test_accy 39.87
2024-10-28 12:07:10,324 [gem.py] => Task 2, Epoch 96/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.98
2024-10-28 12:07:42,218 [gem.py] => Task 2, Epoch 101/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.39
2024-10-28 12:08:13,134 [gem.py] => Task 2, Epoch 106/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.50
2024-10-28 12:08:44,729 [gem.py] => Task 2, Epoch 111/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.46
2024-10-28 12:09:15,293 [gem.py] => Task 2, Epoch 116/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.33
2024-10-28 12:09:46,550 [gem.py] => Task 2, Epoch 121/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.46
2024-10-28 12:10:16,861 [gem.py] => Task 2, Epoch 126/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.50
2024-10-28 12:10:47,271 [gem.py] => Task 2, Epoch 131/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.46
2024-10-28 12:11:19,970 [gem.py] => Task 2, Epoch 136/150 => Loss 0.001, Train_accy 100.00, Test_accy 39.81
2024-10-28 12:11:51,858 [gem.py] => Task 2, Epoch 141/150 => Loss 0.001, Train_accy 100.00, Test_accy 41.20
2024-10-28 12:12:22,791 [gem.py] => Task 2, Epoch 146/150 => Loss 0.001, Train_accy 100.00, Test_accy 40.65
2024-10-28 12:12:47,524 [gem.py] => Task 2, Epoch 150/150 => Loss 0.001, Train_accy 100.00
2024-10-28 12:12:47,531 [base.py] => Reducing exemplars...(55 per classes)
2024-10-28 12:12:49,343 [base.py] => Constructing exemplars...(55 per classes)
2024-10-28 12:12:51,266 [trainer.py] => All params: 3848521
2024-10-28 12:12:52,473 [gem.py] => Exemplar size: 495
2024-10-28 12:12:52,473 [trainer.py] => CNN: {'total': 40.56, '00-04': 40.87, '05-06': 6.42, '07-08': 73.92, 'old': 31.02, 'new': 73.92}
2024-10-28 12:12:52,473 [trainer.py] => NME: {'total': 52.61, '00-04': 55.33, '05-06': 38.0, '07-08': 60.42, 'old': 50.38, 'new': 60.42}
2024-10-28 12:12:52,473 [trainer.py] => CNN top1 curve: [96.3, 68.19, 40.56]
2024-10-28 12:12:52,473 [trainer.py] => CNN top5 curve: [100.0, 98.52, 96.15]
2024-10-28 12:12:52,473 [trainer.py] => NME top1 curve: [96.27, 73.95, 52.61]
2024-10-28 12:12:52,473 [trainer.py] => NME top5 curve: [100.0, 98.43, 95.0]

2024-10-28 12:12:52,473 [trainer.py] => Average Accuracy (CNN): 68.35000000000001
2024-10-28 12:12:52,473 [trainer.py] => Average Accuracy (NME): 74.27666666666666
2024-10-28 12:12:52,474 [trainer.py] => Forgetting (CNN): 43.715
