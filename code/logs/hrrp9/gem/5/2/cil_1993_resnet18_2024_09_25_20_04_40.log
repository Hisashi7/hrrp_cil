2024-09-25 20:04:40,103 [trainer.py] => config: ./exps/gem.json
2024-09-25 20:04:40,103 [trainer.py] => prefix: cil
2024-09-25 20:04:40,103 [trainer.py] => dataset: hrrp9
2024-09-25 20:04:40,103 [trainer.py] => memory_size: 500
2024-09-25 20:04:40,103 [trainer.py] => memory_per_class: 20
2024-09-25 20:04:40,103 [trainer.py] => fixed_memory: False
2024-09-25 20:04:40,103 [trainer.py] => shuffle: True
2024-09-25 20:04:40,104 [trainer.py] => init_cls: 5
2024-09-25 20:04:40,104 [trainer.py] => increment: 2
2024-09-25 20:04:40,104 [trainer.py] => model_name: gem
2024-09-25 20:04:40,104 [trainer.py] => convnet_type: resnet18
2024-09-25 20:04:40,104 [trainer.py] => init_train: False
2024-09-25 20:04:40,104 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-09-25 20:04:40,104 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-09-25 20:04:40,104 [trainer.py] => device: [device(type='cuda', index=1)]
2024-09-25 20:04:40,104 [trainer.py] => seed: 1993
2024-09-25 20:04:40,104 [trainer.py] => epochs: 150
2024-09-25 20:04:40,104 [trainer.py] => lrate: 0.1
2024-09-25 20:04:40,104 [trainer.py] => milestones: [80, 120]
2024-09-25 20:04:40,104 [trainer.py] => lrate_decay: 0.1
2024-09-25 20:04:40,104 [trainer.py] => momentum: 0.1
2024-09-25 20:04:40,104 [trainer.py] => batch_size: 128
2024-09-25 20:04:40,104 [trainer.py] => weight_decay: 0.0002
2024-09-25 20:04:40,104 [trainer.py] => num_workers: 4
2024-09-25 20:04:40,507 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-09-25 20:04:40,839 [trainer.py] => All params: 3843904
2024-09-25 20:04:40,840 [trainer.py] => Trainable params: 3843904
2024-09-25 20:04:40,841 [gem.py] => Learning on 0-5
2024-09-25 20:04:40,998 [gem.py] => init_train?---False
2024-09-25 20:04:41,646 [base.py] => Reducing exemplars...(100 per classes)
2024-09-25 20:04:41,647 [base.py] => Constructing exemplars...(100 per classes)
2024-09-25 20:04:46,142 [gem.py] => Exemplar size: 500
2024-09-25 20:04:46,142 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-09-25 20:04:46,142 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-09-25 20:04:46,142 [trainer.py] => CNN top1 curve: [89.93]
2024-09-25 20:04:46,142 [trainer.py] => CNN top5 curve: [100.0]
2024-09-25 20:04:46,142 [trainer.py] => NME top1 curve: [90.0]
2024-09-25 20:04:46,142 [trainer.py] => NME top5 curve: [100.0]

2024-09-25 20:04:46,142 [trainer.py] => Average Accuracy (CNN): 89.93
2024-09-25 20:04:46,142 [trainer.py] => Average Accuracy (NME): 90.0
2024-09-25 20:04:46,143 [trainer.py] => All params: 3846469
2024-09-25 20:04:46,143 [trainer.py] => Trainable params: 3846469
2024-09-25 20:04:46,144 [gem.py] => Learning on 5-7
2024-09-25 20:04:49,229 [gem.py] => Task 1, Epoch 1/150 => Loss 0.322, Train_accy 89.22, Test_accy 61.62
2024-09-25 20:05:03,842 [gem.py] => Task 1, Epoch 6/150 => Loss 0.010, Train_accy 99.98, Test_accy 61.05
2024-09-25 20:05:18,198 [gem.py] => Task 1, Epoch 11/150 => Loss 0.004, Train_accy 100.00, Test_accy 60.95
2024-09-25 20:05:33,671 [gem.py] => Task 1, Epoch 16/150 => Loss 0.002, Train_accy 100.00, Test_accy 62.81
2024-09-25 20:05:49,048 [gem.py] => Task 1, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.50
2024-09-25 20:06:04,781 [gem.py] => Task 1, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.45
2024-09-25 20:06:21,014 [gem.py] => Task 1, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.83
2024-09-25 20:06:36,579 [gem.py] => Task 1, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 63.98
2024-09-25 20:06:51,974 [gem.py] => Task 1, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.19
2024-09-25 20:07:07,403 [gem.py] => Task 1, Epoch 46/150 => Loss 0.001, Train_accy 100.00, Test_accy 64.57
2024-09-25 20:07:23,455 [gem.py] => Task 1, Epoch 51/150 => Loss 0.001, Train_accy 100.00, Test_accy 65.62
2024-09-25 20:07:39,707 [gem.py] => Task 1, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.02
2024-09-25 20:07:56,081 [gem.py] => Task 1, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.55
2024-09-25 20:08:11,991 [gem.py] => Task 1, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.71
2024-09-25 20:08:28,902 [gem.py] => Task 1, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.00
2024-09-25 20:08:44,869 [gem.py] => Task 1, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.05
2024-09-25 20:09:00,632 [gem.py] => Task 1, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.07
2024-09-25 20:09:16,760 [gem.py] => Task 1, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.14
2024-09-25 20:09:32,960 [gem.py] => Task 1, Epoch 91/150 => Loss 0.001, Train_accy 100.00, Test_accy 66.60
2024-09-25 20:09:48,731 [gem.py] => Task 1, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.88
2024-09-25 20:10:04,255 [gem.py] => Task 1, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.12
2024-09-25 20:10:19,504 [gem.py] => Task 1, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.33
2024-09-25 20:10:36,075 [gem.py] => Task 1, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.76
2024-09-25 20:10:52,331 [gem.py] => Task 1, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.50
2024-09-25 20:11:08,279 [gem.py] => Task 1, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 65.69
2024-09-25 20:11:23,644 [gem.py] => Task 1, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.69
2024-09-25 20:11:39,506 [gem.py] => Task 1, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.57
2024-09-25 20:11:56,077 [gem.py] => Task 1, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.79
2024-09-25 20:12:12,215 [gem.py] => Task 1, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.86
2024-09-25 20:12:27,905 [gem.py] => Task 1, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 66.81
2024-09-25 20:12:40,121 [gem.py] => Task 1, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-09-25 20:12:40,137 [base.py] => Reducing exemplars...(71 per classes)
2024-09-25 20:12:41,366 [base.py] => Constructing exemplars...(71 per classes)
2024-09-25 20:12:43,960 [gem.py] => Exemplar size: 497
2024-09-25 20:12:43,960 [trainer.py] => CNN: {'total': 65.6, '00-04': 75.13, '05-06': 41.75, 'old': 75.13, 'new': 41.75}
2024-09-25 20:12:43,960 [trainer.py] => NME: {'total': 75.71, '00-04': 73.73, '05-06': 80.67, 'old': 73.73, 'new': 80.67}
2024-09-25 20:12:43,960 [trainer.py] => CNN top1 curve: [89.93, 65.6]
2024-09-25 20:12:43,960 [trainer.py] => CNN top5 curve: [100.0, 98.57]
2024-09-25 20:12:43,960 [trainer.py] => NME top1 curve: [90.0, 75.71]
2024-09-25 20:12:43,960 [trainer.py] => NME top5 curve: [100.0, 98.45]

2024-09-25 20:12:43,960 [trainer.py] => Average Accuracy (CNN): 77.765
2024-09-25 20:12:43,960 [trainer.py] => Average Accuracy (NME): 82.85499999999999
2024-09-25 20:12:43,961 [trainer.py] => All params: 3847495
2024-09-25 20:12:43,961 [trainer.py] => Trainable params: 3847495
2024-09-25 20:12:43,962 [gem.py] => Learning on 7-9
2024-09-25 20:12:48,505 [gem.py] => Task 2, Epoch 1/150 => Loss 0.182, Train_accy 94.42, Test_accy 52.52
2024-09-25 20:13:08,743 [gem.py] => Task 2, Epoch 6/150 => Loss 0.005, Train_accy 100.00, Test_accy 57.63
2024-09-25 20:13:29,897 [gem.py] => Task 2, Epoch 11/150 => Loss 0.002, Train_accy 100.00, Test_accy 56.91
2024-09-25 20:13:55,454 [gem.py] => Task 2, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.13
2024-09-25 20:14:19,820 [gem.py] => Task 2, Epoch 21/150 => Loss 0.022, Train_accy 99.45, Test_accy 55.41
2024-09-25 20:14:43,019 [gem.py] => Task 2, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.15
2024-09-25 20:15:08,498 [gem.py] => Task 2, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.22
2024-09-25 20:15:33,852 [gem.py] => Task 2, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 58.80
2024-09-25 20:15:59,731 [gem.py] => Task 2, Epoch 41/150 => Loss 0.001, Train_accy 100.00, Test_accy 57.30
2024-09-25 20:16:25,659 [gem.py] => Task 2, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.30
2024-09-25 20:16:51,906 [gem.py] => Task 2, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 54.89
2024-09-25 20:17:19,024 [gem.py] => Task 2, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 52.52
2024-09-25 20:17:45,974 [gem.py] => Task 2, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.81
2024-09-25 20:18:12,096 [gem.py] => Task 2, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.69
2024-09-25 20:18:38,061 [gem.py] => Task 2, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.39
2024-09-25 20:19:02,565 [gem.py] => Task 2, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.19
2024-09-25 20:19:26,747 [gem.py] => Task 2, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 54.70
2024-09-25 20:19:51,455 [gem.py] => Task 2, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 54.96
2024-09-25 20:20:16,245 [gem.py] => Task 2, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.15
2024-09-25 20:20:42,637 [gem.py] => Task 2, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 54.48
2024-09-25 20:21:02,735 [gem.py] => Task 2, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 54.70
2024-09-25 20:21:21,955 [gem.py] => Task 2, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.57
2024-09-25 20:21:41,923 [gem.py] => Task 2, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 56.20
2024-09-25 20:22:00,612 [gem.py] => Task 2, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.24
2024-09-25 20:22:19,135 [gem.py] => Task 2, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.56
2024-09-25 20:22:38,301 [gem.py] => Task 2, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.22
2024-09-25 20:22:57,879 [gem.py] => Task 2, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.41
2024-09-25 20:23:17,668 [gem.py] => Task 2, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.67
2024-09-25 20:23:37,671 [gem.py] => Task 2, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 54.98
2024-09-25 20:23:56,623 [gem.py] => Task 2, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 55.50
2024-09-25 20:24:11,700 [gem.py] => Task 2, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-09-25 20:24:11,708 [base.py] => Reducing exemplars...(55 per classes)
2024-09-25 20:24:13,407 [base.py] => Constructing exemplars...(55 per classes)
2024-09-25 20:24:16,253 [gem.py] => Exemplar size: 495
2024-09-25 20:24:16,254 [trainer.py] => CNN: {'total': 55.33, '00-04': 57.4, '05-06': 16.17, '07-08': 89.33, 'old': 45.62, 'new': 89.33}
2024-09-25 20:24:16,254 [trainer.py] => NME: {'total': 60.94, '00-04': 58.67, '05-06': 41.92, '07-08': 85.67, 'old': 53.88, 'new': 85.67}
2024-09-25 20:24:16,254 [trainer.py] => CNN top1 curve: [89.93, 65.6, 55.33]
2024-09-25 20:24:16,254 [trainer.py] => CNN top5 curve: [100.0, 98.57, 94.74]
2024-09-25 20:24:16,254 [trainer.py] => NME top1 curve: [90.0, 75.71, 60.94]
2024-09-25 20:24:16,254 [trainer.py] => NME top5 curve: [100.0, 98.45, 95.31]

2024-09-25 20:24:16,254 [trainer.py] => Average Accuracy (CNN): 70.28666666666668
2024-09-25 20:24:16,254 [trainer.py] => Average Accuracy (NME): 75.55
2024-09-25 20:24:16,255 [trainer.py] => Forgetting (CNN): 29.055000000000003
