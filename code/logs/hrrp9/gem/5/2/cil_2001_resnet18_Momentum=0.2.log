2024-10-28 09:19:55,286 [trainer.py] => config: ./exps/gem.json
2024-10-28 09:19:55,286 [trainer.py] => prefix: cil
2024-10-28 09:19:55,286 [trainer.py] => dataset: hrrp9
2024-10-28 09:19:55,286 [trainer.py] => memory_size: 500
2024-10-28 09:19:55,286 [trainer.py] => memory_per_class: 20
2024-10-28 09:19:55,287 [trainer.py] => fixed_memory: False
2024-10-28 09:19:55,287 [trainer.py] => shuffle: True
2024-10-28 09:19:55,287 [trainer.py] => init_cls: 5
2024-10-28 09:19:55,287 [trainer.py] => increment: 2
2024-10-28 09:19:55,287 [trainer.py] => model_name: gem
2024-10-28 09:19:55,287 [trainer.py] => convnet_type: resnet18
2024-10-28 09:19:55,287 [trainer.py] => init_train: False
2024-10-28 09:19:55,287 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42503.pth
2024-10-28 09:19:55,287 [trainer.py] => fc_path1: checkpoints/init_train/fc_42503.pth
2024-10-28 09:19:55,288 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_35172.pth
2024-10-28 09:19:55,288 [trainer.py] => fc_path: checkpoints/init_train/fc_35172.pth
2024-10-28 09:19:55,288 [trainer.py] => device: [device(type='cuda', index=4)]
2024-10-28 09:19:55,288 [trainer.py] => seed: 2001
2024-10-28 09:19:55,288 [trainer.py] => epochs: 150
2024-10-28 09:19:55,288 [trainer.py] => lrate: 0.1
2024-10-28 09:19:55,288 [trainer.py] => milestones: [50, 80, 120]
2024-10-28 09:19:55,288 [trainer.py] => lrate_decay: 0.1
2024-10-28 09:19:55,288 [trainer.py] => momentum: 0.2
2024-10-28 09:19:55,288 [trainer.py] => batch_size: 128
2024-10-28 09:19:55,288 [trainer.py] => weight_decay: 0.0002
2024-10-28 09:19:55,289 [trainer.py] => num_workers: 4
2024-10-28 09:19:55,865 [data_manager.py] => [3, 5, 1, 7, 2, 8, 6, 4, 0]
2024-10-28 09:19:57,068 [trainer.py] => All params: 3843904
2024-10-28 09:19:57,069 [trainer.py] => Trainable params: 3843904
2024-10-28 09:19:57,073 [gem.py] => Learning on 0-5
2024-10-28 09:19:57,386 [gem.py] => init_train?---False
2024-10-28 09:19:58,093 [base.py] => Reducing exemplars...(100 per classes)
2024-10-28 09:19:58,094 [base.py] => Constructing exemplars...(100 per classes)
2024-10-28 09:20:04,884 [trainer.py] => All params: 3846469
2024-10-28 09:20:05,775 [gem.py] => Exemplar size: 500
2024-10-28 09:20:05,776 [trainer.py] => CNN: {'total': 90.13, '00-04': 90.13, 'old': 0, 'new': 90.13}
2024-10-28 09:20:05,776 [trainer.py] => NME: {'total': 89.53, '00-04': 89.53, 'old': 0, 'new': 89.53}
2024-10-28 09:20:05,776 [trainer.py] => CNN top1 curve: [90.13]
2024-10-28 09:20:05,776 [trainer.py] => CNN top5 curve: [100.0]
2024-10-28 09:20:05,776 [trainer.py] => NME top1 curve: [89.53]
2024-10-28 09:20:05,776 [trainer.py] => NME top5 curve: [100.0]

2024-10-28 09:20:05,776 [trainer.py] => Average Accuracy (CNN): 90.13
2024-10-28 09:20:05,776 [trainer.py] => Average Accuracy (NME): 89.53
2024-10-28 09:20:05,777 [trainer.py] => All params: 3846469
2024-10-28 09:20:05,777 [trainer.py] => Trainable params: 3846469
2024-10-28 09:20:05,778 [gem.py] => Learning on 5-7
2024-10-28 09:20:13,339 [gem.py] => Task 1, Epoch 1/150 => Loss 0.261, Train_accy 91.35, Test_accy 50.93
2024-10-28 09:20:52,971 [gem.py] => Task 1, Epoch 6/150 => Loss 0.005, Train_accy 100.00, Test_accy 59.67
2024-10-28 09:21:41,692 [gem.py] => Task 1, Epoch 11/150 => Loss 0.002, Train_accy 100.00, Test_accy 60.00
2024-10-28 09:22:28,819 [gem.py] => Task 1, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 61.43
2024-10-28 09:23:22,841 [gem.py] => Task 1, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.26
2024-10-28 09:24:13,553 [gem.py] => Task 1, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 62.64
2024-10-28 09:25:06,381 [gem.py] => Task 1, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 61.40
2024-10-28 09:25:47,120 [gem.py] => Task 1, Epoch 36/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.00
2024-10-28 09:26:32,438 [gem.py] => Task 1, Epoch 41/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.19
2024-10-28 09:27:15,527 [gem.py] => Task 1, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.26
2024-10-28 09:28:03,078 [gem.py] => Task 1, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.88
2024-10-28 09:28:48,906 [gem.py] => Task 1, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.90
2024-10-28 09:29:37,095 [gem.py] => Task 1, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.02
2024-10-28 09:30:29,240 [gem.py] => Task 1, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.93
2024-10-28 09:31:19,490 [gem.py] => Task 1, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.00
2024-10-28 09:32:08,717 [gem.py] => Task 1, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.86
2024-10-28 09:32:59,481 [gem.py] => Task 1, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.00
2024-10-28 09:33:52,622 [gem.py] => Task 1, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.71
2024-10-28 09:34:44,037 [gem.py] => Task 1, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.19
2024-10-28 09:35:34,195 [gem.py] => Task 1, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.43
2024-10-28 09:36:23,312 [gem.py] => Task 1, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.31
2024-10-28 09:37:14,062 [gem.py] => Task 1, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.88
2024-10-28 09:38:04,333 [gem.py] => Task 1, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 62.90
2024-10-28 09:38:53,743 [gem.py] => Task 1, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.29
2024-10-28 09:39:45,502 [gem.py] => Task 1, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.26
2024-10-28 09:40:38,848 [gem.py] => Task 1, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.10
2024-10-28 09:41:27,528 [gem.py] => Task 1, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.24
2024-10-28 09:42:16,740 [gem.py] => Task 1, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.50
2024-10-28 09:43:06,651 [gem.py] => Task 1, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.24
2024-10-28 09:43:55,163 [gem.py] => Task 1, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 63.10
2024-10-28 09:44:34,685 [gem.py] => Task 1, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-10-28 09:44:34,693 [base.py] => Reducing exemplars...(71 per classes)
2024-10-28 09:44:36,929 [base.py] => Constructing exemplars...(71 per classes)
2024-10-28 09:44:40,273 [trainer.py] => All params: 3847495
2024-10-28 09:44:41,706 [gem.py] => Exemplar size: 497
2024-10-28 09:44:41,706 [trainer.py] => CNN: {'total': 62.83, '00-04': 69.8, '05-06': 45.42, 'old': 69.8, 'new': 45.42}
2024-10-28 09:44:41,706 [trainer.py] => NME: {'total': 64.71, '00-04': 62.3, '05-06': 70.75, 'old': 62.3, 'new': 70.75}
2024-10-28 09:44:41,707 [trainer.py] => CNN top1 curve: [90.13, 62.83]
2024-10-28 09:44:41,707 [trainer.py] => CNN top5 curve: [100.0, 97.45]
2024-10-28 09:44:41,707 [trainer.py] => NME top1 curve: [89.53, 64.71]
2024-10-28 09:44:41,707 [trainer.py] => NME top5 curve: [100.0, 96.88]

2024-10-28 09:44:41,707 [trainer.py] => Average Accuracy (CNN): 76.47999999999999
2024-10-28 09:44:41,707 [trainer.py] => Average Accuracy (NME): 77.12
2024-10-28 09:44:41,707 [trainer.py] => All params: 3847495
2024-10-28 09:44:41,716 [trainer.py] => Trainable params: 3847495
2024-10-28 09:44:41,718 [gem.py] => Learning on 7-9
2024-10-28 09:44:55,516 [gem.py] => Task 2, Epoch 1/150 => Loss 0.267, Train_accy 90.80, Test_accy 40.57
2024-10-28 09:45:59,000 [gem.py] => Task 2, Epoch 6/150 => Loss 0.006, Train_accy 100.00, Test_accy 51.28
2024-10-28 09:47:15,867 [gem.py] => Task 2, Epoch 11/150 => Loss 0.002, Train_accy 100.00, Test_accy 49.26
2024-10-28 09:48:35,291 [gem.py] => Task 2, Epoch 16/150 => Loss 0.001, Train_accy 100.00, Test_accy 48.19
2024-10-28 09:49:55,393 [gem.py] => Task 2, Epoch 21/150 => Loss 0.001, Train_accy 100.00, Test_accy 48.15
2024-10-28 09:51:14,094 [gem.py] => Task 2, Epoch 26/150 => Loss 0.001, Train_accy 100.00, Test_accy 47.50
2024-10-28 09:52:33,631 [gem.py] => Task 2, Epoch 31/150 => Loss 0.001, Train_accy 100.00, Test_accy 49.78
2024-10-28 09:53:43,156 [gem.py] => Task 2, Epoch 36/150 => Loss 0.001, Train_accy 100.00, Test_accy 45.26
2024-10-28 09:54:43,920 [gem.py] => Task 2, Epoch 41/150 => Loss 0.000, Train_accy 100.00, Test_accy 46.04
2024-10-28 09:55:54,714 [gem.py] => Task 2, Epoch 46/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.72
2024-10-28 09:57:08,633 [gem.py] => Task 2, Epoch 51/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.67
2024-10-28 09:58:24,347 [gem.py] => Task 2, Epoch 56/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.93
2024-10-28 09:59:35,797 [gem.py] => Task 2, Epoch 61/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.54
2024-10-28 10:00:50,656 [gem.py] => Task 2, Epoch 66/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.61
2024-10-28 10:02:07,398 [gem.py] => Task 2, Epoch 71/150 => Loss 0.000, Train_accy 100.00, Test_accy 46.13
2024-10-28 10:03:29,464 [gem.py] => Task 2, Epoch 76/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.54
2024-10-28 10:04:46,494 [gem.py] => Task 2, Epoch 81/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.85
2024-10-28 10:06:02,607 [gem.py] => Task 2, Epoch 86/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.30
2024-10-28 10:07:13,223 [gem.py] => Task 2, Epoch 91/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.48
2024-10-28 10:08:26,900 [gem.py] => Task 2, Epoch 96/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.91
2024-10-28 10:09:40,416 [gem.py] => Task 2, Epoch 101/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.72
2024-10-28 10:10:54,107 [gem.py] => Task 2, Epoch 106/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.57
2024-10-28 10:12:08,687 [gem.py] => Task 2, Epoch 111/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.80
2024-10-28 10:13:23,060 [gem.py] => Task 2, Epoch 116/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.91
2024-10-28 10:14:37,869 [gem.py] => Task 2, Epoch 121/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.85
2024-10-28 10:15:52,329 [gem.py] => Task 2, Epoch 126/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.87
2024-10-28 10:17:03,473 [gem.py] => Task 2, Epoch 131/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.70
2024-10-28 10:18:18,740 [gem.py] => Task 2, Epoch 136/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.76
2024-10-28 10:19:29,076 [gem.py] => Task 2, Epoch 141/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.72
2024-10-28 10:20:43,437 [gem.py] => Task 2, Epoch 146/150 => Loss 0.000, Train_accy 100.00, Test_accy 45.91
2024-10-28 10:21:38,121 [gem.py] => Task 2, Epoch 150/150 => Loss 0.000, Train_accy 100.00
2024-10-28 10:21:38,131 [base.py] => Reducing exemplars...(55 per classes)
2024-10-28 10:21:40,745 [base.py] => Constructing exemplars...(55 per classes)
2024-10-28 10:21:43,517 [trainer.py] => All params: 3848521
2024-10-28 10:21:45,264 [gem.py] => Exemplar size: 495
2024-10-28 10:21:45,266 [trainer.py] => CNN: {'total': 45.44, '00-04': 50.17, '05-06': 10.5, '07-08': 68.58, 'old': 38.83, 'new': 68.58}
2024-10-28 10:21:45,266 [trainer.py] => NME: {'total': 50.89, '00-04': 55.4, '05-06': 26.67, '07-08': 63.83, 'old': 47.19, 'new': 63.83}
2024-10-28 10:21:45,266 [trainer.py] => CNN top1 curve: [90.13, 62.83, 45.44]
2024-10-28 10:21:45,266 [trainer.py] => CNN top5 curve: [100.0, 97.45, 91.48]
2024-10-28 10:21:45,267 [trainer.py] => NME top1 curve: [89.53, 64.71, 50.89]
2024-10-28 10:21:45,267 [trainer.py] => NME top5 curve: [100.0, 96.88, 92.56]

2024-10-28 10:21:45,267 [trainer.py] => Average Accuracy (CNN): 66.13333333333333
2024-10-28 10:21:45,268 [trainer.py] => Average Accuracy (NME): 68.37666666666667
2024-10-28 10:21:45,269 [trainer.py] => Forgetting (CNN): 37.44
