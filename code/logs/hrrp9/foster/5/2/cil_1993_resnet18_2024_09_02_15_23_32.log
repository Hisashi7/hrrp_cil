2024-09-02 15:23:32,665 [trainer.py] => config: ./exps/foster.json
2024-09-02 15:23:32,665 [trainer.py] => prefix: cil
2024-09-02 15:23:32,665 [trainer.py] => dataset: hrrp9
2024-09-02 15:23:32,665 [trainer.py] => memory_size: 500
2024-09-02 15:23:32,665 [trainer.py] => memory_per_class: 20
2024-09-02 15:23:32,665 [trainer.py] => fixed_memory: False
2024-09-02 15:23:32,665 [trainer.py] => shuffle: True
2024-09-02 15:23:32,665 [trainer.py] => init_cls: 5
2024-09-02 15:23:32,665 [trainer.py] => increment: 2
2024-09-02 15:23:32,665 [trainer.py] => model_name: foster
2024-09-02 15:23:32,665 [trainer.py] => convnet_type: resnet18
2024-09-02 15:23:32,665 [trainer.py] => init_train: False
2024-09-02 15:23:32,665 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-09-02 15:23:32,665 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-09-02 15:23:32,665 [trainer.py] => device: [device(type='cuda', index=3)]
2024-09-02 15:23:32,665 [trainer.py] => seed: 1993
2024-09-02 15:23:32,666 [trainer.py] => beta1: 0.96
2024-09-02 15:23:32,666 [trainer.py] => beta2: 0.97
2024-09-02 15:23:32,666 [trainer.py] => oofc: ft
2024-09-02 15:23:32,666 [trainer.py] => is_teacher_wa: True
2024-09-02 15:23:32,666 [trainer.py] => is_student_wa: True
2024-09-02 15:23:32,666 [trainer.py] => lambda_okd: 0
2024-09-02 15:23:32,666 [trainer.py] => wa_value: 1
2024-09-02 15:23:32,666 [trainer.py] => init_epochs: 0
2024-09-02 15:23:32,666 [trainer.py] => init_lr: 0.1
2024-09-02 15:23:32,666 [trainer.py] => init_weight_decay: 0.0005
2024-09-02 15:23:32,666 [trainer.py] => boosting_epochs: 200
2024-09-02 15:23:32,666 [trainer.py] => compression_epochs: 100
2024-09-02 15:23:32,666 [trainer.py] => lr: 0.1
2024-09-02 15:23:32,666 [trainer.py] => batch_size: 128
2024-09-02 15:23:32,666 [trainer.py] => weight_decay: 0.0005
2024-09-02 15:23:32,666 [trainer.py] => num_workers: 8
2024-09-02 15:23:32,666 [trainer.py] => T: 2
2024-09-02 15:23:33,232 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-09-02 15:23:33,292 [trainer.py] => All params: 0
2024-09-02 15:23:33,292 [trainer.py] => Trainable params: 0
2024-09-02 15:23:35,391 [foster.py] => Learning on 0-5
2024-09-02 15:23:35,392 [foster.py] => All params: 3849034
2024-09-02 15:23:35,393 [foster.py] => Trainable params: 3849034
2024-09-02 15:23:37,119 [foster.py] => init_train?---False
2024-09-02 15:23:40,817 [base.py] => Reducing exemplars...(100 per classes)
2024-09-02 15:23:40,817 [base.py] => Constructing exemplars...(100 per classes)
2024-09-02 15:23:54,045 [foster.py] => Exemplar size: 500
2024-09-02 15:23:54,045 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-09-02 15:23:54,045 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-09-02 15:23:54,046 [trainer.py] => CNN top1 curve: [89.93]
2024-09-02 15:23:54,046 [trainer.py] => CNN top5 curve: [100.0]
2024-09-02 15:23:54,046 [trainer.py] => NME top1 curve: [90.0]
2024-09-02 15:23:54,046 [trainer.py] => NME top5 curve: [100.0]

2024-09-02 15:23:54,046 [trainer.py] => Average Accuracy (CNN): 89.93
2024-09-02 15:23:54,046 [trainer.py] => Average Accuracy (NME): 90.0
2024-09-02 15:23:54,046 [trainer.py] => All params: 3849034
2024-09-02 15:23:54,047 [trainer.py] => Trainable params: 3849034
2024-09-02 15:23:54,090 [foster.py] => Learning on 5-7
2024-09-02 15:23:54,091 [foster.py] => All params: 7701139
2024-09-02 15:23:54,091 [foster.py] => Trainable params: 3854670
2024-09-02 15:23:54,115 [foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-09-02 15:23:57,945 [foster.py] => Task 1, Epoch 1/200 => Loss 1.482, Loss_clf 0.721, Loss_fe 0.761, Loss_kd 0.000, Train_accy 81.60, Test_accy 29.14
2024-09-02 15:24:09,378 [foster.py] => Task 1, Epoch 6/200 => Loss 0.238, Loss_clf 0.051, Loss_fe 0.188, Loss_kd 0.000, Train_accy 98.49, Test_accy 71.50
2024-09-02 15:24:20,066 [foster.py] => Task 1, Epoch 11/200 => Loss 0.144, Loss_clf 0.038, Loss_fe 0.107, Loss_kd 0.000, Train_accy 99.00, Test_accy 59.67
2024-09-02 15:24:31,709 [foster.py] => Task 1, Epoch 16/200 => Loss 0.045, Loss_clf 0.013, Loss_fe 0.033, Loss_kd 0.000, Train_accy 99.76, Test_accy 67.76
2024-09-02 15:24:42,744 [foster.py] => Task 1, Epoch 21/200 => Loss 0.028, Loss_clf 0.010, Loss_fe 0.018, Loss_kd 0.000, Train_accy 99.87, Test_accy 63.10
2024-09-02 15:24:54,210 [foster.py] => Task 1, Epoch 26/200 => Loss 0.063, Loss_clf 0.020, Loss_fe 0.043, Loss_kd 0.000, Train_accy 99.60, Test_accy 71.69
2024-09-02 15:25:05,296 [foster.py] => Task 1, Epoch 31/200 => Loss 0.027, Loss_clf 0.008, Loss_fe 0.018, Loss_kd 0.000, Train_accy 99.91, Test_accy 69.10
2024-09-02 15:25:16,771 [foster.py] => Task 1, Epoch 36/200 => Loss 0.007, Loss_clf 0.002, Loss_fe 0.004, Loss_kd 0.000, Train_accy 99.98, Test_accy 69.36
2024-09-02 15:25:28,954 [foster.py] => Task 1, Epoch 41/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 70.90
2024-09-02 15:25:40,704 [foster.py] => Task 1, Epoch 46/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 71.38
2024-09-02 15:25:51,437 [foster.py] => Task 1, Epoch 51/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 70.95
2024-09-02 15:26:02,795 [foster.py] => Task 1, Epoch 56/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 70.95
2024-09-02 15:26:14,198 [foster.py] => Task 1, Epoch 61/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 70.90
2024-09-02 15:26:25,777 [foster.py] => Task 1, Epoch 66/200 => Loss 0.300, Loss_clf 0.100, Loss_fe 0.200, Loss_kd 0.000, Train_accy 96.62, Test_accy 56.45
2024-09-02 15:26:36,961 [foster.py] => Task 1, Epoch 71/200 => Loss 0.020, Loss_clf 0.006, Loss_fe 0.014, Loss_kd 0.000, Train_accy 99.91, Test_accy 67.95
2024-09-02 15:26:48,024 [foster.py] => Task 1, Epoch 76/200 => Loss 0.001, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.86
2024-09-02 15:27:00,078 [foster.py] => Task 1, Epoch 81/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.55
2024-09-02 15:27:12,378 [foster.py] => Task 1, Epoch 86/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.26
2024-09-02 15:27:23,403 [foster.py] => Task 1, Epoch 91/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 69.31
2024-09-02 15:27:34,785 [foster.py] => Task 1, Epoch 96/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.95
2024-09-02 15:27:46,417 [foster.py] => Task 1, Epoch 101/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 68.19
2024-09-02 15:27:57,214 [foster.py] => Task 1, Epoch 106/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.24
2024-09-02 15:28:08,663 [foster.py] => Task 1, Epoch 111/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.33
2024-09-02 15:28:20,668 [foster.py] => Task 1, Epoch 116/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 67.45
2024-09-02 15:28:31,547 [foster.py] => Task 1, Epoch 121/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 66.24
2024-09-02 15:28:43,029 [foster.py] => Task 1, Epoch 126/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.83
2024-09-02 15:28:54,393 [foster.py] => Task 1, Epoch 131/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.95
2024-09-02 15:29:05,207 [foster.py] => Task 1, Epoch 136/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.86
2024-09-02 15:29:17,398 [foster.py] => Task 1, Epoch 141/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.95
2024-09-02 15:29:28,754 [foster.py] => Task 1, Epoch 146/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.90
2024-09-02 15:29:40,261 [foster.py] => Task 1, Epoch 151/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.79
2024-09-02 15:29:51,652 [foster.py] => Task 1, Epoch 156/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.76
2024-09-02 15:30:03,188 [foster.py] => Task 1, Epoch 161/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.93
2024-09-02 15:30:14,682 [foster.py] => Task 1, Epoch 166/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.12
2024-09-02 15:30:26,874 [foster.py] => Task 1, Epoch 171/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.12
2024-09-02 15:30:37,577 [foster.py] => Task 1, Epoch 176/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.52
2024-09-02 15:30:48,097 [foster.py] => Task 1, Epoch 181/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.33
2024-09-02 15:30:59,455 [foster.py] => Task 1, Epoch 186/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.38
2024-09-02 15:31:09,960 [foster.py] => Task 1, Epoch 191/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.88
2024-09-02 15:31:21,099 [foster.py] => Task 1, Epoch 196/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.81
2024-09-02 15:31:28,717 [foster.py] => Task 1, Epoch 200/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00
2024-09-02 15:31:28,718 [inc_net.py] => align weights, gamma = 0.6539922952651978 
2024-09-02 15:31:28,719 [foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-09-02 15:31:32,279 [foster.py] => SNet: Task 1, Epoch 1/100 => Loss 1.202,  Train_accy 72.36, Test_accy 51.95
2024-09-02 15:31:44,062 [foster.py] => SNet: Task 1, Epoch 6/100 => Loss 0.704,  Train_accy 100.00, Test_accy 70.98
2024-09-02 15:31:55,551 [foster.py] => SNet: Task 1, Epoch 11/100 => Loss 0.693,  Train_accy 100.00, Test_accy 71.52
2024-09-02 15:32:07,011 [foster.py] => SNet: Task 1, Epoch 16/100 => Loss 0.692,  Train_accy 100.00, Test_accy 72.60
2024-09-02 15:32:18,726 [foster.py] => SNet: Task 1, Epoch 21/100 => Loss 0.688,  Train_accy 100.00, Test_accy 73.14
2024-09-02 15:32:31,108 [foster.py] => SNet: Task 1, Epoch 26/100 => Loss 0.687,  Train_accy 100.00, Test_accy 73.33
2024-09-02 15:32:43,264 [foster.py] => SNet: Task 1, Epoch 31/100 => Loss 0.685,  Train_accy 100.00, Test_accy 71.52
2024-09-02 15:32:55,472 [foster.py] => SNet: Task 1, Epoch 36/100 => Loss 0.685,  Train_accy 100.00, Test_accy 72.95
2024-09-02 15:33:07,222 [foster.py] => SNet: Task 1, Epoch 41/100 => Loss 0.687,  Train_accy 100.00, Test_accy 73.48
2024-09-02 15:33:18,729 [foster.py] => SNet: Task 1, Epoch 46/100 => Loss 0.685,  Train_accy 100.00, Test_accy 71.05
2024-09-02 15:33:30,621 [foster.py] => SNet: Task 1, Epoch 51/100 => Loss 0.684,  Train_accy 100.00, Test_accy 72.88
2024-09-02 15:33:41,564 [foster.py] => SNet: Task 1, Epoch 56/100 => Loss 0.685,  Train_accy 100.00, Test_accy 72.29
2024-09-02 15:33:52,377 [foster.py] => SNet: Task 1, Epoch 61/100 => Loss 0.684,  Train_accy 100.00, Test_accy 72.86
2024-09-02 15:34:04,017 [foster.py] => SNet: Task 1, Epoch 66/100 => Loss 0.684,  Train_accy 100.00, Test_accy 72.40
2024-09-02 15:34:15,550 [foster.py] => SNet: Task 1, Epoch 71/100 => Loss 0.683,  Train_accy 100.00, Test_accy 72.05
2024-09-02 15:34:26,361 [foster.py] => SNet: Task 1, Epoch 76/100 => Loss 0.685,  Train_accy 100.00, Test_accy 73.02
2024-09-02 15:34:37,251 [foster.py] => SNet: Task 1, Epoch 81/100 => Loss 0.684,  Train_accy 100.00, Test_accy 72.24
2024-09-02 15:34:48,966 [foster.py] => SNet: Task 1, Epoch 86/100 => Loss 0.683,  Train_accy 100.00, Test_accy 72.48
2024-09-02 15:35:00,926 [foster.py] => SNet: Task 1, Epoch 91/100 => Loss 0.683,  Train_accy 100.00, Test_accy 72.48
2024-09-02 15:35:12,274 [foster.py] => SNet: Task 1, Epoch 96/100 => Loss 0.684,  Train_accy 100.00, Test_accy 72.95
2024-09-02 15:35:20,395 [foster.py] => SNet: Task 1, Epoch 100/100 => Loss 0.684,  Train_accy 100.00
2024-09-02 15:35:20,396 [inc_net.py] => align weights, gamma = 1.1825617551803589 
2024-09-02 15:35:21,869 [foster.py] => darknet eval: 
2024-09-02 15:35:21,869 [foster.py] => CNN top1 curve: 70.24
2024-09-02 15:35:21,869 [foster.py] => CNN top5 curve: 98.6
2024-09-02 15:35:21,870 [base.py] => Reducing exemplars...(71 per classes)
2024-09-02 15:35:25,699 [base.py] => Constructing exemplars...(71 per classes)
2024-09-02 15:35:33,679 [foster.py] => Exemplar size: 497
2024-09-02 15:35:33,679 [trainer.py] => CNN: {'total': 71.21, '00-04': 61.03, '05-06': 96.67, 'old': 61.03, 'new': 96.67}
2024-09-02 15:35:33,688 [trainer.py] => NME: {'total': 70.52, '00-04': 60.43, '05-06': 95.75, 'old': 60.43, 'new': 95.75}
2024-09-02 15:35:33,688 [trainer.py] => CNN top1 curve: [89.93, 71.21]
2024-09-02 15:35:33,689 [trainer.py] => CNN top5 curve: [100.0, 98.74]
2024-09-02 15:35:33,689 [trainer.py] => NME top1 curve: [90.0, 70.52]
2024-09-02 15:35:33,689 [trainer.py] => NME top5 curve: [100.0, 98.83]

2024-09-02 15:35:33,689 [trainer.py] => Average Accuracy (CNN): 80.57
2024-09-02 15:35:33,689 [trainer.py] => Average Accuracy (NME): 80.25999999999999
2024-09-02 15:35:33,690 [trainer.py] => All params: 7701139
2024-09-02 15:35:33,690 [trainer.py] => Trainable params: 3854670
2024-09-02 15:35:33,745 [foster.py] => Learning on 7-9
2024-09-02 15:35:33,746 [foster.py] => All params: 7705241
2024-09-02 15:35:33,746 [foster.py] => Trainable params: 3857746
2024-09-02 15:35:33,773 [foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-09-02 15:35:37,578 [foster.py] => Task 2, Epoch 1/200 => Loss 1.337, Loss_clf 0.596, Loss_fe 0.740, Loss_kd 0.000, Train_accy 84.66, Test_accy 22.41
2024-09-02 15:35:48,658 [foster.py] => Task 2, Epoch 6/200 => Loss 0.289, Loss_clf 0.031, Loss_fe 0.259, Loss_kd 0.000, Train_accy 99.31, Test_accy 50.74
2024-09-02 15:35:59,941 [foster.py] => Task 2, Epoch 11/200 => Loss 0.253, Loss_clf 0.037, Loss_fe 0.217, Loss_kd 0.000, Train_accy 99.20, Test_accy 59.52
2024-09-02 15:36:11,490 [foster.py] => Task 2, Epoch 16/200 => Loss 0.110, Loss_clf 0.013, Loss_fe 0.097, Loss_kd 0.000, Train_accy 99.69, Test_accy 65.78
2024-09-02 15:36:23,126 [foster.py] => Task 2, Epoch 21/200 => Loss 0.110, Loss_clf 0.015, Loss_fe 0.095, Loss_kd 0.000, Train_accy 99.64, Test_accy 64.96
2024-09-02 15:36:34,563 [foster.py] => Task 2, Epoch 26/200 => Loss 0.006, Loss_clf 0.002, Loss_fe 0.003, Loss_kd 0.000, Train_accy 100.00, Test_accy 62.06
2024-09-02 15:36:45,662 [foster.py] => Task 2, Epoch 31/200 => Loss 0.019, Loss_clf 0.003, Loss_fe 0.016, Loss_kd 0.000, Train_accy 99.96, Test_accy 64.37
2024-09-02 15:36:57,751 [foster.py] => Task 2, Epoch 36/200 => Loss 0.008, Loss_clf 0.002, Loss_fe 0.006, Loss_kd 0.000, Train_accy 100.00, Test_accy 66.56
2024-09-02 15:37:09,749 [foster.py] => Task 2, Epoch 41/200 => Loss 0.175, Loss_clf 0.041, Loss_fe 0.134, Loss_kd 0.000, Train_accy 98.80, Test_accy 48.74
2024-09-02 15:37:21,436 [foster.py] => Task 2, Epoch 46/200 => Loss 0.004, Loss_clf 0.002, Loss_fe 0.003, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.98
2024-09-02 15:37:33,334 [foster.py] => Task 2, Epoch 51/200 => Loss 0.072, Loss_clf 0.015, Loss_fe 0.057, Loss_kd 0.000, Train_accy 99.67, Test_accy 53.59
2024-09-02 15:37:45,314 [foster.py] => Task 2, Epoch 56/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.87
2024-09-02 15:37:57,039 [foster.py] => Task 2, Epoch 61/200 => Loss 0.016, Loss_clf 0.004, Loss_fe 0.012, Loss_kd 0.000, Train_accy 99.98, Test_accy 63.70
2024-09-02 15:38:08,321 [foster.py] => Task 2, Epoch 66/200 => Loss 0.010, Loss_clf 0.003, Loss_fe 0.007, Loss_kd 0.000, Train_accy 99.98, Test_accy 59.96
2024-09-02 15:38:20,129 [foster.py] => Task 2, Epoch 71/200 => Loss 0.007, Loss_clf 0.002, Loss_fe 0.005, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.85
2024-09-02 15:38:31,699 [foster.py] => Task 2, Epoch 76/200 => Loss 0.040, Loss_clf 0.010, Loss_fe 0.029, Loss_kd 0.000, Train_accy 99.84, Test_accy 68.83
2024-09-02 15:38:43,696 [foster.py] => Task 2, Epoch 81/200 => Loss 0.014, Loss_clf 0.004, Loss_fe 0.010, Loss_kd 0.000, Train_accy 99.98, Test_accy 58.83
2024-09-02 15:38:54,876 [foster.py] => Task 2, Epoch 86/200 => Loss 0.009, Loss_clf 0.003, Loss_fe 0.006, Loss_kd 0.000, Train_accy 99.98, Test_accy 67.94
2024-09-02 15:39:06,459 [foster.py] => Task 2, Epoch 91/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.96
2024-09-02 15:39:18,250 [foster.py] => Task 2, Epoch 96/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.30
2024-09-02 15:39:29,817 [foster.py] => Task 2, Epoch 101/200 => Loss 0.004, Loss_clf 0.002, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 60.67
2024-09-02 15:39:42,116 [foster.py] => Task 2, Epoch 106/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 66.06
2024-09-02 15:39:54,209 [foster.py] => Task 2, Epoch 111/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 66.57
2024-09-02 15:40:06,068 [foster.py] => Task 2, Epoch 116/200 => Loss 0.004, Loss_clf 0.001, Loss_fe 0.003, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.50
2024-09-02 15:40:18,223 [foster.py] => Task 2, Epoch 121/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.63
2024-09-02 15:40:29,896 [foster.py] => Task 2, Epoch 126/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.76
2024-09-02 15:40:41,214 [foster.py] => Task 2, Epoch 131/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.63
2024-09-02 15:40:53,262 [foster.py] => Task 2, Epoch 136/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.31
2024-09-02 15:41:04,826 [foster.py] => Task 2, Epoch 141/200 => Loss 0.003, Loss_clf 0.002, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.94
2024-09-02 15:41:16,816 [foster.py] => Task 2, Epoch 146/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.87
2024-09-02 15:41:28,608 [foster.py] => Task 2, Epoch 151/200 => Loss 0.005, Loss_clf 0.002, Loss_fe 0.003, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.17
2024-09-02 15:41:40,569 [foster.py] => Task 2, Epoch 156/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.98
2024-09-02 15:41:52,624 [foster.py] => Task 2, Epoch 161/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.15
2024-09-02 15:42:04,534 [foster.py] => Task 2, Epoch 166/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.41
2024-09-02 15:42:16,090 [foster.py] => Task 2, Epoch 171/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.15
2024-09-02 15:42:27,989 [foster.py] => Task 2, Epoch 176/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 65.09
2024-09-02 15:42:39,541 [foster.py] => Task 2, Epoch 181/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.57
2024-09-02 15:42:51,306 [foster.py] => Task 2, Epoch 186/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.19
2024-09-02 15:43:03,407 [foster.py] => Task 2, Epoch 191/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 63.94
2024-09-02 15:43:15,527 [foster.py] => Task 2, Epoch 196/200 => Loss 0.002, Loss_clf 0.001, Loss_fe 0.001, Loss_kd 0.000, Train_accy 100.00, Test_accy 64.43
2024-09-02 15:43:23,526 [foster.py] => Task 2, Epoch 200/200 => Loss 0.003, Loss_clf 0.001, Loss_fe 0.002, Loss_kd 0.000, Train_accy 100.00
2024-09-02 15:43:23,527 [inc_net.py] => align weights, gamma = 0.6145681738853455 
2024-09-02 15:43:23,528 [foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-09-02 15:43:26,879 [foster.py] => SNet: Task 2, Epoch 1/100 => Loss 1.435,  Train_accy 76.50, Test_accy 36.26
2024-09-02 15:43:38,775 [foster.py] => SNet: Task 2, Epoch 6/100 => Loss 0.996,  Train_accy 100.00, Test_accy 68.33
2024-09-02 15:43:50,911 [foster.py] => SNet: Task 2, Epoch 11/100 => Loss 0.989,  Train_accy 100.00, Test_accy 67.87
2024-09-02 15:44:03,143 [foster.py] => SNet: Task 2, Epoch 16/100 => Loss 0.986,  Train_accy 100.00, Test_accy 71.76
2024-09-02 15:44:15,197 [foster.py] => SNet: Task 2, Epoch 21/100 => Loss 0.985,  Train_accy 100.00, Test_accy 70.83
2024-09-02 15:44:26,993 [foster.py] => SNet: Task 2, Epoch 26/100 => Loss 0.984,  Train_accy 100.00, Test_accy 72.09
2024-09-02 15:44:38,427 [foster.py] => SNet: Task 2, Epoch 31/100 => Loss 0.983,  Train_accy 100.00, Test_accy 70.41
2024-09-02 15:44:49,777 [foster.py] => SNet: Task 2, Epoch 36/100 => Loss 0.985,  Train_accy 100.00, Test_accy 70.85
2024-09-02 15:45:01,851 [foster.py] => SNet: Task 2, Epoch 41/100 => Loss 0.986,  Train_accy 100.00, Test_accy 70.09
2024-09-02 15:45:14,002 [foster.py] => SNet: Task 2, Epoch 46/100 => Loss 0.983,  Train_accy 100.00, Test_accy 70.80
2024-09-02 15:45:25,982 [foster.py] => SNet: Task 2, Epoch 51/100 => Loss 0.983,  Train_accy 100.00, Test_accy 70.81
2024-09-02 15:45:37,734 [foster.py] => SNet: Task 2, Epoch 56/100 => Loss 0.982,  Train_accy 100.00, Test_accy 70.20
2024-09-02 15:45:49,777 [foster.py] => SNet: Task 2, Epoch 61/100 => Loss 0.983,  Train_accy 100.00, Test_accy 71.19
2024-09-02 15:46:02,242 [foster.py] => SNet: Task 2, Epoch 66/100 => Loss 0.980,  Train_accy 100.00, Test_accy 70.13
2024-09-02 15:46:14,481 [foster.py] => SNet: Task 2, Epoch 71/100 => Loss 0.980,  Train_accy 100.00, Test_accy 71.24
2024-09-02 15:46:25,952 [foster.py] => SNet: Task 2, Epoch 76/100 => Loss 0.981,  Train_accy 100.00, Test_accy 71.09
2024-09-02 15:46:37,590 [foster.py] => SNet: Task 2, Epoch 81/100 => Loss 0.981,  Train_accy 100.00, Test_accy 70.54
2024-09-02 15:46:49,830 [foster.py] => SNet: Task 2, Epoch 86/100 => Loss 0.984,  Train_accy 100.00, Test_accy 70.74
2024-09-02 15:47:01,830 [foster.py] => SNet: Task 2, Epoch 91/100 => Loss 0.981,  Train_accy 100.00, Test_accy 71.00
2024-09-02 15:47:14,218 [foster.py] => SNet: Task 2, Epoch 96/100 => Loss 0.980,  Train_accy 100.00, Test_accy 70.76
2024-09-02 15:47:22,719 [foster.py] => SNet: Task 2, Epoch 100/100 => Loss 0.980,  Train_accy 100.00
2024-09-02 15:47:22,719 [inc_net.py] => align weights, gamma = 1.199683666229248 
2024-09-02 15:47:24,371 [foster.py] => darknet eval: 
2024-09-02 15:47:24,371 [foster.py] => CNN top1 curve: 68.24
2024-09-02 15:47:24,371 [foster.py] => CNN top5 curve: 97.13
2024-09-02 15:47:24,372 [base.py] => Reducing exemplars...(55 per classes)
2024-09-02 15:47:29,440 [base.py] => Constructing exemplars...(55 per classes)
2024-09-02 15:47:36,954 [foster.py] => Exemplar size: 495
2024-09-02 15:47:36,954 [trainer.py] => CNN: {'total': 68.04, '00-04': 51.5, '05-06': 80.5, '07-08': 96.92, 'old': 59.79, 'new': 96.92}
2024-09-02 15:47:36,954 [trainer.py] => NME: {'total': 68.5, '00-04': 54.87, '05-06': 74.75, '07-08': 96.33, 'old': 60.55, 'new': 96.33}
2024-09-02 15:47:36,954 [trainer.py] => CNN top1 curve: [89.93, 71.21, 68.04]
2024-09-02 15:47:36,954 [trainer.py] => CNN top5 curve: [100.0, 98.74, 96.24]
2024-09-02 15:47:36,954 [trainer.py] => NME top1 curve: [90.0, 70.52, 68.5]
2024-09-02 15:47:36,955 [trainer.py] => NME top5 curve: [100.0, 98.83, 95.69]

2024-09-02 15:47:36,955 [trainer.py] => Average Accuracy (CNN): 76.39333333333333
2024-09-02 15:47:36,955 [trainer.py] => Average Accuracy (NME): 76.33999999999999
2024-09-02 15:47:36,955 [trainer.py] => Forgetting (CNN): 27.300000000000004
