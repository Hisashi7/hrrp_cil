2024-10-25 10:48:01,137 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 10:48:01,137 [trainer.py] => prefix: cil
2024-10-25 10:48:01,137 [trainer.py] => dataset: hrrp9
2024-10-25 10:48:01,137 [trainer.py] => memory_size: 500
2024-10-25 10:48:01,138 [trainer.py] => memory_per_class: 20
2024-10-25 10:48:01,138 [trainer.py] => fixed_memory: False
2024-10-25 10:48:01,138 [trainer.py] => shuffle: True
2024-10-25 10:48:01,138 [trainer.py] => init_cls: 5
2024-10-25 10:48:01,138 [trainer.py] => increment: 2
2024-10-25 10:48:01,138 [trainer.py] => model_name: POD_foster
2024-10-25 10:48:01,138 [trainer.py] => convnet_type: resnet18
2024-10-25 10:48:01,138 [trainer.py] => init_train: False
2024-10-25 10:48:01,138 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 10:48:01,138 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 10:48:01,139 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 10:48:01,139 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 10:48:01,139 [trainer.py] => device: [device(type='cuda', index=5)]
2024-10-25 10:48:01,139 [trainer.py] => seed: 1993
2024-10-25 10:48:01,139 [trainer.py] => beta1: 0.96
2024-10-25 10:48:01,139 [trainer.py] => beta2: 0.97
2024-10-25 10:48:01,139 [trainer.py] => oofc: ft
2024-10-25 10:48:01,139 [trainer.py] => is_teacher_wa: False
2024-10-25 10:48:01,139 [trainer.py] => is_student_wa: True
2024-10-25 10:48:01,139 [trainer.py] => lambda_okd: 0
2024-10-25 10:48:01,139 [trainer.py] => wa_value: 1
2024-10-25 10:48:01,139 [trainer.py] => init_epochs: 0
2024-10-25 10:48:01,139 [trainer.py] => init_lr: 0.1
2024-10-25 10:48:01,139 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 10:48:01,139 [trainer.py] => boosting_epochs: 150
2024-10-25 10:48:01,139 [trainer.py] => compression_epochs: 120
2024-10-25 10:48:01,140 [trainer.py] => lr: 0.1
2024-10-25 10:48:01,140 [trainer.py] => batch_size: 128
2024-10-25 10:48:01,140 [trainer.py] => weight_decay: 0.0005
2024-10-25 10:48:01,140 [trainer.py] => num_workers: 8
2024-10-25 10:48:01,140 [trainer.py] => momentum: 0.9
2024-10-25 10:48:01,140 [trainer.py] => T: 2
2024-10-25 10:48:01,140 [trainer.py] => lambda_c_base: 0.8
2024-10-25 10:48:01,140 [trainer.py] => lambda_f_base: 1.0
2024-10-25 10:48:01,140 [trainer.py] => POD: w
2024-10-25 10:48:01,811 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 10:48:01,869 [trainer.py] => All params: 0
2024-10-25 10:48:01,870 [trainer.py] => Trainable params: 0
2024-10-25 10:48:02,734 [pod_foster.py] => Learning on 0-5
2024-10-25 10:48:02,735 [pod_foster.py] => All params: 3849034
2024-10-25 10:48:02,735 [pod_foster.py] => Trainable params: 3849034
2024-10-25 10:48:02,770 [pod_foster.py] => Adaptive factor: 0
2024-10-25 10:48:02,927 [pod_foster.py] => init_train?---False
2024-10-25 10:48:03,767 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 10:48:03,767 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 10:48:09,199 [trainer.py] => All params: 3849034
2024-10-25 10:48:10,238 [pod_foster.py] => Exemplar size: 500
2024-10-25 10:48:10,238 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 10:48:10,238 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 10:48:10,238 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 10:48:10,238 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 10:48:10,238 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 10:48:10,238 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 10:48:10,238 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 10:48:10,239 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 10:48:10,239 [trainer.py] => All params: 3849034
2024-10-25 10:48:10,239 [trainer.py] => Trainable params: 3849034
2024-10-25 10:48:10,274 [pod_foster.py] => Learning on 5-7
2024-10-25 10:48:10,275 [pod_foster.py] => All params: 7701139
2024-10-25 10:48:10,276 [pod_foster.py] => Trainable params: 3854670
2024-10-25 10:48:10,288 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 10:48:10,297 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 10:48:13,149 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.969, Loss_clf 0.708, Loss_fe 0.661, Loss_pod 0.402, Loss_flat 0.198, Train_accy 83.13, Test_accy 54.45
2024-10-25 10:48:21,967 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.355, Loss_clf 0.020, Loss_fe 0.042, Loss_pod 0.223, Loss_flat 0.071, Train_accy 99.82, Test_accy 71.38
2024-10-25 10:48:30,777 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.228, Loss_clf 0.008, Loss_fe 0.012, Loss_pod 0.168, Loss_flat 0.040, Train_accy 100.00, Test_accy 70.69
2024-10-25 10:48:40,372 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.197, Loss_clf 0.006, Loss_fe 0.009, Loss_pod 0.150, Loss_flat 0.032, Train_accy 100.00, Test_accy 70.24
2024-10-25 10:48:49,806 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.195, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.147, Loss_flat 0.030, Train_accy 100.00, Test_accy 64.57
2024-10-25 10:48:59,165 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.407, Loss_clf 0.019, Loss_fe 0.034, Loss_pod 0.270, Loss_flat 0.083, Train_accy 99.76, Test_accy 69.74
2024-10-25 10:49:08,464 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.228, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.173, Loss_flat 0.038, Train_accy 100.00, Test_accy 69.62
2024-10-25 10:49:17,590 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.189, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.149, Loss_flat 0.028, Train_accy 100.00, Test_accy 67.98
2024-10-25 10:49:26,683 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.170, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.134, Loss_flat 0.025, Train_accy 100.00, Test_accy 68.62
2024-10-25 10:49:35,985 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.163, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.130, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.95
2024-10-25 10:49:45,126 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.161, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.130, Loss_flat 0.021, Train_accy 100.00, Test_accy 67.29
2024-10-25 10:49:54,312 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.146, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.117, Loss_flat 0.020, Train_accy 100.00, Test_accy 68.21
2024-10-25 10:50:03,726 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.164, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.127, Loss_flat 0.024, Train_accy 100.00, Test_accy 66.57
2024-10-25 10:50:13,057 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.163, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.127, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.19
2024-10-25 10:50:22,264 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.148, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.118, Loss_flat 0.020, Train_accy 100.00, Test_accy 67.88
2024-10-25 10:50:31,295 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.138, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.110, Loss_flat 0.019, Train_accy 100.00, Test_accy 66.76
2024-10-25 10:50:40,647 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.136, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.108, Loss_flat 0.018, Train_accy 100.00, Test_accy 68.67
2024-10-25 10:50:49,913 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.136, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.107, Loss_flat 0.019, Train_accy 100.00, Test_accy 67.79
2024-10-25 10:50:59,417 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.129, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.102, Loss_flat 0.018, Train_accy 100.00, Test_accy 67.17
2024-10-25 10:51:08,793 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.144, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.108, Loss_flat 0.022, Train_accy 100.00, Test_accy 69.88
2024-10-25 10:51:18,123 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.122, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.095, Loss_flat 0.018, Train_accy 100.00, Test_accy 67.95
2024-10-25 10:51:27,336 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.094, Loss_flat 0.018, Train_accy 100.00, Test_accy 68.71
2024-10-25 10:51:36,770 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.093, Loss_flat 0.018, Train_accy 100.00, Test_accy 67.79
2024-10-25 10:51:45,814 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.116, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.090, Loss_flat 0.016, Train_accy 100.00, Test_accy 68.88
2024-10-25 10:51:54,728 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.110, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.084, Loss_flat 0.016, Train_accy 100.00, Test_accy 67.60
2024-10-25 10:52:03,694 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.110, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.084, Loss_flat 0.017, Train_accy 100.00, Test_accy 67.21
2024-10-25 10:52:12,656 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.109, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.084, Loss_flat 0.017, Train_accy 100.00, Test_accy 67.21
2024-10-25 10:52:21,907 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.103, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.078, Loss_flat 0.016, Train_accy 100.00, Test_accy 68.21
2024-10-25 10:52:30,827 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.103, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.078, Loss_flat 0.016, Train_accy 100.00, Test_accy 68.05
2024-10-25 10:52:40,239 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.107, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.082, Loss_flat 0.016, Train_accy 100.00, Test_accy 68.83
2024-10-25 10:52:46,944 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.105, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.079, Loss_flat 0.017, Train_accy 100.00
2024-10-25 10:52:46,945 [pod_foster.py] => do not weight align teacher!
2024-10-25 10:52:46,946 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 10:52:49,083 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 0.989,  Train_accy 71.31, Test_accy 37.29
2024-10-25 10:52:57,073 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.356,  Train_accy 100.00, Test_accy 64.62
2024-10-25 10:53:05,258 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.348,  Train_accy 100.00, Test_accy 66.29
2024-10-25 10:53:13,262 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.345,  Train_accy 100.00, Test_accy 67.31
2024-10-25 10:53:21,102 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.344,  Train_accy 100.00, Test_accy 68.00
2024-10-25 10:53:29,052 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.342,  Train_accy 100.00, Test_accy 68.36
2024-10-25 10:53:37,136 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.345,  Train_accy 100.00, Test_accy 67.10
2024-10-25 10:53:45,224 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.345,  Train_accy 100.00, Test_accy 67.02
2024-10-25 10:53:53,272 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.342,  Train_accy 100.00, Test_accy 67.24
2024-10-25 10:54:01,254 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.342,  Train_accy 100.00, Test_accy 67.57
2024-10-25 10:54:09,386 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.341,  Train_accy 100.00, Test_accy 68.79
2024-10-25 10:54:17,348 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.344,  Train_accy 100.00, Test_accy 67.74
2024-10-25 10:54:25,436 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.341,  Train_accy 100.00, Test_accy 67.93
2024-10-25 10:54:33,807 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.342,  Train_accy 100.00, Test_accy 67.48
2024-10-25 10:54:41,512 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.340,  Train_accy 100.00, Test_accy 68.29
2024-10-25 10:54:49,558 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.341,  Train_accy 100.00, Test_accy 69.12
2024-10-25 10:54:57,452 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.340,  Train_accy 100.00, Test_accy 68.98
2024-10-25 10:55:05,367 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.341,  Train_accy 100.00, Test_accy 68.90
2024-10-25 10:55:13,423 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.342,  Train_accy 100.00, Test_accy 69.74
2024-10-25 10:55:21,454 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.341,  Train_accy 100.00, Test_accy 68.52
2024-10-25 10:55:29,411 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.340,  Train_accy 100.00, Test_accy 69.24
2024-10-25 10:55:37,716 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.340,  Train_accy 100.00, Test_accy 68.81
2024-10-25 10:55:45,648 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.339,  Train_accy 100.00, Test_accy 70.00
2024-10-25 10:55:53,643 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.340,  Train_accy 100.00, Test_accy 69.14
2024-10-25 10:55:59,360 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.341,  Train_accy 100.00
2024-10-25 10:55:59,361 [inc_net.py] => align weights, gamma = 1.160679817199707 
2024-10-25 10:56:00,106 [pod_foster.py] => darknet eval: 
2024-10-25 10:56:00,107 [pod_foster.py] => CNN top1 curve: 65.02
2024-10-25 10:56:00,107 [pod_foster.py] => CNN top5 curve: 98.69
2024-10-25 10:56:00,108 [pod_foster.py] => All params after compression: 3851086
2024-10-25 10:56:00,109 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 10:56:01,266 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 10:56:04,225 [trainer.py] => All params: 7701139
2024-10-25 10:56:06,957 [pod_foster.py] => Exemplar size: 497
2024-10-25 10:56:06,958 [trainer.py] => CNN: {'total': 68.52, '00-04': 56.83, '05-06': 97.75, 'old': 56.83, 'new': 97.75}
2024-10-25 10:56:06,958 [trainer.py] => NME: {'total': 73.45, '00-04': 77.93, '05-06': 62.25, 'old': 77.93, 'new': 62.25}
2024-10-25 10:56:06,958 [trainer.py] => CNN top1 curve: [89.93, 68.52]
2024-10-25 10:56:06,958 [trainer.py] => CNN top5 curve: [100.0, 98.57]
2024-10-25 10:56:06,958 [trainer.py] => NME top1 curve: [90.0, 73.45]
2024-10-25 10:56:06,958 [trainer.py] => NME top5 curve: [100.0, 99.05]

2024-10-25 10:56:06,958 [trainer.py] => Average Accuracy (CNN): 79.225
2024-10-25 10:56:06,958 [trainer.py] => Average Accuracy (NME): 81.725
2024-10-25 10:56:06,959 [trainer.py] => All params: 7701139
2024-10-25 10:56:06,959 [trainer.py] => Trainable params: 3854670
2024-10-25 10:56:07,009 [pod_foster.py] => Learning on 7-9
2024-10-25 10:56:07,010 [pod_foster.py] => All params: 7705241
2024-10-25 10:56:07,010 [pod_foster.py] => Trainable params: 3857746
2024-10-25 10:56:07,039 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 10:56:07,049 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 10:56:09,729 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.126, Loss_clf 0.678, Loss_fe 0.658, Loss_pod 0.535, Loss_flat 0.254, Train_accy 84.46, Test_accy 57.91
2024-10-25 10:56:19,062 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.398, Loss_clf 0.011, Loss_fe 0.063, Loss_pod 0.255, Loss_flat 0.068, Train_accy 99.84, Test_accy 69.20
2024-10-25 10:56:28,581 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.283, Loss_clf 0.008, Loss_fe 0.019, Loss_pod 0.208, Loss_flat 0.048, Train_accy 99.93, Test_accy 70.30
2024-10-25 10:56:38,152 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.316, Loss_clf 0.007, Loss_fe 0.015, Loss_pod 0.240, Loss_flat 0.054, Train_accy 99.98, Test_accy 68.80
2024-10-25 10:56:47,689 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.223, Loss_clf 0.005, Loss_fe 0.009, Loss_pod 0.178, Loss_flat 0.031, Train_accy 100.00, Test_accy 71.78
2024-10-25 10:56:56,957 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.212, Loss_clf 0.005, Loss_fe 0.009, Loss_pod 0.170, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.98
2024-10-25 10:57:06,231 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.216, Loss_clf 0.005, Loss_fe 0.008, Loss_pod 0.172, Loss_flat 0.031, Train_accy 99.98, Test_accy 69.04
2024-10-25 10:57:15,489 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.196, Loss_clf 0.004, Loss_fe 0.007, Loss_pod 0.158, Loss_flat 0.026, Train_accy 100.00, Test_accy 68.65
2024-10-25 10:57:24,713 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.249, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.192, Loss_flat 0.042, Train_accy 100.00, Test_accy 69.52
2024-10-25 10:57:34,370 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.201, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.162, Loss_flat 0.028, Train_accy 100.00, Test_accy 71.56
2024-10-25 10:57:43,693 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.184, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.149, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.30
2024-10-25 10:57:53,107 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.181, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.146, Loss_flat 0.023, Train_accy 100.00, Test_accy 65.78
2024-10-25 10:58:02,303 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.189, Loss_clf 0.005, Loss_fe 0.008, Loss_pod 0.149, Loss_flat 0.027, Train_accy 100.00, Test_accy 68.17
2024-10-25 10:58:11,680 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.197, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.155, Loss_flat 0.028, Train_accy 100.00, Test_accy 73.85
2024-10-25 10:58:21,004 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.220, Loss_clf 0.008, Loss_fe 0.011, Loss_pod 0.170, Loss_flat 0.031, Train_accy 99.93, Test_accy 69.48
2024-10-25 10:58:30,400 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.163, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.132, Loss_flat 0.021, Train_accy 100.00, Test_accy 66.94
2024-10-25 10:58:39,929 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.157, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.126, Loss_flat 0.021, Train_accy 100.00, Test_accy 68.28
2024-10-25 10:58:49,328 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.160, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.130, Loss_flat 0.020, Train_accy 100.00, Test_accy 66.59
2024-10-25 10:58:58,486 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.152, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.122, Loss_flat 0.020, Train_accy 100.00, Test_accy 67.74
2024-10-25 10:59:08,020 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.145, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.117, Loss_flat 0.018, Train_accy 100.00, Test_accy 68.31
2024-10-25 10:59:17,409 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.152, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.123, Loss_flat 0.019, Train_accy 100.00, Test_accy 67.11
2024-10-25 10:59:26,755 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.169, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.131, Loss_flat 0.025, Train_accy 100.00, Test_accy 68.37
2024-10-25 10:59:36,246 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.154, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.121, Loss_flat 0.021, Train_accy 99.98, Test_accy 68.81
2024-10-25 10:59:45,790 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.146, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.114, Loss_flat 0.019, Train_accy 100.00, Test_accy 67.02
2024-10-25 10:59:55,387 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.133, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.105, Loss_flat 0.018, Train_accy 100.00, Test_accy 68.20
2024-10-25 11:00:04,626 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.103, Loss_flat 0.018, Train_accy 100.00, Test_accy 68.69
2024-10-25 11:00:14,039 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.124, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.097, Loss_flat 0.018, Train_accy 100.00, Test_accy 69.00
2024-10-25 11:00:23,606 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.094, Loss_flat 0.017, Train_accy 100.00, Test_accy 69.50
2024-10-25 11:00:32,920 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.118, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.092, Loss_flat 0.017, Train_accy 100.00, Test_accy 69.00
2024-10-25 11:00:42,402 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.122, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.094, Loss_flat 0.017, Train_accy 100.00, Test_accy 66.35
2024-10-25 11:00:49,185 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.115, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.089, Loss_flat 0.016, Train_accy 100.00
2024-10-25 11:00:49,186 [pod_foster.py] => do not weight align teacher!
2024-10-25 11:00:49,187 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 11:00:51,398 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.157,  Train_accy 71.94, Test_accy 28.22
2024-10-25 11:00:59,660 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 0.454,  Train_accy 99.82, Test_accy 62.52
2024-10-25 11:01:07,537 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 0.432,  Train_accy 100.00, Test_accy 67.67
2024-10-25 11:01:15,721 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 0.431,  Train_accy 100.00, Test_accy 65.09
2024-10-25 11:01:24,019 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 0.430,  Train_accy 99.98, Test_accy 67.52
2024-10-25 11:01:32,189 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 0.426,  Train_accy 100.00, Test_accy 68.54
2024-10-25 11:01:40,466 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 0.425,  Train_accy 100.00, Test_accy 66.56
2024-10-25 11:01:48,243 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 0.426,  Train_accy 100.00, Test_accy 69.20
2024-10-25 11:01:56,336 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 0.428,  Train_accy 100.00, Test_accy 67.37
2024-10-25 11:02:04,577 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 0.424,  Train_accy 100.00, Test_accy 67.74
2024-10-25 11:02:12,845 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 0.425,  Train_accy 100.00, Test_accy 67.20
2024-10-25 11:02:20,925 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 0.422,  Train_accy 100.00, Test_accy 68.65
2024-10-25 11:02:29,253 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 0.424,  Train_accy 100.00, Test_accy 66.96
2024-10-25 11:02:37,352 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 0.425,  Train_accy 100.00, Test_accy 67.46
2024-10-25 11:02:45,708 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 0.424,  Train_accy 100.00, Test_accy 68.98
2024-10-25 11:02:53,974 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 0.423,  Train_accy 100.00, Test_accy 68.26
2024-10-25 11:03:02,108 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 0.423,  Train_accy 100.00, Test_accy 67.02
2024-10-25 11:03:10,162 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 0.424,  Train_accy 100.00, Test_accy 66.41
2024-10-25 11:03:18,354 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 0.423,  Train_accy 100.00, Test_accy 67.72
2024-10-25 11:03:26,741 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 0.425,  Train_accy 100.00, Test_accy 66.98
2024-10-25 11:03:35,105 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 0.425,  Train_accy 100.00, Test_accy 67.28
2024-10-25 11:03:43,209 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 0.423,  Train_accy 100.00, Test_accy 67.67
2024-10-25 11:03:51,567 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 0.422,  Train_accy 100.00, Test_accy 68.13
2024-10-25 11:03:59,768 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 0.425,  Train_accy 100.00, Test_accy 67.91
2024-10-25 11:04:05,643 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 0.423,  Train_accy 100.00
2024-10-25 11:04:05,644 [inc_net.py] => align weights, gamma = 0.9997043609619141 
2024-10-25 11:04:06,324 [pod_foster.py] => darknet eval: 
2024-10-25 11:04:06,324 [pod_foster.py] => CNN top1 curve: 68.17
2024-10-25 11:04:06,324 [pod_foster.py] => CNN top5 curve: 95.69
2024-10-25 11:04:06,326 [pod_foster.py] => All params after compression: 3853138
2024-10-25 11:04:06,327 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 11:04:07,904 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 11:04:10,513 [trainer.py] => All params: 7705241
2024-10-25 11:04:12,332 [pod_foster.py] => Exemplar size: 495
2024-10-25 11:04:12,332 [trainer.py] => CNN: {'total': 68.22, '00-04': 51.6, '05-06': 81.42, '07-08': 96.58, 'old': 60.12, 'new': 96.58}
2024-10-25 11:04:12,332 [trainer.py] => NME: {'total': 72.48, '00-04': 63.7, '05-06': 84.17, '07-08': 82.75, 'old': 69.55, 'new': 82.75}
2024-10-25 11:04:12,332 [trainer.py] => CNN top1 curve: [89.93, 68.52, 68.22]
2024-10-25 11:04:12,332 [trainer.py] => CNN top5 curve: [100.0, 98.57, 96.02]
2024-10-25 11:04:12,332 [trainer.py] => NME top1 curve: [90.0, 73.45, 72.48]
2024-10-25 11:04:12,332 [trainer.py] => NME top5 curve: [100.0, 99.05, 95.83]

2024-10-25 11:04:12,333 [trainer.py] => Average Accuracy (CNN): 75.55666666666666
2024-10-25 11:04:12,333 [trainer.py] => Average Accuracy (NME): 78.64333333333333
2024-10-25 11:04:12,333 [trainer.py] => Forgetting (CNN): 27.330000000000002
