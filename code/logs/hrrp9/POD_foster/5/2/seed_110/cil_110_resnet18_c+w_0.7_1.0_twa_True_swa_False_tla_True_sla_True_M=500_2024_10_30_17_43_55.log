2024-10-30 17:43:55,663 [trainer.py] => config: ./exps/POD_foster.json
2024-10-30 17:43:55,663 [trainer.py] => prefix: cil
2024-10-30 17:43:55,664 [trainer.py] => dataset: hrrp9
2024-10-30 17:43:55,664 [trainer.py] => memory_size: 500
2024-10-30 17:43:55,664 [trainer.py] => memory_per_class: 20
2024-10-30 17:43:55,665 [trainer.py] => fixed_memory: False
2024-10-30 17:43:55,665 [trainer.py] => shuffle: True
2024-10-30 17:43:55,665 [trainer.py] => init_cls: 5
2024-10-30 17:43:55,666 [trainer.py] => increment: 2
2024-10-30 17:43:55,666 [trainer.py] => model_name: POD_foster
2024-10-30 17:43:55,666 [trainer.py] => convnet_type: resnet18
2024-10-30 17:43:55,667 [trainer.py] => init_train: False
2024-10-30 17:43:55,667 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_35172.pth
2024-10-30 17:43:55,668 [trainer.py] => fc_path2: checkpoints/init_train/fc_35172.pth
2024-10-30 17:43:55,668 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42503.pth
2024-10-30 17:43:55,669 [trainer.py] => fc_path1: checkpoints/init_train/fc_42503.pth
2024-10-30 17:43:55,669 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42871.pth
2024-10-30 17:43:55,670 [trainer.py] => fc_path: checkpoints/init_train/fc_42871.pth
2024-10-30 17:43:55,670 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-30 17:43:55,670 [trainer.py] => seed: 110
2024-10-30 17:43:55,671 [trainer.py] => beta1: 0.96
2024-10-30 17:43:55,671 [trainer.py] => beta2: 0.97
2024-10-30 17:43:55,671 [trainer.py] => oofc: ft
2024-10-30 17:43:55,672 [trainer.py] => is_teacher_wa: True
2024-10-30 17:43:55,672 [trainer.py] => is_student_wa: False
2024-10-30 17:43:55,673 [trainer.py] => is_teacher_la: True
2024-10-30 17:43:55,673 [trainer.py] => is_student_la: True
2024-10-30 17:43:55,674 [trainer.py] => lambda_okd: 0
2024-10-30 17:43:55,674 [trainer.py] => wa_value: 1
2024-10-30 17:43:55,675 [trainer.py] => init_epochs: 0
2024-10-30 17:43:55,675 [trainer.py] => init_lr: 0.1
2024-10-30 17:43:55,676 [trainer.py] => init_weight_decay: 0.0005
2024-10-30 17:43:55,676 [trainer.py] => boosting_epochs: 150
2024-10-30 17:43:55,676 [trainer.py] => compression_epochs: 120
2024-10-30 17:43:55,677 [trainer.py] => lr: 0.1
2024-10-30 17:43:55,677 [trainer.py] => batch_size: 128
2024-10-30 17:43:55,678 [trainer.py] => weight_decay: 0.0005
2024-10-30 17:43:55,678 [trainer.py] => num_workers: 8
2024-10-30 17:43:55,678 [trainer.py] => momentum: 0.9
2024-10-30 17:43:55,679 [trainer.py] => T: 2
2024-10-30 17:43:55,680 [trainer.py] => lambda_c_base: 0.7
2024-10-30 17:43:55,680 [trainer.py] => lambda_f_base: 1.0
2024-10-30 17:43:55,680 [trainer.py] => POD: c+w
2024-10-30 17:43:56,454 [data_manager.py] => [4, 2, 8, 7, 1, 6, 5, 3, 0]
2024-10-30 17:43:56,524 [trainer.py] => All params: 0
2024-10-30 17:43:56,524 [trainer.py] => Trainable params: 0
2024-10-30 17:43:57,097 [pod_foster.py] => Learning on 0-5
2024-10-30 17:43:57,098 [pod_foster.py] => All params: 3849034
2024-10-30 17:43:57,098 [pod_foster.py] => Trainable params: 3849034
2024-10-30 17:43:57,302 [pod_foster.py] => Adaptive factor: 0
2024-10-30 17:43:57,324 [pod_foster.py] => init_train?---False
2024-10-30 17:43:58,874 [base.py] => Reducing exemplars...(100 per classes)
2024-10-30 17:43:58,876 [base.py] => Constructing exemplars...(100 per classes)
2024-10-30 17:44:12,682 [trainer.py] => All params: 3849034
2024-10-30 17:44:14,751 [pod_foster.py] => Exemplar size: 500
2024-10-30 17:44:14,752 [trainer.py] => CNN: {'total': 96.3, '00-04': 96.3, 'old': 0, 'new': 96.3}
2024-10-30 17:44:14,753 [trainer.py] => NME: {'total': 96.27, '00-04': 96.27, 'old': 0, 'new': 96.27}
2024-10-30 17:44:14,753 [trainer.py] => CNN top1 curve: [96.3]
2024-10-30 17:44:14,753 [trainer.py] => CNN top5 curve: [100.0]
2024-10-30 17:44:14,753 [trainer.py] => NME top1 curve: [96.27]
2024-10-30 17:44:14,754 [trainer.py] => NME top5 curve: [100.0]

2024-10-30 17:44:14,754 [trainer.py] => Average Accuracy (CNN): 96.3
2024-10-30 17:44:14,754 [trainer.py] => Average Accuracy (NME): 96.27
2024-10-30 17:44:14,755 [trainer.py] => All params: 3849034
2024-10-30 17:44:14,757 [trainer.py] => Trainable params: 3849034
2024-10-30 17:44:14,817 [pod_foster.py] => Learning on 5-7
2024-10-30 17:44:14,819 [pod_foster.py] => All params: 7701139
2024-10-30 17:44:14,820 [pod_foster.py] => Trainable params: 3854670
2024-10-30 17:44:14,967 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-30 17:44:14,971 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-30 17:44:21,211 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 2.014, Loss_clf 0.793, Loss_fe 0.655, Loss_pod 0.318, Loss_flat 0.249, Train_accy 84.56, Test_accy 68.98
2024-10-30 17:44:37,219 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.329, Loss_clf 0.019, Loss_fe 0.045, Loss_pod 0.170, Loss_flat 0.095, Train_accy 99.73, Test_accy 85.60
2024-10-30 17:44:55,752 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.182, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.111, Loss_flat 0.055, Train_accy 100.00, Test_accy 80.33
2024-10-30 17:45:12,049 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.172, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.107, Loss_flat 0.051, Train_accy 100.00, Test_accy 83.90
2024-10-30 17:45:28,203 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.163, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.098, Loss_flat 0.048, Train_accy 99.98, Test_accy 80.24
2024-10-30 17:45:44,028 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.383, Loss_clf 0.021, Loss_fe 0.046, Loss_pod 0.205, Loss_flat 0.111, Train_accy 99.51, Test_accy 81.19
2024-10-30 17:45:59,292 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.246, Loss_clf 0.015, Loss_fe 0.017, Loss_pod 0.145, Loss_flat 0.068, Train_accy 99.69, Test_accy 78.29
2024-10-30 17:46:11,677 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.163, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.107, Loss_flat 0.045, Train_accy 100.00, Test_accy 79.64
2024-10-30 17:46:23,260 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.131, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.088, Loss_flat 0.035, Train_accy 100.00, Test_accy 78.43
2024-10-30 17:46:34,908 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.080, Loss_flat 0.031, Train_accy 100.00, Test_accy 81.95
2024-10-30 17:46:46,738 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.108, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.073, Loss_flat 0.027, Train_accy 100.00, Test_accy 77.52
2024-10-30 17:46:58,947 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.107, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.072, Loss_flat 0.028, Train_accy 100.00, Test_accy 80.50
2024-10-30 17:47:11,153 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.111, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.071, Loss_flat 0.030, Train_accy 100.00, Test_accy 79.69
2024-10-30 17:47:25,948 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.377, Loss_clf 0.031, Loss_fe 0.059, Loss_pod 0.189, Loss_flat 0.098, Train_accy 99.18, Test_accy 80.69
2024-10-30 17:47:41,766 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.121, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.079, Loss_flat 0.033, Train_accy 100.00, Test_accy 82.00
2024-10-30 17:47:58,776 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.105, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.071, Loss_flat 0.027, Train_accy 100.00, Test_accy 80.71
2024-10-30 17:48:16,923 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.096, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.064, Loss_flat 0.024, Train_accy 100.00, Test_accy 79.86
2024-10-30 17:48:34,346 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.096, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.065, Loss_flat 0.024, Train_accy 100.00, Test_accy 78.55
2024-10-30 17:48:52,382 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.091, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.060, Loss_flat 0.023, Train_accy 100.00, Test_accy 79.48
2024-10-30 17:49:10,073 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.099, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.063, Loss_flat 0.027, Train_accy 100.00, Test_accy 78.90
2024-10-30 17:49:27,143 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.089, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.057, Loss_flat 0.023, Train_accy 100.00, Test_accy 78.83
2024-10-30 17:49:41,194 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.084, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.054, Loss_flat 0.023, Train_accy 100.00, Test_accy 79.62
2024-10-30 17:49:53,133 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.084, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.053, Loss_flat 0.023, Train_accy 100.00, Test_accy 78.62
2024-10-30 17:50:04,889 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.081, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.051, Loss_flat 0.022, Train_accy 100.00, Test_accy 79.21
2024-10-30 17:50:16,668 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.075, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.047, Loss_flat 0.021, Train_accy 100.00, Test_accy 78.90
2024-10-30 17:50:28,266 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.076, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.047, Loss_flat 0.021, Train_accy 100.00, Test_accy 78.24
2024-10-30 17:50:39,587 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.074, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.047, Loss_flat 0.020, Train_accy 100.00, Test_accy 78.76
2024-10-30 17:50:54,392 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.074, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.045, Loss_flat 0.021, Train_accy 100.00, Test_accy 79.12
2024-10-30 17:51:10,253 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.073, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.021, Train_accy 100.00, Test_accy 79.05
2024-10-30 17:51:26,557 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.076, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.047, Loss_flat 0.021, Train_accy 100.00, Test_accy 79.43
2024-10-30 17:51:38,298 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.072, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.021, Train_accy 100.00
2024-10-30 17:51:38,300 [inc_net.py] => align weights, gamma = 0.5081154704093933 
2024-10-30 17:51:38,303 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-30 17:51:42,356 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.317,  Train_accy 71.40, Test_accy 69.07
2024-10-30 17:51:59,418 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.017,  Train_accy 95.82, Test_accy 85.60
2024-10-30 17:52:14,945 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.011,  Train_accy 96.62, Test_accy 84.19
2024-10-30 17:52:30,795 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.006,  Train_accy 96.93, Test_accy 85.48
2024-10-30 17:52:48,611 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.004,  Train_accy 97.49, Test_accy 86.38
2024-10-30 17:53:02,639 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.001,  Train_accy 97.18, Test_accy 86.81
2024-10-30 17:53:13,156 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.002,  Train_accy 97.09, Test_accy 86.36
2024-10-30 17:53:23,470 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.001,  Train_accy 97.64, Test_accy 85.98
2024-10-30 17:53:33,455 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.002,  Train_accy 97.51, Test_accy 86.00
2024-10-30 17:53:43,493 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.998,  Train_accy 97.58, Test_accy 85.45
2024-10-30 17:53:53,404 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.000,  Train_accy 97.73, Test_accy 86.36
2024-10-30 17:54:03,059 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.998,  Train_accy 97.53, Test_accy 86.50
2024-10-30 17:54:18,387 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.000,  Train_accy 97.89, Test_accy 86.64
2024-10-30 17:54:32,854 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.000,  Train_accy 97.76, Test_accy 86.57
2024-10-30 17:54:48,502 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.999,  Train_accy 97.42, Test_accy 86.29
2024-10-30 17:55:05,241 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.996,  Train_accy 97.71, Test_accy 86.74
2024-10-30 17:55:24,173 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.997,  Train_accy 97.73, Test_accy 86.21
2024-10-30 17:55:41,454 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.998,  Train_accy 97.91, Test_accy 86.83
2024-10-30 17:55:57,778 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.999,  Train_accy 97.78, Test_accy 86.74
2024-10-30 17:56:12,969 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.997,  Train_accy 97.84, Test_accy 86.88
2024-10-30 17:56:24,239 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.995,  Train_accy 97.96, Test_accy 86.50
2024-10-30 17:56:34,116 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.998,  Train_accy 97.80, Test_accy 86.40
2024-10-30 17:56:44,480 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.998,  Train_accy 97.82, Test_accy 86.93
2024-10-30 17:56:54,675 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.997,  Train_accy 97.89, Test_accy 86.57
2024-10-30 17:57:02,439 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.996,  Train_accy 98.02
2024-10-30 17:57:02,439 [pod_foster.py] => do not weight align student!
2024-10-30 17:57:03,122 [pod_foster.py] => darknet eval: 
2024-10-30 17:57:03,123 [pod_foster.py] => CNN top1 curve: 86.43
2024-10-30 17:57:03,123 [pod_foster.py] => CNN top5 curve: 99.76
2024-10-30 17:57:03,123 [pod_foster.py] => CNN: {'total': 86.43, '00-04': 86.53, '05-06': 86.17, 'old': 86.53, 'new': 86.17}
2024-10-30 17:57:03,124 [pod_foster.py] => All params after compression: 3851086
2024-10-30 17:57:03,124 [base.py] => Reducing exemplars...(71 per classes)
2024-10-30 17:57:04,334 [base.py] => Constructing exemplars...(71 per classes)
2024-10-30 17:57:07,647 [trainer.py] => All params: 7701139
2024-10-30 17:57:09,575 [pod_foster.py] => Exemplar size: 497
2024-10-30 17:57:09,575 [trainer.py] => CNN: {'total': 87.6, '00-04': 89.13, '05-06': 83.75, 'old': 89.13, 'new': 83.75}
2024-10-30 17:57:09,575 [trainer.py] => NME: {'total': 82.1, '00-04': 86.47, '05-06': 71.17, 'old': 86.47, 'new': 71.17}
2024-10-30 17:57:09,576 [trainer.py] => CNN top1 curve: [96.3, 87.6]
2024-10-30 17:57:09,576 [trainer.py] => CNN top5 curve: [100.0, 99.71]
2024-10-30 17:57:09,576 [trainer.py] => NME top1 curve: [96.27, 82.1]
2024-10-30 17:57:09,576 [trainer.py] => NME top5 curve: [100.0, 99.74]

2024-10-30 17:57:09,576 [trainer.py] => Average Accuracy (CNN): 91.94999999999999
2024-10-30 17:57:09,576 [trainer.py] => Average Accuracy (NME): 89.185
2024-10-30 17:57:09,577 [trainer.py] => All params: 7701139
2024-10-30 17:57:09,578 [trainer.py] => Trainable params: 3854670
2024-10-30 17:57:09,854 [pod_foster.py] => Learning on 7-9
2024-10-30 17:57:09,855 [pod_foster.py] => All params: 7705241
2024-10-30 17:57:09,856 [pod_foster.py] => Trainable params: 3857746
2024-10-30 17:57:09,882 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-30 17:57:09,889 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-30 17:57:13,051 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.933, Loss_clf 0.678, Loss_fe 0.728, Loss_pod 0.312, Loss_flat 0.215, Train_accy 82.68, Test_accy 50.87
2024-10-30 17:57:24,673 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.418, Loss_clf 0.028, Loss_fe 0.071, Loss_pod 0.200, Loss_flat 0.119, Train_accy 99.53, Test_accy 55.17
2024-10-30 17:57:41,508 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.192, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.115, Loss_flat 0.057, Train_accy 100.00, Test_accy 62.63
2024-10-30 17:57:57,287 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.554, Loss_clf 0.059, Loss_fe 0.124, Loss_pod 0.239, Loss_flat 0.131, Train_accy 98.24, Test_accy 55.44
2024-10-30 17:58:14,289 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.176, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.112, Loss_flat 0.052, Train_accy 99.98, Test_accy 60.63
2024-10-30 17:58:29,335 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.192, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.118, Loss_flat 0.058, Train_accy 99.93, Test_accy 61.11
2024-10-30 17:58:45,316 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.144, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.093, Loss_flat 0.041, Train_accy 100.00, Test_accy 58.11
2024-10-30 17:59:02,193 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.144, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.094, Loss_flat 0.040, Train_accy 100.00, Test_accy 59.19
2024-10-30 17:59:18,993 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.249, Loss_clf 0.008, Loss_fe 0.012, Loss_pod 0.151, Loss_flat 0.078, Train_accy 99.96, Test_accy 62.15
2024-10-30 17:59:31,622 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.145, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.095, Loss_flat 0.040, Train_accy 100.00, Test_accy 56.94
2024-10-30 17:59:43,596 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.132, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.087, Loss_flat 0.035, Train_accy 100.00, Test_accy 59.48
2024-10-30 17:59:56,037 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.125, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.082, Loss_flat 0.033, Train_accy 100.00, Test_accy 57.89
2024-10-30 18:00:08,190 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.198, Loss_clf 0.009, Loss_fe 0.013, Loss_pod 0.121, Loss_flat 0.055, Train_accy 99.91, Test_accy 61.63
2024-10-30 18:00:19,806 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.203, Loss_clf 0.010, Loss_fe 0.014, Loss_pod 0.119, Loss_flat 0.059, Train_accy 99.84, Test_accy 62.41
2024-10-30 18:00:31,476 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.201, Loss_clf 0.012, Loss_fe 0.014, Loss_pod 0.117, Loss_flat 0.057, Train_accy 99.82, Test_accy 56.98
2024-10-30 18:00:46,710 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.117, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.077, Loss_flat 0.031, Train_accy 100.00, Test_accy 58.83
2024-10-30 18:01:02,953 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.145, Loss_clf 0.006, Loss_fe 0.009, Loss_pod 0.088, Loss_flat 0.042, Train_accy 99.93, Test_accy 61.04
2024-10-30 18:01:19,484 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.107, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.071, Loss_flat 0.029, Train_accy 100.00, Test_accy 59.65
2024-10-30 18:01:37,086 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.120, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.078, Loss_flat 0.033, Train_accy 100.00, Test_accy 60.09
2024-10-30 18:01:54,409 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.102, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.066, Loss_flat 0.027, Train_accy 100.00, Test_accy 58.83
2024-10-30 18:02:10,831 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.101, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.064, Loss_flat 0.028, Train_accy 100.00, Test_accy 58.80
2024-10-30 18:02:25,570 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.104, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.064, Loss_flat 0.030, Train_accy 100.00, Test_accy 58.61
2024-10-30 18:02:37,767 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.096, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.060, Loss_flat 0.027, Train_accy 100.00, Test_accy 58.96
2024-10-30 18:02:49,388 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.097, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.060, Loss_flat 0.026, Train_accy 100.00, Test_accy 59.67
2024-10-30 18:03:00,979 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.088, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.055, Loss_flat 0.025, Train_accy 100.00, Test_accy 59.50
2024-10-30 18:03:12,455 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.086, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.054, Loss_flat 0.025, Train_accy 100.00, Test_accy 60.02
2024-10-30 18:03:24,053 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.086, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.053, Loss_flat 0.025, Train_accy 100.00, Test_accy 60.98
2024-10-30 18:03:37,692 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.083, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.051, Loss_flat 0.024, Train_accy 100.00, Test_accy 60.48
2024-10-30 18:03:51,963 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.080, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.048, Loss_flat 0.024, Train_accy 100.00, Test_accy 60.07
2024-10-30 18:04:07,128 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.085, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.051, Loss_flat 0.026, Train_accy 100.00, Test_accy 57.56
2024-10-30 18:04:17,577 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.080, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.048, Loss_flat 0.024, Train_accy 100.00
2024-10-30 18:04:17,579 [inc_net.py] => align weights, gamma = 0.49394845962524414 
2024-10-30 18:04:17,581 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-30 18:04:22,258 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.566,  Train_accy 73.49, Test_accy 66.33
2024-10-30 18:04:38,474 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.308,  Train_accy 95.02, Test_accy 73.61
2024-10-30 18:04:54,131 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.308,  Train_accy 96.13, Test_accy 74.17
2024-10-30 18:05:09,116 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.306,  Train_accy 96.33, Test_accy 75.17
2024-10-30 18:05:23,897 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.303,  Train_accy 96.44, Test_accy 76.07
2024-10-30 18:05:35,754 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.304,  Train_accy 96.44, Test_accy 75.56
2024-10-30 18:05:46,310 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.300,  Train_accy 96.46, Test_accy 75.57
2024-10-30 18:05:56,773 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.302,  Train_accy 96.53, Test_accy 75.65
2024-10-30 18:06:07,097 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.299,  Train_accy 96.51, Test_accy 75.43
2024-10-30 18:06:17,502 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.300,  Train_accy 96.49, Test_accy 75.69
2024-10-30 18:06:27,516 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.300,  Train_accy 96.66, Test_accy 75.93
2024-10-30 18:06:37,398 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.300,  Train_accy 96.42, Test_accy 76.11
2024-10-30 18:06:51,090 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.301,  Train_accy 96.95, Test_accy 77.20
2024-10-30 18:07:05,933 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.298,  Train_accy 96.86, Test_accy 76.11
2024-10-30 18:07:19,308 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.299,  Train_accy 96.82, Test_accy 76.56
2024-10-30 18:07:32,143 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.298,  Train_accy 97.09, Test_accy 77.09
2024-10-30 18:07:44,759 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.300,  Train_accy 96.69, Test_accy 75.98
2024-10-30 18:07:58,500 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.293,  Train_accy 96.84, Test_accy 76.52
2024-10-30 18:08:11,663 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.296,  Train_accy 97.00, Test_accy 76.85
2024-10-30 18:08:25,033 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.295,  Train_accy 96.66, Test_accy 75.93
2024-10-30 18:08:36,509 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.295,  Train_accy 96.86, Test_accy 75.70
2024-10-30 18:08:46,791 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.297,  Train_accy 96.89, Test_accy 76.94
2024-10-30 18:08:57,248 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.299,  Train_accy 96.86, Test_accy 76.50
2024-10-30 18:09:07,525 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.299,  Train_accy 96.69, Test_accy 76.24
2024-10-30 18:09:14,983 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.297,  Train_accy 96.89
2024-10-30 18:09:14,984 [pod_foster.py] => do not weight align student!
2024-10-30 18:09:15,801 [pod_foster.py] => darknet eval: 
2024-10-30 18:09:15,801 [pod_foster.py] => CNN top1 curve: 76.78
2024-10-30 18:09:15,802 [pod_foster.py] => CNN top5 curve: 98.54
2024-10-30 18:09:15,802 [pod_foster.py] => CNN: {'total': 76.78, '00-04': 71.77, '05-06': 83.0, '07-08': 83.08, 'old': 74.98, 'new': 83.08}
2024-10-30 18:09:15,803 [pod_foster.py] => All params after compression: 3853138
2024-10-30 18:09:15,804 [base.py] => Reducing exemplars...(55 per classes)
2024-10-30 18:09:17,213 [base.py] => Constructing exemplars...(55 per classes)
2024-10-30 18:09:20,254 [trainer.py] => All params: 7705241
2024-10-30 18:09:22,505 [pod_foster.py] => Exemplar size: 495
2024-10-30 18:09:22,506 [trainer.py] => CNN: {'total': 76.0, '00-04': 70.4, '05-06': 86.08, '07-08': 79.92, 'old': 74.88, 'new': 79.92}
2024-10-30 18:09:22,506 [trainer.py] => NME: {'total': 70.57, '00-04': 76.13, '05-06': 66.5, '07-08': 60.75, 'old': 73.38, 'new': 60.75}
2024-10-30 18:09:22,506 [trainer.py] => CNN top1 curve: [96.3, 87.6, 76.0]
2024-10-30 18:09:22,506 [trainer.py] => CNN top5 curve: [100.0, 99.71, 98.43]
2024-10-30 18:09:22,506 [trainer.py] => NME top1 curve: [96.27, 82.1, 70.57]
2024-10-30 18:09:22,506 [trainer.py] => NME top5 curve: [100.0, 99.74, 98.26]

2024-10-30 18:09:22,506 [trainer.py] => Average Accuracy (CNN): 86.63333333333333
2024-10-30 18:09:22,506 [trainer.py] => Average Accuracy (NME): 82.98
2024-10-30 18:09:22,507 [trainer.py] => Forgetting (CNN): 12.949999999999996
