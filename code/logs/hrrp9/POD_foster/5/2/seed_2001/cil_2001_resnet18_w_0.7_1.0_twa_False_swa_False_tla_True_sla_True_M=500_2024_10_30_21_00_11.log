2024-10-30 21:00:11,499 [trainer.py] => config: ./exps/POD_foster.json
2024-10-30 21:00:11,499 [trainer.py] => prefix: cil
2024-10-30 21:00:11,499 [trainer.py] => dataset: hrrp9
2024-10-30 21:00:11,500 [trainer.py] => memory_size: 500
2024-10-30 21:00:11,500 [trainer.py] => memory_per_class: 20
2024-10-30 21:00:11,500 [trainer.py] => fixed_memory: False
2024-10-30 21:00:11,501 [trainer.py] => shuffle: True
2024-10-30 21:00:11,501 [trainer.py] => init_cls: 5
2024-10-30 21:00:11,501 [trainer.py] => increment: 2
2024-10-30 21:00:11,501 [trainer.py] => model_name: POD_foster
2024-10-30 21:00:11,502 [trainer.py] => convnet_type: resnet18
2024-10-30 21:00:11,502 [trainer.py] => init_train: False
2024-10-30 21:00:11,502 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_35172.pth
2024-10-30 21:00:11,502 [trainer.py] => fc_path: checkpoints/init_train/fc_35172.pth
2024-10-30 21:00:11,503 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42503.pth
2024-10-30 21:00:11,503 [trainer.py] => fc_path1: checkpoints/init_train/fc_42503.pth
2024-10-30 21:00:11,503 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42871.pth
2024-10-30 21:00:11,503 [trainer.py] => fc_path2: checkpoints/init_train/fc_42871.pth
2024-10-30 21:00:11,504 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-30 21:00:11,504 [trainer.py] => seed: 2001
2024-10-30 21:00:11,504 [trainer.py] => beta1: 0.96
2024-10-30 21:00:11,505 [trainer.py] => beta2: 0.97
2024-10-30 21:00:11,505 [trainer.py] => oofc: ft
2024-10-30 21:00:11,505 [trainer.py] => is_teacher_wa: False
2024-10-30 21:00:11,506 [trainer.py] => is_student_wa: False
2024-10-30 21:00:11,506 [trainer.py] => is_teacher_la: True
2024-10-30 21:00:11,506 [trainer.py] => is_student_la: True
2024-10-30 21:00:11,506 [trainer.py] => lambda_okd: 0
2024-10-30 21:00:11,506 [trainer.py] => wa_value: 1
2024-10-30 21:00:11,507 [trainer.py] => init_epochs: 0
2024-10-30 21:00:11,507 [trainer.py] => init_lr: 0.1
2024-10-30 21:00:11,507 [trainer.py] => init_weight_decay: 0.0005
2024-10-30 21:00:11,508 [trainer.py] => boosting_epochs: 150
2024-10-30 21:00:11,508 [trainer.py] => compression_epochs: 120
2024-10-30 21:00:11,508 [trainer.py] => lr: 0.1
2024-10-30 21:00:11,508 [trainer.py] => batch_size: 128
2024-10-30 21:00:11,509 [trainer.py] => weight_decay: 0.0005
2024-10-30 21:00:11,509 [trainer.py] => num_workers: 8
2024-10-30 21:00:11,509 [trainer.py] => momentum: 0.9
2024-10-30 21:00:11,510 [trainer.py] => T: 2
2024-10-30 21:00:11,510 [trainer.py] => lambda_c_base: 0.7
2024-10-30 21:00:11,510 [trainer.py] => lambda_f_base: 1.0
2024-10-30 21:00:11,510 [trainer.py] => POD: w
2024-10-30 21:00:12,965 [data_manager.py] => [3, 5, 1, 7, 2, 8, 6, 4, 0]
2024-10-30 21:00:13,073 [trainer.py] => All params: 0
2024-10-30 21:00:13,073 [trainer.py] => Trainable params: 0
2024-10-30 21:00:14,051 [pod_foster.py] => Learning on 0-5
2024-10-30 21:00:14,051 [pod_foster.py] => All params: 3849034
2024-10-30 21:00:14,052 [pod_foster.py] => Trainable params: 3849034
2024-10-30 21:00:14,276 [pod_foster.py] => Adaptive factor: 0
2024-10-30 21:00:14,292 [pod_foster.py] => init_train?---False
2024-10-30 21:00:16,282 [base.py] => Reducing exemplars...(100 per classes)
2024-10-30 21:00:16,283 [base.py] => Constructing exemplars...(100 per classes)
2024-10-30 21:00:27,766 [trainer.py] => All params: 3849034
2024-10-30 21:00:29,933 [pod_foster.py] => Exemplar size: 500
2024-10-30 21:00:29,933 [trainer.py] => CNN: {'total': 90.13, '00-04': 90.13, 'old': 0, 'new': 90.13}
2024-10-30 21:00:29,933 [trainer.py] => NME: {'total': 89.53, '00-04': 89.53, 'old': 0, 'new': 89.53}
2024-10-30 21:00:29,934 [trainer.py] => CNN top1 curve: [90.13]
2024-10-30 21:00:29,934 [trainer.py] => CNN top5 curve: [100.0]
2024-10-30 21:00:29,934 [trainer.py] => NME top1 curve: [89.53]
2024-10-30 21:00:29,934 [trainer.py] => NME top5 curve: [100.0]

2024-10-30 21:00:29,939 [trainer.py] => Average Accuracy (CNN): 90.13
2024-10-30 21:00:29,940 [trainer.py] => Average Accuracy (NME): 89.53
2024-10-30 21:00:29,941 [trainer.py] => All params: 3849034
2024-10-30 21:00:29,942 [trainer.py] => Trainable params: 3849034
2024-10-30 21:00:30,152 [pod_foster.py] => Learning on 5-7
2024-10-30 21:00:30,154 [pod_foster.py] => All params: 7701139
2024-10-30 21:00:30,154 [pod_foster.py] => Trainable params: 3854670
2024-10-30 21:00:30,233 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-30 21:00:30,242 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-30 21:00:35,570 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 2.041, Loss_clf 0.625, Loss_fe 0.683, Loss_pod 0.481, Loss_flat 0.251, Train_accy 81.18, Test_accy 65.21
2024-10-30 21:00:52,610 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.341, Loss_clf 0.011, Loss_fe 0.024, Loss_pod 0.233, Loss_flat 0.073, Train_accy 99.98, Test_accy 79.86
2024-10-30 21:01:08,816 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.232, Loss_clf 0.007, Loss_fe 0.010, Loss_pod 0.172, Loss_flat 0.043, Train_accy 100.00, Test_accy 76.31
2024-10-30 21:01:23,808 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.202, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.153, Loss_flat 0.035, Train_accy 100.00, Test_accy 75.12
2024-10-30 21:01:40,100 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 1.426, Loss_clf 0.124, Loss_fe 0.408, Loss_pod 0.614, Loss_flat 0.280, Train_accy 96.07, Test_accy 53.38
2024-10-30 21:01:53,931 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.699, Loss_clf 0.024, Loss_fe 0.101, Loss_pod 0.409, Loss_flat 0.165, Train_accy 99.62, Test_accy 68.31
2024-10-30 21:02:07,301 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.528, Loss_clf 0.021, Loss_fe 0.043, Loss_pod 0.345, Loss_flat 0.120, Train_accy 99.62, Test_accy 73.86
2024-10-30 21:02:19,751 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.361, Loss_clf 0.007, Loss_fe 0.010, Loss_pod 0.272, Loss_flat 0.073, Train_accy 100.00, Test_accy 77.50
2024-10-30 21:02:31,365 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.274, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.217, Loss_flat 0.045, Train_accy 100.00, Test_accy 77.86
2024-10-30 21:02:42,908 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.547, Loss_clf 0.019, Loss_fe 0.058, Loss_pod 0.349, Loss_flat 0.120, Train_accy 99.60, Test_accy 79.50
2024-10-30 21:02:54,332 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.269, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.211, Loss_flat 0.045, Train_accy 100.00, Test_accy 79.36
2024-10-30 21:03:06,110 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.229, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.184, Loss_flat 0.035, Train_accy 100.00, Test_accy 79.76
2024-10-30 21:03:18,031 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.218, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.175, Loss_flat 0.032, Train_accy 100.00, Test_accy 80.64
2024-10-30 21:03:29,729 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.195, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.156, Loss_flat 0.029, Train_accy 100.00, Test_accy 77.69
2024-10-30 21:03:42,967 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.192, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.154, Loss_flat 0.028, Train_accy 100.00, Test_accy 76.67
2024-10-30 21:03:59,527 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.176, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.141, Loss_flat 0.026, Train_accy 100.00, Test_accy 78.86
2024-10-30 21:04:17,783 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.173, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.138, Loss_flat 0.025, Train_accy 100.00, Test_accy 79.60
2024-10-30 21:04:35,425 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.172, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.138, Loss_flat 0.025, Train_accy 100.00, Test_accy 78.00
2024-10-30 21:04:51,682 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.161, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.129, Loss_flat 0.023, Train_accy 100.00, Test_accy 79.52
2024-10-30 21:05:10,470 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.182, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.141, Loss_flat 0.030, Train_accy 100.00, Test_accy 78.29
2024-10-30 21:05:29,893 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.157, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.125, Loss_flat 0.023, Train_accy 100.00, Test_accy 79.05
2024-10-30 21:05:49,558 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.148, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.117, Loss_flat 0.022, Train_accy 100.00, Test_accy 79.21
2024-10-30 21:06:09,589 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.143, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.112, Loss_flat 0.021, Train_accy 100.00, Test_accy 79.57
2024-10-30 21:06:26,249 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.139, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.108, Loss_flat 0.021, Train_accy 100.00, Test_accy 78.83
2024-10-30 21:06:40,211 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.138, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.109, Loss_flat 0.021, Train_accy 100.00, Test_accy 78.74
2024-10-30 21:06:53,127 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.134, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.105, Loss_flat 0.020, Train_accy 100.00, Test_accy 79.05
2024-10-30 21:07:05,904 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.133, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.104, Loss_flat 0.021, Train_accy 100.00, Test_accy 78.48
2024-10-30 21:07:18,531 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.127, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.099, Loss_flat 0.020, Train_accy 100.00, Test_accy 79.10
2024-10-30 21:07:31,562 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.128, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.099, Loss_flat 0.020, Train_accy 100.00, Test_accy 79.19
2024-10-30 21:07:43,941 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.101, Loss_flat 0.020, Train_accy 100.00, Test_accy 79.00
2024-10-30 21:07:53,402 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.127, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.098, Loss_flat 0.020, Train_accy 100.00
2024-10-30 21:07:53,403 [pod_foster.py] => do not weight align teacher!
2024-10-30 21:07:53,403 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-30 21:07:57,375 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 0.989,  Train_accy 67.71, Test_accy 31.00
2024-10-30 21:08:14,497 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.366,  Train_accy 99.98, Test_accy 71.95
2024-10-30 21:08:30,998 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.362,  Train_accy 100.00, Test_accy 75.76
2024-10-30 21:08:50,135 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.359,  Train_accy 100.00, Test_accy 76.07
2024-10-30 21:09:09,117 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.357,  Train_accy 100.00, Test_accy 75.21
2024-10-30 21:09:29,140 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.355,  Train_accy 100.00, Test_accy 77.19
2024-10-30 21:09:50,596 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.357,  Train_accy 100.00, Test_accy 75.55
2024-10-30 21:10:10,526 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.356,  Train_accy 100.00, Test_accy 77.31
2024-10-30 21:10:28,322 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.356,  Train_accy 100.00, Test_accy 76.60
2024-10-30 21:10:43,956 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.355,  Train_accy 100.00, Test_accy 77.26
2024-10-30 21:10:57,064 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.354,  Train_accy 100.00, Test_accy 77.36
2024-10-30 21:11:06,581 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.356,  Train_accy 100.00, Test_accy 76.36
2024-10-30 21:11:16,427 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.353,  Train_accy 100.00, Test_accy 77.05
2024-10-30 21:11:25,957 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.355,  Train_accy 100.00, Test_accy 76.21
2024-10-30 21:11:35,468 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.354,  Train_accy 100.00, Test_accy 78.00
2024-10-30 21:11:45,172 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.354,  Train_accy 100.00, Test_accy 78.12
2024-10-30 21:11:55,672 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.354,  Train_accy 100.00, Test_accy 77.43
2024-10-30 21:12:07,154 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.353,  Train_accy 100.00, Test_accy 77.52
2024-10-30 21:12:19,862 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.354,  Train_accy 100.00, Test_accy 77.48
2024-10-30 21:12:35,404 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.354,  Train_accy 100.00, Test_accy 77.79
2024-10-30 21:12:56,825 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.354,  Train_accy 100.00, Test_accy 77.57
2024-10-30 21:13:23,165 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.353,  Train_accy 100.00, Test_accy 77.67
2024-10-30 21:13:49,318 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.354,  Train_accy 100.00, Test_accy 77.86
2024-10-30 21:14:13,716 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.353,  Train_accy 100.00, Test_accy 77.81
2024-10-30 21:14:34,003 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.354,  Train_accy 100.00
2024-10-30 21:14:34,004 [pod_foster.py] => do not weight align student!
2024-10-30 21:14:36,611 [pod_foster.py] => darknet eval: 
2024-10-30 21:14:36,612 [pod_foster.py] => CNN top1 curve: 77.43
2024-10-30 21:14:36,612 [pod_foster.py] => CNN top5 curve: 99.45
2024-10-30 21:14:36,612 [pod_foster.py] => CNN: {'total': 77.43, '00-04': 70.6, '05-06': 94.5, 'old': 70.6, 'new': 94.5}
2024-10-30 21:14:36,613 [pod_foster.py] => All params after compression: 3851086
2024-10-30 21:14:36,614 [base.py] => Reducing exemplars...(71 per classes)
2024-10-30 21:14:40,917 [base.py] => Constructing exemplars...(71 per classes)
2024-10-30 21:14:53,176 [trainer.py] => All params: 7701139
2024-10-30 21:15:00,269 [pod_foster.py] => Exemplar size: 497
2024-10-30 21:15:00,275 [trainer.py] => CNN: {'total': 79.64, '00-04': 74.07, '05-06': 93.58, 'old': 74.07, 'new': 93.58}
2024-10-30 21:15:00,276 [trainer.py] => NME: {'total': 76.24, '00-04': 83.67, '05-06': 57.67, 'old': 83.67, 'new': 57.67}
2024-10-30 21:15:00,276 [trainer.py] => CNN top1 curve: [90.13, 79.64]
2024-10-30 21:15:00,277 [trainer.py] => CNN top5 curve: [100.0, 99.62]
2024-10-30 21:15:00,277 [trainer.py] => NME top1 curve: [89.53, 76.24]
2024-10-30 21:15:00,277 [trainer.py] => NME top5 curve: [100.0, 99.62]

2024-10-30 21:15:00,277 [trainer.py] => Average Accuracy (CNN): 84.88499999999999
2024-10-30 21:15:00,278 [trainer.py] => Average Accuracy (NME): 82.88499999999999
2024-10-30 21:15:00,279 [trainer.py] => All params: 7701139
2024-10-30 21:15:00,292 [trainer.py] => Trainable params: 3854670
2024-10-30 21:15:00,384 [pod_foster.py] => Learning on 7-9
2024-10-30 21:15:00,386 [pod_foster.py] => All params: 7705241
2024-10-30 21:15:00,387 [pod_foster.py] => Trainable params: 3857746
2024-10-30 21:15:00,599 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-30 21:15:00,615 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-30 21:15:08,373 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.536, Loss_clf 0.884, Loss_fe 0.794, Loss_pod 0.574, Loss_flat 0.284, Train_accy 81.30, Test_accy 54.89
2024-10-30 21:15:39,901 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.557, Loss_clf 0.025, Loss_fe 0.079, Loss_pod 0.333, Loss_flat 0.120, Train_accy 99.51, Test_accy 64.19
2024-10-30 21:16:15,771 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.301, Loss_clf 0.010, Loss_fe 0.017, Loss_pod 0.215, Loss_flat 0.058, Train_accy 99.96, Test_accy 49.22
2024-10-30 21:16:54,037 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.416, Loss_clf 0.016, Loss_fe 0.030, Loss_pod 0.283, Loss_flat 0.086, Train_accy 99.76, Test_accy 66.54
2024-10-30 21:17:28,513 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.271, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.208, Loss_flat 0.048, Train_accy 100.00, Test_accy 66.93
2024-10-30 21:17:57,521 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.292, Loss_clf 0.007, Loss_fe 0.010, Loss_pod 0.220, Loss_flat 0.055, Train_accy 100.00, Test_accy 69.02
2024-10-30 21:18:21,047 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.233, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.184, Loss_flat 0.037, Train_accy 100.00, Test_accy 62.37
2024-10-30 21:18:43,594 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.214, Loss_clf 0.005, Loss_fe 0.008, Loss_pod 0.169, Loss_flat 0.032, Train_accy 100.00, Test_accy 64.83
2024-10-30 21:19:06,150 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.420, Loss_clf 0.014, Loss_fe 0.022, Loss_pod 0.293, Loss_flat 0.091, Train_accy 99.80, Test_accy 69.98
2024-10-30 21:19:27,444 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.229, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.179, Loss_flat 0.039, Train_accy 100.00, Test_accy 64.74
2024-10-30 21:19:48,917 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.198, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.156, Loss_flat 0.030, Train_accy 100.00, Test_accy 65.89
2024-10-30 21:20:10,556 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.192, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.152, Loss_flat 0.029, Train_accy 100.00, Test_accy 64.02
2024-10-30 21:20:32,450 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.240, Loss_clf 0.007, Loss_fe 0.013, Loss_pod 0.180, Loss_flat 0.040, Train_accy 99.96, Test_accy 65.30
2024-10-30 21:20:54,170 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.250, Loss_clf 0.008, Loss_fe 0.012, Loss_pod 0.186, Loss_flat 0.044, Train_accy 99.98, Test_accy 70.24
2024-10-30 21:21:15,843 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.218, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.167, Loss_flat 0.037, Train_accy 99.93, Test_accy 65.07
2024-10-30 21:21:39,449 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.175, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.138, Loss_flat 0.027, Train_accy 100.00, Test_accy 64.70
2024-10-30 21:22:08,944 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.166, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.131, Loss_flat 0.025, Train_accy 100.00, Test_accy 64.80
2024-10-30 21:22:44,270 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.166, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.133, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.43
2024-10-30 21:23:19,270 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.162, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.127, Loss_flat 0.025, Train_accy 100.00, Test_accy 64.72
2024-10-30 21:23:54,076 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.152, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.120, Loss_flat 0.022, Train_accy 100.00, Test_accy 65.11
2024-10-30 21:24:28,630 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.151, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.119, Loss_flat 0.022, Train_accy 100.00, Test_accy 62.81
2024-10-30 21:25:05,763 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.155, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.122, Loss_flat 0.023, Train_accy 100.00, Test_accy 64.61
2024-10-30 21:25:40,014 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.141, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.110, Loss_flat 0.021, Train_accy 100.00, Test_accy 66.13
2024-10-30 21:26:13,896 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.141, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.109, Loss_flat 0.021, Train_accy 100.00, Test_accy 65.96
2024-10-30 21:26:45,710 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.136, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.106, Loss_flat 0.021, Train_accy 100.00, Test_accy 65.11
2024-10-30 21:27:14,173 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.101, Loss_flat 0.020, Train_accy 100.00, Test_accy 65.74
2024-10-30 21:27:42,852 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.135, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.105, Loss_flat 0.021, Train_accy 100.00, Test_accy 64.83
2024-10-30 21:28:06,449 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.130, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.101, Loss_flat 0.020, Train_accy 100.00, Test_accy 66.50
2024-10-30 21:28:28,296 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.123, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.095, Loss_flat 0.019, Train_accy 100.00, Test_accy 65.76
2024-10-30 21:28:48,579 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.126, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.097, Loss_flat 0.020, Train_accy 100.00, Test_accy 64.33
2024-10-30 21:29:03,452 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.126, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.097, Loss_flat 0.020, Train_accy 100.00
2024-10-30 21:29:03,453 [pod_foster.py] => do not weight align teacher!
2024-10-30 21:29:03,458 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-30 21:29:08,477 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.124,  Train_accy 72.89, Test_accy 26.19
2024-10-30 21:29:26,977 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 0.455,  Train_accy 99.98, Test_accy 60.02
2024-10-30 21:29:44,651 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 0.444,  Train_accy 100.00, Test_accy 62.50
2024-10-30 21:30:02,126 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 0.439,  Train_accy 99.98, Test_accy 60.02
2024-10-30 21:30:19,282 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 0.438,  Train_accy 100.00, Test_accy 59.81
2024-10-30 21:30:36,179 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 0.435,  Train_accy 100.00, Test_accy 60.33
2024-10-30 21:30:52,057 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 0.435,  Train_accy 100.00, Test_accy 60.78
2024-10-30 21:31:12,014 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 0.435,  Train_accy 100.00, Test_accy 61.48
2024-10-30 21:31:29,606 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 0.438,  Train_accy 100.00, Test_accy 61.09
2024-10-30 21:31:48,034 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 0.432,  Train_accy 100.00, Test_accy 61.57
2024-10-30 21:32:05,946 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 0.434,  Train_accy 100.00, Test_accy 60.41
2024-10-30 21:32:23,354 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 0.432,  Train_accy 100.00, Test_accy 61.50
2024-10-30 21:32:40,954 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 0.434,  Train_accy 99.98, Test_accy 59.93
2024-10-30 21:32:57,698 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 0.434,  Train_accy 100.00, Test_accy 59.41
2024-10-30 21:33:10,473 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 0.432,  Train_accy 100.00, Test_accy 61.52
2024-10-30 21:33:23,667 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 0.433,  Train_accy 100.00, Test_accy 61.19
2024-10-30 21:33:37,245 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 0.432,  Train_accy 100.00, Test_accy 60.89
2024-10-30 21:33:50,097 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 0.434,  Train_accy 100.00, Test_accy 59.98
2024-10-30 21:34:00,297 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 0.432,  Train_accy 100.00, Test_accy 61.02
2024-10-30 21:34:09,074 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 0.434,  Train_accy 100.00, Test_accy 59.56
2024-10-30 21:34:17,509 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 0.432,  Train_accy 100.00, Test_accy 60.28
2024-10-30 21:34:25,920 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 0.433,  Train_accy 100.00, Test_accy 60.96
2024-10-30 21:34:34,079 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 0.431,  Train_accy 100.00, Test_accy 61.30
2024-10-30 21:34:42,428 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 0.433,  Train_accy 100.00, Test_accy 61.06
2024-10-30 21:34:48,768 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 0.432,  Train_accy 100.00
2024-10-30 21:34:48,769 [pod_foster.py] => do not weight align student!
2024-10-30 21:34:49,486 [pod_foster.py] => darknet eval: 
2024-10-30 21:34:49,486 [pod_foster.py] => CNN top1 curve: 60.8
2024-10-30 21:34:49,486 [pod_foster.py] => CNN top5 curve: 97.5
2024-10-30 21:34:49,486 [pod_foster.py] => CNN: {'total': 60.8, '00-04': 57.67, '05-06': 43.75, '07-08': 85.67, 'old': 53.69, 'new': 85.67}
2024-10-30 21:34:49,488 [pod_foster.py] => All params after compression: 3853138
2024-10-30 21:34:49,488 [base.py] => Reducing exemplars...(55 per classes)
2024-10-30 21:34:50,739 [base.py] => Constructing exemplars...(55 per classes)
2024-10-30 21:34:53,663 [trainer.py] => All params: 7705241
2024-10-30 21:34:55,463 [pod_foster.py] => Exemplar size: 495
2024-10-30 21:34:55,464 [trainer.py] => CNN: {'total': 64.56, '00-04': 60.9, '05-06': 52.0, '07-08': 86.25, 'old': 58.36, 'new': 86.25}
2024-10-30 21:34:55,464 [trainer.py] => NME: {'total': 70.04, '00-04': 74.53, '05-06': 72.0, '07-08': 56.83, 'old': 73.81, 'new': 56.83}
2024-10-30 21:34:55,464 [trainer.py] => CNN top1 curve: [90.13, 79.64, 64.56]
2024-10-30 21:34:55,464 [trainer.py] => CNN top5 curve: [100.0, 99.62, 97.43]
2024-10-30 21:34:55,464 [trainer.py] => NME top1 curve: [89.53, 76.24, 70.04]
2024-10-30 21:34:55,464 [trainer.py] => NME top5 curve: [100.0, 99.62, 97.17]

2024-10-30 21:34:55,464 [trainer.py] => Average Accuracy (CNN): 78.11
2024-10-30 21:34:55,464 [trainer.py] => Average Accuracy (NME): 78.60333333333334
2024-10-30 21:34:55,465 [trainer.py] => Forgetting (CNN): 35.405
