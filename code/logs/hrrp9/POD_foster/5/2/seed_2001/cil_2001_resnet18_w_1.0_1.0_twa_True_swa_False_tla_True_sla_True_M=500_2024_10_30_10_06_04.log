2024-10-30 10:06:04,327 [trainer.py] => config: ./exps/POD_foster.json
2024-10-30 10:06:04,328 [trainer.py] => prefix: cil
2024-10-30 10:06:04,328 [trainer.py] => dataset: hrrp9
2024-10-30 10:06:04,328 [trainer.py] => memory_size: 500
2024-10-30 10:06:04,329 [trainer.py] => memory_per_class: 20
2024-10-30 10:06:04,329 [trainer.py] => fixed_memory: False
2024-10-30 10:06:04,329 [trainer.py] => shuffle: True
2024-10-30 10:06:04,329 [trainer.py] => init_cls: 5
2024-10-30 10:06:04,330 [trainer.py] => increment: 2
2024-10-30 10:06:04,330 [trainer.py] => model_name: POD_foster
2024-10-30 10:06:04,330 [trainer.py] => convnet_type: resnet18
2024-10-30 10:06:04,330 [trainer.py] => init_train: False
2024-10-30 10:06:04,331 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_35172.pth
2024-10-30 10:06:04,331 [trainer.py] => fc_path: checkpoints/init_train/fc_35172.pth
2024-10-30 10:06:04,331 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42503.pth
2024-10-30 10:06:04,332 [trainer.py] => fc_path2: checkpoints/init_train/fc_42503.pth
2024-10-30 10:06:04,332 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_42871.pth
2024-10-30 10:06:04,332 [trainer.py] => fc_path1: checkpoints/init_train/fc_42871.pth
2024-10-30 10:06:04,333 [trainer.py] => device: [device(type='cuda', index=3)]
2024-10-30 10:06:04,333 [trainer.py] => seed: 2001
2024-10-30 10:06:04,333 [trainer.py] => beta1: 0.96
2024-10-30 10:06:04,334 [trainer.py] => beta2: 0.97
2024-10-30 10:06:04,335 [trainer.py] => oofc: ft
2024-10-30 10:06:04,335 [trainer.py] => is_teacher_wa: True
2024-10-30 10:06:04,335 [trainer.py] => is_student_wa: False
2024-10-30 10:06:04,336 [trainer.py] => is_teacher_la: True
2024-10-30 10:06:04,336 [trainer.py] => is_student_la: True
2024-10-30 10:06:04,336 [trainer.py] => lambda_okd: 0
2024-10-30 10:06:04,336 [trainer.py] => wa_value: 1
2024-10-30 10:06:04,337 [trainer.py] => init_epochs: 0
2024-10-30 10:06:04,337 [trainer.py] => init_lr: 0.1
2024-10-30 10:06:04,337 [trainer.py] => init_weight_decay: 0.0005
2024-10-30 10:06:04,338 [trainer.py] => boosting_epochs: 150
2024-10-30 10:06:04,338 [trainer.py] => compression_epochs: 120
2024-10-30 10:06:04,338 [trainer.py] => lr: 0.1
2024-10-30 10:06:04,339 [trainer.py] => batch_size: 128
2024-10-30 10:06:04,339 [trainer.py] => weight_decay: 0.0005
2024-10-30 10:06:04,339 [trainer.py] => num_workers: 8
2024-10-30 10:06:04,339 [trainer.py] => momentum: 0.9
2024-10-30 10:06:04,340 [trainer.py] => T: 2
2024-10-30 10:06:04,340 [trainer.py] => lambda_c_base: 1.0
2024-10-30 10:06:04,340 [trainer.py] => lambda_f_base: 1.0
2024-10-30 10:06:04,340 [trainer.py] => POD: w
2024-10-30 10:06:05,286 [data_manager.py] => [3, 5, 1, 7, 2, 8, 6, 4, 0]
2024-10-30 10:06:05,341 [trainer.py] => All params: 0
2024-10-30 10:06:05,341 [trainer.py] => Trainable params: 0
2024-10-30 10:06:06,136 [pod_foster.py] => Learning on 0-5
2024-10-30 10:06:06,137 [pod_foster.py] => All params: 3849034
2024-10-30 10:06:06,138 [pod_foster.py] => Trainable params: 3849034
2024-10-30 10:06:06,251 [pod_foster.py] => Adaptive factor: 0
2024-10-30 10:06:06,271 [pod_foster.py] => init_train?---False
2024-10-30 10:06:07,742 [base.py] => Reducing exemplars...(100 per classes)
2024-10-30 10:06:07,742 [base.py] => Constructing exemplars...(100 per classes)
2024-10-30 10:06:16,444 [trainer.py] => All params: 3849034
2024-10-30 10:06:18,317 [pod_foster.py] => Exemplar size: 500
2024-10-30 10:06:18,318 [trainer.py] => CNN: {'total': 90.13, '00-04': 90.13, 'old': 0, 'new': 90.13}
2024-10-30 10:06:18,318 [trainer.py] => NME: {'total': 89.53, '00-04': 89.53, 'old': 0, 'new': 89.53}
2024-10-30 10:06:18,319 [trainer.py] => CNN top1 curve: [90.13]
2024-10-30 10:06:18,319 [trainer.py] => CNN top5 curve: [100.0]
2024-10-30 10:06:18,319 [trainer.py] => NME top1 curve: [89.53]
2024-10-30 10:06:18,320 [trainer.py] => NME top5 curve: [100.0]

2024-10-30 10:06:18,320 [trainer.py] => Average Accuracy (CNN): 90.13
2024-10-30 10:06:18,321 [trainer.py] => Average Accuracy (NME): 89.53
2024-10-30 10:06:18,321 [trainer.py] => All params: 3849034
2024-10-30 10:06:18,322 [trainer.py] => Trainable params: 3849034
2024-10-30 10:06:18,395 [pod_foster.py] => Learning on 5-7
2024-10-30 10:06:18,398 [pod_foster.py] => All params: 7701139
2024-10-30 10:06:18,399 [pod_foster.py] => Trainable params: 3854670
2024-10-30 10:06:18,471 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-30 10:06:18,477 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-30 10:06:22,590 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 2.191, Loss_clf 0.628, Loss_fe 0.683, Loss_pod 0.648, Loss_flat 0.232, Train_accy 81.27, Test_accy 67.81
2024-10-30 10:06:34,189 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.389, Loss_clf 0.010, Loss_fe 0.023, Loss_pod 0.293, Loss_flat 0.063, Train_accy 100.00, Test_accy 78.57
2024-10-30 10:06:46,318 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.295, Loss_clf 0.008, Loss_fe 0.012, Loss_pod 0.234, Loss_flat 0.042, Train_accy 100.00, Test_accy 73.62
2024-10-30 10:07:00,516 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.256, Loss_clf 0.006, Loss_fe 0.008, Loss_pod 0.208, Loss_flat 0.033, Train_accy 100.00, Test_accy 73.74
2024-10-30 10:07:14,537 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.969, Loss_clf 0.057, Loss_fe 0.175, Loss_pod 0.569, Loss_flat 0.168, Train_accy 98.38, Test_accy 74.02
2024-10-30 10:07:29,351 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.424, Loss_clf 0.009, Loss_fe 0.013, Loss_pod 0.334, Loss_flat 0.069, Train_accy 99.98, Test_accy 79.17
2024-10-30 10:07:43,895 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.288, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.235, Loss_flat 0.037, Train_accy 100.00, Test_accy 75.83
2024-10-30 10:07:58,290 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.250, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.209, Loss_flat 0.029, Train_accy 100.00, Test_accy 77.26
2024-10-30 10:08:11,314 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.229, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.193, Loss_flat 0.025, Train_accy 100.00, Test_accy 79.45
2024-10-30 10:08:24,178 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.237, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.199, Loss_flat 0.028, Train_accy 100.00, Test_accy 76.14
2024-10-30 10:08:37,214 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.210, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.176, Loss_flat 0.024, Train_accy 100.00, Test_accy 78.17
2024-10-30 10:08:50,443 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.207, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.174, Loss_flat 0.023, Train_accy 100.00, Test_accy 77.79
2024-10-30 10:09:03,969 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.215, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.180, Loss_flat 0.024, Train_accy 100.00, Test_accy 79.02
2024-10-30 10:09:17,219 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.194, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.162, Loss_flat 0.022, Train_accy 100.00, Test_accy 76.02
2024-10-30 10:09:30,830 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.199, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.168, Loss_flat 0.021, Train_accy 100.00, Test_accy 76.40
2024-10-30 10:09:44,002 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.187, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.157, Loss_flat 0.021, Train_accy 100.00, Test_accy 76.88
2024-10-30 10:09:57,391 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.186, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.157, Loss_flat 0.021, Train_accy 100.00, Test_accy 76.12
2024-10-30 10:10:10,519 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.191, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.161, Loss_flat 0.021, Train_accy 100.00, Test_accy 70.67
2024-10-30 10:10:23,705 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.177, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.148, Loss_flat 0.020, Train_accy 100.00, Test_accy 79.43
2024-10-30 10:10:36,418 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.184, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.151, Loss_flat 0.022, Train_accy 100.00, Test_accy 77.74
2024-10-30 10:10:47,885 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.173, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.144, Loss_flat 0.019, Train_accy 100.00, Test_accy 78.60
2024-10-30 10:10:58,642 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.161, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.133, Loss_flat 0.019, Train_accy 100.00, Test_accy 77.69
2024-10-30 10:11:09,274 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.154, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.127, Loss_flat 0.018, Train_accy 100.00, Test_accy 77.52
2024-10-30 10:11:20,096 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.150, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.121, Loss_flat 0.018, Train_accy 100.00, Test_accy 77.17
2024-10-30 10:11:30,843 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.149, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.122, Loss_flat 0.018, Train_accy 100.00, Test_accy 76.90
2024-10-30 10:11:41,588 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.143, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.117, Loss_flat 0.017, Train_accy 100.00, Test_accy 77.64
2024-10-30 10:11:52,144 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.142, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.114, Loss_flat 0.018, Train_accy 100.00, Test_accy 76.98
2024-10-30 10:12:02,579 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.133, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.107, Loss_flat 0.017, Train_accy 100.00, Test_accy 77.95
2024-10-30 10:12:13,352 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.134, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.108, Loss_flat 0.017, Train_accy 100.00, Test_accy 77.79
2024-10-30 10:12:24,063 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.136, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.110, Loss_flat 0.018, Train_accy 100.00, Test_accy 77.88
2024-10-30 10:12:31,651 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.131, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.106, Loss_flat 0.017, Train_accy 100.00
2024-10-30 10:12:31,653 [inc_net.py] => align weights, gamma = 0.4696617126464844 
2024-10-30 10:12:31,654 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-30 10:12:34,250 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.337,  Train_accy 69.40, Test_accy 70.38
2024-10-30 10:12:43,517 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.081,  Train_accy 93.69, Test_accy 80.62
2024-10-30 10:12:52,846 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.075,  Train_accy 94.13, Test_accy 81.14
2024-10-30 10:13:02,198 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.070,  Train_accy 94.24, Test_accy 82.24
2024-10-30 10:13:11,745 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.072,  Train_accy 94.00, Test_accy 82.45
2024-10-30 10:13:20,994 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.067,  Train_accy 94.02, Test_accy 82.55
2024-10-30 10:13:30,602 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.068,  Train_accy 94.76, Test_accy 82.38
2024-10-30 10:13:40,228 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.068,  Train_accy 94.40, Test_accy 83.12
2024-10-30 10:13:49,644 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.066,  Train_accy 94.91, Test_accy 83.36
2024-10-30 10:13:58,925 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.064,  Train_accy 94.82, Test_accy 83.45
2024-10-30 10:14:08,081 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.065,  Train_accy 94.84, Test_accy 83.67
2024-10-30 10:14:17,674 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.062,  Train_accy 94.89, Test_accy 83.31
2024-10-30 10:14:27,207 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.063,  Train_accy 94.67, Test_accy 83.64
2024-10-30 10:14:36,435 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.066,  Train_accy 94.73, Test_accy 83.21
2024-10-30 10:14:45,584 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.066,  Train_accy 94.69, Test_accy 84.02
2024-10-30 10:14:54,722 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.063,  Train_accy 94.56, Test_accy 83.88
2024-10-30 10:15:04,121 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.064,  Train_accy 94.78, Test_accy 83.90
2024-10-30 10:15:13,557 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.063,  Train_accy 94.84, Test_accy 83.81
2024-10-30 10:15:24,652 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.063,  Train_accy 95.00, Test_accy 83.48
2024-10-30 10:15:37,103 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.063,  Train_accy 95.02, Test_accy 83.98
2024-10-30 10:15:48,860 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.061,  Train_accy 94.91, Test_accy 83.86
2024-10-30 10:15:59,718 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.063,  Train_accy 95.27, Test_accy 83.93
2024-10-30 10:16:12,151 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.064,  Train_accy 94.69, Test_accy 83.95
2024-10-30 10:16:24,857 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.061,  Train_accy 94.78, Test_accy 84.02
2024-10-30 10:16:33,427 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.061,  Train_accy 94.78
2024-10-30 10:16:33,427 [pod_foster.py] => do not weight align student!
2024-10-30 10:16:34,367 [pod_foster.py] => darknet eval: 
2024-10-30 10:16:34,368 [pod_foster.py] => CNN top1 curve: 83.88
2024-10-30 10:16:34,368 [pod_foster.py] => CNN top5 curve: 99.45
2024-10-30 10:16:34,369 [pod_foster.py] => CNN: {'total': 83.88, '00-04': 82.93, '05-06': 86.25, 'old': 82.93, 'new': 86.25}
2024-10-30 10:16:34,370 [pod_foster.py] => All params after compression: 3851086
2024-10-30 10:16:34,371 [base.py] => Reducing exemplars...(71 per classes)
2024-10-30 10:16:36,472 [base.py] => Constructing exemplars...(71 per classes)
2024-10-30 10:16:41,280 [trainer.py] => All params: 7701139
2024-10-30 10:16:44,313 [pod_foster.py] => Exemplar size: 497
2024-10-30 10:16:44,321 [trainer.py] => CNN: {'total': 84.26, '00-04': 85.53, '05-06': 81.08, 'old': 85.53, 'new': 81.08}
2024-10-30 10:16:44,322 [trainer.py] => NME: {'total': 75.64, '00-04': 83.07, '05-06': 57.08, 'old': 83.07, 'new': 57.08}
2024-10-30 10:16:44,322 [trainer.py] => CNN top1 curve: [90.13, 84.26]
2024-10-30 10:16:44,322 [trainer.py] => CNN top5 curve: [100.0, 99.5]
2024-10-30 10:16:44,322 [trainer.py] => NME top1 curve: [89.53, 75.64]
2024-10-30 10:16:44,323 [trainer.py] => NME top5 curve: [100.0, 99.55]

2024-10-30 10:16:44,323 [trainer.py] => Average Accuracy (CNN): 87.195
2024-10-30 10:16:44,323 [trainer.py] => Average Accuracy (NME): 82.58500000000001
2024-10-30 10:16:44,324 [trainer.py] => All params: 7701139
2024-10-30 10:16:44,325 [trainer.py] => Trainable params: 3854670
2024-10-30 10:16:44,366 [pod_foster.py] => Learning on 7-9
2024-10-30 10:16:44,368 [pod_foster.py] => All params: 7705241
2024-10-30 10:16:44,369 [pod_foster.py] => Trainable params: 3857746
2024-10-30 10:16:44,464 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-30 10:16:44,469 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-30 10:16:48,372 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.411, Loss_clf 0.749, Loss_fe 0.770, Loss_pod 0.674, Loss_flat 0.218, Train_accy 81.34, Test_accy 50.78
2024-10-30 10:17:01,866 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.547, Loss_clf 0.019, Loss_fe 0.051, Loss_pod 0.391, Loss_flat 0.086, Train_accy 99.76, Test_accy 58.83
2024-10-30 10:17:15,211 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.334, Loss_clf 0.009, Loss_fe 0.014, Loss_pod 0.267, Loss_flat 0.044, Train_accy 100.00, Test_accy 66.41
2024-10-30 10:17:29,796 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.304, Loss_clf 0.007, Loss_fe 0.010, Loss_pod 0.250, Loss_flat 0.037, Train_accy 100.00, Test_accy 67.83
2024-10-30 10:17:43,297 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.379, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.308, Loss_flat 0.053, Train_accy 99.98, Test_accy 64.72
2024-10-30 10:17:56,898 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.297, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.248, Loss_flat 0.034, Train_accy 100.00, Test_accy 61.00
2024-10-30 10:18:10,366 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.279, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.237, Loss_flat 0.030, Train_accy 100.00, Test_accy 61.37
2024-10-30 10:18:23,706 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.261, Loss_clf 0.005, Loss_fe 0.007, Loss_pod 0.222, Loss_flat 0.027, Train_accy 100.00, Test_accy 61.41
2024-10-30 10:18:37,221 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.425, Loss_clf 0.011, Loss_fe 0.014, Loss_pod 0.340, Loss_flat 0.061, Train_accy 99.89, Test_accy 65.31
2024-10-30 10:18:50,713 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.272, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.229, Loss_flat 0.031, Train_accy 100.00, Test_accy 63.46
2024-10-30 10:19:04,819 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.242, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.205, Loss_flat 0.025, Train_accy 100.00, Test_accy 61.96
2024-10-30 10:19:19,423 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.236, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.201, Loss_flat 0.024, Train_accy 100.00, Test_accy 62.69
2024-10-30 10:19:33,302 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.244, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.207, Loss_flat 0.025, Train_accy 100.00, Test_accy 64.20
2024-10-30 10:19:44,502 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.271, Loss_clf 0.006, Loss_fe 0.007, Loss_pod 0.228, Loss_flat 0.030, Train_accy 100.00, Test_accy 64.83
2024-10-30 10:19:55,426 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.236, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.200, Loss_flat 0.026, Train_accy 100.00, Test_accy 63.85
2024-10-30 10:20:06,686 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.212, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.179, Loss_flat 0.022, Train_accy 100.00, Test_accy 61.72
2024-10-30 10:20:17,619 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.214, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.181, Loss_flat 0.023, Train_accy 100.00, Test_accy 62.33
2024-10-30 10:20:28,525 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.212, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.179, Loss_flat 0.022, Train_accy 100.00, Test_accy 63.87
2024-10-30 10:20:39,427 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.203, Loss_clf 0.004, Loss_fe 0.006, Loss_pod 0.171, Loss_flat 0.022, Train_accy 100.00, Test_accy 64.37
2024-10-30 10:20:50,513 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.194, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.164, Loss_flat 0.020, Train_accy 100.00, Test_accy 63.13
2024-10-30 10:21:01,724 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.191, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.161, Loss_flat 0.020, Train_accy 100.00, Test_accy 61.69
2024-10-30 10:21:12,902 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.193, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.162, Loss_flat 0.020, Train_accy 100.00, Test_accy 63.54
2024-10-30 10:21:24,258 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.174, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.145, Loss_flat 0.019, Train_accy 100.00, Test_accy 64.65
2024-10-30 10:21:35,393 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.176, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.146, Loss_flat 0.019, Train_accy 100.00, Test_accy 64.13
2024-10-30 10:21:46,538 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.171, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.143, Loss_flat 0.019, Train_accy 100.00, Test_accy 64.06
2024-10-30 10:21:57,691 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.160, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.133, Loss_flat 0.018, Train_accy 100.00, Test_accy 64.35
2024-10-30 10:22:08,883 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.168, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.140, Loss_flat 0.019, Train_accy 100.00, Test_accy 63.52
2024-10-30 10:22:20,192 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.161, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.134, Loss_flat 0.018, Train_accy 100.00, Test_accy 64.83
2024-10-30 10:22:34,095 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.153, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.126, Loss_flat 0.018, Train_accy 100.00, Test_accy 64.52
2024-10-30 10:22:47,733 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.155, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.128, Loss_flat 0.018, Train_accy 100.00, Test_accy 62.17
2024-10-30 10:22:56,599 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.156, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.128, Loss_flat 0.019, Train_accy 100.00
2024-10-30 10:22:56,600 [inc_net.py] => align weights, gamma = 0.46508076786994934 
2024-10-30 10:22:56,602 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-30 10:23:00,283 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.620,  Train_accy 70.11, Test_accy 66.67
2024-10-30 10:23:12,416 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.402,  Train_accy 94.08, Test_accy 74.54
2024-10-30 10:23:24,871 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.403,  Train_accy 95.04, Test_accy 76.98
2024-10-30 10:23:36,981 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.396,  Train_accy 95.55, Test_accy 77.33
2024-10-30 10:23:48,838 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.395,  Train_accy 95.40, Test_accy 76.89
2024-10-30 10:24:01,172 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.395,  Train_accy 95.60, Test_accy 77.46
2024-10-30 10:24:13,305 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.394,  Train_accy 95.66, Test_accy 78.20
2024-10-30 10:24:27,171 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.394,  Train_accy 95.60, Test_accy 78.00
2024-10-30 10:24:38,750 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.391,  Train_accy 95.93, Test_accy 77.76
2024-10-30 10:24:51,767 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.390,  Train_accy 95.62, Test_accy 78.28
2024-10-30 10:25:04,646 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.391,  Train_accy 95.82, Test_accy 78.74
2024-10-30 10:25:16,873 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.393,  Train_accy 96.31, Test_accy 78.56
2024-10-30 10:25:30,462 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.392,  Train_accy 95.93, Test_accy 78.26
2024-10-30 10:25:43,888 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.389,  Train_accy 96.24, Test_accy 78.46
2024-10-30 10:25:56,809 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.390,  Train_accy 96.11, Test_accy 78.65
2024-10-30 10:26:08,834 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.391,  Train_accy 96.15, Test_accy 78.56
2024-10-30 10:26:20,116 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.392,  Train_accy 96.22, Test_accy 78.91
2024-10-30 10:26:32,858 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.385,  Train_accy 96.31, Test_accy 77.93
2024-10-30 10:26:43,836 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.389,  Train_accy 96.53, Test_accy 78.63
2024-10-30 10:26:53,068 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.386,  Train_accy 96.38, Test_accy 78.19
2024-10-30 10:27:02,709 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.385,  Train_accy 96.20, Test_accy 78.67
2024-10-30 10:27:12,058 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.390,  Train_accy 96.51, Test_accy 78.22
2024-10-30 10:27:21,360 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.390,  Train_accy 96.24, Test_accy 78.56
2024-10-30 10:27:30,299 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.390,  Train_accy 96.42, Test_accy 78.93
2024-10-30 10:27:36,758 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.388,  Train_accy 96.40
2024-10-30 10:27:36,759 [pod_foster.py] => do not weight align student!
2024-10-30 10:27:37,500 [pod_foster.py] => darknet eval: 
2024-10-30 10:27:37,500 [pod_foster.py] => CNN top1 curve: 78.56
2024-10-30 10:27:37,500 [pod_foster.py] => CNN top5 curve: 97.7
2024-10-30 10:27:37,501 [pod_foster.py] => CNN: {'total': 78.56, '00-04': 76.87, '05-06': 80.58, '07-08': 80.75, 'old': 77.93, 'new': 80.75}
2024-10-30 10:27:37,502 [pod_foster.py] => All params after compression: 3853138
2024-10-30 10:27:37,502 [base.py] => Reducing exemplars...(55 per classes)
2024-10-30 10:27:39,192 [base.py] => Constructing exemplars...(55 per classes)
2024-10-30 10:27:42,520 [trainer.py] => All params: 7705241
2024-10-30 10:27:44,825 [pod_foster.py] => Exemplar size: 495
2024-10-30 10:27:44,826 [trainer.py] => CNN: {'total': 79.61, '00-04': 77.27, '05-06': 85.42, '07-08': 79.67, 'old': 79.6, 'new': 79.67}
2024-10-30 10:27:44,826 [trainer.py] => NME: {'total': 68.31, '00-04': 75.43, '05-06': 64.92, '07-08': 53.92, 'old': 72.43, 'new': 53.92}
2024-10-30 10:27:44,826 [trainer.py] => CNN top1 curve: [90.13, 84.26, 79.61]
2024-10-30 10:27:44,826 [trainer.py] => CNN top5 curve: [100.0, 99.5, 97.48]
2024-10-30 10:27:44,826 [trainer.py] => NME top1 curve: [89.53, 75.64, 68.31]
2024-10-30 10:27:44,826 [trainer.py] => NME top5 curve: [100.0, 99.55, 97.04]

2024-10-30 10:27:44,826 [trainer.py] => Average Accuracy (CNN): 84.66666666666667
2024-10-30 10:27:44,826 [trainer.py] => Average Accuracy (NME): 77.82666666666667
2024-10-30 10:27:44,827 [trainer.py] => Forgetting (CNN): 6.43
