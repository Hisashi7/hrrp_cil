2024-10-25 15:46:18,576 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 15:46:18,576 [trainer.py] => prefix: cil
2024-10-25 15:46:18,576 [trainer.py] => dataset: hrrp9
2024-10-25 15:46:18,576 [trainer.py] => memory_size: 500
2024-10-25 15:46:18,576 [trainer.py] => memory_per_class: 20
2024-10-25 15:46:18,577 [trainer.py] => fixed_memory: False
2024-10-25 15:46:18,577 [trainer.py] => shuffle: True
2024-10-25 15:46:18,577 [trainer.py] => init_cls: 5
2024-10-25 15:46:18,577 [trainer.py] => increment: 2
2024-10-25 15:46:18,577 [trainer.py] => model_name: POD_foster
2024-10-25 15:46:18,577 [trainer.py] => convnet_type: resnet18
2024-10-25 15:46:18,577 [trainer.py] => init_train: False
2024-10-25 15:46:18,577 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 15:46:18,577 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 15:46:18,577 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 15:46:18,577 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 15:46:18,577 [trainer.py] => device: [device(type='cuda', index=4)]
2024-10-25 15:46:18,577 [trainer.py] => seed: 1993
2024-10-25 15:46:18,577 [trainer.py] => beta1: 0.96
2024-10-25 15:46:18,577 [trainer.py] => beta2: 0.97
2024-10-25 15:46:18,577 [trainer.py] => oofc: ft
2024-10-25 15:46:18,578 [trainer.py] => is_teacher_wa: True
2024-10-25 15:46:18,578 [trainer.py] => is_student_wa: False
2024-10-25 15:46:18,578 [trainer.py] => lambda_okd: 0
2024-10-25 15:46:18,578 [trainer.py] => wa_value: 1
2024-10-25 15:46:18,578 [trainer.py] => init_epochs: 0
2024-10-25 15:46:18,578 [trainer.py] => init_lr: 0.1
2024-10-25 15:46:18,578 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 15:46:18,578 [trainer.py] => boosting_epochs: 150
2024-10-25 15:46:18,578 [trainer.py] => compression_epochs: 120
2024-10-25 15:46:18,578 [trainer.py] => lr: 0.1
2024-10-25 15:46:18,578 [trainer.py] => batch_size: 128
2024-10-25 15:46:18,578 [trainer.py] => weight_decay: 0.0005
2024-10-25 15:46:18,578 [trainer.py] => num_workers: 8
2024-10-25 15:46:18,578 [trainer.py] => momentum: 0.9
2024-10-25 15:46:18,578 [trainer.py] => T: 2
2024-10-25 15:46:18,579 [trainer.py] => lambda_c_base: 1.1
2024-10-25 15:46:18,579 [trainer.py] => lambda_f_base: 1.0
2024-10-25 15:46:18,579 [trainer.py] => POD: c
2024-10-25 15:46:19,426 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 15:46:19,483 [trainer.py] => All params: 0
2024-10-25 15:46:19,483 [trainer.py] => Trainable params: 0
2024-10-25 15:46:20,715 [pod_foster.py] => Learning on 0-5
2024-10-25 15:46:20,716 [pod_foster.py] => All params: 3849034
2024-10-25 15:46:20,716 [pod_foster.py] => Trainable params: 3849034
2024-10-25 15:46:20,790 [pod_foster.py] => Adaptive factor: 0
2024-10-25 15:46:21,017 [pod_foster.py] => init_train?---False
2024-10-25 15:46:22,038 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 15:46:22,038 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 15:46:27,973 [trainer.py] => All params: 3849034
2024-10-25 15:46:29,177 [pod_foster.py] => Exemplar size: 500
2024-10-25 15:46:29,178 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 15:46:29,178 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 15:46:29,178 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 15:46:29,178 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 15:46:29,178 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 15:46:29,178 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 15:46:29,178 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 15:46:29,178 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 15:46:29,179 [trainer.py] => All params: 3849034
2024-10-25 15:46:29,179 [trainer.py] => Trainable params: 3849034
2024-10-25 15:46:29,222 [pod_foster.py] => Learning on 5-7
2024-10-25 15:46:29,224 [pod_foster.py] => All params: 7701139
2024-10-25 15:46:29,224 [pod_foster.py] => Trainable params: 3854670
2024-10-25 15:46:29,259 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 15:46:29,271 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 15:46:32,384 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 2.113, Loss_clf 0.703, Loss_fe 0.661, Loss_pod 0.536, Loss_flat 0.213, Train_accy 83.33, Test_accy 54.81
2024-10-25 15:46:41,746 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.463, Loss_clf 0.019, Loss_fe 0.038, Loss_pod 0.334, Loss_flat 0.072, Train_accy 99.91, Test_accy 69.95
2024-10-25 15:46:51,175 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.399, Loss_clf 0.013, Loss_fe 0.018, Loss_pod 0.313, Loss_flat 0.054, Train_accy 99.98, Test_accy 69.43
2024-10-25 15:47:01,095 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.485, Loss_clf 0.028, Loss_fe 0.039, Loss_pod 0.350, Loss_flat 0.068, Train_accy 99.51, Test_accy 67.24
2024-10-25 15:47:10,769 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.354, Loss_clf 0.013, Loss_fe 0.014, Loss_pod 0.278, Loss_flat 0.049, Train_accy 100.00, Test_accy 67.00
2024-10-25 15:47:20,928 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.776, Loss_clf 0.050, Loss_fe 0.119, Loss_pod 0.475, Loss_flat 0.132, Train_accy 98.51, Test_accy 71.24
2024-10-25 15:47:31,544 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.425, Loss_clf 0.012, Loss_fe 0.013, Loss_pod 0.341, Loss_flat 0.058, Train_accy 99.98, Test_accy 69.98
2024-10-25 15:47:41,945 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.369, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.309, Loss_flat 0.042, Train_accy 100.00, Test_accy 68.24
2024-10-25 15:47:52,775 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.321, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.265, Loss_flat 0.038, Train_accy 100.00, Test_accy 66.43
2024-10-25 15:48:03,552 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.302, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.250, Loss_flat 0.035, Train_accy 100.00, Test_accy 68.48
2024-10-25 15:48:14,479 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.303, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.252, Loss_flat 0.034, Train_accy 100.00, Test_accy 64.29
2024-10-25 15:48:25,319 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.316, Loss_clf 0.010, Loss_fe 0.013, Loss_pod 0.250, Loss_flat 0.042, Train_accy 99.98, Test_accy 69.86
2024-10-25 15:48:36,979 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.285, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.233, Loss_flat 0.035, Train_accy 100.00, Test_accy 65.48
2024-10-25 15:48:48,682 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.412, Loss_clf 0.020, Loss_fe 0.024, Loss_pod 0.307, Loss_flat 0.061, Train_accy 99.71, Test_accy 66.17
2024-10-25 15:48:59,742 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.280, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.231, Loss_flat 0.033, Train_accy 100.00, Test_accy 68.38
2024-10-25 15:49:11,077 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.250, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.204, Loss_flat 0.031, Train_accy 100.00, Test_accy 67.95
2024-10-25 15:49:22,136 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.227, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.182, Loss_flat 0.030, Train_accy 100.00, Test_accy 67.55
2024-10-25 15:49:32,755 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.225, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.180, Loss_flat 0.030, Train_accy 100.00, Test_accy 66.52
2024-10-25 15:49:43,637 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.229, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.185, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.67
2024-10-25 15:49:54,366 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.215, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.169, Loss_flat 0.030, Train_accy 100.00, Test_accy 66.76
2024-10-25 15:50:05,040 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.209, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.166, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.88
2024-10-25 15:50:16,375 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.191, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.148, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.64
2024-10-25 15:50:27,307 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.193, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.148, Loss_flat 0.029, Train_accy 100.00, Test_accy 65.86
2024-10-25 15:50:38,605 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.188, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.145, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.64
2024-10-25 15:50:49,797 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.174, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.132, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.48
2024-10-25 15:51:00,964 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.179, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.137, Loss_flat 0.027, Train_accy 100.00, Test_accy 66.36
2024-10-25 15:51:11,812 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.170, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.128, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.12
2024-10-25 15:51:23,015 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.159, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.118, Loss_flat 0.026, Train_accy 100.00, Test_accy 67.33
2024-10-25 15:51:34,371 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.155, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.113, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.02
2024-10-25 15:51:45,723 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.163, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.121, Loss_flat 0.026, Train_accy 100.00, Test_accy 67.64
2024-10-25 15:51:53,734 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.153, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.111, Loss_flat 0.027, Train_accy 100.00
2024-10-25 15:51:53,737 [inc_net.py] => align weights, gamma = 0.46547049283981323 
2024-10-25 15:51:53,740 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 15:51:56,475 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.376,  Train_accy 64.09, Test_accy 71.57
2024-10-25 15:52:06,921 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 1.162,  Train_accy 89.84, Test_accy 78.17
2024-10-25 15:52:17,054 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 1.158,  Train_accy 91.47, Test_accy 77.67
2024-10-25 15:52:27,296 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 1.151,  Train_accy 91.91, Test_accy 78.26
2024-10-25 15:52:37,624 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 1.154,  Train_accy 91.80, Test_accy 78.81
2024-10-25 15:52:47,735 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 1.151,  Train_accy 92.24, Test_accy 79.26
2024-10-25 15:52:57,857 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 1.152,  Train_accy 91.62, Test_accy 79.02
2024-10-25 15:53:08,312 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 1.151,  Train_accy 91.91, Test_accy 78.95
2024-10-25 15:53:18,431 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 1.149,  Train_accy 92.38, Test_accy 79.26
2024-10-25 15:53:28,741 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 1.147,  Train_accy 92.36, Test_accy 78.74
2024-10-25 15:53:38,469 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 1.148,  Train_accy 92.36, Test_accy 78.90
2024-10-25 15:53:48,642 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 1.146,  Train_accy 92.20, Test_accy 79.43
2024-10-25 15:53:59,142 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 1.148,  Train_accy 92.33, Test_accy 79.76
2024-10-25 15:54:09,191 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 1.149,  Train_accy 92.76, Test_accy 78.93
2024-10-25 15:54:19,408 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 1.149,  Train_accy 92.31, Test_accy 79.71
2024-10-25 15:54:29,771 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 1.148,  Train_accy 92.33, Test_accy 79.55
2024-10-25 15:54:40,014 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 1.146,  Train_accy 92.36, Test_accy 79.83
2024-10-25 15:54:50,449 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 1.148,  Train_accy 92.58, Test_accy 79.62
2024-10-25 15:55:00,538 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 1.148,  Train_accy 92.47, Test_accy 79.76
2024-10-25 15:55:10,589 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 1.146,  Train_accy 92.64, Test_accy 79.60
2024-10-25 15:55:20,829 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 1.144,  Train_accy 92.49, Test_accy 79.62
2024-10-25 15:55:31,355 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 1.146,  Train_accy 92.42, Test_accy 79.86
2024-10-25 15:55:41,709 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 1.148,  Train_accy 92.84, Test_accy 80.05
2024-10-25 15:55:51,947 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 1.146,  Train_accy 92.84, Test_accy 79.69
2024-10-25 15:55:59,536 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 1.145,  Train_accy 92.53
2024-10-25 15:55:59,536 [pod_foster.py] => do not weight align student!
2024-10-25 15:56:00,304 [pod_foster.py] => darknet eval: 
2024-10-25 15:56:00,304 [pod_foster.py] => CNN top1 curve: 79.93
2024-10-25 15:56:00,304 [pod_foster.py] => CNN top5 curve: 98.69
2024-10-25 15:56:00,306 [pod_foster.py] => All params after compression: 3851086
2024-10-25 15:56:00,306 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 15:56:01,903 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 15:56:08,010 [trainer.py] => All params: 7701139
2024-10-25 15:56:10,398 [pod_foster.py] => Exemplar size: 497
2024-10-25 15:56:10,398 [trainer.py] => CNN: {'total': 80.5, '00-04': 81.03, '05-06': 79.17, 'old': 81.03, 'new': 79.17}
2024-10-25 15:56:10,398 [trainer.py] => NME: {'total': 73.55, '00-04': 79.17, '05-06': 59.5, 'old': 79.17, 'new': 59.5}
2024-10-25 15:56:10,398 [trainer.py] => CNN top1 curve: [89.93, 80.5]
2024-10-25 15:56:10,399 [trainer.py] => CNN top5 curve: [100.0, 98.71]
2024-10-25 15:56:10,399 [trainer.py] => NME top1 curve: [90.0, 73.55]
2024-10-25 15:56:10,399 [trainer.py] => NME top5 curve: [100.0, 98.95]

2024-10-25 15:56:10,399 [trainer.py] => Average Accuracy (CNN): 85.215
2024-10-25 15:56:10,399 [trainer.py] => Average Accuracy (NME): 81.775
2024-10-25 15:56:10,400 [trainer.py] => All params: 7701139
2024-10-25 15:56:10,400 [trainer.py] => Trainable params: 3854670
2024-10-25 15:56:10,456 [pod_foster.py] => Learning on 7-9
2024-10-25 15:56:10,457 [pod_foster.py] => All params: 7705241
2024-10-25 15:56:10,458 [pod_foster.py] => Trainable params: 3857746
2024-10-25 15:56:10,515 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 15:56:10,526 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 15:56:14,004 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 2.169, Loss_clf 0.646, Loss_fe 0.657, Loss_pod 0.668, Loss_flat 0.198, Train_accy 85.37, Test_accy 63.26
2024-10-25 15:56:25,711 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.596, Loss_clf 0.019, Loss_fe 0.059, Loss_pod 0.454, Loss_flat 0.065, Train_accy 99.89, Test_accy 63.96
2024-10-25 15:56:37,433 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.433, Loss_clf 0.013, Loss_fe 0.021, Loss_pod 0.355, Loss_flat 0.043, Train_accy 100.00, Test_accy 67.74
2024-10-25 15:56:49,434 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.395, Loss_clf 0.010, Loss_fe 0.015, Loss_pod 0.331, Loss_flat 0.039, Train_accy 100.00, Test_accy 67.39
2024-10-25 15:57:01,047 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.367, Loss_clf 0.009, Loss_fe 0.012, Loss_pod 0.313, Loss_flat 0.033, Train_accy 100.00, Test_accy 68.87
2024-10-25 15:57:12,681 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.506, Loss_clf 0.023, Loss_fe 0.035, Loss_pod 0.392, Loss_flat 0.056, Train_accy 99.40, Test_accy 57.15
2024-10-25 15:57:24,493 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.364, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.313, Loss_flat 0.032, Train_accy 100.00, Test_accy 64.00
2024-10-25 15:57:35,778 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.331, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.285, Loss_flat 0.029, Train_accy 100.00, Test_accy 64.91
2024-10-25 15:57:47,555 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.435, Loss_clf 0.010, Loss_fe 0.015, Loss_pod 0.358, Loss_flat 0.052, Train_accy 100.00, Test_accy 66.56
2024-10-25 15:57:59,749 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.373, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.321, Loss_flat 0.034, Train_accy 100.00, Test_accy 66.50
2024-10-25 15:58:11,171 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.325, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.279, Loss_flat 0.029, Train_accy 100.00, Test_accy 65.89
2024-10-25 15:58:22,734 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.331, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.284, Loss_flat 0.028, Train_accy 100.00, Test_accy 62.26
2024-10-25 15:58:34,280 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.314, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.264, Loss_flat 0.031, Train_accy 99.98, Test_accy 61.20
2024-10-25 15:58:45,517 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.387, Loss_clf 0.012, Loss_fe 0.017, Loss_pod 0.312, Loss_flat 0.047, Train_accy 99.91, Test_accy 71.44
2024-10-25 15:58:56,782 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.309, Loss_clf 0.008, Loss_fe 0.010, Loss_pod 0.257, Loss_flat 0.034, Train_accy 99.98, Test_accy 67.09
2024-10-25 15:59:07,793 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.278, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.235, Loss_flat 0.028, Train_accy 100.00, Test_accy 65.20
2024-10-25 15:59:18,964 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.288, Loss_clf 0.009, Loss_fe 0.011, Loss_pod 0.236, Loss_flat 0.032, Train_accy 99.98, Test_accy 64.15
2024-10-25 15:59:30,128 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.250, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.209, Loss_flat 0.026, Train_accy 100.00, Test_accy 62.89
2024-10-25 15:59:41,236 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.239, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.198, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.98
2024-10-25 15:59:52,044 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.237, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.197, Loss_flat 0.025, Train_accy 100.00, Test_accy 63.81
2024-10-25 16:00:02,343 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.216, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.175, Loss_flat 0.025, Train_accy 100.00, Test_accy 62.30
2024-10-25 16:00:13,150 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.254, Loss_clf 0.008, Loss_fe 0.009, Loss_pod 0.204, Loss_flat 0.032, Train_accy 100.00, Test_accy 64.65
2024-10-25 16:00:24,035 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.223, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.181, Loss_flat 0.027, Train_accy 100.00, Test_accy 64.98
2024-10-25 16:00:34,467 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.219, Loss_clf 0.009, Loss_fe 0.010, Loss_pod 0.173, Loss_flat 0.027, Train_accy 100.00, Test_accy 62.24
2024-10-25 16:00:45,716 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.188, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.149, Loss_flat 0.024, Train_accy 100.00, Test_accy 64.19
2024-10-25 16:00:56,087 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.188, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.149, Loss_flat 0.025, Train_accy 100.00, Test_accy 64.24
2024-10-25 16:01:06,529 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.177, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.137, Loss_flat 0.025, Train_accy 100.00, Test_accy 64.13
2024-10-25 16:01:17,375 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.166, Loss_clf 0.007, Loss_fe 0.007, Loss_pod 0.128, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.35
2024-10-25 16:01:28,145 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.160, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.122, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.28
2024-10-25 16:01:38,995 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.171, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.130, Loss_flat 0.025, Train_accy 100.00, Test_accy 61.67
2024-10-25 16:01:46,618 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.156, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.118, Loss_flat 0.023, Train_accy 100.00
2024-10-25 16:01:46,619 [inc_net.py] => align weights, gamma = 0.468641459941864 
2024-10-25 16:01:46,620 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 16:01:49,399 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.656,  Train_accy 68.98, Test_accy 67.06
2024-10-25 16:01:59,265 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.473,  Train_accy 91.57, Test_accy 74.44
2024-10-25 16:02:09,288 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.475,  Train_accy 92.48, Test_accy 74.41
2024-10-25 16:02:19,320 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.470,  Train_accy 93.40, Test_accy 75.06
2024-10-25 16:02:28,805 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.471,  Train_accy 93.02, Test_accy 74.80
2024-10-25 16:02:38,739 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.471,  Train_accy 93.60, Test_accy 76.61
2024-10-25 16:02:48,156 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.466,  Train_accy 93.04, Test_accy 76.39
2024-10-25 16:02:57,819 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.468,  Train_accy 93.51, Test_accy 76.87
2024-10-25 16:03:07,248 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.466,  Train_accy 93.75, Test_accy 77.70
2024-10-25 16:03:16,457 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.467,  Train_accy 93.97, Test_accy 76.57
2024-10-25 16:03:26,090 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.468,  Train_accy 93.28, Test_accy 76.35
2024-10-25 16:03:36,097 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.467,  Train_accy 93.46, Test_accy 76.61
2024-10-25 16:03:45,997 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.468,  Train_accy 93.35, Test_accy 76.72
2024-10-25 16:03:55,423 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.464,  Train_accy 94.02, Test_accy 77.35
2024-10-25 16:04:05,175 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.467,  Train_accy 93.71, Test_accy 77.09
2024-10-25 16:04:14,993 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.467,  Train_accy 93.46, Test_accy 77.30
2024-10-25 16:04:24,770 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.469,  Train_accy 93.77, Test_accy 77.02
2024-10-25 16:04:34,187 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.462,  Train_accy 93.71, Test_accy 76.91
2024-10-25 16:04:43,869 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.465,  Train_accy 94.02, Test_accy 77.33
2024-10-25 16:04:53,317 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.460,  Train_accy 93.84, Test_accy 77.59
2024-10-25 16:05:02,594 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.463,  Train_accy 93.91, Test_accy 77.50
2024-10-25 16:05:12,269 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.465,  Train_accy 93.86, Test_accy 77.35
2024-10-25 16:05:21,742 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.466,  Train_accy 93.97, Test_accy 77.69
2024-10-25 16:05:31,219 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.467,  Train_accy 93.77, Test_accy 76.81
2024-10-25 16:05:38,415 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.465,  Train_accy 93.88
2024-10-25 16:05:38,415 [pod_foster.py] => do not weight align student!
2024-10-25 16:05:39,329 [pod_foster.py] => darknet eval: 
2024-10-25 16:05:39,329 [pod_foster.py] => CNN top1 curve: 77.93
2024-10-25 16:05:39,329 [pod_foster.py] => CNN top5 curve: 96.52
2024-10-25 16:05:39,331 [pod_foster.py] => All params after compression: 3853138
2024-10-25 16:05:39,331 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 16:05:41,141 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 16:05:44,618 [trainer.py] => All params: 7705241
2024-10-25 16:05:47,077 [pod_foster.py] => Exemplar size: 495
2024-10-25 16:05:47,078 [trainer.py] => CNN: {'total': 76.76, '00-04': 68.07, '05-06': 88.25, '07-08': 87.0, 'old': 73.83, 'new': 87.0}
2024-10-25 16:05:47,078 [trainer.py] => NME: {'total': 73.31, '00-04': 70.03, '05-06': 75.0, '07-08': 79.83, 'old': 71.45, 'new': 79.83}
2024-10-25 16:05:47,078 [trainer.py] => CNN top1 curve: [89.93, 80.5, 76.76]
2024-10-25 16:05:47,078 [trainer.py] => CNN top5 curve: [100.0, 98.71, 96.28]
2024-10-25 16:05:47,078 [trainer.py] => NME top1 curve: [90.0, 73.55, 73.31]
2024-10-25 16:05:47,078 [trainer.py] => NME top5 curve: [100.0, 98.95, 96.46]

2024-10-25 16:05:47,078 [trainer.py] => Average Accuracy (CNN): 82.39666666666666
2024-10-25 16:05:47,078 [trainer.py] => Average Accuracy (NME): 78.95333333333333
2024-10-25 16:05:47,079 [trainer.py] => Forgetting (CNN): 10.930000000000007
