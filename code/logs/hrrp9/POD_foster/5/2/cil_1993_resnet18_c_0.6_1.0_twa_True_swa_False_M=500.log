2024-10-25 16:25:37,075 [trainer.py] => config: ./exps/POD_foster.json
2024-10-25 16:25:37,076 [trainer.py] => prefix: cil
2024-10-25 16:25:37,076 [trainer.py] => dataset: hrrp9
2024-10-25 16:25:37,076 [trainer.py] => memory_size: 500
2024-10-25 16:25:37,076 [trainer.py] => memory_per_class: 20
2024-10-25 16:25:37,077 [trainer.py] => fixed_memory: False
2024-10-25 16:25:37,077 [trainer.py] => shuffle: True
2024-10-25 16:25:37,077 [trainer.py] => init_cls: 5
2024-10-25 16:25:37,077 [trainer.py] => increment: 2
2024-10-25 16:25:37,077 [trainer.py] => model_name: POD_foster
2024-10-25 16:25:37,078 [trainer.py] => convnet_type: resnet18
2024-10-25 16:25:37,078 [trainer.py] => init_train: False
2024-10-25 16:25:37,078 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2024-10-25 16:25:37,078 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2024-10-25 16:25:37,079 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2024-10-25 16:25:37,079 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2024-10-25 16:25:37,079 [trainer.py] => device: [device(type='cuda', index=2)]
2024-10-25 16:25:37,079 [trainer.py] => seed: 1993
2024-10-25 16:25:37,080 [trainer.py] => beta1: 0.96
2024-10-25 16:25:37,080 [trainer.py] => beta2: 0.97
2024-10-25 16:25:37,080 [trainer.py] => oofc: ft
2024-10-25 16:25:37,081 [trainer.py] => is_teacher_wa: True
2024-10-25 16:25:37,081 [trainer.py] => is_student_wa: False
2024-10-25 16:25:37,081 [trainer.py] => lambda_okd: 0
2024-10-25 16:25:37,082 [trainer.py] => wa_value: 1
2024-10-25 16:25:37,082 [trainer.py] => init_epochs: 0
2024-10-25 16:25:37,082 [trainer.py] => init_lr: 0.1
2024-10-25 16:25:37,082 [trainer.py] => init_weight_decay: 0.0005
2024-10-25 16:25:37,083 [trainer.py] => boosting_epochs: 150
2024-10-25 16:25:37,083 [trainer.py] => compression_epochs: 120
2024-10-25 16:25:37,083 [trainer.py] => lr: 0.1
2024-10-25 16:25:37,084 [trainer.py] => batch_size: 128
2024-10-25 16:25:37,084 [trainer.py] => weight_decay: 0.0005
2024-10-25 16:25:37,084 [trainer.py] => num_workers: 8
2024-10-25 16:25:37,084 [trainer.py] => momentum: 0.9
2024-10-25 16:25:37,085 [trainer.py] => T: 2
2024-10-25 16:25:37,085 [trainer.py] => lambda_c_base: 0.6
2024-10-25 16:25:37,085 [trainer.py] => lambda_f_base: 1.0
2024-10-25 16:25:37,086 [trainer.py] => POD: c
2024-10-25 16:25:37,855 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2024-10-25 16:25:37,911 [trainer.py] => All params: 0
2024-10-25 16:25:37,911 [trainer.py] => Trainable params: 0
2024-10-25 16:25:39,098 [pod_foster.py] => Learning on 0-5
2024-10-25 16:25:39,099 [pod_foster.py] => All params: 3849034
2024-10-25 16:25:39,100 [pod_foster.py] => Trainable params: 3849034
2024-10-25 16:25:39,170 [pod_foster.py] => Adaptive factor: 0
2024-10-25 16:25:39,403 [pod_foster.py] => init_train?---False
2024-10-25 16:25:40,588 [base.py] => Reducing exemplars...(100 per classes)
2024-10-25 16:25:40,588 [base.py] => Constructing exemplars...(100 per classes)
2024-10-25 16:25:47,027 [trainer.py] => All params: 3849034
2024-10-25 16:25:48,444 [pod_foster.py] => Exemplar size: 500
2024-10-25 16:25:48,444 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2024-10-25 16:25:48,444 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2024-10-25 16:25:48,444 [trainer.py] => CNN top1 curve: [89.93]
2024-10-25 16:25:48,445 [trainer.py] => CNN top5 curve: [100.0]
2024-10-25 16:25:48,445 [trainer.py] => NME top1 curve: [90.0]
2024-10-25 16:25:48,445 [trainer.py] => NME top5 curve: [100.0]

2024-10-25 16:25:48,445 [trainer.py] => Average Accuracy (CNN): 89.93
2024-10-25 16:25:48,445 [trainer.py] => Average Accuracy (NME): 90.0
2024-10-25 16:25:48,445 [trainer.py] => All params: 3849034
2024-10-25 16:25:48,446 [trainer.py] => Trainable params: 3849034
2024-10-25 16:25:48,489 [pod_foster.py] => Learning on 5-7
2024-10-25 16:25:48,490 [pod_foster.py] => All params: 7701139
2024-10-25 16:25:48,491 [pod_foster.py] => Trainable params: 3854670
2024-10-25 16:25:48,541 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2024-10-25 16:25:48,550 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2024-10-25 16:25:52,111 [pod_foster.py] => Task 1, Epoch 1/150 => Loss 1.798, Loss_clf 0.703, Loss_fe 0.665, Loss_pod 0.195, Loss_flat 0.235, Train_accy 83.53, Test_accy 52.76
2024-10-25 16:26:02,942 [pod_foster.py] => Task 1, Epoch 6/150 => Loss 0.322, Loss_clf 0.030, Loss_fe 0.066, Loss_pod 0.120, Loss_flat 0.107, Train_accy 99.47, Test_accy 67.24
2024-10-25 16:26:13,964 [pod_foster.py] => Task 1, Epoch 11/150 => Loss 0.139, Loss_clf 0.007, Loss_fe 0.008, Loss_pod 0.069, Loss_flat 0.054, Train_accy 100.00, Test_accy 70.69
2024-10-25 16:26:24,889 [pod_foster.py] => Task 1, Epoch 16/150 => Loss 0.109, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.056, Loss_flat 0.041, Train_accy 100.00, Test_accy 70.14
2024-10-25 16:26:35,870 [pod_foster.py] => Task 1, Epoch 21/150 => Loss 0.104, Loss_clf 0.008, Loss_fe 0.008, Loss_pod 0.052, Loss_flat 0.037, Train_accy 99.98, Test_accy 57.57
2024-10-25 16:26:46,399 [pod_foster.py] => Task 1, Epoch 26/150 => Loss 0.411, Loss_clf 0.040, Loss_fe 0.087, Loss_pod 0.147, Loss_flat 0.138, Train_accy 99.00, Test_accy 74.67
2024-10-25 16:26:56,945 [pod_foster.py] => Task 1, Epoch 31/150 => Loss 0.162, Loss_clf 0.008, Loss_fe 0.008, Loss_pod 0.083, Loss_flat 0.063, Train_accy 100.00, Test_accy 67.31
2024-10-25 16:27:07,563 [pod_foster.py] => Task 1, Epoch 36/150 => Loss 0.118, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.065, Loss_flat 0.042, Train_accy 100.00, Test_accy 66.12
2024-10-25 16:27:18,318 [pod_foster.py] => Task 1, Epoch 41/150 => Loss 0.111, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.061, Loss_flat 0.040, Train_accy 100.00, Test_accy 66.12
2024-10-25 16:27:29,161 [pod_foster.py] => Task 1, Epoch 46/150 => Loss 0.092, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.052, Loss_flat 0.032, Train_accy 100.00, Test_accy 68.14
2024-10-25 16:27:40,041 [pod_foster.py] => Task 1, Epoch 51/150 => Loss 0.086, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.048, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.67
2024-10-25 16:27:50,791 [pod_foster.py] => Task 1, Epoch 56/150 => Loss 0.079, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.64
2024-10-25 16:28:01,714 [pod_foster.py] => Task 1, Epoch 61/150 => Loss 0.087, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.047, Loss_flat 0.031, Train_accy 100.00, Test_accy 67.29
2024-10-25 16:28:12,395 [pod_foster.py] => Task 1, Epoch 66/150 => Loss 0.460, Loss_clf 0.068, Loss_fe 0.113, Loss_pod 0.157, Loss_flat 0.123, Train_accy 97.76, Test_accy 63.17
2024-10-25 16:28:23,346 [pod_foster.py] => Task 1, Epoch 71/150 => Loss 0.109, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.061, Loss_flat 0.038, Train_accy 100.00, Test_accy 68.24
2024-10-25 16:28:34,107 [pod_foster.py] => Task 1, Epoch 76/150 => Loss 0.089, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.050, Loss_flat 0.031, Train_accy 100.00, Test_accy 68.60
2024-10-25 16:28:45,019 [pod_foster.py] => Task 1, Epoch 81/150 => Loss 0.079, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.043, Loss_flat 0.028, Train_accy 100.00, Test_accy 68.36
2024-10-25 16:28:55,924 [pod_foster.py] => Task 1, Epoch 86/150 => Loss 0.077, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.042, Loss_flat 0.027, Train_accy 100.00, Test_accy 67.69
2024-10-25 16:29:06,918 [pod_foster.py] => Task 1, Epoch 91/150 => Loss 0.072, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.039, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.93
2024-10-25 16:29:17,887 [pod_foster.py] => Task 1, Epoch 96/150 => Loss 0.078, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.040, Loss_flat 0.029, Train_accy 100.00, Test_accy 67.50
2024-10-25 16:29:28,996 [pod_foster.py] => Task 1, Epoch 101/150 => Loss 0.066, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.034, Loss_flat 0.025, Train_accy 100.00, Test_accy 67.60
2024-10-25 16:29:40,216 [pod_foster.py] => Task 1, Epoch 106/150 => Loss 0.066, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.034, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.57
2024-10-25 16:29:51,684 [pod_foster.py] => Task 1, Epoch 111/150 => Loss 0.064, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.032, Loss_flat 0.024, Train_accy 100.00, Test_accy 66.93
2024-10-25 16:30:02,849 [pod_foster.py] => Task 1, Epoch 116/150 => Loss 0.061, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.031, Loss_flat 0.023, Train_accy 100.00, Test_accy 68.43
2024-10-25 16:30:13,591 [pod_foster.py] => Task 1, Epoch 121/150 => Loss 0.059, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.028, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.26
2024-10-25 16:30:24,441 [pod_foster.py] => Task 1, Epoch 126/150 => Loss 0.059, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.028, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.17
2024-10-25 16:30:35,552 [pod_foster.py] => Task 1, Epoch 131/150 => Loss 0.058, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.028, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.29
2024-10-25 16:30:46,331 [pod_foster.py] => Task 1, Epoch 136/150 => Loss 0.056, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.026, Loss_flat 0.022, Train_accy 100.00, Test_accy 67.43
2024-10-25 16:30:57,127 [pod_foster.py] => Task 1, Epoch 141/150 => Loss 0.056, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.026, Loss_flat 0.023, Train_accy 100.00, Test_accy 67.55
2024-10-25 16:31:07,937 [pod_foster.py] => Task 1, Epoch 146/150 => Loss 0.057, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.027, Loss_flat 0.022, Train_accy 100.00, Test_accy 68.26
2024-10-25 16:31:15,961 [pod_foster.py] => Task 1, Epoch 150/150 => Loss 0.056, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.026, Loss_flat 0.023, Train_accy 100.00
2024-10-25 16:31:15,962 [inc_net.py] => align weights, gamma = 0.5238828063011169 
2024-10-25 16:31:15,963 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2024-10-25 16:31:18,574 [pod_foster.py] => SNet: Task 1, Epoch 1/120 => Loss 1.262,  Train_accy 69.56, Test_accy 65.64
2024-10-25 16:31:28,154 [pod_foster.py] => SNet: Task 1, Epoch 6/120 => Loss 0.967,  Train_accy 94.67, Test_accy 76.43
2024-10-25 16:31:38,104 [pod_foster.py] => SNet: Task 1, Epoch 11/120 => Loss 0.961,  Train_accy 95.62, Test_accy 76.48
2024-10-25 16:31:47,968 [pod_foster.py] => SNet: Task 1, Epoch 16/120 => Loss 0.953,  Train_accy 95.96, Test_accy 77.29
2024-10-25 16:31:57,839 [pod_foster.py] => SNet: Task 1, Epoch 21/120 => Loss 0.956,  Train_accy 96.11, Test_accy 78.55
2024-10-25 16:32:07,541 [pod_foster.py] => SNet: Task 1, Epoch 26/120 => Loss 0.952,  Train_accy 96.07, Test_accy 78.40
2024-10-25 16:32:16,997 [pod_foster.py] => SNet: Task 1, Epoch 31/120 => Loss 0.955,  Train_accy 96.16, Test_accy 78.14
2024-10-25 16:32:26,530 [pod_foster.py] => SNet: Task 1, Epoch 36/120 => Loss 0.954,  Train_accy 96.09, Test_accy 77.62
2024-10-25 16:32:36,041 [pod_foster.py] => SNet: Task 1, Epoch 41/120 => Loss 0.950,  Train_accy 96.53, Test_accy 78.12
2024-10-25 16:32:45,160 [pod_foster.py] => SNet: Task 1, Epoch 46/120 => Loss 0.948,  Train_accy 96.40, Test_accy 78.21
2024-10-25 16:32:54,806 [pod_foster.py] => SNet: Task 1, Epoch 51/120 => Loss 0.950,  Train_accy 96.64, Test_accy 77.90
2024-10-25 16:33:04,041 [pod_foster.py] => SNet: Task 1, Epoch 56/120 => Loss 0.948,  Train_accy 96.38, Test_accy 78.40
2024-10-25 16:33:13,537 [pod_foster.py] => SNet: Task 1, Epoch 61/120 => Loss 0.949,  Train_accy 96.24, Test_accy 78.07
2024-10-25 16:33:23,079 [pod_foster.py] => SNet: Task 1, Epoch 66/120 => Loss 0.950,  Train_accy 96.82, Test_accy 77.86
2024-10-25 16:33:32,834 [pod_foster.py] => SNet: Task 1, Epoch 71/120 => Loss 0.950,  Train_accy 96.60, Test_accy 79.07
2024-10-25 16:33:42,307 [pod_foster.py] => SNet: Task 1, Epoch 76/120 => Loss 0.949,  Train_accy 96.84, Test_accy 79.10
2024-10-25 16:33:51,810 [pod_foster.py] => SNet: Task 1, Epoch 81/120 => Loss 0.946,  Train_accy 96.38, Test_accy 79.07
2024-10-25 16:34:01,526 [pod_foster.py] => SNet: Task 1, Epoch 86/120 => Loss 0.948,  Train_accy 96.78, Test_accy 78.76
2024-10-25 16:34:11,226 [pod_foster.py] => SNet: Task 1, Epoch 91/120 => Loss 0.950,  Train_accy 96.76, Test_accy 78.90
2024-10-25 16:34:20,894 [pod_foster.py] => SNet: Task 1, Epoch 96/120 => Loss 0.947,  Train_accy 96.87, Test_accy 78.55
2024-10-25 16:34:30,656 [pod_foster.py] => SNet: Task 1, Epoch 101/120 => Loss 0.946,  Train_accy 96.78, Test_accy 78.71
2024-10-25 16:34:40,535 [pod_foster.py] => SNet: Task 1, Epoch 106/120 => Loss 0.947,  Train_accy 96.49, Test_accy 78.90
2024-10-25 16:34:50,143 [pod_foster.py] => SNet: Task 1, Epoch 111/120 => Loss 0.949,  Train_accy 96.71, Test_accy 79.69
2024-10-25 16:34:59,849 [pod_foster.py] => SNet: Task 1, Epoch 116/120 => Loss 0.947,  Train_accy 96.84, Test_accy 78.88
2024-10-25 16:35:06,748 [pod_foster.py] => SNet: Task 1, Epoch 120/120 => Loss 0.946,  Train_accy 96.96
2024-10-25 16:35:06,748 [pod_foster.py] => do not weight align student!
2024-10-25 16:35:07,401 [pod_foster.py] => darknet eval: 
2024-10-25 16:35:07,402 [pod_foster.py] => CNN top1 curve: 78.86
2024-10-25 16:35:07,402 [pod_foster.py] => CNN top5 curve: 98.6
2024-10-25 16:35:07,403 [pod_foster.py] => All params after compression: 3851086
2024-10-25 16:35:07,404 [base.py] => Reducing exemplars...(71 per classes)
2024-10-25 16:35:08,792 [base.py] => Constructing exemplars...(71 per classes)
2024-10-25 16:35:12,597 [trainer.py] => All params: 7701139
2024-10-25 16:35:14,466 [pod_foster.py] => Exemplar size: 497
2024-10-25 16:35:14,467 [trainer.py] => CNN: {'total': 79.88, '00-04': 77.93, '05-06': 84.75, 'old': 77.93, 'new': 84.75}
2024-10-25 16:35:14,467 [trainer.py] => NME: {'total': 73.43, '00-04': 77.97, '05-06': 62.08, 'old': 77.97, 'new': 62.08}
2024-10-25 16:35:14,467 [trainer.py] => CNN top1 curve: [89.93, 79.88]
2024-10-25 16:35:14,467 [trainer.py] => CNN top5 curve: [100.0, 98.74]
2024-10-25 16:35:14,467 [trainer.py] => NME top1 curve: [90.0, 73.43]
2024-10-25 16:35:14,467 [trainer.py] => NME top5 curve: [100.0, 98.98]

2024-10-25 16:35:14,467 [trainer.py] => Average Accuracy (CNN): 84.905
2024-10-25 16:35:14,467 [trainer.py] => Average Accuracy (NME): 81.715
2024-10-25 16:35:14,468 [trainer.py] => All params: 7701139
2024-10-25 16:35:14,469 [trainer.py] => Trainable params: 3854670
2024-10-25 16:35:14,524 [pod_foster.py] => Learning on 7-9
2024-10-25 16:35:14,526 [pod_foster.py] => All params: 7705241
2024-10-25 16:35:14,527 [pod_foster.py] => Trainable params: 3857746
2024-10-25 16:35:14,563 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2024-10-25 16:35:14,575 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2024-10-25 16:35:18,014 [pod_foster.py] => Task 2, Epoch 1/150 => Loss 1.838, Loss_clf 0.702, Loss_fe 0.670, Loss_pod 0.229, Loss_flat 0.237, Train_accy 85.55, Test_accy 58.11
2024-10-25 16:35:29,176 [pod_foster.py] => Task 2, Epoch 6/150 => Loss 0.230, Loss_clf 0.012, Loss_fe 0.039, Loss_pod 0.102, Loss_flat 0.077, Train_accy 99.96, Test_accy 61.26
2024-10-25 16:35:40,229 [pod_foster.py] => Task 2, Epoch 11/150 => Loss 0.124, Loss_clf 0.007, Loss_fe 0.009, Loss_pod 0.067, Loss_flat 0.041, Train_accy 100.00, Test_accy 66.59
2024-10-25 16:35:51,447 [pod_foster.py] => Task 2, Epoch 16/150 => Loss 0.201, Loss_clf 0.009, Loss_fe 0.016, Loss_pod 0.104, Loss_flat 0.071, Train_accy 99.98, Test_accy 66.28
2024-10-25 16:36:02,499 [pod_foster.py] => Task 2, Epoch 21/150 => Loss 0.104, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.061, Loss_flat 0.033, Train_accy 100.00, Test_accy 67.63
2024-10-25 16:36:13,502 [pod_foster.py] => Task 2, Epoch 26/150 => Loss 0.096, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.055, Loss_flat 0.031, Train_accy 100.00, Test_accy 64.87
2024-10-25 16:36:24,526 [pod_foster.py] => Task 2, Epoch 31/150 => Loss 0.097, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.058, Loss_flat 0.030, Train_accy 100.00, Test_accy 63.80
2024-10-25 16:36:35,710 [pod_foster.py] => Task 2, Epoch 36/150 => Loss 0.090, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.054, Loss_flat 0.028, Train_accy 100.00, Test_accy 64.70
2024-10-25 16:36:47,092 [pod_foster.py] => Task 2, Epoch 41/150 => Loss 0.263, Loss_clf 0.014, Loss_fe 0.037, Loss_pod 0.126, Loss_flat 0.086, Train_accy 99.78, Test_accy 68.56
2024-10-25 16:36:58,254 [pod_foster.py] => Task 2, Epoch 46/150 => Loss 0.133, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.076, Loss_flat 0.046, Train_accy 100.00, Test_accy 67.94
2024-10-25 16:37:09,549 [pod_foster.py] => Task 2, Epoch 51/150 => Loss 0.101, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.059, Loss_flat 0.032, Train_accy 100.00, Test_accy 66.28
2024-10-25 16:37:20,816 [pod_foster.py] => Task 2, Epoch 56/150 => Loss 0.099, Loss_clf 0.006, Loss_fe 0.006, Loss_pod 0.056, Loss_flat 0.030, Train_accy 100.00, Test_accy 23.28
2024-10-25 16:37:31,894 [pod_foster.py] => Task 2, Epoch 61/150 => Loss 0.097, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.058, Loss_flat 0.031, Train_accy 100.00, Test_accy 66.78
2024-10-25 16:37:43,254 [pod_foster.py] => Task 2, Epoch 66/150 => Loss 0.095, Loss_clf 0.005, Loss_fe 0.006, Loss_pod 0.053, Loss_flat 0.032, Train_accy 100.00, Test_accy 70.44
2024-10-25 16:37:54,593 [pod_foster.py] => Task 2, Epoch 71/150 => Loss 0.094, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.052, Loss_flat 0.031, Train_accy 99.96, Test_accy 67.19
2024-10-25 16:38:06,380 [pod_foster.py] => Task 2, Epoch 76/150 => Loss 0.078, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.026, Train_accy 100.00, Test_accy 67.22
2024-10-25 16:38:18,484 [pod_foster.py] => Task 2, Epoch 81/150 => Loss 0.082, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.045, Loss_flat 0.028, Train_accy 100.00, Test_accy 66.56
2024-10-25 16:38:30,335 [pod_foster.py] => Task 2, Epoch 86/150 => Loss 0.076, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.044, Loss_flat 0.024, Train_accy 100.00, Test_accy 67.26
2024-10-25 16:38:42,285 [pod_foster.py] => Task 2, Epoch 91/150 => Loss 0.073, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.041, Loss_flat 0.024, Train_accy 100.00, Test_accy 64.52
2024-10-25 16:38:53,962 [pod_foster.py] => Task 2, Epoch 96/150 => Loss 0.067, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.037, Loss_flat 0.023, Train_accy 100.00, Test_accy 65.33
2024-10-25 16:39:05,962 [pod_foster.py] => Task 2, Epoch 101/150 => Loss 0.071, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.040, Loss_flat 0.023, Train_accy 100.00, Test_accy 65.13
2024-10-25 16:39:17,830 [pod_foster.py] => Task 2, Epoch 106/150 => Loss 0.088, Loss_clf 0.004, Loss_fe 0.005, Loss_pod 0.047, Loss_flat 0.032, Train_accy 100.00, Test_accy 65.11
2024-10-25 16:39:29,468 [pod_foster.py] => Task 2, Epoch 111/150 => Loss 0.072, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.038, Loss_flat 0.025, Train_accy 100.00, Test_accy 66.63
2024-10-25 16:39:41,660 [pod_foster.py] => Task 2, Epoch 116/150 => Loss 0.071, Loss_clf 0.005, Loss_fe 0.005, Loss_pod 0.037, Loss_flat 0.024, Train_accy 100.00, Test_accy 65.59
2024-10-25 16:39:53,173 [pod_foster.py] => Task 2, Epoch 121/150 => Loss 0.062, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.032, Loss_flat 0.022, Train_accy 100.00, Test_accy 65.72
2024-10-25 16:40:05,233 [pod_foster.py] => Task 2, Epoch 126/150 => Loss 0.062, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.032, Loss_flat 0.023, Train_accy 100.00, Test_accy 66.59
2024-10-25 16:40:17,096 [pod_foster.py] => Task 2, Epoch 131/150 => Loss 0.060, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.030, Loss_flat 0.023, Train_accy 100.00, Test_accy 66.56
2024-10-25 16:40:28,600 [pod_foster.py] => Task 2, Epoch 136/150 => Loss 0.058, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.029, Loss_flat 0.022, Train_accy 100.00, Test_accy 67.15
2024-10-25 16:40:40,513 [pod_foster.py] => Task 2, Epoch 141/150 => Loss 0.056, Loss_clf 0.003, Loss_fe 0.003, Loss_pod 0.028, Loss_flat 0.022, Train_accy 100.00, Test_accy 67.17
2024-10-25 16:40:52,215 [pod_foster.py] => Task 2, Epoch 146/150 => Loss 0.060, Loss_clf 0.004, Loss_fe 0.004, Loss_pod 0.029, Loss_flat 0.023, Train_accy 100.00, Test_accy 63.28
2024-10-25 16:41:00,561 [pod_foster.py] => Task 2, Epoch 150/150 => Loss 0.055, Loss_clf 0.003, Loss_fe 0.004, Loss_pod 0.026, Loss_flat 0.022, Train_accy 100.00
2024-10-25 16:41:00,566 [inc_net.py] => align weights, gamma = 0.5270062685012817 
2024-10-25 16:41:00,569 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2024-10-25 16:41:03,455 [pod_foster.py] => SNet: Task 2, Epoch 1/120 => Loss 1.544,  Train_accy 75.36, Test_accy 59.81
2024-10-25 16:41:13,804 [pod_foster.py] => SNet: Task 2, Epoch 6/120 => Loss 1.267,  Train_accy 97.31, Test_accy 72.13
2024-10-25 16:41:24,026 [pod_foster.py] => SNet: Task 2, Epoch 11/120 => Loss 1.268,  Train_accy 97.62, Test_accy 72.57
2024-10-25 16:41:34,247 [pod_foster.py] => SNet: Task 2, Epoch 16/120 => Loss 1.263,  Train_accy 98.09, Test_accy 73.59
2024-10-25 16:41:44,730 [pod_foster.py] => SNet: Task 2, Epoch 21/120 => Loss 1.264,  Train_accy 97.95, Test_accy 73.44
2024-10-25 16:41:55,060 [pod_foster.py] => SNet: Task 2, Epoch 26/120 => Loss 1.262,  Train_accy 98.24, Test_accy 74.96
2024-10-25 16:42:05,341 [pod_foster.py] => SNet: Task 2, Epoch 31/120 => Loss 1.259,  Train_accy 98.11, Test_accy 75.22
2024-10-25 16:42:15,632 [pod_foster.py] => SNet: Task 2, Epoch 36/120 => Loss 1.260,  Train_accy 98.20, Test_accy 76.22
2024-10-25 16:42:26,202 [pod_foster.py] => SNet: Task 2, Epoch 41/120 => Loss 1.258,  Train_accy 98.18, Test_accy 76.46
2024-10-25 16:42:36,522 [pod_foster.py] => SNet: Task 2, Epoch 46/120 => Loss 1.258,  Train_accy 98.47, Test_accy 75.72
2024-10-25 16:42:47,084 [pod_foster.py] => SNet: Task 2, Epoch 51/120 => Loss 1.260,  Train_accy 98.27, Test_accy 75.37
2024-10-25 16:42:57,623 [pod_foster.py] => SNet: Task 2, Epoch 56/120 => Loss 1.259,  Train_accy 98.33, Test_accy 76.13
2024-10-25 16:43:07,953 [pod_foster.py] => SNet: Task 2, Epoch 61/120 => Loss 1.259,  Train_accy 98.29, Test_accy 75.59
2024-10-25 16:43:18,443 [pod_foster.py] => SNet: Task 2, Epoch 66/120 => Loss 1.256,  Train_accy 98.42, Test_accy 76.35
2024-10-25 16:43:28,887 [pod_foster.py] => SNet: Task 2, Epoch 71/120 => Loss 1.258,  Train_accy 98.49, Test_accy 76.63
2024-10-25 16:43:39,623 [pod_foster.py] => SNet: Task 2, Epoch 76/120 => Loss 1.258,  Train_accy 98.51, Test_accy 76.26
2024-10-25 16:43:49,825 [pod_foster.py] => SNet: Task 2, Epoch 81/120 => Loss 1.260,  Train_accy 98.55, Test_accy 75.91
2024-10-25 16:44:00,189 [pod_foster.py] => SNet: Task 2, Epoch 86/120 => Loss 1.253,  Train_accy 98.55, Test_accy 75.69
2024-10-25 16:44:10,548 [pod_foster.py] => SNet: Task 2, Epoch 91/120 => Loss 1.256,  Train_accy 98.64, Test_accy 76.20
2024-10-25 16:44:21,059 [pod_foster.py] => SNet: Task 2, Epoch 96/120 => Loss 1.253,  Train_accy 98.62, Test_accy 76.15
2024-10-25 16:44:30,932 [pod_foster.py] => SNet: Task 2, Epoch 101/120 => Loss 1.255,  Train_accy 98.60, Test_accy 76.09
2024-10-25 16:44:41,218 [pod_foster.py] => SNet: Task 2, Epoch 106/120 => Loss 1.256,  Train_accy 98.55, Test_accy 76.33
2024-10-25 16:44:50,895 [pod_foster.py] => SNet: Task 2, Epoch 111/120 => Loss 1.257,  Train_accy 98.35, Test_accy 76.59
2024-10-25 16:45:00,421 [pod_foster.py] => SNet: Task 2, Epoch 116/120 => Loss 1.258,  Train_accy 98.60, Test_accy 75.85
2024-10-25 16:45:07,287 [pod_foster.py] => SNet: Task 2, Epoch 120/120 => Loss 1.256,  Train_accy 98.51
2024-10-25 16:45:07,287 [pod_foster.py] => do not weight align student!
2024-10-25 16:45:08,165 [pod_foster.py] => darknet eval: 
2024-10-25 16:45:08,165 [pod_foster.py] => CNN top1 curve: 76.94
2024-10-25 16:45:08,165 [pod_foster.py] => CNN top5 curve: 96.63
2024-10-25 16:45:08,166 [pod_foster.py] => All params after compression: 3853138
2024-10-25 16:45:08,167 [base.py] => Reducing exemplars...(55 per classes)
2024-10-25 16:45:09,951 [base.py] => Constructing exemplars...(55 per classes)
2024-10-25 16:45:13,217 [trainer.py] => All params: 7705241
2024-10-25 16:45:15,275 [pod_foster.py] => Exemplar size: 495
2024-10-25 16:45:15,275 [trainer.py] => CNN: {'total': 76.26, '00-04': 66.0, '05-06': 87.42, '07-08': 90.75, 'old': 72.12, 'new': 90.75}
2024-10-25 16:45:15,275 [trainer.py] => NME: {'total': 72.28, '00-04': 67.6, '05-06': 73.58, '07-08': 82.67, 'old': 69.31, 'new': 82.67}
2024-10-25 16:45:15,275 [trainer.py] => CNN top1 curve: [89.93, 79.88, 76.26]
2024-10-25 16:45:15,275 [trainer.py] => CNN top5 curve: [100.0, 98.74, 96.35]
2024-10-25 16:45:15,275 [trainer.py] => NME top1 curve: [90.0, 73.43, 72.28]
2024-10-25 16:45:15,275 [trainer.py] => NME top5 curve: [100.0, 98.98, 96.33]

2024-10-25 16:45:15,275 [trainer.py] => Average Accuracy (CNN): 82.02333333333333
2024-10-25 16:45:15,275 [trainer.py] => Average Accuracy (NME): 78.57000000000001
2024-10-25 16:45:15,276 [trainer.py] => Forgetting (CNN): 11.965000000000003
