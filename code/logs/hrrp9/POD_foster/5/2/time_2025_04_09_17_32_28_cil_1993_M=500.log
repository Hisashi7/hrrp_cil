2025-04-09 17:32:28,540 [trainer.py] => config: ./exps/POD_foster.json
2025-04-09 17:32:28,540 [trainer.py] => prefix: cil
2025-04-09 17:32:28,540 [trainer.py] => dataset: hrrp9
2025-04-09 17:32:28,540 [trainer.py] => memory_size: 500
2025-04-09 17:32:28,540 [trainer.py] => memory_per_class: 20
2025-04-09 17:32:28,540 [trainer.py] => fixed_memory: False
2025-04-09 17:32:28,540 [trainer.py] => shuffle: True
2025-04-09 17:32:28,540 [trainer.py] => init_cls: 5
2025-04-09 17:32:28,540 [trainer.py] => increment: 2
2025-04-09 17:32:28,540 [trainer.py] => model_name: POD_foster
2025-04-09 17:32:28,541 [trainer.py] => convnet_type: resnet18
2025-04-09 17:32:28,541 [trainer.py] => init_train: False
2025-04-09 17:32:28,546 [trainer.py] => convnet_path1: checkpoints/init_train/resnet18_35172.pth
2025-04-09 17:32:28,546 [trainer.py] => fc_path1: checkpoints/init_train/fc_35172.pth
2025-04-09 17:32:28,546 [trainer.py] => convnet_path: checkpoints/init_train/resnet18_42503.pth
2025-04-09 17:32:28,546 [trainer.py] => fc_path: checkpoints/init_train/fc_42503.pth
2025-04-09 17:32:28,546 [trainer.py] => convnet_path2: checkpoints/init_train/resnet18_42871.pth
2025-04-09 17:32:28,547 [trainer.py] => fc_path2: checkpoints/init_train/fc_42871.pth
2025-04-09 17:32:28,547 [trainer.py] => device: [device(type='cuda', index=0)]
2025-04-09 17:32:28,547 [trainer.py] => seed: 1993
2025-04-09 17:32:28,547 [trainer.py] => beta1: 0.96
2025-04-09 17:32:28,547 [trainer.py] => beta2: 0.97
2025-04-09 17:32:28,547 [trainer.py] => oofc: ft
2025-04-09 17:32:28,547 [trainer.py] => is_teacher_wa: True
2025-04-09 17:32:28,547 [trainer.py] => is_student_wa: False
2025-04-09 17:32:28,547 [trainer.py] => is_teacher_la: True
2025-04-09 17:32:28,547 [trainer.py] => is_student_la: True
2025-04-09 17:32:28,547 [trainer.py] => lambda_okd: 0
2025-04-09 17:32:28,547 [trainer.py] => wa_value: 1
2025-04-09 17:32:28,547 [trainer.py] => init_epochs: 0
2025-04-09 17:32:28,547 [trainer.py] => init_lr: 0.1
2025-04-09 17:32:28,547 [trainer.py] => init_weight_decay: 0.0005
2025-04-09 17:32:28,547 [trainer.py] => boosting_epochs: 120
2025-04-09 17:32:28,547 [trainer.py] => compression_epochs: 100
2025-04-09 17:32:28,547 [trainer.py] => lr: 0.1
2025-04-09 17:32:28,548 [trainer.py] => batch_size: 128
2025-04-09 17:32:28,548 [trainer.py] => weight_decay: 0.0005
2025-04-09 17:32:28,548 [trainer.py] => num_workers: 8
2025-04-09 17:32:28,548 [trainer.py] => momentum: 0.9
2025-04-09 17:32:28,548 [trainer.py] => T: 2
2025-04-09 17:32:28,548 [trainer.py] => lambda_c_base: 0.8
2025-04-09 17:32:28,548 [trainer.py] => lambda_f_base: 1.0
2025-04-09 17:32:28,548 [trainer.py] => POD: w
2025-04-09 17:32:28,809 [data_manager.py] => [4, 2, 5, 0, 3, 6, 7, 8, 1]
2025-04-09 17:32:28,868 [trainer.py] => All params: 0
2025-04-09 17:32:28,868 [trainer.py] => Trainable params: 0
2025-04-09 17:32:29,017 [pod_foster.py] => Learning on 0-5
2025-04-09 17:32:29,018 [pod_foster.py] => All params: 3849034
2025-04-09 17:32:29,019 [pod_foster.py] => Trainable params: 3849034
2025-04-09 17:32:29,043 [pod_foster.py] => Adaptive factor: 0
2025-04-09 17:32:29,047 [pod_foster.py] => init_train?---False
2025-04-09 17:32:43,990 [base.py] => Reducing exemplars...(100 per classes)
2025-04-09 17:32:43,991 [base.py] => Constructing exemplars...(100 per classes)
2025-04-09 17:33:08,652 [trainer.py] => task:0 training time:39.78s
2025-04-09 17:33:08,652 [trainer.py] => All params: 3849034
2025-04-09 17:33:35,622 [pod_foster.py] => Exemplar size: 500
2025-04-09 17:33:35,623 [trainer.py] => CNN: {'total': 89.93, '00-04': 89.93, 'old': 0, 'new': 89.93}
2025-04-09 17:33:35,623 [trainer.py] => NME: {'total': 90.0, '00-04': 90.0, 'old': 0, 'new': 90.0}
2025-04-09 17:33:35,623 [trainer.py] => CNN top1 curve: [89.93]
2025-04-09 17:33:35,624 [trainer.py] => CNN top5 curve: [100.0]
2025-04-09 17:33:35,624 [trainer.py] => NME top1 curve: [90.0]
2025-04-09 17:33:35,624 [trainer.py] => NME top5 curve: [100.0]

2025-04-09 17:33:35,624 [trainer.py] => Average Accuracy (CNN): 89.93
2025-04-09 17:33:35,624 [trainer.py] => Average Accuracy (NME): 90.0
2025-04-09 17:33:35,625 [trainer.py] => All params: 3849034
2025-04-09 17:33:35,625 [trainer.py] => Trainable params: 3849034
2025-04-09 17:33:35,646 [pod_foster.py] => Learning on 5-7
2025-04-09 17:33:35,646 [pod_foster.py] => All params: 7701139
2025-04-09 17:33:35,647 [pod_foster.py] => Trainable params: 3854670
2025-04-09 17:33:35,658 [pod_foster.py] => Adaptive factor: 1.8708286933869707
2025-04-09 17:33:35,662 [pod_foster.py] => per cls weights : [1.00484344 1.00484344 1.00484344 1.00484344 1.00484344 0.98789141
 0.98789141]
2025-04-09 17:34:04,477 [pod_foster.py] => Task 1, Epoch 1/120 => Loss 1.174, Loss_clf 0.764, Loss_fe 0.000, Loss_pod 0.295, Loss_flat 0.114, Train_accy 81.80, Test_accy 64.88
2025-04-09 17:35:29,638 [pod_foster.py] => Task 1, Epoch 6/120 => Loss 0.213, Loss_clf 0.022, Loss_fe 0.000, Loss_pod 0.159, Loss_flat 0.032, Train_accy 99.96, Test_accy 69.26
2025-04-09 17:36:55,444 [pod_foster.py] => Task 1, Epoch 11/120 => Loss 0.224, Loss_clf 0.027, Loss_fe 0.000, Loss_pod 0.167, Loss_flat 0.029, Train_accy 99.51, Test_accy 70.95
2025-04-09 17:38:22,402 [pod_foster.py] => Task 1, Epoch 16/120 => Loss 0.165, Loss_clf 0.011, Loss_fe 0.000, Loss_pod 0.135, Loss_flat 0.019, Train_accy 100.00, Test_accy 68.50
2025-04-09 17:39:49,128 [pod_foster.py] => Task 1, Epoch 21/120 => Loss 0.167, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.135, Loss_flat 0.019, Train_accy 100.00, Test_accy 68.33
2025-04-09 17:41:15,946 [pod_foster.py] => Task 1, Epoch 26/120 => Loss 0.250, Loss_clf 0.019, Loss_fe 0.000, Loss_pod 0.189, Loss_flat 0.041, Train_accy 99.91, Test_accy 68.38
2025-04-09 17:42:43,123 [pod_foster.py] => Task 1, Epoch 31/120 => Loss 0.187, Loss_clf 0.016, Loss_fe 0.000, Loss_pod 0.146, Loss_flat 0.026, Train_accy 99.91, Test_accy 68.95
2025-04-09 17:44:11,750 [pod_foster.py] => Task 1, Epoch 36/120 => Loss 0.150, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.125, Loss_flat 0.017, Train_accy 100.00, Test_accy 66.74
2025-04-09 17:45:46,478 [pod_foster.py] => Task 1, Epoch 41/120 => Loss 0.138, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.115, Loss_flat 0.014, Train_accy 100.00, Test_accy 66.83
2025-04-09 17:47:15,461 [pod_foster.py] => Task 1, Epoch 46/120 => Loss 0.139, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.117, Loss_flat 0.014, Train_accy 100.00, Test_accy 68.17
2025-04-09 17:48:42,396 [pod_foster.py] => Task 1, Epoch 51/120 => Loss 0.138, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.117, Loss_flat 0.014, Train_accy 100.00, Test_accy 65.48
2025-04-09 17:50:07,963 [pod_foster.py] => Task 1, Epoch 56/120 => Loss 0.128, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.108, Loss_flat 0.013, Train_accy 100.00, Test_accy 67.33
2025-04-09 17:51:32,537 [pod_foster.py] => Task 1, Epoch 61/120 => Loss 0.134, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.112, Loss_flat 0.014, Train_accy 100.00, Test_accy 68.02
2025-04-09 17:52:57,083 [pod_foster.py] => Task 1, Epoch 66/120 => Loss 0.130, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.109, Loss_flat 0.013, Train_accy 100.00, Test_accy 67.43
2025-04-09 17:54:22,268 [pod_foster.py] => Task 1, Epoch 71/120 => Loss 0.124, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.105, Loss_flat 0.012, Train_accy 100.00, Test_accy 67.57
2025-04-09 17:55:47,050 [pod_foster.py] => Task 1, Epoch 76/120 => Loss 0.116, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.098, Loss_flat 0.012, Train_accy 100.00, Test_accy 67.79
2025-04-09 17:57:11,662 [pod_foster.py] => Task 1, Epoch 81/120 => Loss 0.110, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.092, Loss_flat 0.011, Train_accy 100.00, Test_accy 68.40
2025-04-09 17:58:36,194 [pod_foster.py] => Task 1, Epoch 86/120 => Loss 0.109, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.091, Loss_flat 0.012, Train_accy 100.00, Test_accy 66.45
2025-04-09 18:00:01,233 [pod_foster.py] => Task 1, Epoch 91/120 => Loss 0.103, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.086, Loss_flat 0.011, Train_accy 100.00, Test_accy 66.19
2025-04-09 18:01:26,135 [pod_foster.py] => Task 1, Epoch 96/120 => Loss 0.100, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.082, Loss_flat 0.011, Train_accy 100.00, Test_accy 67.14
2025-04-09 18:02:50,723 [pod_foster.py] => Task 1, Epoch 101/120 => Loss 0.095, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.078, Loss_flat 0.010, Train_accy 100.00, Test_accy 66.55
2025-04-09 18:04:15,266 [pod_foster.py] => Task 1, Epoch 106/120 => Loss 0.091, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.075, Loss_flat 0.010, Train_accy 100.00, Test_accy 67.31
2025-04-09 18:05:40,253 [pod_foster.py] => Task 1, Epoch 111/120 => Loss 0.094, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.077, Loss_flat 0.010, Train_accy 100.00, Test_accy 66.05
2025-04-09 18:07:05,811 [pod_foster.py] => Task 1, Epoch 116/120 => Loss 0.089, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.073, Loss_flat 0.010, Train_accy 100.00, Test_accy 67.43
2025-04-09 18:08:02,788 [pod_foster.py] => Task 1, time 14.20s, Epoch 120/120 => Loss 0.090, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.074, Loss_flat 0.010, Train_accy 100.00
2025-04-09 18:08:02,789 [pod_foster.py] => 100 epoches training time:1727.28s
2025-04-09 18:08:02,789 [pod_foster.py] => Average training time of single epoch:14.46s
2025-04-09 18:08:02,789 [inc_net.py] => align weights, gamma = 0.497345894575119 
2025-04-09 18:08:02,791 [pod_foster.py] => per cls weights : [1.01377356 1.01377356 1.01377356 1.01377356 1.01377356 0.96556609
 0.96556609]
2025-04-09 18:08:30,237 [pod_foster.py] => SNet: Task 1, Epoch 1/100 => Loss 1.308,  Train_accy 71.02, Test_accy 60.12
2025-04-09 18:09:53,792 [pod_foster.py] => SNet: Task 1, Epoch 6/100 => Loss 1.019,  Train_accy 97.16, Test_accy 72.93
2025-04-09 18:11:17,587 [pod_foster.py] => SNet: Task 1, Epoch 11/100 => Loss 1.007,  Train_accy 97.64, Test_accy 76.69
2025-04-09 18:12:42,137 [pod_foster.py] => SNet: Task 1, Epoch 16/100 => Loss 1.004,  Train_accy 97.84, Test_accy 77.07
2025-04-09 18:14:06,119 [pod_foster.py] => SNet: Task 1, Epoch 21/100 => Loss 1.001,  Train_accy 98.11, Test_accy 76.29
2025-04-09 18:15:29,483 [pod_foster.py] => SNet: Task 1, Epoch 26/100 => Loss 1.001,  Train_accy 98.04, Test_accy 75.60
2025-04-09 18:16:53,260 [pod_foster.py] => SNet: Task 1, Epoch 31/100 => Loss 0.999,  Train_accy 98.09, Test_accy 75.93
2025-04-09 18:18:17,816 [pod_foster.py] => SNet: Task 1, Epoch 36/100 => Loss 0.998,  Train_accy 98.27, Test_accy 77.81
2025-04-09 18:19:41,800 [pod_foster.py] => SNet: Task 1, Epoch 41/100 => Loss 0.997,  Train_accy 98.49, Test_accy 77.69
2025-04-09 18:21:05,184 [pod_foster.py] => SNet: Task 1, Epoch 46/100 => Loss 0.996,  Train_accy 98.42, Test_accy 78.00
2025-04-09 18:22:28,750 [pod_foster.py] => SNet: Task 1, Epoch 51/100 => Loss 0.998,  Train_accy 98.36, Test_accy 77.52
2025-04-09 18:23:52,919 [pod_foster.py] => SNet: Task 1, Epoch 56/100 => Loss 0.995,  Train_accy 98.42, Test_accy 77.93
2025-04-09 18:25:17,473 [pod_foster.py] => SNet: Task 1, Epoch 61/100 => Loss 0.998,  Train_accy 98.31, Test_accy 77.83
2025-04-09 18:26:40,873 [pod_foster.py] => SNet: Task 1, Epoch 66/100 => Loss 0.999,  Train_accy 98.47, Test_accy 77.45
2025-04-09 18:28:04,414 [pod_foster.py] => SNet: Task 1, Epoch 71/100 => Loss 0.993,  Train_accy 98.69, Test_accy 76.88
2025-04-09 18:29:28,242 [pod_foster.py] => SNet: Task 1, Epoch 76/100 => Loss 0.996,  Train_accy 98.53, Test_accy 77.07
2025-04-09 18:30:52,781 [pod_foster.py] => SNet: Task 1, Epoch 81/100 => Loss 0.994,  Train_accy 98.49, Test_accy 76.93
2025-04-09 18:32:16,563 [pod_foster.py] => SNet: Task 1, Epoch 86/100 => Loss 0.993,  Train_accy 98.67, Test_accy 76.79
2025-04-09 18:33:40,527 [pod_foster.py] => SNet: Task 1, Epoch 91/100 => Loss 0.995,  Train_accy 98.53, Test_accy 76.95
2025-04-09 18:35:03,907 [pod_foster.py] => SNet: Task 1, Epoch 96/100 => Loss 0.995,  Train_accy 98.47, Test_accy 77.50
2025-04-09 18:36:00,692 [pod_foster.py] => SNet: Task 1, Epoch 100/100 => Loss 0.993,  Train_accy 98.51
2025-04-09 18:36:00,692 [pod_foster.py] => do not weight align student!
2025-04-09 18:36:14,277 [pod_foster.py] => darknet eval: 
2025-04-09 18:36:14,277 [pod_foster.py] => CNN top1 curve: 76.79
2025-04-09 18:36:14,277 [pod_foster.py] => CNN top5 curve: 98.62
2025-04-09 18:36:14,277 [pod_foster.py] => CNN: {'total': 76.79, '00-04': 70.77, '05-06': 91.83, 'old': 70.77, 'new': 91.83}
2025-04-09 18:36:14,278 [pod_foster.py] => All params after compression: 3851086
2025-04-09 18:36:14,278 [base.py] => Reducing exemplars...(71 per classes)
2025-04-09 18:36:24,899 [base.py] => Constructing exemplars...(71 per classes)
2025-04-09 18:36:35,771 [trainer.py] => task:1 training time:3780.15s
2025-04-09 18:36:35,773 [trainer.py] => All params: 7701139
2025-04-09 18:37:04,672 [pod_foster.py] => Exemplar size: 497
2025-04-09 18:37:04,672 [trainer.py] => CNN: {'total': 79.79, '00-04': 76.8, '05-06': 87.25, 'old': 76.8, 'new': 87.25}
2025-04-09 18:37:04,672 [trainer.py] => NME: {'total': 73.93, '00-04': 75.4, '05-06': 70.25, 'old': 75.4, 'new': 70.25}
2025-04-09 18:37:04,672 [trainer.py] => CNN top1 curve: [89.93, 79.79]
2025-04-09 18:37:04,672 [trainer.py] => CNN top5 curve: [100.0, 98.86]
2025-04-09 18:37:04,672 [trainer.py] => NME top1 curve: [90.0, 73.93]
2025-04-09 18:37:04,672 [trainer.py] => NME top5 curve: [100.0, 99.14]

2025-04-09 18:37:04,672 [trainer.py] => Average Accuracy (CNN): 84.86000000000001
2025-04-09 18:37:04,672 [trainer.py] => Average Accuracy (NME): 81.965
2025-04-09 18:37:04,672 [trainer.py] => All params: 7701139
2025-04-09 18:37:04,673 [trainer.py] => Trainable params: 3854670
2025-04-09 18:37:04,721 [pod_foster.py] => Learning on 7-9
2025-04-09 18:37:04,722 [pod_foster.py] => All params: 7705241
2025-04-09 18:37:04,722 [pod_foster.py] => Trainable params: 3857746
2025-04-09 18:37:04,738 [pod_foster.py] => Adaptive factor: 2.1213203435596424
2025-04-09 18:37:04,740 [pod_foster.py] => per cls weights : [1.01239929 1.01239929 1.01239929 1.01239929 1.01239929 1.01239929
 1.01239929 0.95660248 0.95660248]
2025-04-09 18:37:33,721 [pod_foster.py] => Task 2, Epoch 1/120 => Loss 1.149, Loss_clf 0.608, Loss_fe 0.000, Loss_pod 0.399, Loss_flat 0.142, Train_accy 85.37, Test_accy 64.98
2025-04-09 18:39:00,857 [pod_foster.py] => Task 2, Epoch 6/120 => Loss 0.247, Loss_clf 0.019, Loss_fe 0.000, Loss_pod 0.196, Loss_flat 0.032, Train_accy 99.87, Test_accy 65.81
2025-04-09 18:40:28,034 [pod_foster.py] => Task 2, Epoch 11/120 => Loss 0.386, Loss_clf 0.060, Loss_fe 0.000, Loss_pod 0.269, Loss_flat 0.057, Train_accy 98.51, Test_accy 68.28
2025-04-09 18:41:55,414 [pod_foster.py] => Task 2, Epoch 16/120 => Loss 0.202, Loss_clf 0.010, Loss_fe 0.000, Loss_pod 0.170, Loss_flat 0.023, Train_accy 100.00, Test_accy 59.59
2025-04-09 18:43:22,778 [pod_foster.py] => Task 2, Epoch 21/120 => Loss 0.189, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.160, Loss_flat 0.020, Train_accy 100.00, Test_accy 64.96
2025-04-09 18:44:49,955 [pod_foster.py] => Task 2, Epoch 26/120 => Loss 0.183, Loss_clf 0.015, Loss_fe 0.000, Loss_pod 0.151, Loss_flat 0.017, Train_accy 99.96, Test_accy 37.67
2025-04-09 18:46:16,911 [pod_foster.py] => Task 2, Epoch 31/120 => Loss 0.201, Loss_clf 0.009, Loss_fe 0.000, Loss_pod 0.166, Loss_flat 0.026, Train_accy 100.00, Test_accy 66.24
2025-04-09 18:47:42,483 [pod_foster.py] => Task 2, Epoch 36/120 => Loss 0.179, Loss_clf 0.013, Loss_fe 0.000, Loss_pod 0.148, Loss_flat 0.018, Train_accy 99.96, Test_accy 67.37
2025-04-09 18:49:08,436 [pod_foster.py] => Task 2, Epoch 41/120 => Loss 0.171, Loss_clf 0.012, Loss_fe 0.000, Loss_pod 0.142, Loss_flat 0.017, Train_accy 99.98, Test_accy 59.44
2025-04-09 18:50:33,232 [pod_foster.py] => Task 2, Epoch 46/120 => Loss 0.161, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.137, Loss_flat 0.016, Train_accy 100.00, Test_accy 65.78
2025-04-09 18:51:58,007 [pod_foster.py] => Task 2, Epoch 51/120 => Loss 0.167, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.144, Loss_flat 0.016, Train_accy 100.00, Test_accy 62.24
2025-04-09 18:53:22,961 [pod_foster.py] => Task 2, Epoch 56/120 => Loss 0.151, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.130, Loss_flat 0.014, Train_accy 100.00, Test_accy 64.20
2025-04-09 18:54:48,156 [pod_foster.py] => Task 2, Epoch 61/120 => Loss 0.149, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.127, Loss_flat 0.014, Train_accy 100.00, Test_accy 62.93
2025-04-09 18:56:13,106 [pod_foster.py] => Task 2, Epoch 66/120 => Loss 0.158, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.136, Loss_flat 0.015, Train_accy 100.00, Test_accy 63.09
2025-04-09 18:57:37,887 [pod_foster.py] => Task 2, Epoch 71/120 => Loss 0.134, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.115, Loss_flat 0.013, Train_accy 100.00, Test_accy 64.13
2025-04-09 18:59:02,854 [pod_foster.py] => Task 2, Epoch 76/120 => Loss 0.139, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.119, Loss_flat 0.013, Train_accy 100.00, Test_accy 63.50
2025-04-09 19:00:28,439 [pod_foster.py] => Task 2, Epoch 81/120 => Loss 0.134, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.115, Loss_flat 0.012, Train_accy 100.00, Test_accy 63.41
2025-04-09 19:01:53,404 [pod_foster.py] => Task 2, Epoch 86/120 => Loss 0.127, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.108, Loss_flat 0.011, Train_accy 100.00, Test_accy 62.87
2025-04-09 19:03:18,374 [pod_foster.py] => Task 2, Epoch 91/120 => Loss 0.117, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.099, Loss_flat 0.011, Train_accy 100.00, Test_accy 64.85
2025-04-09 19:04:43,345 [pod_foster.py] => Task 2, Epoch 96/120 => Loss 0.123, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.105, Loss_flat 0.012, Train_accy 100.00, Test_accy 64.09
2025-04-09 19:06:08,510 [pod_foster.py] => Task 2, Epoch 101/120 => Loss 0.113, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.095, Loss_flat 0.011, Train_accy 100.00, Test_accy 63.61
2025-04-09 19:07:33,888 [pod_foster.py] => Task 2, Epoch 106/120 => Loss 0.109, Loss_clf 0.008, Loss_fe 0.000, Loss_pod 0.090, Loss_flat 0.011, Train_accy 100.00, Test_accy 64.85
2025-04-09 19:08:58,668 [pod_foster.py] => Task 2, Epoch 111/120 => Loss 0.106, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.089, Loss_flat 0.011, Train_accy 100.00, Test_accy 63.87
2025-04-09 19:10:23,448 [pod_foster.py] => Task 2, Epoch 116/120 => Loss 0.112, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.093, Loss_flat 0.011, Train_accy 100.00, Test_accy 63.06
2025-04-09 19:11:20,345 [pod_foster.py] => Task 2, time 14.23s, Epoch 120/120 => Loss 0.104, Loss_clf 0.007, Loss_fe 0.000, Loss_pod 0.086, Loss_flat 0.011, Train_accy 100.00
2025-04-09 19:11:20,345 [pod_foster.py] => 100 epoches training time:1715.50s
2025-04-09 19:11:20,345 [pod_foster.py] => Average training time of single epoch:14.35s
2025-04-09 19:11:20,346 [inc_net.py] => align weights, gamma = 0.5067314505577087 
2025-04-09 19:11:20,346 [pod_foster.py] => per cls weights : [1.02623168 1.02623168 1.02623168 1.02623168 1.02623168 1.02623168
 1.02623168 0.90818914 0.90818914]
2025-04-09 19:11:48,191 [pod_foster.py] => SNet: Task 2, Epoch 1/100 => Loss 1.625,  Train_accy 75.58, Test_accy 51.78
2025-04-09 19:13:12,782 [pod_foster.py] => SNet: Task 2, Epoch 6/100 => Loss 1.364,  Train_accy 98.15, Test_accy 72.94
2025-04-09 19:14:36,939 [pod_foster.py] => SNet: Task 2, Epoch 11/100 => Loss 1.357,  Train_accy 98.58, Test_accy 73.85
2025-04-09 19:16:00,325 [pod_foster.py] => SNet: Task 2, Epoch 16/100 => Loss 1.353,  Train_accy 98.49, Test_accy 75.85
2025-04-09 19:17:24,690 [pod_foster.py] => SNet: Task 2, Epoch 21/100 => Loss 1.352,  Train_accy 98.93, Test_accy 75.11
2025-04-09 19:18:49,258 [pod_foster.py] => SNet: Task 2, Epoch 26/100 => Loss 1.353,  Train_accy 98.67, Test_accy 75.74
2025-04-09 19:20:13,429 [pod_foster.py] => SNet: Task 2, Epoch 31/100 => Loss 1.351,  Train_accy 98.75, Test_accy 75.87
2025-04-09 19:21:37,006 [pod_foster.py] => SNet: Task 2, Epoch 36/100 => Loss 1.350,  Train_accy 98.80, Test_accy 76.31
2025-04-09 19:23:00,386 [pod_foster.py] => SNet: Task 2, Epoch 41/100 => Loss 1.351,  Train_accy 98.75, Test_accy 75.81
2025-04-09 19:24:24,942 [pod_foster.py] => SNet: Task 2, Epoch 46/100 => Loss 1.351,  Train_accy 98.89, Test_accy 75.87
2025-04-09 19:25:49,118 [pod_foster.py] => SNet: Task 2, Epoch 51/100 => Loss 1.351,  Train_accy 99.02, Test_accy 76.00
2025-04-09 19:27:12,696 [pod_foster.py] => SNet: Task 2, Epoch 56/100 => Loss 1.349,  Train_accy 98.89, Test_accy 76.09
2025-04-09 19:28:36,452 [pod_foster.py] => SNet: Task 2, Epoch 61/100 => Loss 1.351,  Train_accy 98.82, Test_accy 76.13
2025-04-09 19:30:00,671 [pod_foster.py] => SNet: Task 2, Epoch 66/100 => Loss 1.349,  Train_accy 98.82, Test_accy 76.26
2025-04-09 19:31:24,815 [pod_foster.py] => SNet: Task 2, Epoch 71/100 => Loss 1.349,  Train_accy 98.93, Test_accy 75.44
2025-04-09 19:32:48,563 [pod_foster.py] => SNet: Task 2, Epoch 76/100 => Loss 1.345,  Train_accy 98.93, Test_accy 74.96
2025-04-09 19:34:12,159 [pod_foster.py] => SNet: Task 2, Epoch 81/100 => Loss 1.349,  Train_accy 98.95, Test_accy 75.98
2025-04-09 19:35:36,719 [pod_foster.py] => SNet: Task 2, Epoch 86/100 => Loss 1.346,  Train_accy 98.80, Test_accy 75.15
2025-04-09 19:37:02,085 [pod_foster.py] => SNet: Task 2, Epoch 91/100 => Loss 1.347,  Train_accy 98.89, Test_accy 75.74
2025-04-09 19:38:28,075 [pod_foster.py] => SNet: Task 2, Epoch 96/100 => Loss 1.349,  Train_accy 98.91, Test_accy 74.96
2025-04-09 19:39:25,590 [pod_foster.py] => SNet: Task 2, Epoch 100/100 => Loss 1.351,  Train_accy 98.98
2025-04-09 19:39:25,590 [pod_foster.py] => do not weight align student!
2025-04-09 19:39:39,458 [pod_foster.py] => darknet eval: 
2025-04-09 19:39:39,458 [pod_foster.py] => CNN top1 curve: 76.72
2025-04-09 19:39:39,458 [pod_foster.py] => CNN top5 curve: 97.09
2025-04-09 19:39:39,458 [pod_foster.py] => CNN: {'total': 76.72, '00-04': 66.23, '05-06': 86.67, '07-08': 93.0, 'old': 72.07, 'new': 93.0}
2025-04-09 19:39:39,458 [pod_foster.py] => All params after compression: 3853138
2025-04-09 19:39:39,459 [base.py] => Reducing exemplars...(55 per classes)
2025-04-09 19:39:54,583 [base.py] => Constructing exemplars...(55 per classes)
2025-04-09 19:40:05,236 [trainer.py] => task:2 training time:3780.56s
2025-04-09 19:40:05,237 [trainer.py] => All params: 7705241
2025-04-09 19:40:33,295 [pod_foster.py] => Exemplar size: 495
2025-04-09 19:40:33,295 [trainer.py] => CNN: {'total': 74.56, '00-04': 60.43, '05-06': 92.33, '07-08': 92.08, 'old': 69.55, 'new': 92.08}
2025-04-09 19:40:33,296 [trainer.py] => NME: {'total': 72.56, '00-04': 61.87, '05-06': 82.58, '07-08': 89.25, 'old': 67.79, 'new': 89.25}
2025-04-09 19:40:33,296 [trainer.py] => CNN top1 curve: [89.93, 79.79, 74.56]
2025-04-09 19:40:33,296 [trainer.py] => CNN top5 curve: [100.0, 98.86, 96.59]
2025-04-09 19:40:33,296 [trainer.py] => NME top1 curve: [90.0, 73.93, 72.56]
2025-04-09 19:40:33,296 [trainer.py] => NME top5 curve: [100.0, 99.14, 96.2]

2025-04-09 19:40:33,296 [trainer.py] => Average Accuracy (CNN): 81.42666666666668
2025-04-09 19:40:33,296 [trainer.py] => Average Accuracy (NME): 78.83
2025-04-09 19:40:33,296 [trainer.py] => Time consumed in all training process:7684.43s
2025-04-09 19:40:33,296 [trainer.py] => Average Time consumed in single task:2533.50s
2025-04-09 19:40:33,347 [trainer.py] => Model state dict saved successfully at: saved_pth\hrrp9\POD_foster\time_2025_04_09_17_32_28_cil_1993_M=500.pth
2025-04-09 19:40:33,348 [trainer.py] => Forgetting (CNN): 14.750000000000004
